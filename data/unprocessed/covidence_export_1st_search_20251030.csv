Covidence #,Study ID,Title,Reviewer Name,Country of first first author affiliation,Data Set Source,Notes on data set,Type of data set,Language of data set,Does data set contain synthetic data?,Is data set public?,Did users have actual psychopathology or were they unselected/people without mental health conditions?,Were responders trained professionals or lay people/unknown?,READI P-1/2:,P-1 On-premise-capable model Considered in tool design? (Y/N),P-1 On-premise-capable model if YES: Notes (paste text passage),P-2 Privacy/confidentiality awareness Considered in tool design? (Y/N),P-2 Privacy/confidentiality awareness if YES: Notes (paste text passage),Publication Outlet Type,Field of Publication Outlet,Day (1-31),Month (1-12),Year (2017-2025),Study Type,Study Type II,If Experimental Research or Population Survey: Application Type,Application Subtype,Type of Clients or Patients,Number of Clients or Patients Included,Application Subtype,If Study Type == Tool Development and Evaluation: Development Approach,Intervention Type,Models Employed,User Experience Assessment,User Experience Assessment Methods:,"Uses some standard, established user experience instrument Y/N",Uses qualitative assessment Y/N,Uses quantitative assessment Y/N,Were results of user experience assessment reported? Y/N,User Experience Assessment Instrument,Other User Experience Assessment Notes,Reference-based metrics:,Lexical Overlap Used (Y/N),Lexical Overlap How it compares against benchmark (B/S/W),Lexical Overlap Benchmark quality (H/L),Lexical Overlap Notes on benchmark quality,Embedding Similarity Used (Y/N),Embedding Similarity How it compares against benchmark (B/S/W),Embedding Similarity Benchmark quality (H/L),Embedding Similarity Notes on benchmark quality,Classification Used (Y/N),Classification How it compares against benchmark (B/S/W),Classification Benchmark quality (H/L),Classification Notes on benchmark quality,Continuous Value Metrics Used (Y/N),Continuous Value Metrics How it compares against benchmark (B/S/W),Continuous Value Metrics Benchmark quality (H/L),Continuous Value Metrics Notes on benchmark quality,Contentual judgment:,Expert Rating Used (Y/N),Expert Rating How it compares against benchmark (B/S/W),Expert Rating Benchmark quality (H/L),Expert Rating Notes on benchmark quality,LLM as a judge Used (Y/N),LLM as a judge How it compares against benchmark (B/S/W),LLM as a judge Benchmark quality (H/L),LLM as a judge Notes on benchmark quality,Other metrics:,Metric 1 Name of metric,Metric 1 How it compares against benchmark (B/S/W),Metric 1 Benchmark quality (H/L),Metric 1 Notes on benchmark quality,Metric 2 Name of metric,Metric 2 How it compares against benchmark (B/S/W),Metric 2 Benchmark quality (H/L),Metric 2 Notes on benchmark quality,Metric 3 Name of metric,Metric 3 How it compares against benchmark (B/S/W),Metric 3 Benchmark quality (H/L),Metric 3 Notes on benchmark quality,READI categories:,S-1 Risk-detection Considered in tool design? (Y/N),S-1 Risk-detection if YES: Notes (paste text passage),S‑2 Content‑safety evaluation conducted Considered in tool design? (Y/N),S‑2 Content‑safety evaluation conducted if YES: Notes (paste text passage),P-1 Protection of user information Considered in tool design? (Y/N),P-1 Protection of user information if YES: Notes (paste text passage),E-1 Reporting of demographic information Considered in tool design? (Y/N),E-1 Reporting of demographic information if YES: Notes (paste text passage),E-2 Outcomes reported by demographic subgroup Considered in tool design? (Y/N),E-2 Outcomes reported by demographic subgroup if YES: Notes (paste text passage),G‑1 Early‑discontinuation data reported Considered in tool design? (Y/N),G‑1 Early‑discontinuation data reported if YES: Notes (paste text passage),G‑2 Over‑use reported or prevented Considered in tool design? (Y/N),G‑2 Over‑use reported or prevented if YES: Notes (paste text passage),F‑1 Validated clinical outcome measures used Considered in tool design? (Y/N),F‑1 Validated clinical outcome measures used if YES: Notes (paste text passage),F‑2 Control condition present Considered in tool design? (Y/N),F‑2 Control condition present if YES: Notes (paste text passage),I‑1 Multilevel feasibility/acceptability data collected Considered in tool design? (Y/N),I‑1 Multilevel feasibility/acceptability data collected if YES: Notes (paste text passage),I‑2 Healthcare‑integration considerations addressed Considered in tool design? (Y/N),I‑2 Healthcare‑integration considerations addressed if YES: Notes (paste text passage),Notes,Safety-Related Discussion,Main Safety-Related Discussion Contents
224,Shidara 2022,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.,Reviewer Two,Other: Japan,External data set,". All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021",Psychotherapy -- chat logs,Japanese,No,No,Unknown,Trained professionals,"",not applicable,no LLM used,Y,"This dataset was anonymized before being provided to the authors. All efforts for this study were made with the approval of Osaka Prefecture.
",Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",11,7,2022,Tool Development and Evaluation,"",Analysis of conversation transcripts,Chatbot,No clients/patients involved,"","",Other: logistic regression classifier,Other: classification model,Other: logistic regression mode,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",No,""
224,Shidara 2022,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.,Richard Gaus,"",External data set,Data from messenger application LINE (line.me) from Osaka Prefectural Government,"","","","","","","","","","","",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,7,2022,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","","Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier","Unspecified, might include formal therapy methods",BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,"no benchmark. used metrics: F1, precision, recall, AUC. Task: classify mental health and other issues (mental health, suicide thoughts, family, physical health, etc.) in counseling sessions",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","",y,BERT is local,n,"",n,"","","","","","","","","",n,"",n,"","",No,""
224,Shidara 2022,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.,Consensus,"",External data set,". All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021","","","","","","","","","","","",Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",11,7,2022,Tool Development and Evaluation,"",Analysis of conversation transcripts,Other: ,"","","","Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier","Unspecified, might include formal therapy methods",BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,w,l,benchmark: TF-IDF-based classification via logistic regression model,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,BERT is local,n,"",n,"","","","","",n,"","","",n,"",n,"","",No,""
214,Melo 2024,Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care,Reviewer Two,Other: Portugal,No dataset used for development or evaluation,"","","","","","","","",n,"",n,"",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",09,02,2024,Population survey,Tool Development and Evaluation,Client-facing application,"",Patients with disorder explicitly based on ICD or DSM,"","",Custom pipeline with integrated LLM which is only prompted,"Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods",GPT-3.5 family,Yes,"","","","","","",secondary outcome was patient satisfaction on a Likert scale ,"","","","","","","","","","","","","","","","","","","","","","","","","","","","HOQOL-BREF
score",B,"","", patient satisfaction Likert scale ,"","","","","","","","",Y,"n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist","","",Y,"To respect patient confidentiality and data security
, 
all pa-
tient identifiers were removed before data analysis. Chat-
GPT interactions were conducted securely
, 
with data stored
on encrypted, password-protected devices. The AI system
did not store or retain any data post-session, ensuring par-
ticipant anonymity and data security","","","","","","","","",Y,"We included patients aged between 18 to 65 years who
were currently undergoing psychiatric inpatient care and
had received a mental health disorder diagnosis according
to the DSM-5. Each patient
’
s assistant physician evaluated
the presence of criteria for a DSM-5 diagnosis.",Y,"They were then randomly allocated to either the con-
trol or intervention group using the 
“
coin flip
” method","","",Y,"n conclusion, our pilot study suggests that AI chatbots,
such as ChatGPT
, 
can positively impact the quality of life
of psychiatric inpatients while being well-received. Despite
the limitations inherent in a pilot study
, 
such as a small
sample size and the use of convenience sampling, our find-
ings provide valuable insights into the potential role of AI
in psychiatric care","intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes",No,""
214,Melo 2024,Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care,Richard Gaus,Other: Portugal,No dataset used for development or evaluation,"","","","","","","","",n,"",y,"Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",9,2,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,12,"","","Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,"",n,n,y,y,"","As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",WHOQOL-BREF score change,b,h,intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points),"","","","","","","","","",y,attending psychiatrist was checking responses,y,attending psychiatrist was checking responses,n,ChatGPT is not confidential,n,"",n,"",n,"",y,only a set number of sessions was administered,y,World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF),y,control group that received standard care,n,"",n,"","",No,""
214,Melo 2024,Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care,Consensus,Other: Portugal,No dataset used for development or evaluation,"","","","","","","","",n,"",y,"Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",9,2,2024,Direct LLM performance evaluation (only via prompting),Other: ,Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,12,"",Other: ,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,"",n,n,y,y,"","As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",WHOQOL-BREF score change,B,h,intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points),"","","","","","","","","",y,"n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist

attending psychiatrist was checking responses",n,"",n,"Even though authors write ""The AI system
did not store or retain any data post-session, ensuring par-
ticipant anonymity and data security
 "", ChatGPT is not confidential",n,"",n,"",n,"",y,only a set number of sessions was administered,y,World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF),y,control group that received standard care,n,"",n,"","intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes",No,""
212,Spiegel 2024,Feasibility of combining spatial computing and AI for mental health support in anxiety and depression,Reviewer Two,USA,Self-collected data,VR/AI therapy sessions with GPT-4 avatar in biophilic scenes; sessions recorded and transcribed; qualitative thematic analysis after single ~30-min session per participant. • Derived: Not indicated.,Psychotherapy -- speech transcripts,English,No,"",Unknown,Trained professionals,"",N,"",Y,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26,1,2024,Tool Development and Evaluation,"",Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),"","",Custom pipeline with integrated LLM which is only prompted,Other CBT techniques,GPT-4 / GPT-4o family,Yes,"","",Y,"",Y,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,Dec. Helsinki,"","","","","","","","","","","","","","","","","",No,only abstract
212,Spiegel 2024,Feasibility of combining spatial computing and AI for mental health support in anxiety and depression,Richard Gaus,USA,Self-collected data,"Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”",Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,"",n,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26,1,2024,Tool Development and Evaluation,"",Client-facing application,Other: speech-based conversation system,People with some symptoms but not disorder (determined by symptom scales or questionnaires),14,"",Custom pipeline with integrated LLM which is only prompted,"CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,"",n,y,n,y,"","Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",y,"The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.",y,"conversations are
processed through a HIPAA-compliant server: audio is recorded, transcribed by speech-to-text AI, and responded to by LLM (GPT-4)",y,Table-1,n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
212,Spiegel 2024,Feasibility of combining spatial computing and AI for mental health support in anxiety and depression,Consensus,USA,Self-collected data,"Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”",Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,"",N,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26,1,2024,Tool Development and Evaluation,"",Client-facing application,Other: Spoken dialogue system,People with some symptoms but not disorder (determined by symptom scales or questionnaires),14,"",Custom pipeline with integrated LLM which is only prompted,"CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,"",n,y,n,y,"","Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",y,"The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.",y,"conversations are
processed through a HIPAA-compliant server: audio is recorded, transcribed by speech-to-text AI, and responded to by LLM (GPT-4)

Dec. Helsinki",y,Table-1,n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,only abstract
204,Yu 2024,An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system,Reviewer Two,UK,Self-collected data but derived from external data set,"created an own based on therapy transcript documents from different websites

As we could not find a suitable data set, we created one … We used real-world therapy transcript documents from websites and converted the HTML conversation texts…
",Psychotherapy -- speech transcripts,English,No,Other: did not find info. ,Psychopathology,Trained professionals,"",N,"",no,"only:  such as personal details, to protect privacy and reduce computation complexities
",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4,6,2024,Tool Development and Evaluation,"",Client-facing application,Chatbot,"People with some symptoms but not disorder (determined by symptom scales or questionnaires); Other: Human evaluations relied on two groups: volunteers who believe they are suffering mental 
health issue, mental healthcare professionals and researchers.",10,"",Only fine-tuning,"Unspecified, might include formal therapy methods",GPT-3 family; GPT-3.5 family,Yes,"",N,Y,Y,Y,"","1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5)","",Y,BLEU score (B),L,"ChatGPTbased conversations 
(Approach 1), fine-tuned DialoGPT transformer 
conversations (Approach 2), and fine-tuned DialoGPT 
transformer conversations combined with the GPT3 prompts 
API (Approach 3)","","","","","","","","",Y,Perplexity evaluations (B),L,"ChatGPTbased conversations 
(Approach 1), fine-tuned DialoGPT transformer 
conversations (Approach 2), and fine-tuned DialoGPT 
transformer conversations combined with the GPT3 prompts 
API (Approach 3)","",Y,no benchmark,"",s. user experience,"","","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Human evaluations relied on two groups: 
mental healthcare professionals and researchers who believe they are suffering mental 
health issue. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness and satisfaction of the 
chatbot, its conversation quality, and potential areas of 
improvement. ",Y,"We want to emphasize, though, that our chatbot, while 
a step forward, is not a replacement for human therapists. 
Instead, we envision it as an auxiliary resource that can 
provide support in scenarios where human resources are 
stretched thin, or as an additional tool to complement 
traditional therapeutic processes.

 Integration with clinical systems: future research 
could look into integrating the chatbot with chatbot to provide more personalized and context-
aware support. It could also facilitate better 
coordination with healthcare professionals, alerting 
them when the chatbot identifies potential serious 
concerns.
existing clinical systems. This would allow the ","",Yes,"Risk assessment and safety measures: healthcare policies need to scrutinize organizations proposing tools using LLMs in domain-specific areas, such as mental health or healthcare in general. This is due to their tendency to hallucinate and provide inaccurate information to the user. 
In addition, organizations or governments need to employ strategies to educate the public on the limitations of these technologies and their appropriate usage. Providing the proper resources and training aids individuals in evaluating the information received and making informed decisions. 
Balancing support and harm prevention: if the user’s input is about harming themselves, the chatbot did not provide enough suggestions that the user needed to get help from nearby human services or telephone numbers, the dynamic nature of emotions needs to be at the heart of the development of LLMs. Conversational agents must be able to engage with the individual and adapt to their needs, not through stated guidelines but through the evolution of the chatbot’s character through continual tone analysis. This adaptative approach would involve the chatbot not only understanding and responding to the user’s emotions but also evolving its behaviour and developing a unique personality over time.
Privacy and informed consent:  Policies should address data storage, consent management, and secure communication protocols to protect user confidentiality"
204,Yu 2024,An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system,Richard Gaus,UK,External data set,"We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: ""This site is fiction and all the characters are fictional.""",Psychotherapy -- chat logs,English,Other: synthetic; human-generated (fictional),No,Other: not applicable,Other: not applicable,"",n,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4,6,2024,Tool Development and Evaluation,"",Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),10,"","Other: Fine-tuning of DialogGPT. This fine-tuned model is integrated into a custom chatbot pipeline, together with ChatGPT 3.5","Unspecified, might include formal therapy methods",GPT-2 family; GPT-3.5 family,Yes,"",n,y,y,y,"","To gather crucial user insights and evaluate the chatbots 
performance from a user’s perspective, we conducted 
surveys among a select group of mental health users and 
carers. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness of the 
chatbot, its conversation quality, and potential areas of 
improvement. Below, we detail the survey’s findings:
- Willingness for continued chatbot usage: when 
questioned about their willingness to engage with 
the chatbot again after the initial interaction, an 
overwhelming majority (90%) expressed a positive 
intent to reuse the service
- Rating of conversation quality (human-likeness): 
participants rated the chatbot’s conversational 
quality in terms of its human-like language on a 
scale from 1 to 5. The chatbot received an average 
score of 4.3, indicating a high degree of satisfaction 
with the chatbot’s language quality.
- Rating of conversation quality (supportiveness): 
when assessing the chatbot’s supportive nature in 
the conversation, participants gave an average score 
of 4.2 of 5, reflecting their positive experience in 
terms of perceived support.
- Overall rating of the chatbot application: when 
asked to provide an overall rating of the chatbot 
application, participants gave an average score 
of 4.6 out of 5. Notably, no participant rated the 
application lower than 4, indicating a high level of 
user satisfaction.
-  Positive feedback: participants were invited to 
share any positive feedback about their experience. 
We got 8 responses for this question and the most 
important points are the LLM-based chatbot can 
always provide useful suggestions and they feel very 
safe to talk to someone who are always available 
and talkable about their issue and sadness.
- Areas for improvement: we also encouraged 
users to suggest areas where the chatbot could 
be improved. The survey participants found the 
chatbot to be generally helpful, but suggested 
improvements such as exposing the training data 
to more diverse circumstances, enhancing the 
emotional support aspect, avoiding risky responses 
to sensitive inquiries, reducing repetition of 
examples, and focusing on more teaching sessions 
to make the interactions feel less robotic and more 
like conversing with a human friend.
- User interface (UI) suggestions: participants were 
also asked to provide suggestions for improving 
the chatbot’s UI functionalities to enhance its 
usefulness and usability. The feedback is very useful 
for us to implement further improved version. The 
suggestions include voice and image combined 
responses, able to track chat history, virtual reality 
(VR) or mixed reality innovation and realistic 
human tongues enhancement.","",y,b,l,benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts),n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,"no benchmark. however, experts rated the chatbot highly in absolute terms.",n,"","","","",perplexity,b,l,benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts),"","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",y,rated by both patients and professionals,y,"e.g. "" The suggestion to integrate structured therapy 
techniques and conduct research on the efficacy 
of chatbots in mental health support indicates the 
potential for collaboration between the chatbot 
application and professionals in the field. Future 
policies could encourage partnerships between 
developers and professionals in psychology and 
counseling to enhance the chatbots effectiveness 
and provide a well-rounded approach to mental 
healthcare (23).""","",Yes,"Discussions for future health policy of using LLM-based 
chatbot systems
As we dive into the future of health policies, addressing 
the evolving role of LLMs within the domain is essential. 
This is due to their inherent potential but also the unique 
challenges. Below are key points intended to guide 
developing and implementing LLM-based chatbots within 
healthcare settings.
- Risk assessment and safety measures: healthcare 
policies need to scrutinize organizations proposing 
tools using LLMs in domain-specific areas, such 
as mental health or healthcare in general. This is 
due to their tendency to hallucinate and provide 
inaccurate information to the user. In 2018, a 
survey published in the National Health Literacy 
stated that only 11% of the general population 
strongly believed in their ability to appraise the 
reliability of healthcare information. This low 
confidence level indicates a significant risk of 
individuals accepting potentially misleading or 
inaccurate responses as factual.
In addition, organizations or governments need to 
employ strategies to educate the public on the limitations of 
these technologies and their appropriate usage. Providing 
the proper resources and training aids individuals in 
evaluating the information received and making informed 
decisions.

etc."
204,Yu 2024,An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system,Consensus,UK,External data set,"We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: ""This site is fiction and all the characters are fictional.""",Other: not applicable,English,Other: synthetic; human-generated (fictional),No,Other: not applicable,Other: not applicable,"",N,"",n,"only:  such as personal details, to protect privacy and reduce computation complexities
",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4,6,2024,Tool Development and Evaluation,"",Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),10,"","Other: Fine-tuning of DialogGPT. This fine-tuned model is integrated into a custom chatbot pipeline, together with ChatGPT 3.5","Unspecified, might include formal therapy methods",GPT-2 family; GPT-3.5 family,Yes,"",N,Y,Y,Y,"","1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5)","",Y,b,L,"benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)

ChatGPTbased conversations
(Approach 1), fine-tuned DialoGPT transformer
conversations (Approach 2), and fine-tuned DialoGPT
transformer conversations combined with the GPT3 prompts
API (Approach 3)",n,"","","",n,"","","",n,"","","","",Y,no benchmark,no benchmark,"no benchmark. however, experts rated the chatbot highly in absolute terms.",n,"","","","",perplexity,b,l,benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts),"","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Human evaluations relied on two groups: 
mental healthcare professionals and researchers who believe they are suffering mental 
health issue. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness and satisfaction of the 
chatbot, its conversation quality, and potential areas of 
improvement. ",Y,"We want to emphasize, though, that our chatbot, while 
a step forward, is not a replacement for human therapists. 
Instead, we envision it as an auxiliary resource that can 
provide support in scenarios where human resources are 
stretched thin, or as an additional tool to complement 
traditional therapeutic processes.

 Integration with clinical systems: future research 
could look into integrating the chatbot with chatbot to provide more personalized and context-
aware support. It could also facilitate better 
coordination with healthcare professionals, alerting 
them when the chatbot identifies potential serious 
concerns.
existing clinical systems. This would allow the ","",Yes,"Risk assessment and safety measures: healthcare policies need to scrutinize organizations proposing tools using LLMs in domain-specific areas, such as mental health or healthcare in general. This is due to their tendency to hallucinate and provide inaccurate information to the user. 
In addition, organizations or governments need to employ strategies to educate the public on the limitations of these technologies and their appropriate usage. Providing the proper resources and training aids individuals in evaluating the information received and making informed decisions. 
Balancing support and harm prevention: if the user’s input is about harming themselves, the chatbot did not provide enough suggestions that the user needed to get help from nearby human services or telephone numbers, the dynamic nature of emotions needs to be at the heart of the development of LLMs. Conversational agents must be able to engage with the individual and adapt to their needs, not through stated guidelines but through the evolution of the chatbot’s character through continual tone analysis. This adaptative approach would involve the chatbot not only understanding and responding to the user’s emotions but also evolving its behaviour and developing a unique personality over time.
Privacy and informed consent:  Policies should address data storage, consent management, and secure communication protocols to protect user confidentiality"
203,Bird 2024,Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,Reviewer Two,UK,External data set,"CounselChat2,
provided through the HuggingFace Hub platform3. ","","","",Yes,"","","",Y,mistral,N,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26,6,2024,"",Direct LLM performance evaluation (only via prompting),"","","","","","","Unspecified, might include formal therapy methods",Mistral family,No,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",See Table 3,W,H,"",Linguistic Features as mentioned under 3.3,W,H,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Yes,legal and discrimination risks are discussed; to little refferal to professional help 
203,Bird 2024,Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,Richard Gaus,UK,External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,"",y,mistral 7b,n,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26,6,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","",Mix of formal therapy methods,Mistral family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",lexical diversity and richness,w,h,human psychologist responses in the CounselChat transcripts,readability scores,unclear,unclear,it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.,various other traditional NLP metrics,"","","","",n,"",n,"",y,on-premise model (MISTRAL),n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
203,Bird 2024,Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,Consensus,UK,External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,"",y,mistral 7b,n,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26,6,2024,Direct LLM performance evaluation (only via prompting),Other: ,Client-facing application,Chatbot,No clients/patients involved,"","","","Unspecified, might include formal therapy methods",Mistral family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",lexical diversity and richness,W,H,human psychologist responses in the CounselChat transcripts,Linguistic Features as mentioned under 3.3,unclear,H,it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.,"","","","","",n,"",n,"",y,on-premise model (MISTRAL),n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,legal and discrimination risks are discussed; to little refferal to professional help 
200,Lai 2023,Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs),Reviewer Two,Other: Australia,Other: Mix of external and self collected via data crawling,"PsyQA and Crawl of various chinese social media platforms Tianya, Zhihu, Yixinli

PsyQA (external); web-crawled psychology corpus (unnamed). Description: Crawled Chinese psychology-forum data (Yixinli, Tianya) to pretrain PanGu-350M; then fine-tuned on 56,000 PsyQA Q&A pairs.
",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Unknown,"",N,"",N,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",22,12,2023,Direct LLM performance evaluation (only via prompting),"",Client-facing application,"",General population,"","","","Informal counseling (e.g., emotional support conversation)",GPT-2 family; GPT-3 family; Other: PanGu (similar to GPT-3)and WenZhong (GPT-2),Yes,"",n,n,n,n,"","Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.","",y,b,l,ROUGE-L - PanGu better than WenZhong,"","","","","","","","","","","","","",y,w,h,"Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model","","","","","",Perplexity,b,l,PanGu better than WenZhong,Distinct 1,b,l,PanGu better than WenZhong,"Distinct 2
",b,l,PanGu better than WenZhong,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls. Developing mechanisms to handle potentially sensitive
or harmful user queries is crucial. The chatbot should have appropriate safeguards and
guidelines to avoid providing inaccurate or harmful advice. Integrating a reporting system
where users can report problematic responses or seek human intervention can help miti-
gate potential risks. Furthermore, monitoring and auditing the chatbot’s performance and
behaviour can help identify and rectify biases or discriminatory patterns. Regular evalua-
tions by domain experts and user feedback analysis can improve the chatbot’s reliability,
fairness, and inclusivity."
200,Lai 2023,Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs),Richard Gaus,Other: Australia,External data set,"training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.",Other: mixed,Chinese,No,No,Unknown,Unknown,"",y,"",y,"Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",22,12,2023,Tool Development and Evaluation,"",Client-facing application,Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.,No clients/patients involved,"","",Other: both original training of the transformer model and subsequent fine-tuning,"Unspecified, might include formal therapy methods",GPT-2 family; Other: PanGu,No users involved,"","","","","","","","",y,no benchmark,no benchmark,ROUGE-L. no benchmark,n,"","","",n,"","","",n,"","","","",y,w,h,"psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set",n,"","","","",perplexity,no benchmark,no benchmark,no benchmark,"distinct-1, -2",no benchmark,no benchmark,no benchmark,"","","","","",n,"",n,"",y,all used models are on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"As with any AI-based system, ethical considerations and user privacy are paramount. Future work should address these concerns by implementing robust privacy protection mechanisms and ensuring transparency in data usage. This includes obtaining explicit user consent for data collection and usage, anonymising sensitive user information, and imple- menting strict data access controls. Developing mechanisms to handle potentially sensitive or harmful user queries is crucial. The chatbot should have appropriate safeguards and guidelines to avoid providing inaccurate or harmful advice. Integrating a reporting system where users can report problematic responses or seek human intervention can help miti- gate potential risks. Furthermore, monitoring and auditing the chatbot’s performance and behaviour can help identify and rectify biases or discriminatory patterns. Regular evalua- tions by domain experts and user feedback analysis can improve the chatbot’s reliability, fairness, and inclusivity."
200,Lai 2023,Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs),Consensus,Other: Australia,External data set,"training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.",Other: mixed,Chinese,No,No,Unselected,Unknown,"",y,"",y,"Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",22,12,2023,Tool Development and Evaluation,"",Client-facing application,Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.,No clients/patients involved,"","",Other: both original training of the transformer model and subsequent fine-tuning,"Unspecified, might include formal therapy methods","GPT-2 family; Other: PanGu, WenZhong (based on GPT-2)",No users involved,"","","","","","","Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.","",y,no benchmark,no benchmark,ROUGE-L - PanGu better than WenZhong,n,"","","",n,"","","",n,"","","","",y,w,h,"psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set

Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model",n,"","","","",Perplexity,no benchmark,no benchmark,no benchmark,Distinct 1,no benchmark,no benchmark,no benchmark,"Distinct 2
",no benchmark,no benchmark,no benchmark,"",n,"",n,"",y,all used models are on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls. Developing mechanisms to handle potentially sensitive
or harmful user queries is crucial. The chatbot should have appropriate safeguards and
guidelines to avoid providing inaccurate or harmful advice. Integrating a reporting system
where users can report problematic responses or seek human intervention can help miti-
gate potential risks. Furthermore, monitoring and auditing the chatbot’s performance and
behaviour can help identify and rectify biases or discriminatory patterns. Regular evalua-
tions by domain experts and user feedback analysis can improve the chatbot’s reliability,
fairness, and inclusivity."
198,Kumar 2024,"Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)",Reviewer Two,India,External data set,"publically available online data resources;
Kaggle mental-health conversation/Q&A sets; SAMHSA SOAR sample Medical Summary Reports
",Other: unclear,English,Other: unclear,Yes,"","","",Y,Llama,Y,"User privacy and data security are paramount. The system operates under strict ethical guidelines and secure data storage protocols.”
",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,03,2024,Tool Development and Evaluation,"",Client-facing application,"",No clients/patients involved,"","",Custom pipeline with integrated LLM which is only prompted,"Informal counseling (e.g., emotional support conversation)",Llama 2 family,No users involved,"","","","","","","","","","","","","","","","",Yes ,Better,Low,Benchmark are different models,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"User privacy and data 
security are paramount. The system operates under 
strict ethical guidelines and secure data storage 
protocols","","","","","","","","","","","","","","","","Authors mention that model is designed as a ""solution that could be used on common 
hardware by users without having the knowledge and 
technical proficiency regarding large language models""","",No,""
198,Kumar 2024,"Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)",Richard Gaus,India,External data set,Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website,Other: mixed,Other: unknown,Other: unknown,No,Other: unknown,Other: unknown,"",y,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14,3,2024,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Custom pipeline with integrated LLM which is only prompted,Other: unclear,Llama 2 family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Human rating, not sure by whom",no benchmark,no benchmark,different models were rated by human non-experts. no comparison with any benchmark.,"","","","","","","","","",n,"",n,"",y,all on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
198,Kumar 2024,"Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)",Consensus,India,External data set,Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website,Other: mixed,Other: unknown,Other: unknown,No,Other: unknown,Other: unknown,"",Y,Llama,n,"User privacy and data security are paramount. The system operates under strict ethical guidelines and secure data storage protocols.”
",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14,3,2024,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Custom pipeline with integrated LLM which is only prompted,"Unspecified, might include formal therapy methods",Llama 2 family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Human rating, not sure by whom",no benchmark,no benchmark,different models were rated by human non-experts. no comparison with any benchmark.,"","","","","","","","","",n,"",n,"",y,"all on-premise

but not P-2 (hipaa or gdpr)",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
194,Nazarova 2023,Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference,Reviewer Two,Other: Kyrgyz Republic,Self-collected data,"","","","","","","","",N,"",N,"","","Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2,11,2023,Tool Development and Evaluation,"",Client-facing application,"",Other: unclear,"","",Custom pipeline with integrated LLM which is only prompted,Other CBT techniques,GPT-3 family,No,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",No,""
194,Nazarova 2023,Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference,Richard Gaus,Other: Kyrgyzstan,Self-collected data,"The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.",Other: descriptions of cognitive distortions,Other: unknown,"Other: synthetic, human created",No,Unselected,Other: not applicable,"",n,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2,11,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,"Other: 68 university students recruited using convenience sampling (social media campaign, institutional newsletter)",68,"",Only fine-tuning,CBT: Cognitive restructuring; Other CBT techniques,GPT-3 family,No,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.,n,"","","","",n,"","","",n,"","","","",AAQ + CDS,s,h,aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.,"","","","","","","","","",n,"",n,"",n,GPT-3 API,n,"",n,"",y,fig 6 participants flow,y,number of messages sent is reported,y,"unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures",y,"After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. ",n,"",n,"","",No,""
194,Nazarova 2023,Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference,Consensus,Other: Kyrgyzstan,Self-collected data,"The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.",Other: descriptions of cognitive distortions,Other: unknown,"Other: synthetic, human created",No,Other: not applicable,Other: not applicable,"",N,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2,11,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,General population,68,"",Only fine-tuning,CBT: Cognitive restructuring; Other CBT techniques,GPT-3 family,No,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.,n,"","","","",n,"","","",n,"","","","",AAQ + CDS,s,h,aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.,"","","","","","","","","",n,"",n,"",n,GPT-3 API,n,"",n,"",y,fig 6 participants flow,y,number of messages sent is reported,y,"unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures",y,"After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. ",n,"",n,"","",No,""
147,YounghunLee 2024,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Reviewer Two,USA,Self-collected data,"Counseling/chat conversations annotated for utterance‐level features; session-level features and summaries also produced (some via prompts). Annotation: Human annotators used a codebook of counseling strategy features
",Emotional support dialogue -- chat logs,English,No,No,Unselected,Trained professionals,"",N,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,3,2024,Tool Development and Evaluation,Direct LLM performance evaluation (only via prompting),Analysis of conversation transcripts,"",No clients/patients involved,"",Treatment fidelity feedback,"","Unspecified, might include formal therapy methods",BERT family,No,"","","","","","","","","","","","","","","","",Y,B,L,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",No,""
147,YounghunLee 2024,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Richard Gaus,USA,External data set,from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.,Emotional support dialogue -- chat logs,English,No,No,Unknown,Trained professionals,"",n,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,3,2024,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only,"Unspecified, might include formal therapy methods",BERT family; GPT-3 family; GPT-3.5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,"1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,ChatGPT used,n,"Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
147,YounghunLee 2024,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Consensus,USA,External data set,"from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.

Counseling/chat conversations annotated for utterance‐level features; session-level features and summaries also produced (some via prompts). Annotation: Human annotators used a codebook of counseling strategy features",Emotional support dialogue -- chat logs,English,No,No,Unknown,Trained professionals,"",N,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,3,2024,Tool Development and Evaluation,Direct LLM performance evaluation (only via prompting),Analysis of conversation transcripts,"","","",Other: ,Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only,"Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,W,L,"Table 4: Benchmark is fine-tuned DistilBERT model. ChatGPT is compared against this and performs worse.
Table 2: Prompted GPT-3 model (text-davinci-003) is compared against DistilBERT.

1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,ChatGPT used,n,"Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
134,HongbinNa 2024,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,Reviewer Two,Other: Australia,Self-collected data but derived from external data set,"PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
CBT QA dataset. Derived from: PsyQA → CBT QA (ChatGPT-generated via CBT prompt). Short description: Chinese mental-health Q&A where responses are generated with a structured CBT prompt and used to fine-tune CBT-LLM.

",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,"Other: LLM responses, not human responders
","",N,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",20,5,2024,Tool Development and Evaluation,"","","","","","",Custom pipeline with integrated LLM which is only prompted,Other CBT techniques,GPT-3.5 family,"","","","","","","","","",Y,B,"L

",similar chinese language LLMs,Y,B,"L
",similar chinese language LLMs,N,"","","",N,"","","","",Y,B,L,Relevance; CBT Structure; Helpfulness,N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",No,""
134,HongbinNa 2024,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,Richard Gaus,Other: Australia,Self-collected data but derived from external data set,"External: PsyQA
Derived: CBT QA",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Other: not applicable,"",y,"",n,they say they release the data but where is it?,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",20,5,2024,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Only fine-tuning,Other CBT techniques,Other: Baichuan-7B,No users involved,"","","","","","","","",y,b,l,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",y,b,l,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",y,no benchmark,no benchmark,"accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection.",n,"","","","",y,b,l,Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.,n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
134,HongbinNa 2024,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,Consensus,Other: Australia,Self-collected data but derived from external data set,"PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
CBT QA dataset. Derived from: PsyQA → CBT QA (ChatGPT-generated via CBT prompt). Short description: Chinese mental-health Q&A where responses are generated with a structured CBT prompt and used to fine-tune CBT-LLM.

",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Other: not applicable,"",y,"",N,they say they release the data but where is it?,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",20,5,2024,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Only fine-tuning,Other CBT techniques,"GPT-3.5 family; Qwen family; Other: Baichuan-7B, Llama 1, Alpaca 1",No users involved,"","","","","","","","",Y,B,L,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",Y,B,L,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",y,no benchmark,no benchmark,"accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection. Task: Cognitive distortion detection in client questions.",N,"","","","",Y,B,L,"Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.

Measures: Relevance, CBT structure, helpfulness",N,"","","","","","","","","","","","","","","","","",n,"",n,"",y,on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
128,Ma 2023,Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support,Reviewer Two,USA,Self-collected data,"","","","","","","","","","","","",Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),28,7,2023,Conceptual or theoretical work (e.g. on ethics or safety),"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
128,Ma 2023,Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support,Richard Gaus,USA,No dataset used for development or evaluation,"","","","","","","","","","","","",Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",9,11,2024,Population survey,"",Client-facing application,Chatbot,Other: reddit users (r/Replika),462,"","","Informal counseling (e.g., emotional support conversation)",GPT-3 family,Yes,"",n,y,n,y,"","Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Yes,"Section Challenge 1: Harmful content. Description of unsafe functions of the chatbot. It follows only a superficial discussion of this in the discussion section, though."
128,Ma 2023,Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support,Consensus,USA,No dataset used for development or evaluation,"","","","","","","","","","","","",Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",9,11,2024,Population survey,"",Client-facing application,Chatbot,General population,462,"","","Informal counseling (e.g., emotional support conversation)",GPT-3 family,Yes,"",n,y,n,y,"","Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Yes,"Section Challenge 1: Harmful content. Description of unsafe functions of the chatbot. It follows only a superficial discussion of this in the discussion section, though."
124,Yao 2022,"Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings",Reviewer Two,China,External data set,"PsyQA
he data set contains … about 22,000 pieces of psychological counseling data… Topics included… growth, emotion, love… About 8% of the answers come from national second-class psychological counselors; 35% … from volunteers…” / “The data source for the knowledge graph is PsyQA …
",Internet data -- mental health Q&A,Chinese,No,Other: unclear,Unselected,Other: mixed,"",N,"",N,"",Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),8,11,2024,Tool Development and Evaluation,Direct LLM performance evaluation (only via prompting),Analysis of conversation transcripts,Chatbot,Other: The questions in PsyQA data set cover a variety of user groups.,"","",Custom pipeline with integrated LLM which is only prompted,"Informal counseling (e.g., emotional support conversation)",GPT-2 family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","",N,"","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",proof-of-concept prototype rather than a full-fledged clinical tool,No,""
124,Yao 2022,"Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings",Richard Gaus,China,External data set,PsyQA,Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Trained professionals,"",y,"",n,"",Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",28,10,2022,Tool Development and Evaluation,"",Client-facing application,Other: One-turn chatbot,No clients/patients involved,"","","Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.","Unspecified, might include formal therapy methods",BERT family; GPT-2 family,No,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"even though this is a crisis call intervention system, there is not detection of acute risk",n,GPT-2 outputs are not checked,y,BERT and GPT-2 is on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","No evaluation of the conversation system. Study only displayed some conversation sample ""look this is good"".",No,""
124,Yao 2022,"Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings",Consensus,China,External data set,PsyQA,Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Trained professionals,"",y,"",n,"",Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",28,10,2022,Tool Development and Evaluation,Other: ,Client-facing application,"Other: ""one turn chatbot"", i.e., user inputs ""feelings and confusion"" and system makes analysis and outputs intervention text once. There is no turn-based interaction with the tool.",No clients/patients involved,"","","Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.","Unspecified, might include formal therapy methods",BERT family; GPT-2 family,No,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"even though this is a crisis call intervention system, there is not detection of acute risk",n,GPT-2 outputs are not checked,y,BERT and GPT-2 is on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","proof-of-concept prototype rather than a full-fledged clinical tool

No evaluation of the conversation system. Study only displayed some conversation sample ""look this is good"".",No,""
120,Liu 2023,"Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I",Reviewer Two,China,Self-collected data but derived from external data set,"MESConv dataset generated from YT videos

Large-scale multi-modal emotional-support dialogues with utterance-level emotion annotations and strategy labels; used for Emotion/Strategy/Response tasks.
",Emotional support dialogue -- speech transcripts,Other: did not find only guessed,Yes,No,Unselected,Unknown,"",N,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5,11,2023,Tool Development and Evaluation,"",Client-facing application,"",No clients/patients involved,"","",Other: ,"Informal counseling (e.g., emotional support conversation)",Other: none,No,"",N,"","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","Emotion
task",B,L, recognizing the utterance-level emotion of the help-seeke,Strategy task,B,L,predicting support strategies,Response task,B,L,"generating supportive
response","","","","","","","","","","","","","","","","","","","","","","","","",No,""
120,Liu 2023,"Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I",Richard Gaus,China,External data set,"MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube)",Other: Emotional support dialogue -- multimodal data,Other: unknown,No,No,Unselected,Unknown,"",y,BlenderBot,n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5,11,2023,Tool Development and Evaluation,"",Client-facing application,"Other: conversation system with video, audio, and text input",No clients/patients involved,"","","Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response","Unspecified, might include formal therapy methods",Other: BlenderBot,No users involved,"","","","","","","","",y,b,l,benchmark: other conversation systems,n,"","","",y,b,l,benchmark: other conversation systems,n,"","","","",n,"","","",n,"","","","",perplexity,b,l,benchmark: other conversation systems,"","","","","","","","","",n,"",n,"",y,all on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
120,Liu 2023,"Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I",Consensus,China,Self-collected data but derived from external data set,"MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube)

Large-scale multi-modal emotional-support dialogues with utterance-level emotion annotations and strategy labels; used for Emotion/Strategy/Response tasks.",Other: Emotional support dialogue -- multimodal data,Other: unknown,No,No,Unselected,Unknown,"",y,BlenderBot,N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5,11,2023,Tool Development and Evaluation,"",Client-facing application,"Other: conversation system with video, audio, and text input",No clients/patients involved,"","","Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response","Unspecified, might include formal therapy methods",Other: BlenderBot,No users involved,"","","","","","","","",y,b,l,benchmark: other conversation systems,n,"","","",y,b,l,benchmark: other conversation systems. Task: emotion classification in current utterance of help-seeker,N,"","","","",n,"","","",n,"","","","",perplexity,b,l,benchmark: other conversation systems,"","","","","","","","","",n,"",n,"",y,all on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
119,Srivastava 2023,Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023,Reviewer Two,India,External data set,"HOPE;
Switchboard Dialogue-act Corpus; (also mentions a “Dialog-act corpus”). Description: Counseling conversations with dialogue-act labels (HOPE) and a general telephone dialogue-act corpus (Switchboard) used for evaluation/generalizability. Derived: Not indicated.
",Psychotherapy -- chat logs,English,Yes,Other: unclear,Unknown,Other: not applicable,"",N,"",Y,"",Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",30,4,2023,Tool Development and Evaluation,"","","","","","","Other: not sure, ""READER is built on transformer to jointly predict a potential dialogue-act for the
next utterance and to generate an appropriate
response""","Informal counseling (e.g., emotional support conversation)",GPT-2 family,No users involved,"","","","","","","","",Y,B,L,"DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",Y,B,L,"DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",N,"","","",N,"","","","",Y,no benchmark,"","","","","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","","",""
119,Srivastava 2023,Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023,Richard Gaus,India,External data set,"HOPE
212 multi-turn English psychotherapy sessions (≈ 12.9 K utterances) between therapist and patient, transcribed from YouTube videos",Psychotherapy -- speech transcripts,English,No,No,Unknown,Unknown,"",y,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",30,4,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","","Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization","Unspecified, might include formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",y,b,l,"metrics: ROUGE, METEOR. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...)",y,b,l,"metrics: BERTScore. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...)",n,"","","",n,"","","","",y,b,l,"metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Ethical Considerations and Future Work. Considering the severity of the research area, we make sure that at each step, we maintain the privacy of the personal data of clients."
119,Srivastava 2023,Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023,Consensus,India,External data set,"HOPE
212 multi-turn English psychotherapy sessions (≈ 12.9 K utterances) between therapist and patient, transcribed from YouTube videos

HOPE;
Switchboard Dialogue-act Corpus; (also mentions a “Dialog-act corpus”). Description: Counseling conversations with dialogue-act labels (HOPE) and a general telephone dialogue-act corpus (Switchboard) used for evaluation/generalizability. Derived: Not indicated.",Psychotherapy -- speech transcripts,English,No,No,Unknown,Unknown,"",y,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",30,4,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","","Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization","Unspecified, might include formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",Y,B,L,"metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",Y,B,L,"metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",N,"","","",N,"","","","",Y,b,l,"metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Ethical Considerations and Future Work. Considering the severity of the research area, we make sure that at each step, we maintain the privacy of the personal data of clients."
116,Qiu 2023,"A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II",Reviewer Two,China,Self-collected data,"Multi-turn real counseling chat sessions collected on a Chinese online text-based free counseling platform, plus public QA; filtered to 7,935 sessions; labeled with an 8-category safety taxonomy; stratified split 90/10.
",Emotional support dialogue -- chat logs,Chinese,No,No,Unselected,Unknown,"",N,Chat GPT 3.5 used + BERT ,N,Too little,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,10,2023,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,B,L,ChatGPT (GPT 3.5),N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",Yes,"Dialogue safety and unpredictable behavior of LLMs: ""However, there are growing concerns about dialogue safety, especially for open-domain conversational AI, due to the uncontrollable generation derived from the intrinsic unpredictable nature of neural language models."" [...] "" Furthermore, most efforts to detect harmful content [5,6,10] prioritize identifying offensive language in casual conversations due to the
extensive development of dialogue systems for casual chatting. In particular, as digital mental health services [11,12] become increasingly common, it is crucial to develop new taxonomies and approaches that can accurately identify and address unserviceable content in mental health support conversations. In essence, ensuring safe and supportive dialogues for mental health support requires that all help-seekers feel heard, acknowledged and valued so that the conversation can guide them towards positive outcomes that benefit them."""
116,Qiu 2023,"A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II",Richard Gaus,China,Self-collected data,"We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

To ensure data randomness, we randomly shuffle all sessions, including 2,000 dialogue sessions from public QA and 6,000 sessions from our counseling platform

https://github.com/qiuhuachuan/DialogueSafety",Emotional support dialogue -- chat logs,Chinese,No,Yes,Unknown,Unknown,"",n,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,10,2023,Other: Development of new taxonomy for utterances of LLM psychotherapists. Fine-tuning of BERT. Direct evaluation of ChatGPT 3.5.,"",Analysis of conversation transcripts,"","","","","","Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,"accuracy, precision, recall, F1 for classifying utterances",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",y,study is about this exactly,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"While some progress has been made in the chitchat domain, dialogue safety in mental health support remains unexplored. Existing taxonomies [4,7–9] for dialogue safety mainly focus on chitchat settings, but they may not be suitable for identifying inappropriate content that violates mental health principles in mental health support conversations. Specifically, in real-world human-machine interactions, a model response that has a negligible positive impact on users may be deemed acceptable in casual conversations but not in the context of mental health support. Furthermore, most efforts to detect harmful content [5, 6,10] prioritize identifying offensive language in casual conversations due to the extensive development of dialogue systems for casual chatting. In particular, as digital mental health services [11,12] become increasingly common, it is crucial to develop new taxonomies and approaches that can accurately identify and address unserviceable content in mental health support conversations.

ff."
116,Qiu 2023,"A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II",Consensus,China,Self-collected data,"We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

Multi-turn real counseling chat sessions collected on a Chinese online text-based free counseling platform",Emotional support dialogue -- chat logs,Chinese,No,No,Unknown,Unknown,"",N,Chat GPT 3.5 used + BERT ,N,Too little,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,10,2023,Tool Development and Evaluation,Direct LLM performance evaluation (only via prompting),Analysis of conversation transcripts,"","","","",Only fine-tuning,"Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,W,L,"accuracy, precision, recall, F1 for classifying utterances. Task: classification of mental health dialogue turns into safe/various types of unsafe responses. benchmark: fine-tuned BERT-base and RoBERTa-large",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",Yes,"While some progress has been made in the chitchat domain, dialogue safety in mental health support remains unexplored. Existing taxonomies [4,7–9] for dialogue safety mainly focus on chitchat settings, but they may not be suitable for identifying inappropriate content that violates mental health principles in mental health support conversations. Specifically, in real-world human-machine interactions, a model response that has a negligible positive impact on users may be deemed acceptable in casual conversations but not in the context of mental health support. Furthermore, most efforts to detect harmful content [5, 6,10] prioritize identifying offensive language in casual conversations due to the extensive development of dialogue systems for casual chatting. In particular, as digital mental health services [11,12] become increasingly common, it is crucial to develop new taxonomies and approaches that can accurately identify and address unserviceable content in mental health support conversations.

ff."
115,Koz\l{}owski 2023,"Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I",Reviewer Two,Other: Poland,Other: Translation of English datasets to Polish ,"DailyDialog and EmpatheticDialogues as basis; translated and merged: CORTEX; (enriched by Polish Common Crawl);

A Polish emotion-labeled text corpus was semi-supervisedly expanded with unlabeled web data to enlarge the training set
",Other: emotion-labeled text corpus,Other: Polish,No,Yes,Other: not applicable,Other: not applicable,"",N,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3,7,2023,Tool Development and Evaluation,"","","","","","",Only fine-tuning,Other CBT techniques,BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,no benchmark,"","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","","","","",""
115,Koz\l{}owski 2023,"Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I",Richard Gaus,"",Self-collected data but derived from external data set,"Name of new dataset: CORTEX. Derived from DailyDialog, EmpatheticDialogues","","","","","","","","","","","",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3,7,2023,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,"sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"","","","","","","","","",n,"",n,"","",No,""
115,Koz\l{}owski 2023,"Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I",Consensus,"",Self-collected data but derived from external data set,DailyDialog and EmpatheticDialogues as basis; translated and merged: CORTEX; (enriched by Polish Common Crawl) ,"","","","","","","","","","","",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3,7,2023,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,no benchmark,no benchmark,"sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification",N,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",NA,"",NA,"",n,"",NA,"",n,"",n,"","","",""
101,Berrezueta-Guzman 2024,Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement,Reviewer Two,Germany,Self-collected data,"","","","","","","","",N,"",N,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",19,3,2024,Other: experimental research,"",Client-facing application,"",No clients/patients involved,"","","","Unspecified, might include formal therapy methods","Other: ""custom GPT"", version not further specified",No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,no benchmark,no benchmark,no benchmark,N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Based on the analysis of the results, we will consider that implementing a custom
ChatGPT in a robot to support ADHD therapies presents considerable potential. Its advan-
tages include personalization, where ChatGPT can tailor interactions to each patient’s unique
needs and responses, thus potentially enhancing the therapeutic experience. Consistency is
another benefit, as a ChatGPT-equipped robot can offer stable support, which is crucial in
ADHD therapies where routine and predictability play vital roles. Additionally, ChatGPT’s
capability to understand and generate natural language can significantly increase the en-
gagement and interactivity of therapy sessions for children with ADHD, thus making them
more dynamic and effective. However, we also found a range of complex challenges and
considerations. Key among these is the need for emotional intelligence.
Significantly, ChatGPT and its use by a robotic assistant should complement, not
replace, human therapists (as also mentioned by the studies [14,21]), as the human element
is critical, especially for children with ADHD. ","""Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality.""",Yes,"Over-reliance on technology poses a risk, thus potentially impacting the development of social skills and real-life coping mechanisms, and technical limitations, such as misunderstanding inputs or handling complex scenarios, must be addressed.
[..]

Currently, LLMs face difficulties in ensuring safety and stability, mainly when deal-
ing with prompt injection (PI) that includes sensitive topics, which generate adversarial replies [54]. There is room for improvement regarding jailbreaking attempts in LLMs, thus dealing with inconsistent LLM replies by reverse engineering the prompt to jailbreak or hack the system. However, new ChatGPT models (ChatGPT 4) decreased the rate of generated adversarial content [55], which is based on semantic understanding [56]. Moreover, malicious content and hallucinations could be mitigated by adding a reinforcement learning layer to act as a filter before sending the output to the user [57]."
101,Berrezueta-Guzman 2024,Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement,Richard Gaus,Germany,Self-collected data but derived from external data set,"","","","","","","","",n,"",y," The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",19,3,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,"no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.)",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,Sections 4.2. Principal Complexities and Ethical Responsibilities Inherent in Its Deployment and 4.3. Future Challenges
101,Berrezueta-Guzman 2024,Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement,Consensus,Germany,Self-collected data,Some PDFs for configuring a custom GPT (type of PDFs not further specified),"","","","","","","",n,"",N,"Not sufficient: The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",19,3,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,no benchmark,no benchmark,"no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.)",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",n,"","""Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality.""",Yes,"Over-reliance on technology poses a risk, thus potentially impacting the development of social skills and real-life coping mechanisms, and technical limitations, such as misunderstanding inputs or handling complex scenarios, must be addressed.
[..]

Currently, LLMs face difficulties in ensuring safety and stability, mainly when deal-
ing with prompt injection (PI) that includes sensitive topics, which generate adversarial replies [54]. There is room for improvement regarding jailbreaking attempts in LLMs, thus dealing with inconsistent LLM replies by reverse engineering the prompt to jailbreak or hack the system. However, new ChatGPT models (ChatGPT 4) decreased the rate of generated adversarial content [55], which is based on semantic understanding [56]. Moreover, malicious content and hallucinations could be mitigated by adding a reinforcement learning layer to act as a filter before sending the output to the user [57].



Sections 4.2. Principal Complexities and Ethical Responsibilities Inherent in Its Deployment and 4.3. Future Challenges"
98,,Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation,Reviewer Two,Other: Canada,External data set,MIBot v5.1 dataset,Emotional support dialogue -- chat logs,English,No,Other: unclear,Unknown,Unknown,"",N,"",N,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26,9,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,"",No clients/patients involved,"","","",CBT: Motivational interviewing,GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,no benchmark,no benchmark,no benchmark,N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",No,""
98,,Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation,Richard Gaus,Other: Canada,External data set,"Dataset was used for evaluation only.

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539])",Emotional support dialogue -- chat logs,English,Yes,No,Unselected,Other: chatbot response,"",n,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26,9,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","",CBT: Motivational interviewing,GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,rating scale to determine quality of backward looking reflections (Textbox 5). ,n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
98,,Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation,Consensus,Other: Canada,External data set,"Dataset was used for evaluation only.

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539])",Emotional support dialogue -- chat logs,English,Yes,No,Psychopathology,Other: not applicable,"",N,"",N,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26,9,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","",CBT: Motivational interviewing,GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","","","","","","","","","","",human rating (unclear if expert or not),no benchmark,no benchmark,rating scale to determine quality of backward looking reflections (Textbox 5).,"","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",No,""
95,Adhikary 2024,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,Reviewer Two,India,External data set,"To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,"","","","","",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23,07,2024,Direct LLM performance evaluation (only via prompting),"",Analysis of conversation transcripts,"",No clients/patients involved,"","","","Informal counseling (e.g., emotional support conversation)","",No,"","","","","","","","",Y,"",L,other models,Y,"",L,other models,"","","","","","","","","",Y,"",L,No relative comparisons where conducted; the experts rated in absolute terms,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Yes,"Hallucination:

Importantly, all 3 models exhibited
a significant number of cases with no hallucination observed,
indicating reliable performance and implying their ability to
maintain fidelity to the original content

Potential legal problems:

However, the models did not do as well with the structure
separation of the information. The SH, PD, and reflection
sections frequently overlapped, posing clinical and legal
problems. History is considered clinically sacrosanct and should
not be contaminated by the therapist’s interpretation, and it is
also citable in legal cases as client evidence, while
interpretations are not.
PD: patient discovery; SH: symptom and history"
95,Adhikary 2024,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,Richard Gaus,India,Self-collected data but derived from external data set,"external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS",Psychotherapy -- speech transcripts,English,No,No,Unknown,Unknown,"",y,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23,7,2024,Direct LLM performance evaluation (only via prompting),"",Analysis of conversation transcripts,"","","","","","Unspecified, might include formal therapy methods","BART family; T5 family; GPT-2 family; Llama 2 family; Mistral family; Other: Phi-2, GPT-J, GPT-Neo",No users involved,"","","","","","","","",y,no benchmark,no benchmark,"no benchmark. metric: ROUGE-1, -2, -L",y,no benchmark,no benchmark,BERTScore,n,"","","",n,"","","","",y,no benchmark,no benchmark,"no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,they used exclusively on-premise models,n,"",n,"",n,"",n,"",n,"",n,"",n,acceptability data collected only from one stakeholder level (clinicians),n,"","",No,""
95,Adhikary 2024,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,Consensus,India,Self-collected data but derived from external data set,"external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS

To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube",Psychotherapy -- speech transcripts,English,No,No,Unknown,Unknown,"","","","","",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23,7,2024,Direct LLM performance evaluation (only via prompting),"",Analysis of conversation transcripts,"",No clients/patients involved,"","","","Unspecified, might include formal therapy methods","BART family; T5 family; GPT-2 family; Llama 2 family; Mistral family; Other: Phi-2, GPT-J, GPT-Neo",No users involved,"","","","","","","","",Y,no benchmark,no benchmark,"no benchmark. metric: ROUGE-1, -2, -L",Y,no benchmark,no benchmark,BERTScore,n,"","","",n,"","","","",Y,no benchmark,no benchmark,"no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,they used exclusively on-premise models,n,"",n,"",n,"",n,"",n,"",n,"",n,acceptability data collected only from one stakeholder level (clinicians),n,"","",Yes,"Hallucination:

Importantly, all 3 models exhibited
a significant number of cases with no hallucination observed,
indicating reliable performance and implying their ability to
maintain fidelity to the original content

Potential legal problems:

However, the models did not do as well with the structure
separation of the information. The SH, PD, and reflection
sections frequently overlapped, posing clinical and legal
problems. History is considered clinically sacrosanct and should
not be contaminated by the therapist’s interpretation, and it is
also citable in legal cases as client evidence, while
interpretations are not.
PD: patient discovery; SH: symptom and history"
93,Zech 2022,Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.,Reviewer Two,USA,Self-collected data,derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform ,"Other: therapist performance task—text responses, not patient dialogues",English,No,Yes,Other: Providers/ not patients,Trained professionals,"",N,albeit not really applicable,N,albeit not really applicable,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",16,08,2022,Tool Development and Evaluation,Tool Development and Evaluation,Analysis of conversation transcripts,"","","","",Only fine-tuning,"",BERT family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","",yes ,same ,H,"the standard deviation
of ratings generated by DistilBERT (SD = 0.230) were much
closer to the distribution of human ratings (SD = 0.308) versus
that of the SVR (SD = 0.146","","","","","","","","","","","","","","","","","","","","","","","","",Y,"Table 2 summarizes key descriptive statistics related to our
sample of task-taking therapists. Of the 978 task takers, 81.6%
(n = 798) were women. This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%). The majority of therapists had
at least three years of experience providing therapy and a
significant minority had 11 or more years of experience (n =
321, 32.8%). However, most therapists had much less
experience providing messaging therapy; 818 (83.6%) had
between zero and two years of messaging therapy experience","","","","","","","","","","","","","","","",No,""
93,Zech 2022,Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.,Richard Gaus,"","Other: Unsure. Authors describe the data collection process as if they collected it themselves. But they state: ""The dataset for the present study is derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com):","","","","","","","","","","","","",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",16,8,2022,Tool Development and Evaluation,"",Other: Unsure between analysis of conversation transcripts or therapist-facing application with treatment fidelity feedback,"","","",Treatment fidelity feedback,Only fine-tuning,Mix of formal therapy methods,BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",y,b,l,benchmark is support vector regressor (lower capacity model),"",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,BERT is on-premise,n,"Some demographics of therapists are shown but only male/female, no ethnic etc. data",n,"",n,"",n,"",y,FIS-T is validated,n,"",n,"",n,"","This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%)",No,""
93,Zech 2022,Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.,Consensus,"",Self-collected data but derived from external data set,derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com),"","","","","","","","","","","",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",16,8,2022,Tool Development and Evaluation,Other: ,Analysis of conversation transcripts,"","","",Other: ,Only fine-tuning,Mix of formal therapy methods,BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",y,b,l,benchmark is support vector regressor (lower capacity model),"",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"BERT is on-premise,
not GDPR",n,"Some demographics of therapists are shown but only male/female, no ethnic etc. data",n,"",n,"",n,"",y,FIS-T is validated,n,"",n,"",n,"","This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%)",No,""
92,Imel 2024,Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.,Reviewer Two,"",External data set,Protocall random sample of crisis calls,"","","","","","","","","","","",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",19,7,2024,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,"Unspecified, might include formal therapy methods",BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,S,H,Human Interrater agreement,N,"","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"""With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation.""",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"""Specific applications of machine learning–based evaluation may go beyond after-action call-level summaries of statement-level evaluations that cannot affect the quality of care received by the caller in the moment. Specifically, real-time evaluation may also be possible. For example, machine-learning–enabled call center software could label interventions as they occur, providing just-in-time feedback and suggestions to call takers when specific interventions are not provided."" (...) ""..stakeholders include call takers who can use this information to learn from previous calls, supervisors and administrators who may better identify and then direct resources to call takers who are struggling with their performance, and funders who may begin to include population-level data on the quality of conversations - complementing existing metrics on answer rates and wait times."" ","",No,""
92,Imel 2024,Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.,Richard Gaus,"",External data set,Random sample of 476 Protocall crisis calls,"","","","","","","","","","","",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",19,7,2024,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,Mix of formal therapy methods,BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,s,h,"metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other).",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",y,the tool itself is a suicide risk detection tool,n,"",y,on-premise BERT model,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
92,Imel 2024,Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.,Consensus,"",External data set,Random sample of 476 Protocall crisis calls,"","","","","","","","","","","",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",19,7,2024,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,"Unspecified, might include formal therapy methods",BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,S,H,"metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other). Task: classify crisis calls transcripts into 10 suicide risk labels.",N,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",Y,"""With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation.""

the tool itself is a suicide risk detection tool",N,"",y,on-premise BERT model,N,"",N,"",NA,"",NA,"",N,"",NA,"",N,"",Y,"""Specific applications of machine learning–based evaluation may go beyond after-action call-level summaries of statement-level evaluations that cannot affect the quality of care received by the caller in the moment. Specifically, real-time evaluation may also be possible. For example, machine-learning–enabled call center software could label interventions as they occur, providing just-in-time feedback and suggestions to call takers when specific interventions are not provided."" (...) ""..stakeholders include call takers who can use this information to learn from previous calls, supervisors and administrators who may better identify and then direct resources to call takers who are struggling with their performance, and funders who may begin to include population-level data on the quality of conversations - complementing existing metrics on answer rates and wait times."" ","",No,""
91,Atzil-Slonim 2024,Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy,Reviewer Two,Other: Isreal,Self-collected data,"","","","","",Psychopathology,"","",N,"",N,"",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",18,01,2024,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,Psychodynamic psychotherapy,BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","","
Y",W,H,F1-Micro Score against human labeling,N,"","","","","N
","","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",No,""
91,Atzil-Slonim 2024,Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy,Richard Gaus,Other: Israel,Self-collected data,"","","","",No,Psychopathology,"","","","","","",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",18,1,2024,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","","",Mix of formal therapy methods,BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,w,h,they used f1 and kappa to evaluate the emotion labeling system. the benchmark was a human annotator.,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,only on-premise model used (BERT),y,"Some demographic information given: The clients were all above age 18 (Mage =
39.06, SD = 13.67, range 20–77), and most were women (58.9%).
Of the clients, 92% were native Hebrew speakers and 92% were
born in Israel. Of the clients, 53.5% had at least a bachelor’s degree;
53.5% were single and 8.9% were in a committed relationship
but unmarried; 23.2% were married and 14.2% were divorced
or widowed. ",n,"",n,"",n,"",y,"The Outcome Rating Scale (ORS), Profile of Mood States",n,"",n,"",n,"this is all: Automatic emotion recognition models can be integrated into existing feedback systems to provide an indication of the levels of
emotional coherence in psychotherapy sessions and allow therapists
to modify their interventions accordingly.","",No,""
91,Atzil-Slonim 2024,Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy,Consensus,Other: Israel,Self-collected data,"","","","","",Psychopathology,"","","","","","",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",18,1,2024,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,Mix of formal therapy methods,BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,W,H,they used f1-micro and kappa to evaluate the emotion labeling system. the benchmark was a human annotator. Task: classify emotions in client utterances,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",n,"",n,"",y,only on-premise model used (BERT),y,"Some demographic information given: The clients were all above age 18 (Mage =
39.06, SD = 13.67, range 20–77), and most were women (58.9%).
Of the clients, 92% were native Hebrew speakers and 92% were
born in Israel. Of the clients, 53.5% had at least a bachelor’s degree;
53.5% were single and 8.9% were in a committed relationship
but unmarried; 23.2% were married and 14.2% were divorced
or widowed. ",n,"",n,"",n,"",y,"",n,"",n,"",n,"this is all: Automatic emotion recognition models can be integrated into existing feedback systems to provide an indication of the levels of
emotional coherence in psychotherapy sessions and allow therapists
to modify their interventions accordingly.",This is one of the few studies that also included psychodynamic psychotherapy in the intervention.,No,""
89,Das 2022,Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues,Reviewer Two,USA,External data set,"Subreddits in the following categories: (a) Coping and Therapy (C-Th): 7Cup
sofTea, Existential_crisis, getting_over_it, Grief-
Support, helpmecope, hardshipmates, HereToHelp,
itgetsbetter, LostALovedOne, offmychest, MMFB,
Miscarriage, reasonstolive, SuicideBereavement,
therapy; (b) Mood Disorders (MD): depression, de-
pressed, lonely, mentalhealth; (c) Psychosis and
Anxiety (P-An): anxiety, BipolarReddit, socialanxi-
ety; and (d) Trauma and Abuse (Tr-A): abuse, sur-
vivors, Anger, emotionalabuse, PTSDcombat.
Alexander Street Press video transcripts",Internet data -- mental health Q&A,"",Yes,No,Psychopathology,Trained professionals,"",unclear/yes,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",26,5,2022,"Other: Tool Development and Evaluation, Direct LLM performance evaluation","","","","","","",Only fine-tuning,"Unspecified, might include formal therapy methods",GPT-2 family; Other: DialoGPT,No users involved,"","","","","","","","",Y,no benchmark,"","",Y,no benchmark,"","",N,"","","",N,"","","","",Y,no benchmark,"","",N,"","","","",Lexical Diversity,no benchmark,"","",Average length,no benchmark,"","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","","",""
89,Das 2022,Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues,Richard Gaus,USA,Self-collected data,"self-created ""Empathic Conversation dataset""",Emotional support dialogue -- chat logs,English,No,Other: synthetic human-generated,Unselected,Unknown,"","","","","",Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26,5,2022,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Other: fine-tuning + transfer learning,"Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",y,b,l,benchmark are human responses.,y,s,l,benchmark are human responses.,"","","","","","","","","",y,b,h,benchmark are human responses.,"","","","","","","","","","","","","","","","","","",n,"",n,"",y,gpt-2 is on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
89,Das 2022,Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues,Consensus,USA,Self-collected data,"self-created ""Empathic Conversation dataset""",Emotional support dialogue -- chat logs,English,Other: Yes; human actors,No,Unselected,Unknown,"",y,"",N,"",Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26,5,2022,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Other: fine-tuning + transfer learning,"Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Unspecified, might include formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",Y,b,l,benchmark are human responses,Y,w,l,benchmark are human responses.,N,"","","",N,"","","","",Y,no benchmark,"","",N,"","","","",Lexical Diversity,w,l,"",Average length,unclear,l,"","","","","","",N,"",N,"",y,gpt-2 is on-premise,N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",Other: ,""
88,Malhotra 2022,Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations,Reviewer Two,India,Self-collected data,"HOPE
Short description: Multi-turn, therapist–patient counseling dialogues labeled with dialog acts; used to evaluate SPARTA vs baselines.
",Emotional support dialogue -- speech transcripts,"",No,No,Unknown,Unknown,"",unclear,"",N,nothing,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21,2,2022,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","No clients/patients involved; Other: youtube, The data collection
process provides us 12.9𝐾 utterances from 212 counseling therapy
sessions – all of them are dyadic conversations only.",212 counseling sessions (not clear how many patients),"",Custom pipeline with integrated LLM which is only prompted,Mix of formal therapy methods,"Other: ""For speaker-invariant representations, we employ a pre-trained RoBERTa language model which is further fine-tuned on DAC task.""",No users involved,"",N,N,N,N,"","","",N,"","","",N,"","","",Y,B,H,"Several benchmarks used, including ones that were more recently developed. ""In comparison, SPARTA-TAA obtains significant improvements over all
baselines.""",N,"","","","",N,"","","",N,"","","","",Macro-F1,B,H,"""SPARTA-TAA obtains significant improvements over all
baselines. It reports improvements of +8.64%, +8.58%, and +6.29%
in macro-F1 (60.29), weighted-F1 (64.53), and accuracy (64.75%),
respectively, as compared to CASA""",Weighted-F1,B,H,"",Accuracy,B,H,"","",N,"",N,"",Y,"To ensure confidentiality, we randomly assign synthetic
names to all patients and therapists in all examples",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",Yes,"Further, the deployment of any such technology will be
done keeping in mind the safety-risks and mitigating any sources
of bias that may arise"
88,Malhotra 2022,Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations,Richard Gaus,"",Self-collected data but derived from external data set,Name of new dataset: HOPE. Derived from transcripts from YouTube counseling sessions. They added self-created annotations to these data.,"","","","","","","","","","","",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21,2,2022,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","","Other: they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.",Mix of formal therapy methods,BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,b,l,"benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. for dialogue-act classification of conversational turns",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,all on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"We understand that the building computational models
in mental-health avenues has high-stakes associated with it and
ethical considerations, therefore, become necessary. No technology
will work perfectly in solving the problems related to mental health
[26]. It is important to note that we do not make any diagnostic
claims. Further, the deployment of any such technology will be
done keeping in mind the safety-risks and mitigating any sources
of bias that may arise."
88,Malhotra 2022,Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations,Consensus,"",Self-collected data but derived from external data set,Name of new dataset: HOPE. Derived from transcripts from YouTube counseling sessions. They added self-created annotations to these data.,"","","","","","","","","","","",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21,2,2022,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","","Other: fine-tuning + wrapper.
they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.",Mix of formal therapy methods,BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,B,l,"Several benchmarks used, including ones that were more recently developed. ""In comparison, SPARTA-TAA obtains significant improvements over all
baselines.""

benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. Task: Dialogue-act classification of conversational turns",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"all on-premise; To ensure confidentiality, we randomly assign synthetic
names to all patients and therapists in all examples",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"We understand that the building computational models
in mental-health avenues has high-stakes associated with it and
ethical considerations, therefore, become necessary. No technology
will work perfectly in solving the problems related to mental health
[26]. It is important to note that we do not make any diagnostic
claims. Further, the deployment of any such technology will be
done keeping in mind the safety-risks and mitigating any sources
of bias that may arise."
86,Eberhardt 2024,Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy,Reviewer Two,Germany,Self-collected data,"","","","","","","","",Y,"",N,"",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",15,2,2024,Tool Development and Evaluation,"",Client-facing application,"",Patients recruited in hospital or outpatient treatment facility,"","","",Other CBT techniques,BERT family,No,"",n,n,n,n,"","","","","","","","","","","","","","","","","","","","","","","",TBD - Unsure,"","","",TBD - Unsure,"",Construct Validity using MTMM and Hierarchical  linear models HLM,"","",No benchmark,Criterion Validity,"","",No benchmark,"","","","","",n,"",n,"",n,"",y,"We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.",n,"",n,"",n,"",n,"",n,"",n,"",y,"The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).","",Yes,"While the insights obtained from this study are
comprehensive, they remain initial and should be
interpreted with caution. Future research should
include more patients, apply corrected testing, and
integrate more session transcripts. We are exploring
more advanced transcription methods to reduce pre-
vious limitations and build upon these initial findings
for subsequent research on sentiment analysis in
psychotherapy.
Conclusion
Sentiment analysis has demonstrated potential,
revealing several meaningful associations and provid-
ing initial evidence supporting its validity as a
measure of the fundamental emotional tone in"
86,Eberhardt 2024,Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy,Richard Gaus,Germany,Self-collected data,therapy transcriptions from real patients (that they recruited themselves),Psychotherapy -- speech transcripts,Other: German,No,No,Psychopathology,Trained professionals,"","","","","",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28,2,2024,Direct LLM performance evaluation (only via prompting),"",Analysis of conversation transcripts,"",Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,35,"","",Other CBT techniques,BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",N,"","","",N,"","","","",construct validity,no benchmark,no benchmark,no benchmark,criterion validity,no benchmark,no benchmark,no benchmark,"","","","","",n,"",n,"",y,on-premise,n,"",n,"",n,"",n,"",y,"measured phq-9, gad, etc. in patients",n,"",n,"",n,"","",No,""
86,Eberhardt 2024,Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy,Consensus,Germany,Self-collected data,therapy transcriptions from real patients (that they recruited themselves),"","","","","","","","","","","",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28,2,2024,Direct LLM performance evaluation (only via prompting),"",Analysis of conversation transcripts,"",Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,35,"","",Other CBT techniques,BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",Construct Validity using MTMM and Hierarchical  linear models HLM,no benchmark,no benchmark,no benchmark,Criterion Validity,no benchmark,no benchmark,no benchmark,"","","","","",n,"",n,"",y,"P-1: on-premise, P-2: true (Written, informed consent allowing anonymized data to be used for research purposes was obtained from all participants)",y,"See Table S1

We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.",n,"",n,"",n,"",n,"measured phq-9, gad, etc. in patients but did not predict those from the transcripts",n,"",n,"",n,"some description in following paragraph but too superficial:

The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).","",Yes,"While the insights obtained from this study are
comprehensive, they remain initial and should be
interpreted with caution. Future research should
include more patients, apply corrected testing, and
integrate more session transcripts. We are exploring
more advanced transcription methods to reduce pre-
vious limitations and build upon these initial findings
for subsequent research on sentiment analysis in
psychotherapy.
Conclusion
Sentiment analysis has demonstrated potential,
revealing several meaningful associations and provid-
ing initial evidence supporting its validity as a
measure of the fundamental emotional tone in
"
81,Uglova 2024,Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example),Reviewer Two,Other: Russia,Self-collected data,Survey of 236 participants evaluating ChatGPT-generated vs. psychologist-generated psychological recommendations.,Other: psychological recommendation texts used as experimental stimuli,Other: unclear presumeably russian,Yes,No,Unselected,Trained professionals,"",N,"",N,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),3,1,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,General population,"","",Custom pipeline with integrated LLM which is only prompted,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,"",N,N,Y,N,"During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).","""During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used).""​","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",Y,"
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.",N,"",N,"",N,"",N,"",Y,"A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.",N,"",N,"","",No,""
81,Uglova 2024,Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example),Richard Gaus,Other: Russia,No dataset used for development or evaluation,"","","","","","","","",n,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,10,2023,Direct LLM performance evaluation (only via prompting),Population survey,Client-facing application,Other: one-turn recommendations generated by LLM,Other: unclear,236,"","","Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",Yes,"",n,n,y,y,"","Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions (""score"", ""activity"", ""informativeness""), while psychologist responses were rated higher in ""strength"".","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
81,Uglova 2024,Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example),Consensus,Other: Russia,No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,"",N,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,10,2023,Direct LLM performance evaluation (only via prompting),Population survey,Client-facing application,Other: one-turn recommendations generated by LLM,General population,236,"",Other: ,"Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",Yes,"",n,n,y,y,"During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).","Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions (""score"", ""activity"", ""informativeness""), while psychologist responses were rated higher in ""strength"".

""During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used).""​","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.",n,"",n,"",n,"",n,"",Y,"A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.",n,"",n,"","",No,""
77,Fu 2024,Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study,Reviewer Two,Other: Hong Kong,Self-collected data,"Open Up text-counseling logs; after filtering → 44,810 valid sessions / 3,231,830 messages; subset with postsession feedback 5,240 sessions / 533,609 messages; stratified sample 131 sessions / 6,169 messages used for annotation & evaluation.
",Emotional support dialogue -- chat logs,Chinese,No,No,Psychopathology,Lay people,"",N,"",N,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,1,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,"",General population,"","","","Informal counseling (e.g., emotional support conversation)",GPT-3.5 family; GPT-4 / GPT-4o family,No,"",n,n,n,n,"","","","","","","","","","","unsure, see page 8",y,w,l,"","","","","","",y,"",H,"unsure, see figure 5",y,b,L,GPT 4 better than GPT3.5,"",Linguistic Inquiry and word count (LIWC),w,l,worst of all,Logistic Regression,w,l,"",Long short tem measure,w,l,"","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"since counseling sessions are inherently sensitive and
private, using LLMs like ChatGPT to analyze sentiment in a
counseling context raises several ethical considerations. First,data privacy is of major concern. The use of LLMs to analyze
counseling session data must ensure anonymity and
confidentiality and accord with ethics approval. Second, the
decision-making process of LLMs is not fully interpretable,
especially since we do not know what materials were provided
during the training stage, which poses challenges in justifying
their outputs. Third, LLMs might generate biased statements
or unsafe advice, which should receive more attention,
particularly in sensitive settings like counseling. Addressing these ethical considerations enables us to advance our research
responsibly."
77,Fu 2024,Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study,Richard Gaus,China,External data set,"Open Up is a free24/7 web-based, text-based counseling service in Hong Kong that enables people aged between 11 and 35 years to anonymously chat with paid staff (staff counselors or social workers) or trained volunteers. 5240 sessions with 533,609 messages (Figure 3). We stratified the 5240 sessions based on the number of messages in each session. There were a total of 131 unique message count groups among the 5240 sessions.",Emotional support dialogue -- chat logs,Chinese,No,No,Unselected,Trained professionals,"",n,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,1,2024,Direct LLM performance evaluation (only via prompting),"",Analysis of conversation transcripts,"","","","","","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",y,b,l,"benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"The use of LLMs to analyze
counseling session data must ensure anonymity and
confidentiality and accord with ethics approval. Second, the
decision-making process of LLMs is not fully interpretable,
especially since we do not know what materials were provided
during the training stage, which poses challenges in justifying
their outputs. Third, LLMs might generate biased statements
or unsafe advice, which should receive more attention,
particularly in sensitive settings like counseling. Addressing
these ethical considerations enables us to advance our research
responsibly."
77,Fu 2024,Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study,Consensus,China,External data set,"Open Up is a free24/7 web-based, text-based counseling service in Hong Kong that enables people aged between 11 and 35 years to anonymously chat with paid staff (staff counselors or social workers) or trained volunteers. 5240 sessions with 533,609 messages (Figure 3). We stratified the 5240 sessions based on the number of messages in each session. There were a total of 131 unique message count groups among the 5240 sessions.",Emotional support dialogue -- chat logs,Chinese,No,No,Unselected,Trained professionals,"",N,"",N,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,1,2024,Direct LLM performance evaluation (only via prompting),"",Analysis of conversation transcripts,"","","","","","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,b,l,"benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"P-1: no, P-2: no (no compliance with health data regulation claimed, no safeguard described)",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"since counseling sessions are inherently sensitive and
private, using LLMs like ChatGPT to analyze sentiment in a
counseling context raises several ethical considerations. First,data privacy is of major concern. The use of LLMs to analyze
counseling session data must ensure anonymity and
confidentiality and accord with ethics approval. Second, the
decision-making process of LLMs is not fully interpretable,
especially since we do not know what materials were provided
during the training stage, which poses challenges in justifying
their outputs. Third, LLMs might generate biased statements
or unsafe advice, which should receive more attention,
particularly in sensitive settings like counseling. Addressing
these ethical considerations enables us to advance our research
responsibly."
75,Furukawa 2023,Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study,Reviewer Two,Other: Japan,External data set," FLATT (Fun to Learn to Act and Think 
through Technology) trial + HCT (Healthy Campus Trial)","","","","","","","","","","","",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",7,7,2023,Tool Development and Evaluation,Direct LLM performance evaluation (only via prompting),Client-facing application,Other: smartphone CBT app (Kokoro App),Patients with disorder explicitly based on ICD or DSM; General population,1626,"",Custom pipeline with integrated LLM which is only prompted,CBT: Cognitive restructuring,Other: Japanese Text-to-Text Transfer Transformer,No,"","","","","","","","",N,"","","",N,"","","",Y,"","","",N,"","","","",Y,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"","N
","",N,"",N,"",N,"",Y,PHQ-9,N,"",N,"",N,"","",No,""
75,Furukawa 2023,Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study,Richard Gaus,Other: Japan,External data set,FLATT dataset (Fun to Learn to Act and Think through Technology),Other: automatic thought-feeling pairs,Japanese,No,No,Psychopathology,Other: not applicable,"",y,"",n,"",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",7,7,2023,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,CBT: Cognitive restructuring,T5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,"accuracy, F1, precision, recall for prediction of feeling based on automatic thought",n,"","","","",n,"","","",n,"","","","",reduction of negative feelings,no benchmark,no benchmark,no benchmark,"","","","","","","","","",n,"",n,"",y,T5 is on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
75,Furukawa 2023,Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study,Consensus,Other: Japan,External data set,FLATT dataset (Fun to Learn to Act and Think through Technology),Other: automatic thought-feeling pairs,Japanese,No,No,Psychopathology,Other: not applicable,"",y,T5 model,n,"",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",7,7,2023,Tool Development and Evaluation,Other: ,Analysis of conversation transcripts,Other: ,No clients/patients involved,"","",Only fine-tuning,CBT: Cognitive restructuring,T5 family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,no benchmark,no benchmark,"accuracy, F1, precision, recall for prediction of feeling based on automatic thought (feeling-thought pairs)",N,"","","","",n,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",y,T5 is on-premise,"N
","",N,"",N,"",N,"",n,"",N,"",N,"",N,"","",No,""
73,Shen 2020,Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context,Reviewer Two,USA,External data set,Motivational Interviewing (MI) counseling dataset from Perez-Rosas et al. (2016); Alexander Street dataset ,Psychotherapy -- speech transcripts,English,No,Other: unclear,Unselected,Lay people,"",Y,gpt 2,N,too little,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1,7,2020,Tool Development and Evaluation,"","","","","","",Only fine-tuning,CBT: Motivational interviewing,GPT-2 family,No users involved,"","","","","","","","",Y,B,L,seq2seq,Y,B,L,seq2seq,"","","","","","","","","",Y,B,L,seq2seq,"","","","","",Diversity,S,L,seq2seq,"","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","","",""
73,Shen 2020,Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context,Richard Gaus,USA,External data set,"Motivational Interviewing counseling dataset (Perez-Rosas 2016)

The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The dataset is derived from a collection of 284 video recordings of counseling encounters using MI. The recordings were collected from various sources, including two clinical trials, students’ counseling sessions from a graduate level MI course, wellness coaching phone calls, and demonstrations of MI strategies in brief medical encounters.",Psychotherapy -- speech transcripts,English,No,No,Other: mixed,Other: mixed,"",y,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1,7,2020,Tool Development and Evaluation,"",Client-facing application,Other: reflection generation,No clients/patients involved,"","",Other: fine-tuning + retrieval and context expansion,CBT: Motivational interviewing,GPT-2 family,No users involved,"","","","","","","","",y,b,l,benchmark is a simple seq2seq model,y,b,l,benchmark is a simple seq2seq model,n,"","","",n,"","","","",y,s,h,benchmark is ground truth (human created reflections) and output of simple seq2seq model,n,"","","","",diversity,s,l,benchmark is a simple seq2seq model,"","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
73,Shen 2020,Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context,Consensus,USA,External data set,"Motivational Interviewing counseling dataset (Perez-Rosas 2016)

The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The dataset is derived from a collection of 284 video recordings of counseling encounters using MI. The recordings were collected from various sources, including two clinical trials, students’ counseling sessions from a graduate level MI course, wellness coaching phone calls, and demonstrations of MI strategies in brief medical encounters.",Psychotherapy -- speech transcripts,English,No,No,Unselected,Lay people,"",Y,gpt 2,N,too little,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1,7,2020,Tool Development and Evaluation,"",Therapist-facing application,Other: reflection generation,"","",Utterance suggestions,Other: fine-tuning + retrieval and content expansion,CBT: Motivational interviewing,GPT-2 family,No users involved,"","","","","","","","",Y,B,L,seq2seq,Y,B,l,seq2seq,n,"","","",n,"","","","",Y,s,h,benchmark is ground truth (human created reflections) and output of simple seq2seq model,n,"","","","",diversity,s,l,seq2seq,"","","","","","","","","",N,"",N,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Other: ,""
72,Jeong 2024,Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment,Reviewer Two,Other: Korea,Self-collected data,42 tinitus patients as raw data + augmented dataset as test samples,"Other: CBT patient diaries + audiometry/THI clinical data
",English,Yes,No,Psychopathology,Unknown,"",N,"",Y?,"“Informed consent was obtained… participants were assured that no personal sensitive data would be collected… All data collected was kept confidential and anonymous, ensuring complete privacy and data protection.”
",Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25,03,2025,Tool Development and Evaluation,"",Therapist-facing application,"","","",Other: Probability of Advanced Tinitus Therapy success ,Only fine-tuning,CBT: Cognitive restructuring; Other CBT techniques,Other: Google T5 Transformer ,No,"","","","","","","","","","","","","","","","","","","","",Yes,B,No,"","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"We conducted a retrospective anal-
ysis of medical records documenting tinnitus treatment in
those patients. The data was completely anonymized for
VOLUME 12, 2024 
52415
Y. Jeong et al.: Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis
the study","","","","","","","","","","","","","","",Y,"","",No,""
72,Jeong 2024,Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment,Richard Gaus,Other: Korea,Self-collected data,"The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients",Other: cbt diary entries,Other: Korean,No,No,Psychopathology,Other: not applicable,"",y,"",n,"",Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",29,3,2024,Tool Development and Evaluation,"",Other: Analysis of CBT diary data,"",Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires),42,"",Other: Fine-tuning plus other elements like clustering,Other CBT techniques,T5 family; GPT-2 family,No,"","","","","","","","",y,no benchmark,no benchmark,Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.,n,"","","",n,"","","",y,no benchmark,no benchmark,Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though.,"",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,all models on-premise,n,"",n,"",n,"",n,"",y,Tinnitus Handicap Inventory (THI),n,"",n,"",n,"","",No,""
72,Jeong 2024,Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment,Consensus,Other: Korea,Self-collected data,"42 tinitus patients as raw data + augmented dataset as test samples

The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients",Other: cbt diary entries,English,No,No,Psychopathology,Other: not applicable,"",y,"",n,"“Informed consent was obtained… participants were assured that no personal sensitive data would be collected… All data collected was kept confidential and anonymous, ensuring complete privacy and data protection.”
",Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",29,3,2024,Tool Development and Evaluation,"",Other: Analysis of CBT diary data,"",Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires),42,Other: ,Other: Fine-tuning plus other elements like clustering,CBT: Cognitive restructuring; Other CBT techniques,T5 family; GPT-2 family,No,"","","","","","","","",y,no benchmark,no benchmark,Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.,n,"","","",n,"","","",y,no benchmark,no benchmark,"Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though (Table 10, 11)","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"all models on-premise

privacy compliant: We conducted a retrospective anal-
ysis of medical records documenting tinnitus treatment in
those patients. The data was completely anonymized for
VOLUME 12, 2024 
52415
Y. Jeong et al.: Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis
the study",n,"",n,"",n,"",n,"",y,Tinnitus Handicap Inventory (THI),n,"",n,"",n,"","",No,""
70,Saha 2022,Towards Motivational and Empathetic Response Generation in Online Mental Health Support,Reviewer Two,India,External data set,MotiVAte,"","","","","","","",N,"",N,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11,07,2022,Tool Development and Evaluation,"","","","","","",Other: own,"Informal counseling (e.g., emotional support conversation)",GPT-2 family,No,"","","","","","","","",Y,B,L,"ML models (HRED, SEQ2SEQ)",N,"","","",N,"","","",N,"","","","",Y,B,L,"ML models (HRED, SEQ2SEQ)",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Yes,"We acknowledge that in designing compu-
tational models for mental health support, there is a risk that re-
sponses trying to aid can have the opposite effect, which can be
lethal resulting in self-harm. Thus, risk mitigation steps are appro-
priate in this context. We stress on the fact that the system does not
intend to make any clinical diagnosis or treatment of the disorder.
It focuses on the VA to learn a generation policy. In such cases, the
VA is always focused on providing comfort and motivation to the
support seekers. This is perfectly benign"
70,Saha 2022,Towards Motivational and Empathetic Response Generation in Online Mental Health Support,Richard Gaus,India,External data set,"MotiVAte

This dataset comprises of 4k dyadic conversations between the depressed support seekers and the VA imparting appropriate suggestion, hope and motivation resulting in a total of 14,809 utterances. The conversations of the MotiVAte dataset are collected from peer-to-peer support forum and modified to represent dyadic conversations for an end-to-end online mental health support system.",Emotional support dialogue -- chat logs,English,No,No,Unselected,Lay people,"",y,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,7,2022,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework,Peer support conversation,GPT-2 family,No users involved,"","","","","","","","",y,b,l,Benchmark is other NLP model,y,b,l,Benchmark is other NLP model,n,"","","",n,"","","","",y,b,l,Benchmark is other NLP model,"","","","","",sentiment polarity,s,l,Benchmark is other NLP model,change of empathy scores for the ERF module,no benchmark,no benchmark,no benchmark,"","","","","",n,"",n,"",y,all on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Ethical Concerns. We acknowledge that in designing compu-
tational models for mental health support, there is a risk that re-
sponses trying to aid can have the opposite effect, which can be
lethal resulting in self-harm. Thus, risk mitigation steps are appro-
priate in this context. We stress on the fact that the system does not
intend to make any clinical diagnosis or treatment of the disorder.
It focuses on the VA to learn a generation policy. In such cases, the
VA is always focused on providing comfort and motivation to the
support seekers. This is perfectly benign."
70,Saha 2022,Towards Motivational and Empathetic Response Generation in Online Mental Health Support,Consensus,India,External data set,"MotiVAte

This dataset comprises of 4k dyadic conversations between the depressed support seekers and the VA imparting appropriate suggestion, hope and motivation resulting in a total of 14,809 utterances. The conversations of the MotiVAte dataset are collected from peer-to-peer support forum and modified to represent dyadic conversations for an end-to-end online mental health support system.",Emotional support dialogue -- chat logs,English,No,No,Unselected,Lay people,"",y,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,7,2022,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework,"Peer support conversation; Informal counseling (e.g., emotional support conversation)",GPT-2 family,No users involved,"","","","","","","","",Y,B,L,"ML models (HRED, SEQ2SEQ)",y,b,l,"ML models (HRED, SEQ2SEQ)",N,"","","",N,"","","","",Y,B,L,"ML models (HRED, SEQ2SEQ)",N,"","","","",sentiment polarity,s,l,Benchmark is other NLP model,change of empathy scores for the ERF module,no benchmark,no benchmark,no benchmark,"","","","","",n,"",n,"",y,all on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"We acknowledge that in designing compu-
tational models for mental health support, there is a risk that re-
sponses trying to aid can have the opposite effect, which can be
lethal resulting in self-harm. Thus, risk mitigation steps are appro-
priate in this context. We stress on the fact that the system does not
intend to make any clinical diagnosis or treatment of the disorder.
It focuses on the VA to learn a generation policy. In such cases, the
VA is always focused on providing comfort and motivation to the
support seekers. This is perfectly benign"
63,Brown 2023,A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,Reviewer Two,Other: Canada,Self-collected data,"Four chatbot versions tested with separate groups of ambivalent smokers; pre/post and 1-week follow-up (readiness, confidence, importance), quit attempts, and perceived empathy; core chat = 5 scripted questions + (for most versions) generated MI reflections.  • Derived: Not indicated.
","Other: MI chatbot intervention chat logs + survey responses
",English,Yes,No,Unselected,Lay people,"",N,nothing found,N,"",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",17,10,2023,Tool Development and Evaluation,"",Client-facing application,"",General population,"","",Custom pipeline with integrated LLM which is only prompted,CBT: Motivational interviewing,GPT-2 family,Yes,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",eaviness of Smoking Index (HSI) measures.,W,L,"","","","","","","","","","","","","","","","",Y,"","","","","","","","","","","","","","","","",No,""
63,Brown 2023,A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,Richard Gaus,Other: Canada,Self-collected data,"In version 1 of the generator, the fine-tuning question-and-response data set came from 2 sources: the first was our prior work [40,41], and the second data source was from earlier deployments of MIBot, before the creation of MIBot v4.7. The reflections used came from a variety of sources: from previous versions of this chatbot that were deemed to be acceptable MI reflections by MI-literate researchers or actual reflections produced by MI-literate researchers or MI-expert clinicians.

To address the rate of poor reflections, we developed version 2 of the generator with 2 significant enhancements. First, a larger set of 301 fine-tuning triplets were collected over approximately 10 months of deploying the chatbot, making use of the various responses from smokers who had been recruited in a similar manner, as described in the Participant Recruitment and Screening subsection. This second data set did not include any of the data from the earlier chatbot version [40,41]. Only MI-consistent reflections were used, which were sourced from MI clinicians, MI-literate researchers, or version 1 of the generator. The labeling and selection of the MI-consistent reflections were improved by using multiple human raters and a carefully controlled decision tree to determine the validity of the reflections. The new rating scheme itself was stricter than the one used in version 1, which caused the hit rate to go down—not because the generation was worse but because of the stricter rating. The hit rate of the new generator was measured to be 55.1% (166/301) on a set of reflections.",Other: Motivational interviewing reflections,English,Yes,No,Unknown,Trained professionals,"",y,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17,10,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),349,"",Other: fine-tuning + custom pipeline (see Reflection Generation Training),CBT: Motivational interviewing,BERT family; GPT-2 family,Yes,"",y,n,y,y,consultation and relational empathy survey (CARE),"consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","readiness ruler (patient symptom report, smoking related, validated measure)","no benchmark. in absolute terms favorable, with reduction in smoking, increase in confidence, importance, readiness.",no benchmark,no benchmark,"","","","","","","","","",n,"",y,used reflection quality classifier,y,on-premise models,y,reported in multimedia appendix 3,n,"",y,"Of the 654 participants who accepted and consented to the study,
105 (16.1%) did not finish the entire study. We speculate that
this dropout was caused by several factor",n,"",y,use of the readiness ruler,n,"",n,"",n,"","",Yes,"It is important to consider the possibility that the 3.7% (13/349)
of the participants who resolved their ambivalence to continue
smoking were hurt by the interaction with the chatbot. We
manually reviewed each of these conversations, and for 85.1%
(297/349) of the conversations, we did not see evidence of
harmful statements made by the chatbot that could have
contributed to this resolution. For the other 14.9% (52/349) of
the conversations, the chatbot produced poor reflections, which
may have caused participants to be less likely to quit or to
believe that they had less of a chance to do so; for example, in
response to a participant expressing the idea that quitting cold
turkey was their best approach to quitting, the chatbot responded,
“A smoker can’t really do that,” which is quite inappropriate."
63,Brown 2023,A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,Consensus,Other: Canada,Self-collected data,"In version 1 of the generator, the fine-tuning question-and-response data set came from 2 sources: the first was our prior work [40,41], and the second data source was from earlier deployments of MIBot, before the creation of MIBot v4.7. The reflections used came from a variety of sources: from previous versions of this chatbot that were deemed to be acceptable MI reflections by MI-literate researchers or actual reflections produced by MI-literate researchers or MI-expert clinicians.

To address the rate of poor reflections, we developed version 2 of the generator with 2 significant enhancements. First, a larger set of 301 fine-tuning triplets were collected over approximately 10 months of deploying the chatbot, making use of the various responses from smokers who had been recruited in a similar manner, as described in the Participant Recruitment and Screening subsection. This second data set did not include any of the data from the earlier chatbot version [40,41]. Only MI-consistent reflections were used, which were sourced from MI clinicians, MI-literate researchers, or version 1 of the generator. The labeling and selection of the MI-consistent reflections were improved by using multiple human raters and a carefully controlled decision tree to determine the validity of the reflections. The new rating scheme itself was stricter than the one used in version 1, which caused the hit rate to go down—not because the generation was worse but because of the stricter rating. The hit rate of the new generator was measured to be 55.1% (166/301) on a set of reflections.",Other: Motivational interviewing reflections,English,Yes,No,Psychopathology,Trained professionals,"",y,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17,10,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),349,"",Other: fine-tuning + custom pipeline (see Reflection Generation Training),CBT: Motivational interviewing,BERT family; GPT-2 family,Yes,"",y,n,y,y,consultation and relational empathy survey (CARE),"consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","readiness ruler (patient symptom report, smoking related, validated measure)","no benchmark. in absolute terms favorable, with reduction in smoking, increase in confidence, importance, readiness.",no benchmark,no benchmark,"","","","","","","","","",n,"",y,used reflection quality classifier,y,on-premise models,Y,reported in multimedia appendix 3,n,"",y,"Of the 654 participants who accepted and consented to the study,
105 (16.1%) did not finish the entire study. We speculate that
this dropout was caused by several factor",n,"",y,use of the readiness ruler,n,"",n,"",n,"","",Yes,"It is important to consider the possibility that the 3.7% (13/349)
of the participants who resolved their ambivalence to continue
smoking were hurt by the interaction with the chatbot. We
manually reviewed each of these conversations, and for 85.1%
(297/349) of the conversations, we did not see evidence of
harmful statements made by the chatbot that could have
contributed to this resolution. For the other 14.9% (52/349) of
the conversations, the chatbot produced poor reflections, which
may have caused participants to be less likely to quit or to
believe that they had less of a chance to do so; for example, in
response to a participant expressing the idea that quitting cold
turkey was their best approach to quitting, the chatbot responded,
“A smoker can’t really do that,” which is quite inappropriate."
62,Ma 2024,Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support,Reviewer Two,USA,Self-collected data,"Qualitative study using an online survey + interviews about LGBTQ+ users’ experiences with LLM-based chatbots for mental wellness; questions cover usage, apps used, frequency, and detailed experiences.
","Other: Other (qualitative survey & interview data about chatbot use)
",English,No,"",Unselected,"Other: AI chatbots referenced; study data from human participants
","",not applicable,"",not applicable,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11,05,2024,Conceptual or theoretical work (e.g. on ethics or safety),"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",qualitative study - survey with 31 participants from recruited over Reddit,"",""
62,Ma 2024,Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support,Richard Gaus,USA,No dataset used for development or evaluation,"","","","","","","","","","","","",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11,5,2024,Population survey,"",Client-facing application,Chatbot,"Other: chatbot usersfrom three sub-Reddits: r/Snapchat, r/Anima, and r/Parradot",31,"","","Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified; Other: various LLM-based chatbots (Replika - Snapchat My AI - Chai - Character.ai - Anima - Paradot - Kuki)",Yes,"",n,y,n,y,"","Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions)","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Yes,"The fact that our LGBTQ+ participants occasionally received
inappropriate or potentially detrimental advice from the chatbots
revealed an inherent unpredictability in these models. For example,
when participants asked chatbots for suggestions about workplace
homophobia, LLMs advised them to quit their jobs without consid
ering any fnancial or personal consequences that such decisions
would cause them. Chatbots also assumed that the participants’
environment was LGBTQ+ friendly when the opposite was true.
Therefore, LLM-based chatbots are potentially more dangerousthan
pre-LLM chatbots because while pre-LLM chatbots lack the linguis
tic prowess LLM-based chatbots possess, their responses do not
deviate from scripted interactions. LLM-based chatbots, while they
can indeed ofer responses that are engaging and fexible, run risks
of giving gibberish and harmful advice due to this unpredictability.
Granted, LLM-based chatbot designers cannot safeguard against all
problematic output, but future endeavors should be spent trying to
harness the strengths of LLMs while minimizing their dangers

6.2.1 Implementing Context-Sensitive Conversational Guardrails."
62,Ma 2024,Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support,Consensus,USA,No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,"",Other: ,Other: ,"","","","","",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11,5,2024,Population survey,"",Client-facing application,Chatbot,General population,31,"","","Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified; Other: various LLM-based chatbots: Replika, Snapchat My AI, Chai, Character.ai, Anima, Paradot, ChatGPT, Kuki",Yes,"",n,y,n,y,"","Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions)","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",qualitative study - survey with 31 participants from recruited over Reddit,Yes,"The fact that our LGBTQ+ participants occasionally received
inappropriate or potentially detrimental advice from the chatbots
revealed an inherent unpredictability in these models. For example,
when participants asked chatbots for suggestions about workplace
homophobia, LLMs advised them to quit their jobs without consid
ering any fnancial or personal consequences that such decisions
would cause them. Chatbots also assumed that the participants’
environment was LGBTQ+ friendly when the opposite was true.
Therefore, LLM-based chatbots are potentially more dangerousthan
pre-LLM chatbots because while pre-LLM chatbots lack the linguis
tic prowess LLM-based chatbots possess, their responses do not
deviate from scripted interactions. LLM-based chatbots, while they
can indeed ofer responses that are engaging and fexible, run risks
of giving gibberish and harmful advice due to this unpredictability.
Granted, LLM-based chatbot designers cannot safeguard against all
problematic output, but future endeavors should be spent trying to
harness the strengths of LLMs while minimizing their dangers

6.2.1 Implementing Context-Sensitive Conversational Guardrails."
61,Ding 2022,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,Reviewer Two,USA,Self-collected data,"Dataset collected (Ben-Zeev et al., 2020) and rated (Tauscher et al., 2022) in previous studies conducted by same institution","","","","","","","",N,"",N,"",Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",10,7,2022,Tool Development and Evaluation,"",Other: Analysis of text-message conversations between clients and clinicians,"","","","",Only fine-tuning,"Unspecified, might include formal therapy methods",BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,B,L,base BERT model with AUPRC <0.52,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",No,""
61,Ding 2022,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,Richard Gaus,USA,External data set,"From our previous work (Tauscher et al., 2022), we utilized data from a randomized controlled trial of a community-based text-message intervention for individuals with serious mental illness (Ben- Zeev et al., 2020).

""The maximum intervention dose was three “exchanges,” wherein an exchange is defined as a cluster of thematically connected back-and-forth messages between mobile interventionist and participant (e.g. three outgoing messages and participant responses). Texting strategies included: reminders (e.g., appointments, prescription refills), information provision (e.g., psychoeducation, links to regional events and resources), cognitive challenges (e.g., restructuring dysfunctional beliefs about voices, questioning the validity of self-sabotaging automatic beliefs), self-monitoring/self-reflection (e.g., guidance on self-evaluation of affect, journaling of symptomatic experiences), relaxation techniques (e.g., diaphragmatic breathing, guided imagery), social skills training (e.g., initiating conversations, maintaining eye contact), supportive messages (e.g., affirmations, inspirational quotes), and in-vivo instruction (e.g., pre-scheduled real-time support as the patient attempted a new activity)."" (from Ben-Zeev et al.)

in addition they augmented their data using different strategies (3.2 Augmentation of text data)","","","","","","","","","","","",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",10,7,2022,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,CBT: Cognitive restructuring,BERT family; GPT-2 family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,b,l,Benchmark: BERT (no augmentation). metric: AUPRC,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,all used models are on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
61,Ding 2022,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,Consensus,USA,External data set,"From our previous work (Tauscher et al., 2022), we utilized data from a randomized controlled trial of a community-based text-message intervention for individuals with serious mental illness (Ben- Zeev et al., 2020).

""The maximum intervention dose was three “exchanges,” wherein an exchange is defined as a cluster of thematically connected back-and-forth messages between mobile interventionist and participant (e.g. three outgoing messages and participant responses). Texting strategies included: reminders (e.g., appointments, prescription refills), information provision (e.g., psychoeducation, links to regional events and resources), cognitive challenges (e.g., restructuring dysfunctional beliefs about voices, questioning the validity of self-sabotaging automatic beliefs), self-monitoring/self-reflection (e.g., guidance on self-evaluation of affect, journaling of symptomatic experiences), relaxation techniques (e.g., diaphragmatic breathing, guided imagery), social skills training (e.g., initiating conversations, maintaining eye contact), supportive messages (e.g., affirmations, inspirational quotes), and in-vivo instruction (e.g., pre-scheduled real-time support as the patient attempted a new activity)."" (from Ben-Zeev et al.)

in addition they augmented their data using different strategies (3.2 Augmentation of text data)","","","","","","","","","","","",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",10,7,2022,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,CBT: Cognitive restructuring,BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",Y,B,L,base BERT model with AUPRC <0.52. Task: classifying cognitive distortions,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",y,all used models are on-premise,N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",No,""
60,Sharma 2024,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,Reviewer Two,USA,Self-collected data,"interaction logs from a self-guided reframing tool (user inputs thoughts; LLM generates reframes);
","Other: self-guided mental-health tool interaction logs
",English,Yes,Other: ,Unselected,"Other: self-guided mental-health
","",N,GPT3,N,too little,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11,5,2024,Tool Development and Evaluation,"",Client-facing application,"",General population,"","",Custom pipeline with integrated LLM which is only prompted,CBT: Cognitive restructuring,GPT-3 family,No,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","
","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",no alpha adjusting (table 3),Yes,"See: 8.3 
Ethics and Safety

"
60,Sharma 2024,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,Richard Gaus,USA,External data set,"We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].

4.1 Curated Situations & Negative Thoughts
 We start by curating data sources for situations and negative thoughts.
 Thought Records Dataset (Burger et al., 2021).
 This dataset contains hypothetical and real-world situations, thoughts and emotional processes reported by crowdworkers on Amazon Mechanical Turk. We manually curate 180 pairs of diverse situations with negative thoughts from this dataset.
 Mental Health America (MHA). Situations and thoughts from crowdworkers may not reflect the broad range of mental health challenges that people face in real-life. To incorporate more real-world situations and thoughts, we ran a survey on the MHA website (screening.mhanational.org). MHA visitors (who typically use the website for screening of mental illnesses) were asked to describe any negative thoughts and the associated situations they were struggling with. We manually curate 120 pairs of self-reported situations and thoughts to ensure broad coverage of relevant topics based on high diversity and manual filtering.

https://github.com/behavioral-data/Cognitive-Reframing",Other: thought records,English,No,Yes,Unselected,Other: not applicable,"",n,"",n,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11,5,2024,Tool Development and Evaluation,"",Client-facing application,Other: reframed thought generator,"Other: Visitors to Mental Health America (MHA, large mental health website that provides mental health resources and tools to millions of users).
""Many MHA visitors are interested in mental health resources including self-guided systems.""",15531,"","Other: multiple different:
selection of thinking traps: fine-tuning
writing of reframes: retrieval-enhanced in-context learning",CBT: Cognitive restructuring,GPT-3 family,Yes,"",n,y,y,y,"","We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",reduction in emotion intensity,no benchmark,no benchmark,no benchmark. only the pre-post-difference in this metric was measured.,"","","","","","","","","",n,"",y,see 4.2 -> Safety considerations,n,data are sent to GPT-3 API,y,"",y,see table 4!,n,"",n,"",n,"",y,"We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions",n,"",n,"","",Yes,"4.2 System Design, Safety Considerations
6.4 Analysis of the Language Model Generated Content that was Flagged Inappropriate
Section 8.3 Ethics and Safety
(1) Non-maleficence, (2) Beneficence, (3) Respect for Autonomy, (4) Justice, (5) Explicability"
60,Sharma 2024,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,Consensus,USA,External data set,"We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].

4.1 Curated Situations & Negative Thoughts
 We start by curating data sources for situations and negative thoughts.
 Thought Records Dataset (Burger et al., 2021).
 This dataset contains hypothetical and real-world situations, thoughts and emotional processes reported by crowdworkers on Amazon Mechanical Turk. We manually curate 180 pairs of diverse situations with negative thoughts from this dataset.
 Mental Health America (MHA). Situations and thoughts from crowdworkers may not reflect the broad range of mental health challenges that people face in real-life. To incorporate more real-world situations and thoughts, we ran a survey on the MHA website (screening.mhanational.org). MHA visitors (who typically use the website for screening of mental illnesses) were asked to describe any negative thoughts and the associated situations they were struggling with. We manually curate 120 pairs of self-reported situations and thoughts to ensure broad coverage of relevant topics based on high diversity and manual filtering.

https://github.com/behavioral-data/Cognitive-Reframing",Other: thought records,English,No,Yes,Unselected,Other: not applicable,"",N,GPT3,N,too little,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11,5,2024,Tool Development and Evaluation,"",Client-facing application,Other: reframed thought generator,General population,15531,"","Other: multiple different:
selection of thinking traps: fine-tuning
writing of reframes: retrieval-enhanced in-context learning",CBT: Cognitive restructuring,GPT-3 family,Yes,"",n,y,y,y,"","We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",reduction in emotion intensity,no benchmark,no benchmark,no benchmark. only the pre-post-difference in this metric was measured.,ux measures,no benchmark,no benchmark,"Reframe Relatability, Reframe Helpfulness, Reframe Memorability, Skill Learnability","","","","","",n,"",y,see 4.2 -> Safety considerations,n,data are sent to GPT-3 API,y,"",y,see table 4!,n,"",n,"",n,"",n,"We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions
BUT: only for different design decisions; no overall other intervention or no intervention as control group!",n,"",n,"",no alpha adjusting (table 3),Yes,"4.2 System Design, Safety Considerations
6.4 Analysis of the Language Model Generated Content that was Flagged Inappropriate
Section 8.3 Ethics and Safety
(1) Non-maleficence, (2) Beneficence, (3) Respect for Autonomy, (4) Justice, (5) Explicability"
59,Bird 2023,Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety,Reviewer Two,UK,External data set,"CounselChat1,
the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 ","","","","","","","",unclear,"",unclear,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),5,7,2023,Tool Development and Evaluation,"","","","","","",Custom pipeline with integrated LLM which is only prompted,"Unspecified, might include formal therapy methods",Llama 3.1 family; Other: own?,No,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",No,""
59,Bird 2023,Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety,Richard Gaus,UK,External data set,"Due to this, data from the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 were selected. Questions
and answers are extracted, and questions are manually generated
dependent on the information available, e.g. for the NHS definition
of depression, questions such as “what is depression?"" are imputed.",Internet data -- mental health Q&A,English,No,No,Other: not applicable,Unknown,"",y,own transformer architecture -> on-premise,n,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),5,7,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Only fine-tuning,"Unspecified, might include formal therapy methods",Other: own transformer architecture,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,"no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token",y,no benckmark,no benchmark,no benchmark. loss value,"",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"own transformer architecture, on-premise",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
59,Bird 2023,Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety,Consensus,UK,External data set,"Due to this, data from the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 were selected. Questions
and answers are extracted, and questions are manually generated
dependent on the information available, e.g. for the NHS definition
of depression, questions such as “what is depression?"" are imputed.",Internet data -- mental health Q&A,English,No,No,Other: not applicable,Trained professionals,"",y,own transformer architecture -> on-premise,n,"",Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),5,7,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","",Other: Other: self-developed and trained transformer architecture,"Unspecified, might include formal therapy methods",Other: own transformer architecture,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,"no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token",y,no benckmark,no benchmark,no benchmark. loss value,"",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"own transformer architecture, on-premise",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
56,Liu 2024,Leveraging ChatGPT to optimize depression intervention through explainable deep learning,Reviewer Two,China,Self-collected data,"1. publicly available dataset comprising HGC, 2. AI-generated content generated by ChatGPT",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Lay people,"",N,Chat GPT,N,"",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6,6,2024,Tool Development and Evaluation,Conceptual or theoretical work (e.g. on ethics or safety),Other: Comparison between AI and human councellor recommendations,"",No clients/patients involved,"","",Only fine-tuning,"Informal counseling (e.g., emotional support conversation)","BERT family; Other: Roberta, TextCNN, Text-LSTM, GPT-unknown version",No users involved,"",n,n,n,n,"","","",Y,s,H,Lexical Overlap within comparative linguistic analysis,Y,s,H,"",Y,s,H,"","","","","","","","","","",Y,"",L,"","",part-of-speech (POS) analysis ,"",L,"",dependency-syntactic-parsing (DEP) analysis,"",L,"","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
56,Liu 2024,Leveraging ChatGPT to optimize depression intervention through explainable deep learning,Richard Gaus,China,External data set,data sourced from Xinli001.com,Internet data -- mental health Q&A,Chinese,No,No,Unselected,Trained professionals,"",n,"",n,"",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6,6,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","various linguistic analyses with no clear meaning (part-of-speech, sentiment, ...)",unclear,h,benchmark: human psychologist responses from transcripts,SHAP values of words in classifier that classifies human vs. ChatGPT responses,unclear,h,benchmark: human psychologist responses from transcripts,"","","","","",n,"they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Although the efficacy of AIGC is theoretically promising,
mental health professionals shoulder the responsibility of
upholding human well-being as paramount—consequently, the
current stage warrants abstention regarding chat-based
assessments as professionally conclusive, pending substantiating
evidence. Our study encapsulates its outcomes in guidelines to
illuminate forthcoming researchers, developers, and practitioners,
enhancing their grasp of mental health literacy pertinent to
downstream tasks."
56,Liu 2024,Leveraging ChatGPT to optimize depression intervention through explainable deep learning,Consensus,China,External data set,data sourced from Xinli001.com. Counsellor responses were included in the dataset. ChatGPT responses were generated by the authors.,Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Trained professionals,"",N,Chat GPT,N,"",Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6,6,2024,Direct LLM performance evaluation (only via prompting),Other: ,Client-facing application,Other: one-turn Q&A Chatbot,No clients/patients involved,"","",Other: ,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",part-of-speech (POS) analysis; dependency-syntactic-parsing (DEP) analysis; semantic-dependency-parsing (SDP) analysis; sentiment analysis,unclear,h,benchmark: human psychologist responses from transcripts,SHAP values of words in classifier that classifies human vs. ChatGPT responses,unclear,h,benchmark: human psychologist responses from transcripts,"","","","","",n,"they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Although the efficacy of AIGC is theoretically promising,
mental health professionals shoulder the responsibility of
upholding human well-being as paramount—consequently, the
current stage warrants abstention regarding chat-based
assessments as professionally conclusive, pending substantiating
evidence. Our study encapsulates its outcomes in guidelines to
illuminate forthcoming researchers, developers, and practitioners,
enhancing their grasp of mental health literacy pertinent to
downstream tasks."
50,Striegl 2024,Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,Reviewer Two,Germany,External data set,"The Emobank dataset (Buechel & Hahn, 2022)
The GoEmotions dataset (Demszky et al., 2020) 
The International Survey on Emotion Antecedents and Reactions (ISEAR) (ISEAR
dataset, https://paperswithcode.com/dataset/isear, accessed 22.08.2023
Validation: The CrowdFlower dataset (Sentiment Analysis in Text - Dataset by crowdflower,
https://data.world/crowdflower/sentiment-analysis-in-text, accessed 07.08.2022)

(see excel)","","","","","","","",N,"",N,"",Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",24,6,2024,Tool Development and Evaluation,"",Other: Analysis of conversation transcripts & client facing,"",Other: healthy individuals without a diagnosed mental health disorder,"","","Other: ""we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset""
""The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output""",Other CBT techniques,BERT family,Yes,"",Y,N,Y,Y,"System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi)","System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.","","","","","","","","","",Y,B,L,see below,Y,B,L,"rule-based approach: "" It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score""","","","","","","","","","","",User rating of Empathig Understanding (EU),S,L,"rule-based approach: "" It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score""","","","","","","","","","",Y,"Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. ",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group)",N,"",Y,"
""Automation of time-consuming tasks in iCBT could, through a positive lens, lead to
improved cost-effectiveness, which is an important point in often over-encumbered and
underfinanced psychiatry treatment and care contexts.""
""
Fully automated iCBT, including the prediction of emotional states coupled with a CA
in charge of the iCBT with no human therapist involvement, would be both unwanted
and unethical. For legal reasons, having a clinical professional involved and ultimately
responsible for treatment is mandatory today and unlikely to change in the foreseeable
future. Only hybrid solutions of man-machine co-involvement are therefore further
discussed here. One such hybrid scenario would be the sole automation of emotion
recognition. This scenario starts with initial machine recognition of emotional states
derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional
states can then be fed to a human clinician as decision support. In theory, this could
render an improved understanding of a patient’s emotional state and also change of
state across time during iCBT. This could ultimately improve treatment tailoring and
effectiveness through the patient perceiving the therapist as more empathic, strengthening
the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy
work modules to better suit the patient’s emotional state""
"" An interesting
but largely untested scenario would be extended automation of emotional recognition
coupled with therapist-supported CA treatment. This would involve not only the potential
benefit of emotion recognition discussed above but also cost-effective semi-automated
treatment. One such implementation would be that the emotionally informed CA drafts
empathically written therapy responses to the patient’s messages and the human therapist
then scrutinizes the responses and signs off on them with or without making prior changes.
A major portion of iCBT costs come from therapists spending time drafting responses to
patients in the treatment portal, unlocking a major potential for cost-saving strategies. An
additional downside risk with this scenario would be that the human therapist—due to
stress or other human factors—signs off on written responses of lower therapeutic quality.
Proper training and structured follow-up of therapists are likely required in this scenario,
which in turn may offset some of the cost-effectiveness of the approach. That stated since
a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored
CA seems in accordance with that overarching aim of iCBT.""","",Yes,"p.17: 

Ethical and legal issues come into play whenever automated machines are integrated in
mental healthcare. When implementing AI in healthcare, Gerke, Minssen & Cohen (2020)
identified four primary ethical issues, which include,(1) informed consent to use, (2) safety
and transparency, (3) algorithmic fairness and biases, and (4) data privacy, as well as five
legal challenges: (1) safety and effectiveness, (2) liability, (3) data protection and privacy,
(4) cybersecurity, and(5) intellectual property law. For AI to be used in the healthcare
context, ethical considerations need to be made concerning informed consent to use.
The aforementioned issues become more complex when they impact clinical decisions.
Safety in the clinical context is a major concern regarding the implementation of AI,
and it is important that developers prioritize reliable training data, transparency on the
data used, and potential shortcomings of the system, e.g., biases. In order to avoid unfair
outcomes, the data needs to be of high quality, unbiased, and therefore from a data science
perspective balanced. Concerning data privacy, patients’ trust through sufficient data
protection measurements is crucial for the successful integration of AI in healthcare. When
AI systems have access to patients’ data, considerations need to be made concerning how
and if patients are informed of the use"
50,Striegl 2024,Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,Richard Gaus,"",External data set,"EmoBank, GoEmotions, ISEAR, CrowdFlower","","","","","","","","","","","",Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",24,6,2024,Tool Development and Evaluation,"",Client-facing application,Chatbot,General population,20,"","Other: BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture",Other CBT techniques,BERT family,Yes,"",y,n,y,y,"empathic understanding (EU), system usability (SUS), acceptance (CSQi)","Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based","",n,"","","",n,"","","",y,b,l,benchmark is simple rule-based emotion recognition system. Task: classification of clustered dimensional values (five distinct categories),y,b,l,"benchmark is simple rule-based emotion recognition system.
second, better benchmark: state-of-the-art valence and arousal recognition systems from other papers (Park et al., etc.). still, this is no human. Task: rating of valence, arousal, dominance in conversations","",n,"","","",n,"","","","","","","","","","","","","","","","","",y,"Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. ",n,"",y,all on-premise,n,"",n,"",n,"",n,"",n,"",y,"Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group).",n,"",y,"Fully automated iCBT, including the prediction of emotional states coupled with a CA in charge of the iCBT with no human therapist involvement, would be both unwanted and unethical. For legal reasons, having a clinical professional involved and ultimately responsible for treatment is mandatory today and unlikely to change in the foreseeable future. Only hybrid solutions of man-machine co-involvement are therefore further discussed here. One such hybrid scenario would be the sole automation of emotion recognition. This scenario starts with initial machine recognition of emotional states derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional states can then be fed to a human clinician as decision support. In theory, this could render an improved understanding of a patient’s emotional state and also change of state across time during iCBT. This could ultimately improve treatment tailoring and effectiveness through the patient perceiving the therapist as more empathic, strengthening the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy work modules to better suit the patient’s emotional state. A potential risk with this approach would be the drift of the therapist’s own emotional assessment influenced by the machine’s estimated emotional state of the patient which may be wrong or biased. An interesting but largely untested scenario would be extended automation of emotional recognition coupled with therapist-supported CA treatment. This would involve not only the potential benefit of emotion recognition discussed above but also cost-effective semi-automated treatment. One such implementation would be that the emotionally informed CA drafts empathically written therapy responses to the patient’s messages and the human therapist then scrutinizes the responses and signs off on them with or without making prior changes. A major portion of iCBT costs come from therapists spending time drafting responses to patients in the treatment portal, unlocking a major potential for cost-saving strategies. An additional downside risk with this scenario would be that the human therapist—due to stress or other human factors—signs off on written responses of lower therapeutic quality. Proper training and structured follow-up of therapists are likely required in this scenario, which in turn may offset some of the cost-effectiveness of the approach. That stated since a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored CA seems in accordance with that overarching aim of iCBT.","",Yes,"Implementation of AI tools in the mental healthcare context presents both opportunities and challenges. When assessing previous research on AI in mental health care, it is clear that there are flaws in research methodology and quality, such as not reporting external validation, high risk of bias, and a lack of transparency (Tornero-Costa et al., 2023). Ethical and legal issues come into play whenever automated machines are integrated in mental healthcare. When implementing AI in healthcare, Gerke, Minssen & Cohen (2020) identified four primary ethical issues, which include,(1) informed consent to use, (2) safety and transparency, (3) algorithmic fairness and biases, and (4) data privacy, as well as five legal challenges: (1) safety and effectiveness, (2) liability, (3) data protection and privacy, (4) cybersecurity, and(5) intellectual property law. For AI to be used in the healthcare context, ethical considerations need to be made concerning informed consent to use. The aforementioned issues become more complex when they impact clinical decisions. Safety in the clinical context is a major concern regarding the implementation of AI, and it is important that developers prioritize reliable training data, transparency on the data used, and potential shortcomings of the system, e.g., biases. In order to avoid unfair outcomes, the data needs to be of high quality, unbiased, and therefore from a data science perspective balanced. Concerning data privacy, patients’ trust through sufficient data protection measurements is crucial for the successful integration of AI in healthcare. When AI systems have access to patients’ data, considerations need to be made concerning how and if patients are informed of the use.

Fully automated iCBT, including the prediction of emotional states coupled with a CA in charge of the iCBT with no human therapist involvement, would be both unwanted and unethical. For legal reasons, having a clinical professional involved and ultimately responsible for treatment is mandatory today and unlikely to change in the foreseeable future. Only hybrid solutions of man-machine co-involvement are therefore further discussed here"
50,Striegl 2024,Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,Consensus,"",External data set,"The Emobank dataset (Buechel & Hahn, 2022)
The GoEmotions dataset (Demszky et al., 2020) 
The International Survey on Emotion Antecedents and Reactions (ISEAR) (ISEAR
dataset, https://paperswithcode.com/dataset/isear, accessed 22.08.2023
Validation: The CrowdFlower dataset (Sentiment Analysis in Text - Dataset by crowdflower,
https://data.world/crowdflower/sentiment-analysis-in-text, accessed 07.08.2022)","","","","","","","","","","","",Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",24,6,2024,Tool Development and Evaluation,"",Other: Analysis of conversation transcripts; Client-facing application,Chatbot,General population,20,"","Other: ""we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset""
""The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output""

BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture",Other CBT techniques,BERT family,Yes,"",Y,N,Y,Y,"empathic understanding (EU), system usability (SUS), acceptance (CSQi)","System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.

Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based","",n,"","","",n,"","","",y,b,l,benchmark is simple rule-based emotion recognition system. Task: classification of clustered dimensional values (five distinct categories),y,b,l,"benchmark is simple rule-based emotion recognition system.
second, better benchmark: state-of-the-art valence and arousal recognition systems from other papers (Park et al., etc.). still, this is no human. Task: rating of valence, arousal, dominance in conversations

rule-based approach: "" It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score""","",n,"","","",n,"","","","","","","","","","","","","","","","","",Y,"Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. ",N,"",y,all on-premise,N,"",N,"",N,"",N,"",N,"",Y,"Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group).",N,"",Y,"
""Automation of time-consuming tasks in iCBT could, through a positive lens, lead to
improved cost-effectiveness, which is an important point in often over-encumbered and
underfinanced psychiatry treatment and care contexts.""
""
Fully automated iCBT, including the prediction of emotional states coupled with a CA
in charge of the iCBT with no human therapist involvement, would be both unwanted
and unethical. For legal reasons, having a clinical professional involved and ultimately
responsible for treatment is mandatory today and unlikely to change in the foreseeable
future. Only hybrid solutions of man-machine co-involvement are therefore further
discussed here. One such hybrid scenario would be the sole automation of emotion
recognition. This scenario starts with initial machine recognition of emotional states
derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional
states can then be fed to a human clinician as decision support. In theory, this could
render an improved understanding of a patient’s emotional state and also change of
state across time during iCBT. This could ultimately improve treatment tailoring and
effectiveness through the patient perceiving the therapist as more empathic, strengthening
the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy
work modules to better suit the patient’s emotional state""
"" An interesting
but largely untested scenario would be extended automation of emotional recognition
coupled with therapist-supported CA treatment. This would involve not only the potential
benefit of emotion recognition discussed above but also cost-effective semi-automated
treatment. One such implementation would be that the emotionally informed CA drafts
empathically written therapy responses to the patient’s messages and the human therapist
then scrutinizes the responses and signs off on them with or without making prior changes.
A major portion of iCBT costs come from therapists spending time drafting responses to
patients in the treatment portal, unlocking a major potential for cost-saving strategies. An
additional downside risk with this scenario would be that the human therapist—due to
stress or other human factors—signs off on written responses of lower therapeutic quality.
Proper training and structured follow-up of therapists are likely required in this scenario,
which in turn may offset some of the cost-effectiveness of the approach. That stated since
a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored
CA seems in accordance with that overarching aim of iCBT.""","",Yes,"Implementation of AI tools in the mental healthcare context presents both opportunities and challenges. When assessing previous research on AI in mental health care, it is clear that there are flaws in research methodology and quality, such as not reporting external validation, high risk of bias, and a lack of transparency (Tornero-Costa et al., 2023). Ethical and legal issues come into play whenever automated machines are integrated in mental healthcare. When implementing AI in healthcare, Gerke, Minssen & Cohen (2020) identified four primary ethical issues, which include,(1) informed consent to use, (2) safety and transparency, (3) algorithmic fairness and biases, and (4) data privacy, as well as five legal challenges: (1) safety and effectiveness, (2) liability, (3) data protection and privacy, (4) cybersecurity, and(5) intellectual property law. For AI to be used in the healthcare context, ethical considerations need to be made concerning informed consent to use. The aforementioned issues become more complex when they impact clinical decisions. Safety in the clinical context is a major concern regarding the implementation of AI, and it is important that developers prioritize reliable training data, transparency on the data used, and potential shortcomings of the system, e.g., biases. In order to avoid unfair outcomes, the data needs to be of high quality, unbiased, and therefore from a data science perspective balanced. Concerning data privacy, patients’ trust through sufficient data protection measurements is crucial for the successful integration of AI in healthcare. When AI systems have access to patients’ data, considerations need to be made concerning how and if patients are informed of the use.

Fully automated iCBT, including the prediction of emotional states coupled with a CA in charge of the iCBT with no human therapist involvement, would be both unwanted and unethical. For legal reasons, having a clinical professional involved and ultimately responsible for treatment is mandatory today and unlikely to change in the foreseeable future. Only hybrid solutions of man-machine co-involvement are therefore further discussed here

p.17: 

Ethical and legal issues come into play whenever automated machines are integrated in
mental healthcare. When implementing AI in healthcare, Gerke, Minssen & Cohen (2020)
identified four primary ethical issues, which include,(1) informed consent to use, (2) safety
and transparency, (3) algorithmic fairness and biases, and (4) data privacy, as well as five
legal challenges: (1) safety and effectiveness, (2) liability, (3) data protection and privacy,
(4) cybersecurity, and(5) intellectual property law. For AI to be used in the healthcare
context, ethical considerations need to be made concerning informed consent to use.
The aforementioned issues become more complex when they impact clinical decisions.
Safety in the clinical context is a major concern regarding the implementation of AI,
and it is important that developers prioritize reliable training data, transparency on the
data used, and potential shortcomings of the system, e.g., biases. In order to avoid unfair
outcomes, the data needs to be of high quality, unbiased, and therefore from a data science
perspective balanced. Concerning data privacy, patients’ trust through sufficient data
protection measurements is crucial for the successful integration of AI in healthcare. When
AI systems have access to patients’ data, considerations need to be made concerning how
and if patients are informed of the use"
41,Brocki 2023,Deep Learning Mental Health Dialogue System,Reviewer Two,Other: Poland,Self-collected data,"“Pushshift Reddit Dataset which includes 651 million submissions and 5.6 billion comments…” • “We have fine-tuned this model on transcripts of counseling and psychotherapy sessions… Our… dataset consisted of 14,300 patient’s prompt and counselor’s answer pairs.”
","","","","","","","",N,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",23,1,2023,Tool Development and Evaluation,"",Client-facing application,"",General population,"","","",Other: Person Centered Therapy (PCT) (Carl Rogers),"Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer
[26] model which generates a beam of candidate responses.
In the second stage a number of smaller, more specialized
Transformer-based models)",Yes,"","","","","","","Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Yes,"- Serena may contain potential bias, incoherence, and
distaste, despite heuristics and post-processing attempts to
mitigate such responses.
- Toxic language (pretrained on Reddit comments)"
41,Brocki 2023,Deep Learning Mental Health Dialogue System,Richard Gaus,Other: Poland,External data set,"Alexander Street Press, Counseling and Psychotherapy Transcripts: Volume I",Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,"",y,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,2,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","","Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers",Other: Person-centered therapy,BERT family; Other: own architecture,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",y,"To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.",y,only on-premise models,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"In addition, as a generative deep learning
model, Serena may contain potential bias, incoherence, and
distaste, despite heuristics and post-processing attempts to
mitigate such responses."
41,Brocki 2023,Deep Learning Mental Health Dialogue System,Consensus,Other: Poland,External data set,"Alexander Street Press, Counseling and Psychotherapy Transcripts: Volume I",Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,"",y,"",N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,2,2023,Tool Development and Evaluation,"",Client-facing application,Chatbot,No clients/patients involved,"","","Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers",Other: Person Centered Therapy (PCT) (Carl Rogers),"BERT family; Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer [26] model which generates a beam of candidate responses. In the second stage a number of smaller, more specialized Transformer-based models)",Yes,"","","","","","","Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.

Results: Not clear/not reported","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",n,"",y,"To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.",y,only on-premise models,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","no evaluation was performed. Only a sample conversation was shown ""look this is good"".",Yes,"- Serena may contain potential bias, incoherence, and
distaste, despite heuristics and post-processing attempts to
mitigate such responses.
- Toxic language (pretrained on Reddit comments)"
34,Meywirth 2024,Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,Reviewer Two,Germany,Self-collected data,"brief questionnaire; users interacted ~10 minutes; prompts designed for ChatGPT; qualitative feedback collected
",Other: qualitative interviews,Other: german,No,No,Unknown,Other: not applicable,"",N,Chat GPT,N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",27,5,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,"",General population,"","","",CBT: Motivational interviewing,GPT-3.5 family,Yes,"",N,Y,N,Y,"","Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their
goals and inquire about their interests. Notably, no users reported feeling pressured or
convinced by the prototype to change aspects of their lives against their wishes. Several
users found the prototype to be particularly valuable for creating quick and helpful
weekly plans or recipes. However, it was observed that one user found it premature
to establish concrete plans at that stage. Overall, while some users initially felt that
the prototype’s responses were not aligned with their preferences, they noted that it
was possible to clarify their intentions, and the prototype quickly adapted accordingly.
Despite this, some users felt that the assessment of their goals and life circumstances
at the beginning of the training was too superficial, leading to suggestions for a more
thorough initial assessment process.
Users highlighted several aspects of the prototype’s communication style. Most users
found the conversation to be clear and direct. In contrast, one user reported that the pro-
totype struggled to understand his goals and sometimes provided contradictory advice,
a concern not raised by others. On the positive side, one user appreciated the prototype’s
approach of asking questions rather than giving fixed instructions, which encouraged 
engagement and avoided a patronizing tone. Some users noted the positive and moti-
vating tone of the coach, particularly when asking if they were willing to continue with
suggested steps and advice. The use of a positive tone was seen as a motivating factor by
one user and was also appreciated by another who found it encouraging. Users generally
found concrete advice more helpful than generic recommendations. Additionally, one
user valued the coach’s acknowledgment of setbacks and the importance of enjoying the
process. However, some users expressed a desire for shorter, more direct conversations
that focused less on delivering general knowledge, and one user specifically requested
a more emotional and less matter-of-fact tone of voice.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",Yes,"p.93: 

Notably, the prototype was designed to avoid recommending changes in aspects of
users’ lives they did not wish to alter, underlining the importance of user autonomy in
the coaching process. Despite these positive aspects, the evaluation also underscored the
inherent challenges in achieving precise control over the outputs of non-deterministic
systems like LLMs. This limitation emphasizes the need for ongoing, comprehensive
assessments to ensure the safety and reliability of deploying such innovative technolo-
gies. Moving forward, enhancing personalization and ensuring the alignment of inter-
ventions with users’ unique circumstances and preferences will be crucial in refining
and advancing the field of LLM-based coaching for lifestyle behavior change"
34,Meywirth 2024,Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,Richard Gaus,Germany,No dataset used for development or evaluation,"","","","","","","","",n,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3,6,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,"Other: unclear (it just says ""users"")",11,"","",CBT: Motivational interviewing,GPT-3.5 family,Yes,"",n,y,n,y,"","Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Notably, the prototype was designed to avoid recommending changes in aspects of users’ lives they did not wish to alter, underlining the importance of user autonomy in the coaching process. Despite these positive aspects, the evaluation also underscored the inherent challenges in achieving precise control over the outputs of non-deterministic systems like LLMs. This limitation emphasizes the need for ongoing, comprehensive assessments to ensure the safety and reliability of deploying such innovative technolo- gies. Moving forward, enhancing personalization and ensuring the alignment of inter- ventions with users’ unique circumstances and preferences will be crucial in refining and advancing the field of LLM-based coaching for lifestyle behavior change."
34,Meywirth 2024,Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,Consensus,Germany,No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,"",N,Chat GPT,N,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",27,5,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,General population,11,"","",CBT: Motivational interviewing,GPT-3.5 family,Yes,"",N,Y,N,Y,"","Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",Yes,"p.93: 

Notably, the prototype was designed to avoid recommending changes in aspects of
users’ lives they did not wish to alter, underlining the importance of user autonomy in
the coaching process. Despite these positive aspects, the evaluation also underscored the
inherent challenges in achieving precise control over the outputs of non-deterministic
systems like LLMs. This limitation emphasizes the need for ongoing, comprehensive
assessments to ensure the safety and reliability of deploying such innovative technolo-
gies. Moving forward, enhancing personalization and ensuring the alignment of inter-
ventions with users’ unique circumstances and preferences will be crucial in refining
and advancing the field of LLM-based coaching for lifestyle behavior change"
32,Sharma 2021,Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach,Reviewer Two,USA,External data set,"TalkLife
Real peer-to-peer conversations on a mental-health support platform (TalkLife) used to train and evaluate “Partner,” a reinforcement-learning model for empathic rewriting. Includes posts by seekers and peer supporters, with empathy scores (0–6) labeled using the Sharma et al. (2020) empathy classifier and 180 expert “empathic rewritings.
",Internet data -- mental health forum,English,Yes,No,Other: for me unclear does the comment suffice?,Lay people,"",N,"",N,too little,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",19,4,2021,Tool Development and Evaluation,"",Other: Conversation Rewriting,"",No clients/patients involved,"",Utterance suggestions,Other: PARTNER,"Informal counseling (e.g., emotional support conversation)",GPT-2 family,No,"","","","","","","","",N,"","","",Y (Specifity),B,L,"",N,"","","",Y (Perplexity),W,L,"","",N,"","","",N,"","","","",Empathy classification (Sharma et al),B,L,Other LLMs,Edit Rate,W,L,Other LLMs,"","","","","","","","","","","","","","","","","","","","","","","","","","","","",Yes,"Impor-
tantly, this machine-in-the-loop approach can help mitigate some
of the risks related to toxicity and safety of AI systems in settings
of suicidal ideation, self-harm, or insensitive comments related to

Towards preventing unsafe rewritings. We acknowledge that
building computational models for intervention in high-stakes set-
tings such as mental health necessitates ethical considerations.
There is a risk that in attempting to help, responses could have
the opposite effect, which could be deadly in cases of self-harm.
No current computational approach will identify and respond to
harm-related utterances perfectly [43]. Thus, risk mitigation steps
are appropriate in this context. Here, we remove all posts that con-
tain a pre-defined unsafe regular expression (e.g., ∗commit suicide∗)
from our analyses and training in collaboration with mental health
professionals. Future work testing or deploying AI systems should
assess safety-related risk, and also potential sources of bias (e.g.,
race, ethnicity, age, or gender bias in training data or models)."
32,Sharma 2021,Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach,Richard Gaus,USA,Self-collected data,"TalkLife (talklife.co) is the largest online peer-to-peer support platform for mental health support. It enables conversations between people seeking support (support seekers) and people providing support (peer supporters) in a thread-like setting. We call the post authored by a support seeker as seeker post, and the response by a peer supporter as response post. Table 1 describes the statistics of conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question ""Is the seeker talking about a mental health related issue or situation in his/her post?"". Using this annotated dataset, we train a standard text classifier based on BERT [15] (achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-related conversations. This dataset contains 3.33M interactions from 1.48M seeker posts.

https://github.com/behavioral-data/Empathy-Mental-Health/tree/master",Internet data -- mental health forum,English,No,Yes,Unselected,Lay people,"",y,"",n,"",Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19,4,2021,Tool Development and Evaluation,"",Therapist-facing application,"","","",Utterance suggestions,Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.,Peer support conversation,BERT family; GPT-2 family,No users involved,"","","","","","","","",y,b,l,BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations,y,b,l,"""specificity""","","","","","","","","","",y,w,h,Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.,y,b,l,"BERT-based empathy scoring model.
benchmarks are other LLMs, not fine tuned to empathy tasks","",perplexity,w,l,PARTNER against other LLMs.,lexical_diversity,w,l,PARTNER against other LLMs.,automatic empathy rating,b,l,change in empathy. PARTNER against other LLMs.,"",n,"",n,"",y,all on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Towards preventing unsafe rewritings. We acknowledge thatbuilding computational models for intervention in high-stakes set-tings such as mental health necessitates ethical considerations.There is a risk that in attempting to help, responses could havethe opposite effect, which could be deadly in cases of self-harm.No current computational approach will identify and respond toharm-related utterances perfectly [43]. Thus, risk mitigation stepsare appropriate in this context. Here, we remove all posts that con-tain a pre-defined unsafe regular expression (e.g., ∗commit suicide∗)from our analyses and training in collaboration with mental health professionals. Future work testing or deploying AI systems shouldassess safety-related risk, and also potential sources of bias (e.g.,race, ethnicity, age, or gender bias in training data or models)."
32,Sharma 2021,Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach,Consensus,USA,Self-collected data but derived from external data set,"TalkLife (talklife.co) is the largest online peer-to-peer support platform for mental health support. It enables conversations between people seeking support (support seekers) and people providing support (peer supporters) in a thread-like setting. We call the post authored by a support seeker as seeker post, and the response by a peer supporter as response post. Table 1 describes the statistics of conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question ""Is the seeker talking about a mental health related issue or situation in his/her post?"". Using this annotated dataset, we train a standard text classifier based on BERT [15] (achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-related conversations. This dataset contains 3.33M interactions from 1.48M seeker posts.

Non-public: For accessing the TalkLife portion of our dataset for non-commercial use, please contact the TalkLife team here.",Internet data -- mental health forum,English,No,No,Unselected,Lay people,"",y,"",N,too little,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19,4,2021,Tool Development and Evaluation,"",Therapist-facing application,"",No clients/patients involved,"",Utterance suggestions,Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.,Peer support conversation,BERT family; GPT-2 family,No users involved,"","","","","","","","",y,b,l,BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations,Y,B,L,specificity is an embedding similarity metric here,N,"","","","","","","","",y,w,h,Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.,N,"","","","",Empathy classification (Sharma et al),B,L,Other LLMs,"Perplexity, Diversity, Sentence coherence, edit rate",W,L,Other LLMs,expert rating 2,b,l,"Expert judgments of empathy, fluency, specificity of PARTNER against other LLMs.","",n,"",n,"",y,all on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Impor-
tantly, this machine-in-the-loop approach can help mitigate some
of the risks related to toxicity and safety of AI systems in settings
of suicidal ideation, self-harm, or insensitive comments related to

Towards preventing unsafe rewritings. We acknowledge that
building computational models for intervention in high-stakes set-
tings such as mental health necessitates ethical considerations.
There is a risk that in attempting to help, responses could have
the opposite effect, which could be deadly in cases of self-harm.
No current computational approach will identify and respond to
harm-related utterances perfectly [43]. Thus, risk mitigation steps
are appropriate in this context. Here, we remove all posts that con-
tain a pre-defined unsafe regular expression (e.g., ∗commit suicide∗)
from our analyses and training in collaboration with mental health
professionals. Future work testing or deploying AI systems should
assess safety-related risk, and also potential sources of bias (e.g.,
race, ethnicity, age, or gender bias in training data or models)."
30,Farhat 2024,ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane,Reviewer Two,India,"","","","","","","","","",N,"",N,"",Other: Letter to the Editor ,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",21,07,2023,"Opinion, commentary, perspective, correspondence","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
30,Farhat 2024,ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane,Richard Gaus,India,No dataset used for development or evaluation,"","","","","","","","",n,"",n,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",21,7,2023,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","","Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"ChatGPT could recommend the wrong medications:
certain prompts resulted in the model generating a list of prescribed 
medications for the subject condition. This could potentially 
harm society or individuals, particularly those dealing with 
depression or other mental health issues."
30,Farhat 2024,ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane,Consensus,India,No dataset used for development or evaluation,no dataset. ChatGPT is used as is.,"","","","","","","",N,"",N,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",21,7,2023,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","","Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",No real evaluation. Only a sample conversatino is shown,Yes,"ChatGPT could recommend the wrong medications:
certain prompts resulted in the model generating a list of prescribed 
medications for the subject condition. This could potentially 
harm society or individuals, particularly those dealing with 
depression or other mental health issues."
26,Hodson 2024,Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.,Reviewer Two,UK,Self-collected data,"oth ChatGPT-4 and Bard responded to 20 tasks at each
stage of the study --> no new dataset created ",Psychotherapy -- chat logs,English,No,No,Unselected,Lay people,"",N,"",N,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,07,2024,Direct LLM performance evaluation (only via prompting),"","","",No clients/patients involved,"","","",CBT: Cognitive restructuring,GPT-4 / GPT-4o family; Other: Bard ,No users involved,"","","","","","","","","","","","","","","","",Yes ,No benchmark,no benchmark,"GPT4 scored 44/60 and Bard 46/60, but the study did not aim to compare those two,  ","","","","","",Yes ,no benchmark,no benchmark,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions",very short study ,No,""
26,Hodson 2024,Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.,Richard Gaus,UK,Self-collected data,Data were 20 thoughts written by two CBT therapists (each wrote 10 thoughts).,Other: automatic thought records,English,Other: synthetic: human generated,No,Other: not applicable,Other: not applicable,"",n,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,7,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Other: reframed thought generator,No clients/patients involved,"","","",CBT: Cognitive restructuring,GPT-4 / GPT-4o family; Gemini / Bard family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,"","",no benchmark.,n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
26,Hodson 2024,Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.,Consensus,UK,Self-collected data,"oth ChatGPT-4 and Bard responded to 20 tasks at each
stage of the study --> no new dataset created ",Other: automatic thought records,English,Other: synthetic: human generated,No,Other: not applicable,Other: not applicable,"",N,"",N,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,7,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Other: reframed thought generator,No clients/patients involved,"","","",CBT: Cognitive restructuring,BERT family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,no benchmark.,n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",Y,"Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions",very short study ,No,""
18,Imel 2024,Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning,Reviewer Two,USA,External data set,"Data were obtained from an online and mobile therapy app (Talkspace) for services provided
between 2014 and 2019.",Psychotherapy -- chat logs,English,No,No,Psychopathology,Trained professionals,"",unclear,did not find information,Y,"“The deidentified content of messages was retained…” / “clients and therapists agree to the use of their anonymized data for quality assurance and for research.” / “This study was thus deemed exempt from full institutional review board review…
",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",22,1,2024,Other: use of machine learning to evaluate clinical content (ML was used to improve quality),"",Analysis of conversation transcripts,"","","","","","Informal counseling (e.g., emotional support conversation)",Other: ML model to structure responses,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","",y,"","","","","","","",y,"","","","","","","","",No,""
18,Imel 2024,Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning,Richard Gaus,"",External data set,Talkspace data,"","","","","","","","","","","",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",22,1,2024,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,Mix of formal therapy methods,Other: not sure - it just says transformer model,No users involved,"","","","","","","","",n,"","","",n,"","","",y,w,h,benchmark are ratings by humans on MISC and Topics (see appendix eTable 2),y,w,h,benchmark are ratings by humans on CTRS (see appendix eTable 2),"","","","","","","","","","","","","","","","","","","","","","","",n,"",n,"",y,on-premise transformer model,y,see Table client demographic characteristics,n,"",n,"",n,"",y,PHQ-8 was measured in clients,n,"",n,"",n,"",the transformer model predicted items subscales from different clinical questionnaires,No,""
18,Imel 2024,Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning,Consensus,"",External data set,"Data were obtained from an online and mobile therapy app (Talkspace) for services provided
between 2014 and 2019.","","","","","","","","","","","",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",22,1,2024,Tool Development and Evaluation,"",Analysis of conversation transcripts,"","","","",Only fine-tuning,Mix of formal therapy methods,"Other: not sure, it just says transformer model",No users involved,"","","","","","","","",N,"","","",N,"","","",y,s,h,benchmark are ratings by humans on MISC and Topics (see appendix eTable 2). Task: classification of client messages into content types,y,s,h,benchmark are ratings by humans on CTRS (see appendix eTable 2),"",N,"","","",N,"","","","","","","","","","","","","","","","","",n,"",n,"",y,on-premise transformer model,y,see Table client demographic characteristics,n,"",n,"",n,"",y,PHQ-8 was measured in clients,n,"",n,"",n,"",the transformer model predicted items subscales from different clinical questionnaires,No,""
17,Naher 2024,Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues,Reviewer Two,Other: Bangladesh,External data set,"Kaggle (n.d.)  This dataset encompasses 80 distinct tags, each containing numerous conversational prompts and corresponding 
responses","","","","","","","",N,"",Y,"Privacy and confidentiality … conversations … may contain sensitive information … implement robust security measures to protect user privacy and ensure the confidentiality of conversations.
",Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",23,5,2024,Direct LLM performance evaluation (only via prompting),"","","","","","",Custom pipeline with integrated LLM which is only prompted,"Informal counseling (e.g., emotional support conversation)",Other: GPT family not specified,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N (if you consider the author him/herself an expert?),"","","",N,"","","","",Sentiment Analysis,B,H,(Benchamark Kaggle data set) sentiment analysis,Response Quality,B,H,Author rated himself .. (vested interests??),Word analysis/ Count,B,H,"","","","","","","","","","","","","","","","","","","","","","","","","",Yes,"One ethical concern associated 
with using ChatGPT as a mental health agent is the privacy 
and confidentiality of user data. Conversations with ChatGPT 
may contain sensitive information, and there is a risk of this 
data being compromised or misused. Researchers and devel
opers must implement robust security measures to protect 
user privacy and ensure the confidentiality of conversations"
17,Naher 2024,Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues,Richard Gaus,Other: Bangladesh,External data set,Kaggle mental health conversational dataset https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data,Internet data -- mental health Q&A,English,Other: unknown,Yes,Unknown,Unknown,"",n,"",n,"",Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23,5,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",sentiment analysis score,b,h,benchmark are responses in the kaggle dataset,non expert rating,b,h,benchmark are responses in the kaggle dataset,"","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Accuracy and reliability Accuracy and reliability of ChatG
PT’s responses in addressing mental health concerns should 
be validated. While ChatGPT can provide valuable support 
and information, there is a risk of it providing inaccurate 
or misleading advice. It is essential to validate ChatGPT’s 
responses through rigorous testing and validation processes 
to minimize the potential harm caused by misinformation, 
specially for the extra information and external resources 
provided by the ChatGPT.

Informed consent Additionally, obtaining informed consent 
from users engaging with ChatGPT as a mental health agent 
is crucial. Users should be made aware of the limitations of 
ChatGPT, the purpose of the interaction, and any potential 
risks involved. Providing users with clear information and 
options for opting out of the conversation can help ensure 
that their rights and autonomy are respected. However, at 
the same time, users can be reluctant to use chatbots for 
mental health which can create a dilemma for the specialist."
17,Naher 2024,Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues,Consensus,Other: Bangladesh,External data set,"Kaggle mental health conversational dataset https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data

Kaggle (n.d.)  This dataset encompasses 80 distinct tags, each containing numerous conversational prompts and corresponding responses",Internet data -- mental health Q&A,English,Other: unknown,Yes,Unknown,Unknown,"",N,"",n,"Privacy and confidentiality … conversations … may contain sensitive information … implement robust security measures to protect user privacy and ensure the confidentiality of conversations.
",Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23,5,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","",Other: ,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",sentiment analysis score,b,H,benchmark are responses in the kaggle dataset,Response Quality rated by non experts,b,H,"benchmark are responses in the kaggle dataset, but author rated himself (vested interests???)",Word analysis/ Count,more words,H,"","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"Accuracy and reliability Accuracy and reliability of ChatG
PT’s responses in addressing mental health concerns should 
be validated. While ChatGPT can provide valuable support 
and information, there is a risk of it providing inaccurate 
or misleading advice. It is essential to validate ChatGPT’s 
responses through rigorous testing and validation processes 
to minimize the potential harm caused by misinformation, 
specially for the extra information and external resources 
provided by the ChatGPT.

Informed consent Additionally, obtaining informed consent 
from users engaging with ChatGPT as a mental health agent 
is crucial. Users should be made aware of the limitations of 
ChatGPT, the purpose of the interaction, and any potential 
risks involved. Providing users with clear information and 
options for opting out of the conversation can help ensure 
that their rights and autonomy are respected. However, at 
the same time, users can be reluctant to use chatbots for 
mental health which can create a dilemma for the specialist."
14,Alanzi 2024,ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.,Reviewer Two,Other: Saudi Arabia,Self-collected data,"","","","","",Psychopathology,Other: ,"",N,"",N,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",7,10,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,"",Patients recruited in hospital or outpatient treatment facility,"","","","Other CBT techniques; Other: ACT, ET, MBCT, DBT",GPT-3.5 family,Yes,"","","","","","","Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","",Yes ,"Measures
were taken to ensure participant confidentiality and data
security, including anonymizing responses and storing
data on secure servers. A robust data anonymization
process to protect participants’ privacy. Personal identifiers
were removed at the time of data collection, and partici-
pants were assigned unique codes to ensure anonymity.",Yes ,"able 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
",Yes ,Table 3 and Table 4,"","","","",Yes ,patients have a diagnosed anxiety disorder ,"","","","",Yes ,"
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee","",Yes,"For addressing “privacy concerns”
• Enhanced encryption: Implementing end-to-end
encryption to ensure that all conversations remain
confidential and secure from unauthorized access.
• Transparent data policies: Clearly communicating
data usage policies to users, explaining how their
data is stored, used, and protected.
• Anonymization: Ensuring that all user data is anon-
ymized to protect individual identities and further
assure users of their privacy.
For addressing “ethical concerns”
• Ethical guidelines: Developing and adhering to strict
ethical guidelines for AI use in psychotherapy,
including obtaining informed consent and respecting
user autonomy.
• Human oversight: Incorporating human oversight
into AI interventions to ensure that ethical standards
are maintained and to provide support when needed.
• Ethics training: Providing training for developers and
practitioners on the ethical use of AI in mental health
care.
For addressing “lack of human connection” concern
• Empathy training: Improving the AI’s ability to simu-
late empathetic responses through advanced natural
language processing techniques.
• Hybrid models: Combining AI therapy with human
support, where AI handles routine interactions, and
human therapists intervene when deeper emotional
connections are required.
• User feedback: Regularly collecting and incorporat-
ing user feedback to make the AI interactions more
personalized and human-like."
14,Alanzi 2024,ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.,Richard Gaus,Other: Saudi Arabia,No dataset used for development or evaluation,"","","","","","","","",n,"",n,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",7,10,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility,399,"","",Mix of formal therapy methods,GPT-3.5 family,Yes,"",n,n,y,y,"","The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",user experience measures (see above),"","","","","","","","","","","","",n,"",n,"",n,"",y,"Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
",y,"section ""Perceptions across demographic groups""",n,"",n,"",n,"",n,"",n,"",y,"The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in need.","",No,""
14,Alanzi 2024,ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.,Consensus,Other: Saudi Arabia,No dataset used for development or evaluation,"","","","","",Other: ,"","",n,"",n,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",7,10,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility,399,"","",Mix of formal therapy methods,GPT-3.5 family,Yes,"",n,n,y,y,"","The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.

Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",y,"Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
",y,"Table 3 and Table 4
section ""Perceptions across demographic groups""",n,"",n,"",n,"",n,"",n,"",y,"
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee","",Yes,"For addressing “privacy concerns”
• Enhanced encryption: Implementing end-to-end
encryption to ensure that all conversations remain
confidential and secure from unauthorized access.
• Transparent data policies: Clearly communicating
data usage policies to users, explaining how their
data is stored, used, and protected.
• Anonymization: Ensuring that all user data is anon-
ymized to protect individual identities and further
assure users of their privacy.
For addressing “ethical concerns”
• Ethical guidelines: Developing and adhering to strict
ethical guidelines for AI use in psychotherapy,
including obtaining informed consent and respecting
user autonomy.
• Human oversight: Incorporating human oversight
into AI interventions to ensure that ethical standards
are maintained and to provide support when needed.
• Ethics training: Providing training for developers and
practitioners on the ethical use of AI in mental health
care.
For addressing “lack of human connection” concern
• Empathy training: Improving the AI’s ability to simu-
late empathetic responses through advanced natural
language processing techniques.
• Hybrid models: Combining AI therapy with human
support, where AI handles routine interactions, and
human therapists intervene when deeper emotional
connections are required.
• User feedback: Regularly collecting and incorporat-
ing user feedback to make the AI interactions more
personalized and human-like."
13,Jain 2024,Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses,Reviewer Two,India,Self-collected data,"Name: none specified. Description: Self-collected open-ended mental-health problem statements and corresponding ChatGPT- and human-generated responses evaluated by participants.
",Internet data -- mental health Q&A,English,Yes,No,Unselected,Lay people,"",N,"",N,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27,4,2024,Population survey,"",Client-facing application,Chatbot,General population,111 individuals (82 females and 29 males),"",Custom pipeline with integrated LLM which is only prompted,Other: Perception of AI-generated responses in mental health support,GPT-3.5 family,Yes,"",Y,N,Y,Y,"Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).","Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",quantification of effect,"","","","","","","","","","","","",N,"",Y,"Although proper care was taken that no 
triggering problem was used in the survey, the participants were also 
informed about the counseling services available at our university if the 
need arose. As the responses were circulated to experts, no such 
comment which was rejected on this basis (misleading or harmful) was 
included in the study.",Y,"All data collected was kept confidential and anonymous, ensuring complete privacy and data protection""",Y,"f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).",N,"",N,"",N,"",Y,"we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022)",N,"",N,"",N,"","",Yes,"""As the responses were circulated to experts, no such comment which was rejected on this basis (misleading or harmful) was included in the study."" ... 
""Although proper care was taken that no triggering problem was used in the survey, the participants were also informed about the counseling services available at our university if the need arose."""
13,Jain 2024,Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses,Richard Gaus,India,Self-collected data,"Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown.",Other: emotional experience descriptions,English,No,No,Unselected,Other: not applicable,"",n,"",n,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27,4,2024,Population survey,"",Client-facing application,Chatbot,"Other: young adults, convenience sampling, online forms and in-person visits to classrooms",111,"","","Informal counseling (e.g., emotional support conversation)","Other: They say ""ChatGPT"". Not clear which version. Most likely 3.5 because they mention ""Building on our previous exploration of GPT-3.5, one of the most advanced Natural Language Processing (NLP) technologies, this follow-up study aims to understand the perception dynamics of AI-generated responses in the realm of mental health support for young people.""",Yes,"",n,n,y,y,"","Users rated AI responses more authentic, professional, and prarctical when blinded. when unblinded, the rated human responses higher (except professionalism).","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",No,""
13,Jain 2024,Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses,Consensus,India,Self-collected data,"Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown.

Name: none specified. Description: Self-collected open-ended mental-health problem statements and corresponding ChatGPT- and human-generated responses evaluated by participants.",Other: emotional experience descriptions,English,Yes,No,Unselected,Trained professionals,"",N,"",N,"",Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27,4,2024,Population survey,"",Client-facing application,Chatbot,General population,111,"",Other: ,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,"",Y,n,y,y,"Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).","Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",n,"",n,"",n,"f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).",N,"",N,"",N,"",n,"we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022)",N,"",N,"",N,"","",Yes,"""As the responses were circulated to experts, no such comment which was rejected on this basis (misleading or harmful) was included in the study."" ... 
""Although proper care was taken that no triggering problem was used in the survey, the participants were also informed about the counseling services available at our university if the need arose."""
9,Heston 2023,Safety of Large Language Models in Addressing Depression.,Reviewer Two,USA,Self-collected data,"","","","","","","","",N,"",N,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",18,12,2023,Direct LLM performance evaluation (only via prompting),"",Client-facing application,"",No clients/patients involved,"","",Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",First refferal to a human,"","","",Shutdown of conversation,"","","","","","","","",y,"",y,"","","","","","","","","","","","","","","","","","","","",Yes,"
The findings suggest that LLMs may not consistently detect and address hazardous psychological states. The
mean points at which conversations were terminated corresponded with severe depression scores on the
PHQ-9 scale, a level of impairment that often mandates immediate intervention to avert self-harm [22].
LLMs that extend risky conversations could consequently imperil users.

To augment patient safety, stringent testing and oversight of LLM applications in the mental health domain
are essential. Several questions remain unresolved: does perpetuating conversations after identifying high-
risk behavior attenuate or exacerbate the likelihood of self-harm? Does the enhanced accessibility provided
by cost-free, online AI agents alleviate or worsen mental health conditions? Are individuals more
predisposed to divulge personal information to an AI agent than a human mental health professional in a
face-to-face encounter? How can the capabilities of LLMs be safely optimized for mental health treatment?

While conversational AI exhibits promising capabilities, compelling
evidence indicates that it may advance at a pace that outstrips associated safety measures [28]. While the
rapid progress in innovation is undeniably fascinating, it is imperative for researchers and developers to
proactively prioritize ethical considerations pertaining to transparency, explainability, bias mitigation, user
privacy, system access controls, and the micro-targeting of vulnerable populations."
9,Heston 2023,Safety of Large Language Models in Addressing Depression.,Richard Gaus,USA,No dataset used for development or evaluation,"""dataset"" were PHQ-9 questions","","","","","","","",n,"",n,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",18,12,2023,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","","","Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",safety (number of conversations turns until initial referral/shutdown of chatbot),no benchmark,no benchmark,no benchmark,"","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,Whole study was about safety. Specifically about referral to human experts on self-harm risk.
9,Heston 2023,Safety of Large Language Models in Addressing Depression.,Consensus,USA,No dataset used for development or evaluation,"""dataset"" were PHQ-9 questions","","","","","","","",N,"",N,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",18,12,2023,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,No clients/patients involved,"","",Other: ,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",safety (number of conversations turns until initial referral/shutdown of chatbot),"","",no benchmark,"","","","","","","","","",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","",Yes,"
The findings suggest that LLMs may not consistently detect and address hazardous psychological states. The
mean points at which conversations were terminated corresponded with severe depression scores on the
PHQ-9 scale, a level of impairment that often mandates immediate intervention to avert self-harm [22].
LLMs that extend risky conversations could consequently imperil users.

To augment patient safety, stringent testing and oversight of LLM applications in the mental health domain
are essential. Several questions remain unresolved: does perpetuating conversations after identifying high-
risk behavior attenuate or exacerbate the likelihood of self-harm? Does the enhanced accessibility provided
by cost-free, online AI agents alleviate or worsen mental health conditions? Are individuals more
predisposed to divulge personal information to an AI agent than a human mental health professional in a
face-to-face encounter? How can the capabilities of LLMs be safely optimized for mental health treatment?

While conversational AI exhibits promising capabilities, compelling
evidence indicates that it may advance at a pace that outstrips associated safety measures [28]. While the
rapid progress in innovation is undeniably fascinating, it is imperative for researchers and developers to
proactively prioritize ethical considerations pertaining to transparency, explainability, bias mitigation, user
privacy, system access controls, and the micro-targeting of vulnerable populations."
3,Alanezi 2024,Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,Reviewer Two,Other: Saudi Arabia,Self-collected data,"24 patients were selected to participate in the study. Before participating in an interview, the patients were thoroughly 
informed of the study’s objectives and given the opportunity to provide informed consent during their outpatient visit. 
Following approval, patients used ChatGPT at home for two weeks to seek mental-health support in their own ways. The 
participants were required to interact with ChatGPT3 for a minimum of 15 minutes per day for 14 days. Twenty to thirty 
interviews were regarded an appropriate sample size for qualitative investigations, particularly those employing inter­
views as a method of data collection.
","","","","",Psychopathology,"","",N,"",N?,"issues such as data privacy, 
consent, confidentiality, and the potential for biases in AI algorithms are the few challenges raised by most of the 
participants. These are inferred from the following statements",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",31,01,2024,Population survey,"",Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,24,"","","CBT: Cognitive restructuring; Psychodynamic psychotherapy; Systemic therapy; Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Other: ChatGBT as a therapist",GPT-3.5 family,Yes,"","","","","","","semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT 
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Yes ,Table S1 and Appendix B,"","","","","","","","","","","","","","","",Yes,"there may be safety concerns in providing mental health support through an AI tool. For example, in 
cases of crisis or emergencies, ChatGPT may not be able to provide appropriate and timely interventions, and patients may 
require immediate human intervention. As an AI-powered tool, ChatGPT lacks the human touch and emotional connection 
that can be important in mental health support.
59 
Building rapport, establishing trust, and providing empathy, which are 
crucial in therapeutic relationships, may be challenging for an AI model like ChatGPT."
3,Alanezi 2024,Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,Richard Gaus,Other: Saudi Arabia,No dataset used for development or evaluation,"","","","","","","","",n,"",n,"",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",31,1,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility,24,"","","Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,"",n,y,n,y,"","Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. ""It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information.""","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",y,"Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B",n,"",n,"",n,"",n,"",n,"",n,"",n,"","Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory",Yes,"There may be safety concerns in providing mental health support through an AI tool. For example, in
cases of crisis or emergencies, ChatGPT may not be able to provide appropriate and timely interventions, and patients may
require immediate human intervention."
3,Alanezi 2024,Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,Consensus,Other: Saudi Arabia,No dataset used for development or evaluation,"","","","","",Other: ,"","",n,"",n,"issues such as data privacy, 
consent, confidentiality, and the potential for biases in AI algorithms are the few challenges raised by most of the 
participants. These are inferred from the following statements",Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",31,1,2024,Direct LLM performance evaluation (only via prompting),"",Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility,24,"","","Unspecified, might include formal therapy methods",GPT-3.5 family,Yes,"",n,y,n,y,"","Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. ""It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information.""

semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",y,"Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B",n,"",n,"",n,"",n,"",n,"",n,"",n,"","Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory


Therapy type: undirected therapy resulting from interacting with ChatGPT with no prespecified initial prompt",Yes,"there may be safety concerns in providing mental health support through an AI tool. For example, in 
cases of crisis or emergencies, ChatGPT may not be able to provide appropriate and timely interventions, and patients may 
require immediate human intervention. As an AI-powered tool, ChatGPT lacks the human touch and emotional connection 
that can be important in mental health support.
59 
Building rapport, establishing trust, and providing empathy, which are 
crucial in therapeutic relationships, may be challenging for an AI model like ChatGPT."
