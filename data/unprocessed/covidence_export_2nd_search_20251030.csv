Covidence #,Study ID,Title,Reviewer Name,Publication Outlet Type,Field of Publication Outlet,Day (1-31),Month (1-12),Year (2017-2025),Country of first first author affiliation,Study Type,Application Type,Application Subtype,Type of Clients or Patients,Number of Clients or Patients Included,Application Subtype,Data Set Source,Notes on data set,Type of data set,Language of data set,Does data set contain synthetic data?,Is data set public?,Did users have actual psychopathology or were they unselected/people without mental health conditions?,Were responders trained professionals or lay people/unknown?,If Study Type == Empirical research involving an LLM: Development Approach,Notes on development approach,Intervention Type,Models Employed,User Experience Assessment,User Experience Assessment Methods:,"Uses some standard, established user experience instrument Y/N",Uses qualitative assessment Y/N,Uses quantitative assessment Y/N,Were results of user experience assessment reported? Y/N,User Experience Assessment Instrument,Other User Experience Assessment Notes,Reference-based metrics:,Lexical Overlap Used (Y/N),Lexical Overlap How it compares against benchmark (B/S/W),Lexical Overlap Benchmark quality (H/L),Lexical Overlap Notes on benchmark quality,Embedding Similarity Used (Y/N),Embedding Similarity How it compares against benchmark (B/S/W),Embedding Similarity Benchmark quality (H/L),Embedding Similarity Notes on benchmark quality,Classification Used (Y/N),Classification How it compares against benchmark (B/S/W),Classification Benchmark quality (H/L),Classification Notes on benchmark quality,Continuous Value Metrics Used (Y/N),Continuous Value Metrics How it compares against benchmark (B/S/W),Continuous Value Metrics Benchmark quality (H/L),Continuous Value Metrics Notes on benchmark quality,Contentual judgment:,Expert Rating Used (Y/N),Expert Rating How it compares against benchmark (B/S/W),Expert Rating Benchmark quality (H/L),Expert Rating Notes on benchmark quality,LLM as a judge Used (Y/N),LLM as a judge How it compares against benchmark (B/S/W),LLM as a judge Benchmark quality (H/L),LLM as a judge Notes on benchmark quality,Automatic metrics:,Perplexity Used (Y/N),Perplexity How it compares against benchmark (B/S/W),Perplexity Benchmark quality (H/L),Perplexity Notes on benchmark quality,Lexical diversity Used (Y/N),Lexical diversity How it compares against benchmark (B/S/W),Lexical diversity Benchmark quality (H/L),Lexical diversity Notes on benchmark quality,Other metrics:,Metric 1 Name of metric,Metric 1 How it compares against benchmark (B/S/W),Metric 1 Benchmark quality (H/L),Metric 1 Notes on benchmark quality,Metric 2 Name of metric,Metric 2 How it compares against benchmark (B/S/W),Metric 2 Benchmark quality (H/L),Metric 2 Notes on benchmark quality,Metric 3 Name of metric,Metric 3 How it compares against benchmark (B/S/W),Metric 3 Benchmark quality (H/L),Metric 3 Notes on benchmark quality,READI categories:,S-1 Risk-detection Considered in tool design? (Y/N),S-1 Risk-detection if YES: Notes (paste text passage),S‑2 Content‑safety evaluation conducted Considered in tool design? (Y/N),S‑2 Content‑safety evaluation conducted if YES: Notes (paste text passage),P-1 On-premise-capable model Considered in tool design? (Y/N),P-1 On-premise-capable model if YES: Notes (paste text passage),P-2 Privacy/confidentiality awareness Considered in tool design? (Y/N),P-2 Privacy/confidentiality awareness if YES: Notes (paste text passage),E-1 Reporting of demographic information Considered in tool design? (Y/N),E-1 Reporting of demographic information if YES: Notes (paste text passage),E-2 Outcomes reported by demographic subgroup Considered in tool design? (Y/N),E-2 Outcomes reported by demographic subgroup if YES: Notes (paste text passage),G‑1 Early‑discontinuation data reported Considered in tool design? (Y/N),G‑1 Early‑discontinuation data reported if YES: Notes (paste text passage),G‑2 Over‑use reported or prevented Considered in tool design? (Y/N),G‑2 Over‑use reported or prevented if YES: Notes (paste text passage),F‑1 Validated clinical outcome measures used Considered in tool design? (Y/N),F‑1 Validated clinical outcome measures used if YES: Notes (paste text passage),F‑2 Control condition present Considered in tool design? (Y/N),F‑2 Control condition present if YES: Notes (paste text passage),I‑1 Multilevel feasibility/acceptability data collected Considered in tool design? (Y/N),I‑1 Multilevel feasibility/acceptability data collected if YES: Notes (paste text passage),I‑2 Healthcare‑integration considerations addressed Considered in tool design? (Y/N),I‑2 Healthcare‑integration considerations addressed if YES: Notes (paste text passage),Notes
258,Siddals 2024,"""It happened to be the perfect thing"": experiences of generative AI chatbots for mental health.",Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",27,10,2024,UK,Population survey,Client-facing application,Multi-turn chatbot,General population,19,"",No dataset used for development or evaluation,"","","","","","","","Other: no model development, qualitative interviews only","Study observes consumer LLM chatbots in the wild (Pi, ChatGPT, Copilot, Kindroid, ChatMind/VOS), not building a new system. ","Unspecified, might include formal therapy methods","ChatGPT, model unspecified; Other: Pi, Copilot, Kindroid, ChatMind/VOS ",Yes,"",N,Y,N,Y,""," range of positive impacts were reported, including improved mood… healing from trauma and loss… improved relationships…

Participants told us that generative AI feels like an emotional sanctuary, offers insightful guidance… joy of connection… bears comparison with human therapy.","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"Nineteen participants (12 male, 7 female)… age 17 to 60… eight countries…","","","","","","","","","","","","","","",""
258,Siddals 2024,"""It happened to be the perfect thing"": experiences of generative AI chatbots for mental health.",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27,10,2024,UK,Population survey,Client-facing application,Multi-turn chatbot,General population,19,"",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified; Other: Pi, Copilot, Kindroid, ChatMind",Yes,"",n,y,n,y,"",Qualitative thematic coding of interview results,"",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
258,Siddals 2024,"""It happened to be the perfect thing"": experiences of generative AI chatbots for mental health.",Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",27,10,2024,UK,Population survey,Client-facing application,Multi-turn chatbot,General population,19,"",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified; Other: Pi, Copilot, Kindroid, ChatMind/VOS ",Yes,"",N,Y,N,Y,""," range of positive impacts were reported, including improved mood… healing from trauma and loss… improved relationships…

Participants told us that generative AI feels like an emotional sanctuary, offers insightful guidance… joy of connection… bears comparison with human therapy.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
251,Xiao 2024,Healme: Harnessing cognitive reframing in large language models for psychotherapy,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,08,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,6,"","External data set, modified","In this study, we leverage an existing raw dataset
focused on cognitive reframing and expand it to
include multiple rounds of dialogue. Specifically,
we conduct a manual review of the selected raw
dataset and select 1,000 well-composed pairs of
(thinking trap, client’s thought) from it.
The raw dataset we utilize in this study is intro-
duced by Maddela et al. (2023). 
https://aclanthology.org/2023.acl-long.763.pdf

Creation of HealMe dataset ",Emotional support dialogue -- chat logs,English,Yes,Yes,Other: Not applicable; AI generated,Other: Not applicable; AI generated,Only fine-tuning,"approach by
selecting the open-source LLM, LLaMA2-7b-chat,
as its base and fine-tuning it to ensure the model
consistently maintains the role of a psychotherapist
with high empathy and guidance capabilities.",CBT: Cognitive restructuring,Llama 2 family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,B,L,Table 4; Benchmarks are ChatGLM3-6b and LLaMA2-7b-chat,Y,-,-,-,"",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",Y,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
251,Xiao 2024,Healme: Harnessing cognitive reframing in large language models for psychotherapy,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,8,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,8,"","External data set, modified","HealMe dataset
https://github.com/elsa66666/HealMe",Emotional support dialogue -- chat logs,English,Yes,Yes,Other: not applicable,Other: not applicable,Only fine-tuning,HealMe = fine-tuned Llama2-7B-Chat,CBT: Cognitive restructuring,Llama 2 family,No,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,b,l,"benchmarks: ChatGLM3-6b, Llama2-7b-Chat",y,no benchmark,no benchmark,Only training data is scored using GPT-4 LLM-as-a-judge (table 2),"",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",y,"Since our testing phases involve real-person clients, we exclusively use offline models to protect user privacy. In our study involving real-person clients, we adhere to the Right to Withdraw (Association et al., 2017), ensuring that participants can withdraw at any time if they experience any discomfort",n,"",n,"",n,"",n,"",y,PANAS,y,two people not undergoing intervention,n,"",n,"",""
251,Xiao 2024,Healme: Harnessing cognitive reframing in large language models for psychotherapy,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,8,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,8,"","External data set, modified","HealMe dataset
https://github.com/elsa66666/HealMe

In this study, we leverage an existing raw dataset
focused on cognitive reframing and expand it to
include multiple rounds of dialogue. Specifically,
we conduct a manual review of the selected raw
dataset and select 1,000 well-composed pairs of
(thinking trap, client’s thought) from it.
The raw dataset we utilize in this study is intro-
duced by Maddela et al. (2023). 
https://aclanthology.org/2023.acl-long.763.pdf

Creation of HealMe dataset ",Emotional support dialogue -- chat logs,English,Yes,Yes,Other: not applicable,Other: not applicable,Only fine-tuning,"HealMe = fine-tuned Llama2-7B-Chat

approach by
selecting the open-source LLM, LLaMA2-7b-chat,
as its base and fine-tuning it to ensure the model
consistently maintains the role of a psychotherapist
with high empathy and guidance capabilities.",CBT: Cognitive restructuring,Llama 2 family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,B,L,Table 4; Benchmarks are ChatGLM3-6b and LLaMA2-7b-chat,Y,no benchmark,no benchmark,Only training data is scored using GPT-4 LLM-as-a-judge (table 2),"",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",Y,"",N,"",N,"",N,"",N,"",N,"",y,PANAS,y,two people not undergoing intervention,N,"",N,"",""
232,G 2024,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",03,06,2024,USA,Empirical research involving an LLM,Other: unsure,"",No clients/patients involved,"","",External data set,"We conduct extensive experiments on datasets derived from
real-world MI sessions addressing alcohol usage disorder. We
examine three prompting baselines and experiment with four
state-of-the-art auto-regressive LLMs, including Llama2 [12],
Falcon [20], Mistral [21], and ChatGPT [22]. ",Other: unclear,Other: unclear,Other: unclear,No,Unknown,Unknown,Only prompting,"CoI breaks MI coding into three sequential prompt stages (Interaction Definition, Involvement Assessment, Valence Analysis) to mimic professional coders using MISC schema.","Unspecified, might include formal therapy methods","Other: Llama2-13B-Chat [12], Falcon-7B-Instruct [20], Mistral-7B-
Instruct [21], and ChatGPT [22)",No users involved,"","","","","","","","","","","","","","","","",Y,B,L,"Quantitative F1 decrease in ablation study shows impact of each stage.
","","","","","","","","","","","","","","","","","","","","","","","","ablation performance comparison
??",B,L,"Performance drop when removing CoI stages (ID, IA, VA).
","","","","","","","","","","","","","",Y (mostly),"Llama2-13B-Chat, Falcon-7B-Instruct, Mistral-7B-Instruct (open-source).","","","","","","","","","","","","",Y,"""Compared to baseline prompting methods (Zeroshot, Few-shot, ZeroCoT).""
","","","","",""
232,G 2024,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3,6,2024,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"","","","",External data set,"Motivational interviewing session transcripts from Borsari et al., “In-session processes of brief motivational interventions in two trials with mandated college students.” Journal of consulting and clinical psychology, vol. 83, pp. 56–67, 2 2015",Psychotherapy -- speech transcripts,English,No,No,Unselected,Trained professionals,Only prompting,"Different styles of prompting: Zero-shot, few-shot, CoT",CBT: Motivational interviewing,GPT-3.5 family; Llama 2 family; Mistral family; Other: Falcon-7B,No users involved,"","","","","","","","",n,"","","",n,"","","",y,b,l,Metric: Micro- and macro-F1. Benchmark: zero- and few-shot prompting with the same LLMs.,n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
232,G 2024,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3,6,2024,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved,"","",External data set,"We conduct extensive experiments on datasets derived from
real-world MI sessions addressing alcohol usage disorder. We
examine three prompting baselines and experiment with four
state-of-the-art auto-regressive LLMs, including Llama2 [12],
Falcon [20], Mistral [21], and ChatGPT [22]. 

Motivational interviewing session transcripts from Borsari et al., “In-session processes of brief motivational interventions in two trials with mandated college students.” Journal of consulting and clinical psychology, vol. 83, pp. 56–67, 2 2015",Psychotherapy -- speech transcripts,English,No,No,Unselected,Trained professionals,Only prompting,"CoI breaks MI coding into three sequential prompt stages (Interaction Definition, Involvement Assessment, Valence Analysis) to mimic professional coders using MISC schema.

Different styles of prompting: Zero-shot, few-shot, CoT",CBT: Motivational interviewing,GPT-3.5 family; Llama 2 family; Mistral family; Other: Falcon-7B,No users involved,"","","","","","","","",n,"","","",n,"","","",y,b,l,"Metric: Micro- and macro-F1. Benchmark: zero- and few-shot prompting with the same LLMs.

Quantitative F1 decrease in ablation study shows impact of each stage.
",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","",Y,"Llama2-13B-Chat, Falcon-7B-Instruct, Mistral-7B-Instruct (open-source).","","","","","","","","","","","","",Y,"""Compared to baseline prompting methods (Zeroshot, Few-shot, ZeroCoT).""
","","","","",""
230,DeDuro 2025,"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans",Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",31,01,2025,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"HOPE (Malhotra, 2022)",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Only prompting,"Two adverserial LLMs generate new counseling dataset. 
Prompted ChatGPT-3.5, Claude Haiku and LLaMAntino to role-play therapist and patient in depression sessions; compared against human HOPE dataset using text-network metrics. ",Other CBT techniques,GPT-3.5 family; Claude family,No users involved,"","","","","","","","","","","","","","","","","","","","",Y,B,L,"Network centrality and average distance values reported (Table 1 and 2).
","","","","","","","","","","","","","","",Y,B,L,"Captured via semantic richness (degree) in Table 2.
","","","","","","","","","","","","","","","","","","",Y(partly),LLaMAntino (Italian LLaMA 2 model) is,"","","","","","","","","","","","","","","","","","","Interestingly, Haiku and ChatGPT, while playing either therapists or 
patients and speaking in English, rarely use jargon expressing anger or 
disgust. Instead, human patients express jargon eliciting disgust and 
anger at higher rates, compatible with random expectations. This dif­
ference could be due to English LLMs might have been fine-tuned to 
more strongly avoid using a negative outlook on things."
230,DeDuro 2025,"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans",Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",31,1,2025,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,HOPE dataset,Emotional support dialogue -- speech transcripts,English,No,Yes,Unselected,Unknown,Only prompting,"","Unspecified, might include formal therapy methods",GPT-3.5 family; Claude family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",various text analyses,unclear,h,benchmark: human therapist responses,various network analyses,unclear,h,benchmark: human therapist responses,"","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
230,DeDuro 2025,"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans",Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",31,1,2025,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"HOPE (Malhotra, 2022)",Emotional support dialogue -- speech transcripts,English,No,Yes,Unknown,Unknown,Only prompting,"Two adverserial LLMs generate new counseling dataset. 
Prompted ChatGPT-3.5, Claude Haiku and LLaMAntino to role-play therapist and patient in depression sessions; compared against human HOPE dataset using text-network metrics. ","Unspecified, might include formal therapy methods",GPT-3.5 family; Llama 2 family; Claude family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",various text analyses,unclear,h,benchmark: human therapist responses,"","","","",network centrality (distance from depression terms),s,h,Network centrality and average distance values reported (Table 1 and 2).,"",n,"",n,"",y,LLaMAntino (Italian LLaMA 2 model) is,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","Interestingly, Haiku and ChatGPT, while playing either therapists or 
patients and speaking in English, rarely use jargon expressing anger or 
disgust. Instead, human patients express jargon eliciting disgust and 
anger at higher rates, compatible with random expectations. This dif­
ference could be due to English LLMs might have been fine-tuned to 
more strongly avoid using a negative outlook on things."
228,Qiu 2023,Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,11,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,SMILECHAT based on bootstrapped PsyTest and source PsyQA ,Emotional support dialogue -- chat logs,Chinese,Yes,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,Uses ChatGPT prompting to synthesize SMILECHAT from PsyQA single-turn items; topics auto-labeled with Qwen1.5-110B-Chat; trains MeChat via parameter-efficient LoRA fine-tuning on ChatGLM2-6B. ,"Unspecified, might include formal therapy methods","GPT-3.5 family; Qwen family; Other: ChatGLM2-6B, ",No users involved,"","","","","","","","",Y,B,L,"BLEU-1/2/3, ROUGE-L improved after fine-tuning",Y,B,L,BERTScore used and increased; also pairwise cosine similarity for semantic diversity analysis,"","","","",Y,B,L,Information entropy of dialogue topics… SMILE 15.02 vs standard 8.40,"",N,"","","","","","","","","","","","",Y,B,L,"Tab. 5

Distinct-1/2/3… Fine-tuned 82.08/95.74/97.81 vs Baseline 52.95/80.74/90.17… §5.1–5.3","",N,"","","","","","","","","","","","","","","","",Y,"fine-tuning experiment on ChatGLM2-6B (open, self-hostable)","","","","","","","","","","","","","","","","","","",""
228,Qiu 2023,Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,11,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","The study uses the PsyQA dataset and creates a new one called SMILECHAT.

https://github.com/qiuhuachuan/smile",Emotional support dialogue -- chat logs,Chinese,Yes,Yes,Other: not applicable,Other: not applicable,Only fine-tuning,fine-tunes a ChatGLM2-6B model on synthetically generated data,"Unspecified, might include formal therapy methods",Other: ChatGLM2-6B,No users involved,"","","","","","","","",y,b,l,benchmark is some baseline model,y,b,l,benchmark is some baseline model,n,"","","",n,"","","","",y,w,h,benchmark are responses of human counselors in PsyTest,n,"","","","",n,"","","",y,b,l,benchmark is some baseline model,"","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
228,Qiu 2023,Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,11,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","The study uses the PsyQA dataset and creates a new one called SMILECHAT.

https://github.com/qiuhuachuan/smile",Emotional support dialogue -- chat logs,Chinese,Yes,Yes,Other: not applicable,Other: not applicable,Only fine-tuning,fine-tunes a ChatGLM2-6B model on synthetically generated data,"Unspecified, might include formal therapy methods",Other: ChatGLM2-6B,No users involved,"","","","","","","","",Y,b,L,"BLEU-1/2/3, ROUGE-L improved after fine-tuning

benchmark is some baseline model",Y,b,L,"BERTScore used and increased; also pairwise cosine similarity for semantic diversity analysis

benchmark is some baseline model",n,"","","",n,"","","","",y,w,h,benchmark are responses of human counselors in PsyTest,n,"","","","",n,"","","",Y,B,L,"Tab. 5

Distinct-1/2/3… Fine-tuned 82.08/95.74/97.81 vs Baseline 52.95/80.74/90.17… §5.1–5.3","","","","","","","","","","","","","","",n,"",n,"",y,"fine-tuning experiment on ChatGLM2-6B (open, self-hostable)",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
226,Adhikary 2024,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23,7,2024,India,Empirical research involving an LLM,Analysis of conversation transcripts,"","","","",External data set,"To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Only prompting,"","Unspecified, might include formal therapy methods",BERT family; T5 family; GPT-2 family; Llama 2 family; Mistral family; Other: Phi-2,No users involved,"","","","","","","","",Y,"","","",Y,"","","","","","","","","","","","",Y,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
219,Maurya 2025,Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",25,04,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"ChatGPT responses collected for 21 psychoeducational prompts across categories (e.g., depression, substance use, wellness); evaluated on six criteria (accuracy, clarity, relevance, empathy, engagement, ethics) via qualitative content analysis (QCA) and Likert ratings.","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","",Y ,none,H,"“reviewers assessed ChatGPT’s psychoeducational responses …” 
","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
219,Maurya 2025,Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",25,4,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,Prompting of ChatGPT with mental health related questions (Table 1),"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,no benchmark,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
219,Maurya 2025,Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",25,4,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"ChatGPT responses collected for 21 psychoeducational prompts across categories (e.g., depression, substance use, wellness); evaluated on six criteria (accuracy, clarity, relevance, empathy, engagement, ethics) via qualitative content analysis (QCA) and Likert ratings.

Prompting of ChatGPT with mental health related questions (Table 1)","Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,"“reviewers assessed ChatGPT’s psychoeducational responses …” 
",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
217,Izmaylov 2023,Combining Psychological Theory with Language Models for Suicide Risk Detection,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2,5,2023,Other: Israel,Empirical research involving an LLM,"","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
214,Chen 2023,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",6,12,2023,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved,"","",External data set,Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.,Other: labeled cognitive distortions,English,No,No,Unknown,Other: not applicable,Only prompting,"",CBT: Cognitive restructuring,GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,b,l,f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.,n,"","","","",y,no benchmark,no benchmark,4.3 Human Evaluation Results,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
214,Chen 2023,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",6,12,2023,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved,"","",External data set,"We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Only prompting,"We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).",CBT: Cognitive restructuring,GPT-3.5 family; GPT-4 / GPT-4o family; Other: Vicuna,No users involved,"","","","","","","","","","","","","","","","",Y,"","","","","","","","",Y,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
214,Chen 2023,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",6,12,2023,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved,"","",External data set,"We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset

Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.

https://www.kaggle.com/datasets/sagarikashreevastava/cognitive-distortion-detetction-dataset",Other: labeled cognitive distortions,English,No,Yes,Unknown,Other: not applicable,Only prompting,"We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).",CBT: Cognitive restructuring,GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,No users involved,"","","","","","","","",n,"","","",n,"","","",Y,b,l,f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.,n,"","","","",y,no benchmark,no benchmark,4.3 Human Evaluation Results,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
212,C 2024,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14,04,2024,Other: Czech Republic,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,https://alexanderstreet.com/,Psychotherapy -- speech transcripts,English,No,Yes,Other: all of the above,Other: all of the above,Fine-tuning + other modules,"to fine-tune the generated instruction
data effectively, we employed the inhibition adaption fine-
tuning method [14] and self-RAG [13] on Llama2-7B [15],
as well as ChatGLM2-6B [16].","Unspecified, might include formal therapy methods",Llama 2 family; Other: ChatGLM2-7,No users involved,"","","","","","","","",Y,B,L,only the inhibited LoRA Finetuning,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Fluency,B,L,"","","","","","","","","","","","","","",Y,on Llama2-7B… as well as ChatGLM2-6B,"","","","","","","","","","","","","","","","","","",""
212,C 2024,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14,4,2024,Other: Czech Republic,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Alexander Street Press therapy transcripts,Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,Fine-tuning + other modules,Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4),"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family; Other: ChatGLM2-6B,No users involved,"","","","","","","","",y,no benchmark,no benchmark,Rouge comparison to Alexander Street Press reference. No benchmark.,n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,"human rating (read, prof, match)",n,"","","","",n,"","","",n,"","","","",Fluency,no benchmark,no benchmark,Not clear what fluency is. The reference paper measures fluency via human rating but the authors of this paper state this is an automatic assessment.,"","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
212,C 2024,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14,4,2024,Other: Czech Republic,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Alexander Street Press therapy transcripts,Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,Fine-tuning + other modules,"Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4)

to fine-tune the generated instruction
data effectively, we employed the inhibition adaption fine-
tuning method [14] and self-RAG [13] on Llama2-7B [15],
as well as ChatGLM2-6B [16].","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family; Other: ChatGLM2-6B,No users involved,"","","","","","","","",Y,no benchmark,no benchmark,"Rouge comparison to Alexander Street Press reference. No benchmark.

only the inhibited LoRA Finetuning",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,"human rating (read, prof, match)",n,"","","","",n,"","","",n,"","","","",Fluency,no benchmark,no benchmark,Not clear what fluency is. The reference paper measures fluency via human rating but the authors of this paper state this is an automatic assessment.,"","","","","","","","","",n,"",n,"",y,on Llama2-7B… as well as ChatGLM2-6B,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
206,Kozłowski 2023,Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
206,Kozłowski 2023,Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
206,Kozłowski 2023,Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
202,J 2025,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",02,12,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","we collect question-answer pairs from open platforms… Yixinli… Zhihu… We crawl books related to psychology from the web and then use Qwen-72B to extract knowledge-based QA… After-school exercises from several books

There are several publicly accessible websites committed to
establishing a psychology platform and offering online solu-
tions for individuals seeking psychological assistance, such as
Yixinli3, Zhihu4, and so on. The data from these platforms
can be regarded as real-world inquiries and the solutions pro-
vided by professionals in response to these inquiries.
ubse-
quently, professional or experienced individuals offer detailed
solutions or advice in response to these questions. We collect
over 267 000 pairs of data from websites.
To accurately evaluate the model performance in the psy-
chology domain, we introduce a benchmark that psychological
professional individuals need to master. The data is sourced
from publicly available examination questions. We follow a standardized data preprocessing procedure. For some data, we
first need to perform optical character recognition (OCR) to
convert images into text. After that, we invite several students to
manually review the collected data to ensure its quality and con-
sistency with the original document. This process involves rec-
tifying formatting errors, eliminating duplicate questions, and
rectifying any instances of garbled characters. Our proposed
psychological benchmark draws inspiration from the format of
the most authoritative psychological counseling examination in
China, comprising two primary components: objective ques-
tions and subjective questions.",Emotional support dialogue -- speech transcripts,Chinese,No,Yes,Other: both,"Other: Trainded pros are for sure included, but others might also be. ",Fine-tuning + other modules,"Full-parameter supervised fine-tuning (1e-5 LR; batch 128; 3 epochs) on 8×A6000; pipeline for multi-turn data (generation → evidence judgment → refinement), plus teacher–student with and without RAG for knowledge QA","Unspecified, might include formal therapy methods",Qwen family,No users involved,"","","","","","","","",Y,B,L,ROUGE-1 and BLEU-4 highest for PsycoLLM; ROUGE-L slightly lower than EmoLLM,Y,B,L,Highest BERTScore among compared models.,Y,B,L,Highest average standard accuracy; >60% pass; elastic accuracy also strong,"","","","","","","","","","","","","","","","","","","","","","","",Elastic accuracy for MMCQ ??,"","","","","","","","","","","","","","","","",Y,Qwen is,"","","","","","","","","","","","","","","","","","",""
202,J 2025,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,Richard Gaus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",3,4,2025,China,Empirical research involving an LLM,Client-facing application,Other: clinical psychology-specific general-purpose LLM,No clients/patients involved,"","","External data set, modified","Data sourced from single-turn QA psychology platforms, multi-turn dialogue synthetically generated using the single-turn data, and psychological knowledge QA",Other: mixed: multi-turn and QA,Chinese,Yes,No,Unselected,Other: mixed,Only fine-tuning,Only fine-tuning of Qwen on their dataset,"Unspecified, might include formal therapy methods",Other: Qwen1.5-14B-Chat,No users involved,"","","","","","","","",y,b,l,"Metrics: Rouge-1, Rouge-L, BLEU-4. Benchmarks: Various other LLMs",y,b,l,Metrics: BERTScore. Benchmarks: Various other LLMs,y,b,l,"""Elastic accuracy"" on their QA dataset.",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
202,J 2025,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",02,12,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","Data sourced from single-turn QA psychology platforms, multi-turn dialogue synthetically generated using the single-turn data, and psychological knowledge QA

we collect question-answer pairs from open platforms… Yixinli… Zhihu… We crawl books related to psychology from the web and then use Qwen-72B to extract knowledge-based QA… After-school exercises from several books

There are several publicly accessible websites committed to
establishing a psychology platform and offering online solu-
tions for individuals seeking psychological assistance, such as
Yixinli3, Zhihu4, and so on. The data from these platforms
can be regarded as real-world inquiries and the solutions pro-
vided by professionals in response to these inquiries.
ubse-
quently, professional or experienced individuals offer detailed
solutions or advice in response to these questions. We collect
over 267 000 pairs of data from websites.
To accurately evaluate the model performance in the psy-
chology domain, we introduce a benchmark that psychological
professional individuals need to master. The data is sourced
from publicly available examination questions. We follow a standardized data preprocessing procedure. For some data, we
first need to perform optical character recognition (OCR) to
convert images into text. After that, we invite several students to
manually review the collected data to ensure its quality and con-
sistency with the original document. This process involves rec-
tifying formatting errors, eliminating duplicate questions, and
rectifying any instances of garbled characters. Our proposed
psychological benchmark draws inspiration from the format of
the most authoritative psychological counseling examination in
China, comprising two primary components: objective ques-
tions and subjective questions.",Other: mixed: multi-turn and QA,Chinese,Yes,No,Unselected,Other: mixed,Only fine-tuning,Only fine-tuning of Qwen on their dataset,"Unspecified, might include formal therapy methods",Qwen family,No users involved,"","","","","","","","",Y,B,L,"Metrics: Rouge-1, Rouge-L, BLEU-4. Benchmarks: Various other LLMs
ROUGE-1 and BLEU-4 highest for PsycoLLM; ROUGE-L slightly lower than EmoLLM",Y,B,L,"Metrics: BERTScore. Benchmarks: Various other LLMs
Highest BERTScore among compared models.",Y,B,L,"""Elastic accuracy"" on their QA dataset.
Highest average standard accuracy; >60% pass; elastic accuracy also strong",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,Qwen is,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
193,H 2024,Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?,Reviewer Two,"","",03,12,2024,China,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved,"","",Self-collected data,"The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.",Emotional support dialogue -- speech transcripts,Other: English and Chinese,No,No,Unknown,Unknown,Prompting + other modules,constructed a CBT-related knowledge base and integrated it with the general models … RAG (Retrieval-Augmented Generation),Other CBT techniques,"GPT-3.5 family; Other: ERNIE-3.5-8K,
iFlytek Spark V3.0 and ChatGLM-3-turbo,",No users involved,"","","","","","","","",Y,B,L,ROUGE/METEOR improved esp. for ChatGPT; mixed for others.,Y,B,L,"BERTScore higher with KB, esp. ChatGPT.","","","","",Y,S,L,BLEURT ↑ with KB; BartScore ↓,"","","","","","","","","","","","","","","","","","","",Sentiment score & PQA ,B,L,"","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
193,H 2024,Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?,Richard Gaus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3,12,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.",Emotional support dialogue -- speech transcripts,Other: Chinese and English mixed,"Other: mix of ""authentic"" and role-played",No,Unknown,Unknown,Prompting + other modules,Prompting with and without RAG,"Unspecified, might include formal therapy methods","GPT-3.5 family; Other: Ernie-3.5-8K, iFlytek Spark V3.0, ChatGLM-3-turbo",No users involved,"","","","","","","","",y,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,no benchmark,n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",y,no benchmark,no benchmark,DISTINCT score. no benchmark,"",BARTScore,no benchmark,no benchmark,no benchmark,BLEURT,no benchmark,no benchmark,no benchmark,"","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
193,H 2024,Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?,Consensus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3,12,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.",Emotional support dialogue -- speech transcripts,Other: English and Chinese,"Other: mix of ""authentic"" and role-played",No,Unknown,Unknown,Prompting + other modules,"Prompting with and without RAG

constructed a CBT-related knowledge base and integrated it with the general models … RAG (Retrieval-Augmented Generation)",Other CBT techniques,"GPT-3.5 family; Other: ERNIE-3.5-8K,
iFlytek Spark V3.0, ChatGLM-3-turbo",No users involved,"","","","","","","","",y,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,no benchmark,n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",y,no benchmark,no benchmark,DISTINCT score. no benchmark,"",BARTScore,no benchmark,no benchmark,no benchmark,BLEURT,no benchmark,no benchmark,no benchmark,"","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
186,Maurya 2024,Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",29,12,2023,USA,Other: Exploratory/pedagogical/model paper (not an empirical evaluation with recruited participants) ,Therapist-facing application,"","","",Patient simulations,No dataset used for development or evaluation,"","","","","","","",Only prompting,"Model was prompted to behave like a counseling client: It was then tested in a fishbowl, triad, and one-to-one formats using facilitation commands (“play,” “pause,” “rewind”) and three processing techniques (“ChatGPT close up,” “counselor close up,” “observer close up”) grounded in Bernard’s Discrimination Model (1979)","Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",No users involved,"","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
186,Maurya 2024,Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",29,12,2023,USA,Empirical research involving an LLM,Therapist-facing application,"","","",Patient simulations,No dataset used for development or evaluation,"","","","","","","",Only prompting,Asking ChatGPT to act as a client,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
186,Maurya 2024,Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",29,12,2023,USA,Empirical research involving an LLM,Therapist-facing application,"","","",Patient simulations,No dataset used for development or evaluation,"","","","","","","",Only prompting,"Model was prompted to behave like a counseling client: It was then tested in a fishbowl, triad, and one-to-one formats using facilitation commands (“play,” “pause,” “rewind”) and three processing techniques (“ChatGPT close up,” “counselor close up,” “observer close up”) grounded in Bernard’s Discrimination Model (1979)

Asking ChatGPT to act as a client","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
185,Brown 2024,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,3,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Other: MI reflection generation,No clients/patients involved,"","","External data set, modified","Only questions and answers from MI chatbot reflections by Brown 2023 -- A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study.
The authors generate reflections based on these question-answer pairs using GPT-4",Other: MI reflections,English,Yes,No,Psychopathology,Other: not applicable,Only fine-tuning; Only prompting,"Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.",CBT: Motivational interviewing,GPT-2 family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,no benchmark,"",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
185,Brown 2024,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,03,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"Other: ''We recruited five annotators to evaluate reflections
from GPT-4 and each distilled student model. The
five annotators consist of four males and one fe-
male at an average age of 23, located in North
America. Each annotator has a basic understand-
ing of MI having read (Miller and Rollnick, 2012) and taken coursework.""","","",Self-collected data,"""Mentioned previously, we use transcripts
from the smoking cessation MI chatbot created by
the authors (Brown et al., 2023). """,Emotional support dialogue -- speech transcripts,English,Yes,No,Unknown,Unknown,Fine-tuning + other modules,"e.g. ""After gathering the dataset of MI conversation
questions, answers, and GPT-4-generated reflec-
tions, we use fine-tuning to distill that reflection
capability in a student model.""",CBT: Motivational interviewing,GPT-2 family; GPT-4 / GPT-4o family,No,"","","","","","","","","","","","","","","","","","","","","","","","","",Y,B,H,human annotators,"","","","","","","","","","","","","","",Cohen Kappa,"","","",MI-adherence,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
185,Brown 2024,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,3,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Other: MI reflection generation,No clients/patients involved,"","","External data set, modified","""Mentioned previously, we use transcripts
from the smoking cessation MI chatbot created by
the authors (Brown et al., 2023). ""

Only questions and answers from MI chatbot reflections by Brown 2023 -- A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study.
The authors generate reflections based on these question-answer pairs using GPT-4",Other: MI reflections,English,Yes,No,Psychopathology,Other: not applicable,Only fine-tuning; Only prompting,"Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.",CBT: Motivational interviewing,GPT-2 family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",Y,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,no benchmark,"",n,"","","",n,"","","","",llm-as-a-judge mi_adherence,no benchmark,no benchmark,no benchmark,"","","","","","","","","",n,"",n,"",y,One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
180,Held 2024,A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10,10,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population; Other,6 Patients (+6 Clinicians),"",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"The addition of multiple
collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s
behavior. Iterative prompt engineering was needed and used to
improve the individual agent’s behavior and how they worked
together.",CBT: Cognitive restructuring,GPT-4 / GPT-4o family,Yes,"",N,Y,N,Y,"","See ""Initial User Feedback""","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user",N,"",Y,"Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy. All
user data are transferred through secure and encrypted https.
All user data are always encrypted and then stored in databases.
Users are also encouraged to refrain from providing personally
identifiable information or personal health information when
they log into Socrates 2.0’s web interface to further decrease
any potential risks",N,"",N,"",N,"",N,"",N,"",N,"",Y," Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible.",N,"",""
180,Held 2024,A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10,10,2024,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,6,"",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"The addition of multiple collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s behavior. Iterative prompt engineering was needed and used to improve the individual agent’s behavior and how they worked together.",CBT: Cognitive restructuring,GPT-4 / GPT-4o family,Yes,"",N,Y,N,Y,"","See ""Initial User Feedback""","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user",N,"",Y,"Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible",N,"",""
180,Held 2024,A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10,10,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,6,"",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"The addition of multiple collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s behavior. Iterative prompt engineering was needed and used to improve the individual agent’s behavior and how they worked together.",CBT: Cognitive restructuring,GPT-4 / GPT-4o family,Yes,"",N,Y,N,Y,"","See ""Initial User Feedback""","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user",N,"",Y,"Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible",N,"Future studies
should examine whether using LLMs would make out-of-session
practice, such as examining one’s thoughts, more engaging for
patients than worksheets [21].",""
173,A 2024,Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8,1,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"See Figure 3. All revolves around prompting of GPT-4, and then there is a ""memory concept"" and other stuff","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
173,A 2024,Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8,1,2024,USA,Other: ,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
173,A 2024,Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8,1,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"See Figure 3. All revolves around prompting of GPT-4, and then there is a ""memory concept"" and other stuff","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
169,K 2022,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
169,K 2022,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
169,K 2022,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
167,Sun 2021,Psyqa: A chinese dataset for generating long counseling text for mental health support,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",01,08,2021,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"PsyQA (+ Yixinly? ""Thus we crawled 50K articles (0.1B tokens in total) related to psychology and mental health support from Yixinli (xinli001.com/info) and train a GPT-2 from scratch based on the corpus."")",Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,"Other: Secondly, the answers in PsyQA are
mostly provided by experienced and well-trained volunteers or professional counselors",Fine-tuning + other modules,"","Unspecified, might include formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",Y,B,L,S2S-Model (without strategy) as benchmark,N,"","","",N,"","","",N,"","","","",Y,W,H,Dataset responses from professionals and trained volunteers as benchmark,N,"","","","",Y,B,L,S2S-Model (without strategy) as benchmark,Y,B,L,S2S-Model (without strategy) as benchmark,"","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
167,Sun 2021,Psyqa: A chinese dataset for generating long counseling text for mental health support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1,8,2021,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,psyqa,Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Trained professionals,Fine-tuning + other modules,Training of GPT-2 from scratch on PsyQA + prepending of strategy token,"Unspecified, might include formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",y,b,l,benchmark: simple seq2seq model,n,"","","",y,w,l,"""controllability"", i.e. match between predicted and actual strategy tokens",n,"","","","",y,w,h,"rating of fluency, coherence, relevance, helpfulness. benchmark: human responses from the dataset.",n,"","","","",y,b,l,benchmark: simple seq2seq model,y,b,l,benchmark: simple seq2seq model,"","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
167,Sun 2021,Psyqa: A chinese dataset for generating long counseling text for mental health support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1,8,2021,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"PsyQA (+ Yixinly? ""Thus we crawled 50K articles (0.1B tokens in total) related to psychology and mental health support from Yixinli (xinli001.com/info) and train a GPT-2 from scratch based on the corpus."")",Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Other: mixed,Fine-tuning + other modules,Training of GPT-2 from scratch on PsyQA + prepending of strategy token,"Unspecified, might include formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",Y,B,L,S2S-Model (without strategy) as benchmark,N,"","","",y,w,l,"""controllability"", i.e. match between predicted and actual strategy tokens",N,"","","","",y,w,h,"rating of fluency, coherence, relevance, helpfulness. benchmark: human responses from the dataset.",n,"","","","",y,b,l,benchmark: simple seq2seq model,y,b,l,benchmark: simple seq2seq model,"","","","","","","","","","","","","","",N,"",N,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
157,S 2024,Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot),Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,12,24,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","","","","","","","",Prompting + other modules,Hybrid pipeline: NLP preprocessing → VADER sentiment scoring with thresholds → template or GPT-3.5-turbo generation → referral logic and resources → logging/monitoring; context tracking for coherence. ,"Unspecified, might include formal therapy methods",GPT-3.5 family; Other: NLTK VADER,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
157,S 2024,Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot),Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,12,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set," For example, it uses conversational 
data gained in previous interactions with the user as well as 
datasets having labeled expressions of sentiment (""emo.txt"" 
and ""Sentiments1.txt"")",Other: ,English,Other: unspecified,Other: unspecified,Unknown,Unknown,Prompting + other modules,"""A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative"" .. ""The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately"" ... ""Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition""","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
157,S 2024,Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot),Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,12,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified"," For example, it uses conversational 
data gained in previous interactions with the user as well as 
datasets having labeled expressions of sentiment (""emo.txt"" 
and ""Sentiments1.txt"")",Other: ,English,Other: unspecified,Other: unspecified,Unknown,Unknown,Prompting + other modules,"""A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative"" .. ""The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately"" ... ""Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition""","Unspecified, might include formal therapy methods",GPT-3.5 family; Other: NLTK VADER,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
156,A 2024,The Role of AI Counselling in Journaling for Mental Health Improvement,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",29,8,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,50,"",No dataset used for development or evaluation,"","","","","","","",Other: unknown,"","Unspecified, might include formal therapy methods",Other: unknown,Yes,"",y,n,y,y,"Counseling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
156,A 2024,The Role of AI Counselling in Journaling for Mental Health Improvement,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,05,2024,India,Empirical research involving an LLM,Client-facing application,Other: Journaling app with AI counselling ,General population,50,"",Self-collected data,-,"Other: Survey responses (quantitative, self-report)",English,No,No,Unselected,Unknown,Prompting + other modules,Built on LangChain framework with memory and journaling integration,Other: Digital journaling with AI counselling  ,"Other: not specified, using LangChain framework",Yes,"",N,N,Y,Y,"","Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b)","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","Enhancing Counselling Skills and Therapeutic 
Conditions (PB-CSTC)

Enhancing Counselling Dispositions and 
Behaviours (PB-CDB)

Learning Counselling and Mental Health Topics 
(PB-LC)","","","","","","","","","","","","",N,"",N,"",N,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
156,A 2024,The Role of AI Counselling in Journaling for Mental Health Improvement,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",29,8,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,50,"",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Prompting + other modules,Built on LangChain framework with memory and journaling integration .. Input/Templates/Prompt Instruction,Other: Digital journaling with AI counselling  ,Other: not specified - using LangChain framework,Yes,"",y,n,y,y,"Counselling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964

Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b)","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
155,Rackoff 2025,Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17,1,2025,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,428,"","","","","","","","","","","","","",Yes,"",Y,N,Y,Y,"modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
155,Rackoff 2025,Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17,1,2025,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,428,"",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods","Other: all types of chatbots, e.g. ChatGPT",Yes,"",y,n,y,y,"Mental Help Seeking Attitudes Scale (MHSAS) with regard to chatbots for mental health support.
Modified version of the measure of structural barriers toward mental health service utilization as reported by Van Doren et al.","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
155,Rackoff 2025,Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17,1,2025,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,428,"",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods",Other: all types of chatbots e.g. ChatGPT,Yes,"",y,n,y,y,"modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
153,Kivlighan 2025,Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
153,Kivlighan 2025,Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
153,Kivlighan 2025,Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
152,D 2025,MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
152,D 2025,MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",7,5,2025,India,Population survey,"","",General population,"","","Other: ""The mental health quiz module uses a custom dataset de-
rived from stress and anxiety indicators, consisting of 40
structured questions with multiple-choice options. Users
receive a random selection of 10 questions, ensuring
reliability and varied experience""",unclear who the users of the MH quiz module are,"",English,"",Yes,Unselected,Unknown,Only fine-tuning,"","",BERT family,Yes,"","","","",Y,"","Feedback from test users: These results suggest that the
system effectively delivers mental health insights while main-
taining user engagement.","","","","","","","","","",Y,"","","Accuracy, F1 Score, Precision and Recall","","","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",This studiy does not make clear who the users are. It seems they had some users who competed a quiz
152,D 2025,MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
150,Rasool 2025,nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
150,Rasool 2025,nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
150,Rasool 2025,nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
148,,Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",20,05,2025,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,counsel-chat,Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Trained professionals,Only fine-tuning,"","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",y,b,L,older LLM,y,b,L,older LLM,"","","","","","","","","","","","","","","","","","","","","","","","","","","",SHAP value,B,L,older LLM,LIME value,B,L,older LLM,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
148,,Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation,Richard Gaus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",20,5,2025,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Psychotherapy -- chat logs,English,No,Yes,Unselected,Trained professionals,Only prompting,"","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
148,,Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation,Consensus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",20,5,2025,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only prompting,"","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
147,S 2024,AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16,12,2024,India,"","","","","","",External data set,"The MSP-Podcast Corpus is a well annotated dataset 
which includes a wide range of emotional expressions 
captured through spontaneous speech. The dataset's 
diversity and the quality of annotations make it an 
excellent resource for making them availed . Its 
annotations ensure reliability, making it invaluable for 
training and evaluating emotion recognition models

Video Dataset: FER+ Dataset  provides quality images with 
annotations for various facial expressions
","Other: A large naturalistic speech database with emotional traces

The FER+ annotations provide a set of new labels for the standard Emotion FER dataset. In FER+, each image has been labeled by 10 crowd-sourced taggers, which provide better quality ground truth for still image emotion than the original FER labels. ",English,No,No,Unselected,Unknown,Only fine-tuning,"",Other CBT techniques,GPT-2 family,No users involved,"","","","","","","","","","","","","","","","",Y,"","","Precision, recall accuracy, F1 Score","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
147,S 2024,AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16,12,2024,India,Empirical research involving an LLM,Client-facing application,Other: Spoken dialog system,No clients/patients involved,"","",External data set,"The fine-tuning process involves the 
calibration of the pre-trained Transformer model with 
specific datasets relevant to cognitive therapy. This 
includes dialogues from CBT sessions, therapeutic 
interventions, and stress management conversations.

The GPT-2 model is fine-tuned using a specialized 
therapeutic dialogue dataset where the process involves 
supervised learning and the openly available trained 
model is adapted for a wide range of therapeutic 
contexts. The goal is to improve the ability of the model 
to provide relevant and empathetic responses during 
therapy sessions. The fine-tuning dataset includes a 
variety of therapeutic dialogues which ensures the 
model can handle different therapeutic scenarios 
effectively.",Other: unknown,Other: unknown,Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of GPT-2 + voice and video interaction system,Other CBT techniques,GPT-2 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,GPT-2,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
147,S 2024,AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16,12,2024,India,Empirical research involving an LLM,Client-facing application,Other: Multimodal dialog system,No clients/patients involved,"","","Other: External data set, but completely unknown","The fine-tuning process involves the 
calibration of the pre-trained Transformer model with 
specific datasets relevant to cognitive therapy. This 
includes dialogues from CBT sessions, therapeutic 
interventions, and stress management conversations.

The GPT-2 model is fine-tuned using a specialized 
therapeutic dialogue dataset where the process involves 
supervised learning and the openly available trained 
model is adapted for a wide range of therapeutic 
contexts. The goal is to improve the ability of the model 
to provide relevant and empathetic responses during 
therapy sessions. The fine-tuning dataset includes a 
variety of therapeutic dialogues which ensures the 
model can handle different therapeutic scenarios 
effectively.",Other: unknown,Other: unknown,Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of GPT-2 + voice and video interaction system,Other CBT techniques,GPT-2 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,GPT-2,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
144,M 2025,Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,1,2025,India,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"Reddit, Twitter an Mental Health Forums as data source",Internet data -- mental health forum,English,No,Yes,Unselected,Unknown,Fine-tuning + other modules,see Fig. 1 ,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"",N,N,N,N,"","","",Y,B,L,BLEU SCORE,Y,B,L,Semantic Similarity,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
144,M 2025,Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,1,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms. It is intended for training and evaluating language models that provide safer, context-aware mental-health responses.",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",y,no benchmark,no benchmark,BLEU score,y,no benchmark,no benchmark,Cosine distance (see Eq. 4). no benchmark,n,"","",Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM,n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
144,M 2025,Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,1,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms. It is intended for training and evaluating language models that provide safer, context-aware mental-health responses.",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier (see Fig. 1),"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"",N,N,N,N,"","","",Y,no benchmark,no benchmark,BLEU SCORE,Y,no benchmark,no benchmark,Semantic Similarity via cosine distance (see Eq. 4),n,"","",Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM,n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
143,S 2024,Mello: A Large Language Model for Mental Health Counselling Conversations,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,09,2024,India,Empirical research involving an LLM,Analysis of conversation transcripts,One-turn chatbot (usually Q&A),No clients/patients involved,"",Patient simulations,"External data set, modified","counsel-chat dataset, accessible on Hugging Face ( 2,700 anonymised
talks between individuals and experienced counsellors on the
website www.counselchat.com)",Psychotherapy -- chat logs,English,No,Yes,Unselected,Other: experienced counsellors,Only fine-tuning,"","Informal counseling (e.g., emotional support conversation)",Mistral family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,B/S,H,human responses from existing literature,"","","","","","","","","","",Emotional Intelligence Scale (EIS),B,H,human responses from existing literature,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
143,S 2024,Mello: A Large Language Model for Mental Health Counselling Conversations,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,9,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Psychotherapy -- chat logs,English,No,Yes,Unselected,Trained professionals,Only fine-tuning,"","Unspecified, might include formal therapy methods",Mistral family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",PsychoBench Empathy Scale,b,l,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",PsychoBench Emotional Intelligence Scale,b,l,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
143,S 2024,Mello: A Large Language Model for Mental Health Counselling Conversations,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,9,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"",Other: ,External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only fine-tuning,"","Unspecified, might include formal therapy methods",Mistral family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",PsychoBench Empathy Scale,b,l,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",PsychoBench Emotional Intelligence Scale,b,l,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
142,Giray 2025,Cases of Using ChatGPT as a Mental Health and Psychological Support Tool,Reviewer Two,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23,12,2024,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,"","",Other: reddit ,reddit r/ChatGPT,Other: Internet data - online forum,English,No,Yes,Unknown,Unknown,Only prompting,"","Informal counseling (e.g., emotional support conversation)",GPT-3 family; GPT-3.5 family; Other,Yes,"",N,Y,N,N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
142,Giray 2025,Cases of Using ChatGPT as a Mental Health and Psychological Support Tool,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23,12,2024,Other: Philippines,Population survey,Client-facing application,Multi-turn chatbot,General population,7,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",n,y,n,y,"","""User experience assessment"" was content of the Reddit posts","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
142,Giray 2025,Cases of Using ChatGPT as a Mental Health and Psychological Support Tool,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23,12,2024,Other: Philippines,Population survey,Client-facing application,Multi-turn chatbot,General population,7,"",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Only prompting,"","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",N,Y,N,y,"","""User experience assessment"" was content of the Reddit posts","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
141,Lalk 2025,Employing large language models for emotion detection in psychotherapy transcripts,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
141,Lalk 2025,Employing large language models for emotion detection in psychotherapy transcripts,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
141,Lalk 2025,Employing large language models for emotion detection in psychotherapy transcripts,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
140,Z 2024,MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,Richard Gaus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",03,06,2024,China,Empirical research involving an LLM,Client-facing application,"",No clients/patients involved,"","",External data set,PsyQA,Emotional support dialogue -- speech transcripts,"",No,Yes,Unknown,Unknown,Prompting + other modules,"Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,Yes,"",N,N,Y,Y,"","Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale","",Y,B,L,"“We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”","","","","",Y,B,L,"The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978","","","","","","","","","","","","","","","","","","",Y,B,L,D1 (Distinct-1) measures the richness of vocabulary in the responses,"","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","",Y,"We compare MENTALER with the following baselines… CoT, TPE, ReAct, Cue-CoT, Chameleon","","","","",""
140,Z 2024,MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3,12,2024,China,Empirical research involving an LLM,Other: ,"",No clients/patients involved,"","",External data set,"PsyQA is an authoritative Chinese dialogue dataset focused
on the field of mental health support. It covers 9 broad
topics and multiple subtopics, encompassing various aspects
of mental health. PsyQA is presented in a question-answer
format, where each example includes a question, a detailed
description of the mental issues, and keywords provided by
anonymous help-seekers. Responses are asynchronously pro-
vided by well-trained volunteers or professional counselors
and involve detailed analysis and guidance in response to the
help-seekers’ questions, aiming to offer mental health support.
Additionally, the answers have been further annotated with
seven psychological strategies based on psychological coun-
seling theory [12], ",Internet data -- mental health Q&A,Chinese,No,Yes,Unknown,Trained professionals,Only prompting,"""We use ChatGPT as our base LLM and access it through
the API with prompts""","Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,"","","","","","","","",Y,"","","",N,"","","",N,"","","",N,"","","","",Y,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
140,Z 2024,MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",03,12,2024,China,Empirical research involving an LLM,Client-facing application,"",No clients/patients involved,"","",External data set,"PsyQA is an authoritative Chinese dialogue dataset focused
on the field of mental health support. It covers 9 broad
topics and multiple subtopics, encompassing various aspects
of mental health. PsyQA is presented in a question-answer
format, where each example includes a question, a detailed
description of the mental issues, and keywords provided by
anonymous help-seekers. Responses are asynchronously pro-
vided by well-trained volunteers or professional counselors
and involve detailed analysis and guidance in response to the
help-seekers’ questions, aiming to offer mental health support.
Additionally, the answers have been further annotated with
seven psychological strategies based on psychological coun-
seling theory [12], ",Internet data -- mental health Q&A,Chinese,No,Yes,Unknown,Trained professionals,Prompting + other modules,"Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",Y,B,L,"“We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”",N,"","","",Y,B,L,"The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978",N,"","","","",Y,"","","Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale",N,"","","","","","","","",Y,B,L,D1 (Distinct-1) measures the richness of vocabulary in the responses,"","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
139,Kopelovich 2025,Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
139,Kopelovich 2025,Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
139,Kopelovich 2025,Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
138,Suffoletto 2025,Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",28,04,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),45,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Prompt engineering improved MI fidelity (Phase II); GPT-4 via secure API; HIPAA-compliant setup. — Quote: “explicit prompting for adherence to validated MI literature and more collaborative, client-centered language enhanced the therapeutic alliance.”",CBT: Motivational interviewing,GPT-4 / GPT-4o family,Yes,"",Y,Y,Y,Y,"SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract)","SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback)","",N,"","","","","","","","","","","","","","","","",Y,B,L,(reviewed transcripts for safety + MI fidelity) — Quote: “All authors… reviewed MICA session transcripts independently to identify any statements… inappropriate” ,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results)","","","","",Y?,"Participants were mostly male (71.1 %), White race (66.7 %), and college educated (82.2 %)","","","","","","","","","","","","","","",""
138,Suffoletto 2025,Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",28,4,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,45,"",Other: External data set + Self-collected data ,"Model not really trained on the dataset, but datasets used to generate suitable prompts. 

Datasets used: AnnoMI + 20 full-length simulated sessions using GPT, seeded with anonymized baseline data from young adult participants with hazardous alcohol use drawn from a recent clinical trial",Other: Psychotherapy -- speech transcripts + Syntethic data ,English,"Other: AnnoMi no, self-collected yes","Other: AnnoMi yes, self-collected no",Unknown,Trained professionals,Only fine-tuning; Only prompting,"We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",Y,Y,Y,Y,System Usability Scale (SUS) + Client Evaluation of Motivational Interviewing (CEMI),"","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. ",Y,"ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. ",N,"",Y,"
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. ",Y,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
138,Suffoletto 2025,Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",28,04,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),45,"",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Only fine-tuning; Only prompting,"We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.","CBT: Motivational interviewing; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",Y,Y,Y,Y,"SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract)","SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback)","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results)",N,"",Y,"
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. ",Y,Table 1,N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
137,Haider 2025,AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",05,05,2025,Other: Pakistan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","{17} J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015.
[18] D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009.
[19] B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014.",Other: ,English,No,Yes,Unknown,Unknown,Prompting + other modules,"Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1)","Informal counseling (e.g., emotional support conversation)",BART family,No users involved,"","","","","","","","",Y,B,L,0.85 cosine similarity score,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract),"","","","",""
137,Haider 2025,AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",05,05,2027,Other: Pakistan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","Books: 
E. J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015. + D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009. + B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014. 

To make sure it was appropriate for entering into the model, the data taken from the books went through a number of preparation steps
in this research: Extraction and cleaning.",Other: Book data,English,No,No,Other: N/A,Trained professionals,Other: RAG,"","Unspecified, might include formal therapy methods","BART family; ChatGPT, model unspecified",No users involved,"","","","","","","","",N,"","","",Y,B,L,Best model of authors compared to models of other works. ,N,"","","",N,"","","","",N,"","","",N,"","","","",Y,W,L,Best model of authors compared to models of other works. ,N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
137,Haider 2025,AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",05,05,2025,Other: Pakistan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","Books: 
E. J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015. + D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009. + B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014. 

To make sure it was appropriate for entering into the model, the data taken from the books went through a number of preparation steps
in this research: Extraction and cleaning.",Other: Book data,English,No,No,Unknown,Unknown,Prompting + other modules,"Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1)","Unspecified, might include formal therapy methods","BART family; ChatGPT, model unspecified",No users involved,"","","","","","","","",N,"","","",Y,B,L,Best model of authors compared to models of other works. ,N,"","","",N,"","","","",N,"","","",N,"","","","",Y,W,L,Best model of authors compared to models of other works. ,N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract),N,"",N,"",""
136,Aleem 2024,Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),9,11,2024,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
136,Aleem 2024,Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),9,11,2024,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
136,Aleem 2024,Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),9,11,2024,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
135,K 2024,MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2,12,2024,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",Self-collected data,PsyQA dataset,Emotional support dialogue -- chat logs,Chinese,No,Yes,Unknown,Unknown,Only prompting,"","Unspecified, might include formal therapy methods",BERT family; Llama 2 family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
135,K 2024,MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,11,2024,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","","External data set, modified","PsyQA, extended by ""helping skills"" column",Internet data -- mental health Q&A,Chinese,"","","","",Prompting + other modules,BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.,"Unspecified, might include formal therapy methods",BERT family; Llama 2 family,No users involved,"","","","","","","","",y,no benchmark,no benchmark,"BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L",n,"","","",y,no benchmark,no benchmark,Accuracy of the BERT helping skill classification model,n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
135,K 2024,MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,11,2024,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","","External data set, modified","PsyQA, extended by ""helping skills"" column",Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Other: Mix of lay and trained,Prompting + other modules,BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.,"Unspecified, might include formal therapy methods",BERT family; Llama 2 family,No users involved,"","","","","","","","",y,no benchmark,no benchmark,"BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L",n,"","","",y,no benchmark,no benchmark,Accuracy of the BERT helping skill classification model,n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
134,J 2025,Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
134,J 2025,Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
134,J 2025,Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
132,,AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",02,04,2025,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,11,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"The design phase mentions fine-tuning and prompt engineering, but the implemented study relied on a simple prompt with GPT-3.5. — Quote: “The design process addressed… fine-tuning the LLMs, and prompt engineering.” / “The initial study used a relatively simple prompt…”",CBT: Motivational interviewing,GPT-3.5 family,Yes,"",N,Y,Y,Y,"","Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” ","","",N ,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",VR maybe as solution for the embodiment problem? 
132,,AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",02,04,2025,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,11,"",External data set,"fine-tuned on the 2 volumes of published counseling and psychotherapy data from
Alexander Street Press. The volumes are searchable collections of transcripts containing real counseling and therapy sessions
and first-person narratives illuminating the experience of mental illness and treatment. The 2 volumes contain 3500 session transcripts and >700,000 utterances between a counselor and a
patient.",Emotional support dialogue -- speech transcripts,English,No,Yes,Unknown,Trained professionals,Only fine-tuning; Only prompting,"""We fine-tuned the model with an 80% to 20% train-test split"" + ""that is, the system is prompted
to keep generating responses""...","Unspecified, might include formal therapy methods",GPT-3.5 family,Yes,"",N,Y,N,Y,"","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
132,,AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",02,04,2025,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,11,"",External data set,"fine-tuned on the 2 volumes of published counseling and psychotherapy data from
Alexander Street Press. The volumes are searchable collections of transcripts containing real counseling and therapy sessions
and first-person narratives illuminating the experience of mental illness and treatment. The 2 volumes contain 3500 session transcripts and >700,000 utterances between a counselor and a
patient.",Emotional support dialogue -- speech transcripts,English,No,Yes,Unknown,Unknown,Only fine-tuning; Only prompting,"""We fine-tuned the model with an 80% to 20% train-test split"" + ""that is, the system is prompted
to keep generating responses""...",CBT: Motivational interviewing,GPT-3.5 family,Yes,"",N,Y,Y,Y,"","Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” ","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",VR maybe as solution for the embodiment problem? 
131,Manole 2024,Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2,12,2024,Other: Romania,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),50,"",No dataset used for development or evaluation,"","","","","",Psychopathology,Other: chatbot,Prompting + other modules,"Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization",CBT: Cognitive restructuring; Other CBT techniques,GPT-4 / GPT-4o family,"","",N,Y,Y,Y,Participant Feedback ,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Anxiety Score ,"","","",Engangement Metrics ,"","","","","","","","",N,"","","",N,"",N,"",Y,"",Y,"",N,"",N,"",N,"",N,"",N,"",N,"",""
131,Manole 2024,Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2,12,2024,Other: Romania,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),50,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"They only ever refer to ""prompt engineering techniques"" used in their chatbot",CBT: Cognitive restructuring; Other CBT techniques,"ChatGPT, model unspecified",Yes,"",n,y,y,y,"",Qualitative feedback reported in 4.4.,"",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","Reduction in anxiety symptoms (BAI, GAD-7)",no benchmark,no benchmark,no benchmark,User satisfaction rating,no benchmark,no benchmark,no benchmark,Daily interaction time,no benchmark,no benchmark,no benchmark,"",n,"",n,"",n,"",n,"",y,Table 2 has just the bare minimum,n,"",y,Table 6 shows how many participants interacted only 0-15 mins daily,y,Table 6 shows daily interaction time over 30 mins,y,Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7),n,"",n,"",y,"Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.",""
131,Manole 2024,Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2,12,2024,Other: Romania,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),50,"",No dataset used for development or evaluation,"","","","","",Other: ,Other: ,Prompting + other modules,"Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization",CBT: Cognitive restructuring; Other CBT techniques,"ChatGPT, model unspecified",Yes,"",n,y,y,y,"",Qualitative feedback reported in 4.4.,"",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","Reduction in anxiety symptoms (BAI, GAD-7)",no benchmark,no benchmark,no benchmark,User satisfaction rating,no benchmark,no benchmark,no benchmark,Daily interaction time,no benchmark,no benchmark,no benchmark,"",N,"",n,"",N,"",N,"",Y,Table 2 has just the bare minimum,n,"",y,Table 6 shows how many participants interacted only 0-15 mins daily,y,Table 6 shows daily interaction time over 30 mins,y,Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7),n,"",n,"",y,"Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.",""
130,Maurya 2025,Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,Reviewer Two,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2,7,2025,India,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),"","","","External data set, modified","Aditya Mental Health Counselling Dataset, Mental Health Counselling Chat, 
Counsel Chat Dataset and Amod-Mental Health Counselling Conversations",Internet data -- mental health Q&A,English,No,Yes,Unselected,Unknown,Only fine-tuning,"","Unspecified, might include formal therapy methods",BART family; T5 family; Other: GODEL(Grounded Open Dialogue Language Model),No users involved,"","","","","","","","",Y,B,L,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets","","","","","","","","","","","","","","","","","","","","","","",Y,B,L,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
130,Maurya 2025,Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2,7,2025,India,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,Aditya Mental Health Counseling https://huggingface.co/datasets/Aditya149/Mental_Health_Counselling_Dataset,Internet data -- mental health Q&A,English,Other: Unknown,Yes,Unknown,Unknown,Only fine-tuning,Different models are simply fine-tuned on provided datasets,"Informal counseling (e.g., emotional support conversation)",BART family; T5 family; GPT-2 family,No users involved,"","","","","","","","",y,no benchmark,no benchmark,Comparison against reference responses in dataset,n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,Perplexity of reference responses in dataset,n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
130,Maurya 2025,Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2,7,2025,India,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,Aditya Mental Health Counseling https://huggingface.co/datasets/Aditya149/Mental_Health_Counselling_Dataset,Internet data -- mental health Q&A,English,Other: Unknown,Yes,Unknown,Unknown,Only fine-tuning,Different models are simply fine-tuned on provided datasets,"Unspecified, might include formal therapy methods",BART family; T5 family; GPT-2 family,No users involved,"","","","","","","","",y,no benchmark,no benchmark,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Comparison against reference responses in dataset",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Perplexity of reference responses in dataset",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
129,S 2025,Multi-Tiered RAG-Based Chatbot for Mental Health Support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,04,2025,Other: Saudi Arabia,Other: engineering/system paper with internal evaluation (no human user study) ,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.","Other CBT techniques; Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,B,L,"Custom metrics (Role Adherence, Answer Relevancy, Faithfulness) reported in tables across tiers of the same model; no external gold standard named","","","","","","","","","",Y,"Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159",N,"","","","","","","","","","","","","","","","","","","","","",""
129,S 2025,Multi-Tiered RAG-Based Chatbot for Mental Health Support,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,4,2025,Other: Saudi Arabia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"unspecified (""credible sources"")",Other: unspecified ,"",Other: unspecified ,Other: unspecified ,Unknown,Unknown,Fine-tuning + other modules," multi-tiered chatbot leveraging Retrieval-
Augmented Generation (RAG) provides dynamic, context-aware
mental health assistance","Other CBT techniques; Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"","",The DeepEval framework + MQG-RAG Evaluation,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
129,S 2025,Multi-Tiered RAG-Based Chatbot for Mental Health Support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,04,2025,Other: Saudi Arabia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Prompting + other modules,"Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.","Other CBT techniques; Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,no benchmark,no benchmark,The DeepEval framework + MQG-RAG Evaluation,"","","","","","","","","","",automatic response quality,no benchmark,no benchmark,llm as a judge: The DeepEval framework + MQG-RAG Evaluation,"","","","","","","","","",Y,"Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159",N,"","","","","","","","","","","","","","","","","","","","","",""
128,Santarpia 2024,"ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence",Reviewer Two,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",03,10,2024,Other: canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).,Psychoanalysis,GPT-4 / GPT-4o family,Yes,"",N,Y,N,Y,"","Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)","",N (all below),"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
128,Santarpia 2024,"ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence",Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",03,10,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: The author served as the sole participant,1,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,"",N,Y,N,Y,"","May not be understood as the ""classical"" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
128,Santarpia 2024,"ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence",Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",03,10,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,1,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).,Psychoanalysis,GPT-4 / GPT-4o family,Yes,"",N,Y,N,Y,"","Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)

May not be understood as the ""classical"" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
127,Schlarb 2024,Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories,Reviewer Two,"","Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",17,12,2024,Germany,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).",Other,"ChatGPT, model unspecified",Yes,"",N,Y,Y,Y,"","As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).","",N,"","","","","","","","","","","","","","","","",N,"","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
127,Schlarb 2024,Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories,Richard Gaus,"","Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",17,12,2024,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: Reviewers trained in developing therapeutic stories,4,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Promts: ""Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis"".","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,-,-,-,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
127,Schlarb 2024,Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories,Consensus,"","Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",17,12,2024,Germany,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Promts: ""Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis"".

Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).",Other: Bedtime stories,"ChatGPT, model unspecified",Yes,"",N,Y,Y,Y,"","As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
126,Campellone 2025,Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23,5,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,160,"",Self-collected data,"Using a dataset of Woebot questions and user responses (pulled from the Gen-W-MA internal testing data) labeled as being on or off topic, we fine-tuned an instance of the text-embedding-ada-002 model using the Azure OpenAI service",Emotional support dialogue -- chat logs,English,No,No,Unselected,Other: Woebot chatbot,Prompting + other modules,davinci-003 + off topic classifier. davinci-003 seems not finetuned,"CBT: Cognitive restructuring; Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,"",y,n,y,y,Client Satisfaction Questionnaire-8 (CSQ-8),"Quantitative: Measures of the user experience with the generative and rules-based DMHIs included user engagement (number of sessions, total active days, and conversational exchanges).
","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",Posttrial guardrail review,no benchmark,no benchmark,"A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). ","Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond)","","","",Safety measures,"","","Safety
was assessed by adverse events monitored during both in-app
conversational exchanges and study assessment points, instances
of concerning language detected in user free-text inputs, and
the posttrial technical guardrail assessment success rate.","",y,"Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language",y,"We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.",n,"","","",y,Table 1,"","",y,dropouts noted,y,"Measures of the user experience with the generative and
rules-based DMHIs included user engagement (number of
sessions, total active days, and conversational exchanges)","","","","","","","","",""
126,Campellone 2025,Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23,5,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,160,"",Self-collected data,"",Emotional support dialogue -- speech transcripts,English,No,No,Unknown,"",Prompting + other modules,"","Informal counseling (e.g., emotional support conversation)",GPT-3 family,Yes,"",Y,N,Y,Y,"Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language",Y,"Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer",N,"",N,"",Y,"",N,"",Y,"",N,"",Y,"",Y,RCT,Y,"",N,"",""
126,Campellone 2025,Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23,5,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,160,"",Self-collected data,"Using a dataset of Woebot questions and user responses (pulled from the Gen-W-MA internal testing data) labeled as being on or off topic, we fine-tuned an instance of the text-embedding-ada-002 model using the Azure OpenAI service",Emotional support dialogue -- chat logs,English,No,No,Unknown,Other: Woebot chatbot,Fine-tuning + other modules,davinci-003 + off topic classifier. davinci-003 seems not finetuned,"CBT: Cognitive restructuring; Informal counseling (e.g., emotional support conversation)",GPT-3.5 family; Other: text-embedding-ada-002,Yes,"",Y,N,Y,Y,"Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9","Quantitative: Measures of the user experience with the generative and rules-based DMHIs included user engagement (number of sessions, total active days, and conversational exchanges).
","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,b,l,benchmark: woebot non-generative/rule-based.,N,"","","","",N,"","","",N,"","","","",Posttrial guardrail review,no benchmark,no benchmark,"A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). ",Instances of potentially concerning language detected,w,l,benchmark: woebot non-generative/rule-based.,"","","","","",Y,"Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language

Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language",Y,"Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer

We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.",N,"",N,"",Y,Table 1,N,"",Y,dropouts noted,y,"Measures of the user experience with the generative and
rules-based DMHIs included user engagement (number of
sessions, total active days, and conversational exchanges)",n,"WAI-SR Bond is used, but this is not symptom or function scale",Y,RCT,n,"",N,"",""
125,Q 2024,PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",08,10,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"PsyAdv Corp consisting out of Efaqa, CpsyCounD, SMILECHAT, PsyQA",Psychotherapy -- speech transcripts,Chinese,Yes,Yes,Unknown,Unknown,Fine-tuning + other modules,"Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.","Unspecified, might include formal therapy methods",Other: InternLM2-7B-Chat with QLoRA,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,B,L,ChatGPT-4 rated using the categories below,"","","","","","","","","","","Comprehensiveness, Professionalism, Safety",B ,L,ChatGPT and GLM-4,Authenticity,W,L,ChatGPT and GLM-4,"","","","","",N,"",N,"",Y,InternLM2-7B is self-hostable,N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
125,Q 2024,PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,11,2024,China,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved,"","",External data set,"PsyQA dataset (and PsyAdv Corp, and CPSyCounE)",Emotional support dialogue -- chat logs,Chinese,No,Yes,Unselected,Unknown,Fine-tuning + other modules,"'we apply the
QLoRA [12] technique for parameter-efficient finetuning of
the generator' .. 'dense retrieval model'... ' we introduce a BERT-
based cross-encoder as a re-ranke'","Informal counseling (e.g., emotional support conversation)",BERT family,No,"",N,N,N,N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
125,Q 2024,PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,11,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","PsyAdv Corp consisting out of Efaqa, CpsyCounD, SMILECHAT, PsyQA",Emotional support dialogue -- chat logs,Chinese,Yes,Yes,Unknown,Unknown,Fine-tuning + other modules,"Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.","Unspecified, might include formal therapy methods",BERT family; Other: InternLM2-7B-Chat with QLoRA,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,B,L,"ChatGPT-4 rated using the categories below. Benchmark: ChatGPT, GLM-4","","","","","","","","","","",automatic safety rating,b,l,ChatGPT and GLM-4,automatic response quality,b,l,ChatGPT and GLM-4,"","","","","",N,"",N,"",Y,InternLM2-7B is self-hostable,N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
124,T 2024,Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14,12,2024,India,"Other: Propsal of a multimodal Model called ""empathy AI""",Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Multimodal pipeline combining BERT embeddings (text), CNN (facial), LSTM/MFCC (audio), LangChain prompting, GPT-4 response generation; frontend in React, auth/session in Firebase. ","Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family; Other: additionally: CNN for Facial Recognition and LSTM for Text-based Emotion Detection,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
124,T 2024,Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),14,12,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Other: unspecified,"",Other: unspecified,Other: unspecified,Other: unspecified,Other: unspecified,Unknown,Unknown,Prompting + other modules,"""LangChain for Prompting"" combined with ""CNN for Facial Recognition,"" ""LSTM for Text-based Emotion Detection,"" ""MFCC or LSTM for Audio Recognition""","Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
124,T 2024,Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14,12,2024,India,Empirical research involving an LLM,Client-facing application,Other: Multi-modal dialogue system,No clients/patients involved,"","",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Prompting + other modules,"""LangChain for Prompting"" combined with ""CNN for Facial Recognition,"" ""LSTM for Text-based Emotion Detection,"" ""MFCC or LSTM for Audio Recognition""","Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
123,Nayinzira 2024,SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",28,10,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,Mental Health Counseling Conversations Dataset,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Fine-tuning + other modules,"dataset fine-tuning, sentiment analysis within RAG models","Unspecified, might include formal therapy methods",GPT-3.5 family; Mistral family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Ragas evaluation metrics to calculate Faithfulness, Answer Relevancy, and Answer Correctness scores for the various
RAG models",B,L,OpenAI against Mistral with RAG modules and sentiment analysis,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
123,Nayinzira 2024,SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",28,10,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis,"Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family; Mistral family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",y,no benchmark,no benchmark,Ragas is LLM-as-a-judge,"",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
123,Nayinzira 2024,SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis,Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",28,10,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis,"Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family; Mistral family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",y,no benchmark,no benchmark,Ragas is LLM-as-a-judge,"",n,"","","",n,"","","","",automatic response quality,no benchmark,no benchmark,Ragas is LLM-as-a-judge. OpenAI against Mistral with RAG modules and sentiment analysis,"","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
119,Grabb 2023,The impact of prompt engineering in large language model performance: a psychiatric example,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",24,10,2023,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Psychoanalysis; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","","","","","","","","","","","","","","",N,"","","","","","","","",N,"","","","","","","","",N,"","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
119,Grabb 2023,The impact of prompt engineering in large language model performance: a psychiatric example,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",24,10,2023,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Four unique questions were asked of ChatGPT 4.0, and each question was asked five separate times. Each question asked to the model 
was performed in a separate, new conversation (10)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
119,Grabb 2023,The impact of prompt engineering in large language model performance: a psychiatric example,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",24,10,2023,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Four unique questions were asked of ChatGPT 4.0, and each question was asked five separate times. Each question asked to the model 
was performed in a separate, new conversation (10)","Psychoanalysis; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
117,Berrezueta-Guzman 2025,Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.,Richard Gaus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),25,5,2025,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"Prompting of GPT-4o + components of a robot (speech-to-text, movement, etc.)","",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,s,h,"Measure: ""Engagement score"". Benchmark: Therapist-led sessions",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",y,Prioritize data privacy and ethical compliance,n,"",n,"",n,"",n,"",n,"",n,"",n,"","","",""
117,Berrezueta-Guzman 2025,Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.,Reviewer Two,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),25,5,2025,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,multiple sets: existing clinical datasets - e.g. from ADHD-200 Global Competition; transcriptions of therapist-patient interactions; DHD support group discussions ,Psychotherapy -- speech transcripts,"Other: german, english ",Yes,Yes,Psychopathology,Trained professionals,Prompting + other modules,"chatgbt 4 was fine-tuned with external (and internal) data sets, fine-tuned through prompt engineering and integrated into a technical framework ","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,"Other: users were not the target group ( here: children), but educators and caregivers rated the effectiviness in potential application with children","",N,Y,N,N,"","Categories of qualitative feedback: helpfullness, Naturalness, Ease of Use, Engangement. ","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,B,H,"To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.",N,"","","","","","","","","","","","","",Integrated interaction ,"","","",Consitency ,"","","","","","","","",N,"",N,"",N,"",Y,"In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.
",N,"",N,"",N,"",N,"",N,"",N,"",Y,"",Y,"
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families",""
117,Berrezueta-Guzman 2025,Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.,Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),25,5,2025,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Prompting + other modules,"Prompting of GPT-4o + components of a robot (speech-to-text, movement, etc.)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,B,H,"To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.",N,"","","","",n,"","","",n,"","","","",Stress test metrics,no benchmark,no benchmark,no benchmark,"","","","","","","","","",N,"",N,"",N,"",Y,"In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.
",N,"",N,"",N,"",N,"",N,"",N,"",n,"",Y,"
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families",""
116,Moore 2025,Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",23,6,25,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Real therapy transctipts from Alexander Street Press,Psychotherapy -- speech transcripts,English,No,Yes,Unknown,Trained professionals,Only prompting,"Only prompting to evaluate capabilities of LLMs in terms of stigma and their responses to mental health symptoms.  In addition to prompting models with the stimuli without in-context examples, we also employed a novel method of prompting models with a portion of real therapy tran-
scripts from Alexander Street Press [6, 7]","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",Y,-,-,"No clear indexmodel, so not necessarly a benchmark (?)","",N,"","","",N,"","","","",Avg. of Stigma Questions,-,-,"Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. ",Appropriateness of responses,B,L,Compared to commercially-available therapy bots; no indexmodel. ,"","","","","",Y,Implicit in S-2,Y,"LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11)",Y,"Partially: GPT not, Llama yes. ",Y,"Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].",""
116,Moore 2025,Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.,Richard Gaus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23,6,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Alexander Street Press therapy transcripts,Psychotherapy -- speech transcripts,English,No,No,Psychopathology,Trained professionals,Only prompting,Only prompting of different LLMs,"Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family; Other: Pi, Noni, Serena, and other commercial chatbots",No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",y,w,h,LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.,"",n,"","","",n,"","","","",Level of stigma,no benchmark,no benchmark,"Figure 1, no benchmark","","","","","","","","","",y,"",y,"",y,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",y,See 6.2 and 7,""
116,Moore 2025,Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",23,6,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Alexander Street Press therapy transcripts,Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,Only prompting,"Only prompting to evaluate capabilities of LLMs in terms of stigma and their responses to mental health symptoms.  In addition to prompting models with the stimuli without in-context examples, we also employed a novel method of prompting models with a portion of real therapy tran-
scripts from Alexander Street Press [6, 7]","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family; Other: Pi - Noni - Serena - and other commercial chatbots,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",Y,w,h,LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.,"",N,"","","",N,"","","","",Avg. of Stigma Questions,no benchmark,no benchmark,"Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. ",automatic safety rating,w,h,LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.,"","","","","",Y,Implicit in S-2,Y,"LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11)",Y,"Partially: GPT not, Llama yes. ",Y,"Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].

See 6.2 and 7",""
114,S 2024,Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms,Richard Gaus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),17,06,2024,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,See Fig. 1: LLMs integrated into robot to specifically adress ADHD Symptoms,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Claude family,No users involved,"","","","","","","","",Y,B,L,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",See Fig. 4-9. ,"","","","","","","","","","","","",N,"",N,"","","","","","","","","","","","","","","","","","","","","",""
114,S 2024,Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17 ,6,2024,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"The implementation of the speak-speak feature involves
specifying the models, prompts (as we are testing in a zero
learning shot environment)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Claude family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
114,S 2024,Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),17,06,2024,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"The implementation of the speak-speak feature involves
specifying the models, prompts (as we are testing in a zero
learning shot environment)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Claude family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","",y,s,l,benchmark: chatgpt without robot integration,"","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"",N,"","","","","","","","","","","","","","","","","","","","","",""
112,Cioffi 2025,Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17,03,2025,Other: Italy,Empirical research involving an LLM,Therapist-facing application,"","","",Treatment fidelity feedback,Self-collected data,-,Other: Psychotherapy -- Therapist Feedback,Other: Italien,Yes,No,Unselected,Trained professionals,Fine-tuning + other modules,"",Other: Gestalt-Therapy,GPT-4 / GPT-4o family,Yes,"",N,Y,Y,Y,"","Satisfaction ratings focused on relational/emotional quality, technical/didactic, treatment support, professional orientation. — Quote: “PCA highlighted four components… relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4)” (Position: Abstract)","",N,"","","","","","","","","","","","","","","","",Y,B,H,valuated by Gestalt psychotherapy trainees,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract)",Y,Yes (trainee evaluations of AI vs. human) — Quote: “evaluated by Gestalt psychotherapy trainees using a Likert scale rating” (Position: Abstract),"","",""
112,Cioffi 2025,Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,03,2025,Other: Italy,Empirical research involving an LLM,Therapist-facing application,"","","",Treatment fidelity feedback,No dataset used for development or evaluation,"","","","","","","",Only prompting,"in a further chat interface (test 2), we
introduced the scope of expertise using a prompt as a pretraining tool, before proceeding
with the presentation of the case. This prompt was generated using ChatGPT, and its
reliability as a pretraining tool for the chat session was verified as a good alternative to
a longer and more complex training process in a previous study we conducted [12]","Unspecified, might include formal therapy methods; Other: Gestalt Therapy techniques, Gestalt supervision",GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,B,H,qualified human supervisor,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
112,Cioffi 2025,Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT,Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),17,03,2025,Other: Italy,Empirical research involving an LLM,Therapist-facing application,"","","",Treatment fidelity feedback,No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Only prompting,"in a further chat interface (test 2), we
introduced the scope of expertise using a prompt as a pretraining tool, before proceeding
with the presentation of the case. This prompt was generated using ChatGPT, and its
reliability as a pretraining tool for the chat session was verified as a good alternative to
a longer and more complex training process in a previous study we conducted [12]","Unspecified, might include formal therapy methods; Other: Gestalt Therapy techniques, Gestalt supervision",GPT-4 / GPT-4o family,Yes,"",N,Y,Y,Y,"","Satisfaction ratings focused on relational/emotional quality, technical/didactic, treatment support, professional orientation. — Quote: “PCA highlighted four components… relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4)” (Position: Abstract)","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,B,H,valuated by Gestalt psychotherapy trainees,N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract)",N,"","","",""
111,Lee 2025,Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24,3,2025,Other: Korea,Empirical research involving an LLM,Therapist-facing application,"",No clients/patients involved,"",Treatment fidelity feedback,External data set," Korean Children Voice Records from
AI-Hub","Other: Speech data from children,  child
counseling service provided by the Professor Youjin Han’s
research lab",Other: Korean,No,Yes,Unselected,Trained professionals,Fine-tuning + other modules,"","","",Yes,"",Y,Y,Y,Y,SEM,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
111,Lee 2025,Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),24,3,2025,Other: Korea,Empirical research involving an LLM,Therapist-facing application,"","Other: ""Child counseling experts""",48,Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition,No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"Prompting of different LLMs + other modules (speech-to-text, editable dashboard, etc.)","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,Yes,"",n,y,y,y,"",User experience assessment of therapists using the tool. There is a quantiative (Likert scale questionnaires) and qualitative (open-ended user questions) assessment,"",n,"","","",n,"","","",n,"","","",n,"","","","",y,b,l,"Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
111,Lee 2025,Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),24,3,2025,Other: Korea,Empirical research involving an LLM,Therapist-facing application,"","Other: ""Child counseling experts""",48,Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition,No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Prompting + other modules,"Prompting of different LLMs + other modules (speech-to-text, editable dashboard, etc.)","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,Yes,"",n,y,y,y,"",User experience assessment of therapists using the tool. There is a quantiative (Likert scale questionnaires) and qualitative (open-ended user questions) assessment,"",n,"","","",n,"","","",n,"","","",n,"","","","",y,b,l,"Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
109,Giri 2025,Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11,09,2025,Other: Finland,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,70,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"GPT-4 outputs compared with curated expert book excerpts; randomized and blinded survey design. — Quote: “participants were blinded in terms of which advice was book-based and which was ChatGPT generated.” (Position: Methods, p. 36)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,N,Y,Y,"","72.86% rated GPT-4 advice practical … 87.94% well-explained … statistically significant” (Position: Results, p. 41–42)

GPT-4 advice perceived as more practical and clearer; effect sizes small-to-medium. — Quote: “differences … statistically significant … d ≈ 0.34 … d ≈ 0.53 …” (Position: Results, p. 41–42)","",N,"","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
109,Giri 2025,Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",02,03,2025,Other: Finnland,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,70,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"For the ChatGPT-generated advice, version GPT-4 was used. On October 12, 2023, 
the following question was asked on a fresh chatbot session and the response was saved:
[...] Since the study’s objective was to evaluate performance of ChatGPT-4 with-
out any refinement or customization, we deliberately refrained from using any prompt 
engineering techniques in our design and opted to use the afore mentioned one-time 
instruction to the LLM. ","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",Lay Rating,B,H (?),Book written by a human expert.,"","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"„Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“",""
109,Giri 2025,Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",02,09,2024,Other: Finland,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,70,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"For the ChatGPT-generated advice, version GPT-4 was used. On October 12, 2023, 
the following question was asked on a fresh chatbot session and the response was saved:
[...] Since the study’s objective was to evaluate performance of ChatGPT-4 with-
out any refinement or customization, we deliberately refrained from using any prompt 
engineering techniques in our design and opted to use the afore mentioned one-time 
instruction to the LLM. ","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,N,Y,Y,"","72.86% rated GPT-4 advice practical … 87.94% well-explained … statistically significant” (Position: Results, p. 41–42)

GPT-4 advice perceived as more practical and clearer; effect sizes small-to-medium. — Quote: “differences … statistically significant … d ≈ 0.34 … d ≈ 0.53 …” (Position: Results, p. 41–42)","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","",Lay Rating,B,H,Book written by a human expert.,"","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"„Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“",""
108,M 2024,Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,12,2024,Other: SriLanka,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),unknown,"",External data set,"",Psychotherapy -- speech transcripts,Other: indian,Yes,No,Unselected,Trained professionals,Only fine-tuning,"",CBT: Cognitive restructuring,GPT-3.5 family,No,"",n,n,n,n,"","","","","","","","","","","",Y,"","",no benchmark,"","","","","",y,"","",no benchmark,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
108,M 2024,Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,12,2024,Other: Sri Lanka,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,cognitive distortion identification dataset,Other: Labeled examples of cognitive distortions,Other: unknown,Yes,No,Unknown,Other: not applicable,Prompting + other modules,GPT-3.5 is only prompted but there is also an intent classifier and cognitive distortion identifier,CBT: Cognitive restructuring,GPT-3.5 family; Other: text-embedding-3-small,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,"Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.",n,"","","","",y,no benchmark,no benchmark,Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
108,M 2024,Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,12,2024,Other: Sri Lanka,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,cognitive distortion identification dataset,Other: Labeled examples of cognitive distortions,Other: unknown,Yes,No,Unknown,Other: not applicable,Prompting + other modules,GPT-3.5 is only prompted but there is also an intent classifier and cognitive distortion identifier,CBT: Cognitive restructuring,GPT-3.5 family; Other: text-embedding-3-small,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,"Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.",n,"","","","",y,no benchmark,no benchmark,Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
107,D 2024,Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,12,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Other: unknown,Some documents for their RAG database but no further information is given.,"","","","","","",Prompting + other modules,"Llama 2 is prompted, but there is also a RAG component","Unspecified, might include formal therapy methods",Llama 2 family,No users involved,"","","","","","","","",y,no benchmark,no benchmark,BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.,n,"","","",y,no benchmark,no benchmark,"Various classification metrics (accuracy, precision, recall, f1) for checking how ""relevant"" the responses are. No information is given on how ""relevance"" is measured here.",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
107,D 2024,Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),23,1,2024,India,Population survey,Client-facing application,Other: unclear,Other: unclear,unclear,"",Other: unclear,"","","","","","","",Prompting + other modules,"","Unspecified, might include formal therapy methods",Llama 2 family,Yes,"",N,Y,Y,N,"","","",Y,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
107,D 2024,Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,12,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Other: unclear,"","","","","","","",Prompting + other modules,"Llama 2 is prompted, but there is also a RAG component","Unspecified, might include formal therapy methods",Llama 2 family,No users involved,"","","","","","","","",Y,no benchmark,no benchmark,BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.,n,"","","",n,"","","Various classification metrics (accuracy, precision, recall, f1) for checking how ""relevant"" the responses are. No information is given on how ""relevance"" is measured here.",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
106,M 2024,Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),15,9,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Primate2022 https://github.com/primate-mh/Primate2022,Internet data -- mental health forum,English,No,Yes,Unselected,Other: not applicable,Prompting + other modules,Prompt engineering + RAG,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","",y,no benchmark,no benchmark,"BLEU, ROUGE with counselor responses as reference (CounselChat)",y,no benchmark,no benchmark,Wonky measurement: embedding similarity between chatbot responses and patient inputs,n,"","","",y,no benchmark,no benchmark,Comparison of chatbot-inferred PHQ-9 scores with human scorers via ICC and Cohen's kappa,"",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
106,M 2024,Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),15,9,2024,Other: Canada ,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"Evalution with Primate2022 dataset, but not for development ","","","","","","",Only fine-tuning,"","Unspecified, might include formal therapy methods",GPT-3.5 family,"","","","","","","","","",Y,"",""," BLEU [57] and ROUGE
[58] metrics",Y,"","",on average 0.93 (±0.03) similarity in the embedding space.,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
106,M 2024,Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),15,9,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Primate2022 https://github.com/primate-mh/Primate2022 (but only for evaluation),Internet data -- mental health forum,English,No,Yes,Unselected,"Other: not applicable (only initial posts, no responses)",Prompting + other modules,Prompt engineering + RAG,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","",Y,no benchmark,no benchmark,"BLEU, ROUGE with counselor responses as reference (CounselChat)",Y,no benchmark,no benchmark,"on average 0.93 (±0.03) similarity in the embedding space.

Wonky measurement: embedding similarity between chatbot responses and patient inputs",n,"","","",y,no benchmark,no benchmark,Comparison of chatbot-inferred PHQ-9 scores with human scorers via ICC and Cohen's kappa,"",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
104,P 2025,Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,4,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,"The chatbot’s corpus comprises 200 manually curated context-response pairs that cover themes like stress, anxiety, depression, and motivation","Other: ""context-response pairs"" -- what is that?",Other: unknown,Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of all-MiniLM-L6-v2 for relevance detection + response generation by Gemini 1.5 Flash,"Unspecified, might include formal therapy methods",BERT family; Gemini / Bard family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,Classification into relevant/irrelevant via all-MiniLM-L6-v2,n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",y,4) Crisis Intervention and Ethical Safeguards,n,"",n,Gemini 1.5 Flash,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
104,P 2025,Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,4,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,"The chatbot’s corpus comprises 200 manually curated
context-response pairs that cover themes like stress, anxiety, 
depression, and motivation",Other: Self curated ,Other: ,Yes,No,Other: no users in study ,"",Prompting + other modules,"The proposed chatbot system aims to enhance mental health 
assistance through generative AI and embedding-based 
relevance detection. Unlike rule-based chatbots with 
predefined responses, our chatbot responds dynamically to 
diverse user inputs. The core innovations include few-shot 
learning for personalized conversation, semantic similarity 
filtering to ensure relevance, and generative response 
generation using Google Gemini API. ","Unspecified, might include formal therapy methods",Gemini / Bard family,No users involved,"","","","","","","","","","","","","","","","",Y,B,L,"Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries) ","","","","","","","","","","","","","","","","","","","","","","","","Response Quality Assessments (evaluating coherence,
empathy, tone) ","","","","User Interaction Studies (questionnaires and session 
reviews)","","","","","","","","",Y,"The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response",Y,"The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response1",N,"",Y,"To ensure ethical deployment, the system avoids medical 
advice, doesn’t store personal data, and complies with data 
protection standards. Sensitive inputs are not logged or 
reused. ",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
104,P 2025,Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17,4,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,"The chatbot’s corpus comprises 200 manually curated context-response pairs that cover themes like stress, anxiety, depression, and motivation","Other: unclear (authors call it ""context-response pairs""",Other: unknown,Other: unknown,No,Unknown,Unknown,Prompting + other modules,"The proposed chatbot system aims to enhance mental health 
assistance through generative AI and embedding-based 
relevance detection. Unlike rule-based chatbots with 
predefined responses, our chatbot responds dynamically to 
diverse user inputs. The core innovations include few-shot 
learning for personalized conversation, semantic similarity 
filtering to ensure relevance, and generative response 
generation using Google Gemini API. ","Unspecified, might include formal therapy methods",BERT family; Gemini / Bard family,No users involved,"","","","","","","","",n,"","","",n,"","","",Y,no benchmark,no benchmark,"Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries)

Classification into relevant/irrelevant via all-MiniLM-L6-v2",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",Y,"The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response

4) Crisis Intervention and Ethical Safeguards",n,"",N,Gemini 1.5 Flash,n,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
103,Shiwakoti 2025,Addressing the Challenges of Mental Health Conversations with Large Language Models,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",28,04,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,HOPE (Mental Health cOunselling of PatiEnts),Emotional support dialogue -- speech transcripts,English,No,Yes,Unselected,Unknown,Fine-tuning + other modules,"The authors adapted ChatMGL, a GPT-2–based model originally trained on STEM Q&A with supervised fine-tuning and PPO, for mental health dialogue. They dropped PPO, focusing instead on supervised fine-tuning with Hugging Face’s SFTT and Delta Tuning (a parameter-efficient method that updates only part of the weights). ","Unspecified, might include formal therapy methods",BERT family; GPT-2 family,No users involved,"","","","","","","","",Y,B,L,"METEOR and ROUGE tested, but Strawman models",Y,B,L,"",Y,B,L,"","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
103,Shiwakoti 2025,Addressing the Challenges of Mental Health Conversations with Large Language Models,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",28,5,2025,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved,"","",External data set,"We use the HOPE (Mental Health cOunselling of PatiEnts) [8]
dataset for our experiments. The dataset is specifically designed
for mental health counseling and dialogue-act classification tasks.
The HOPE dataset includes approximately 12,800 utterances ex-
tracted from 212 mental health counseling sessions, sourced from
publicly available YouTube videos. ",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,"To this end, we propose a set of mod-
ifications: supervised fine-tuning on a carefully designed mental
health dataset and incorporating dialogue-act labels that capture
the unique structure and emotional undercurrents of therapeutic
sessions. By evaluating these adaptations, we show how ChatMGL
can be transformed into a more context-aware and empathetic
system, capable of generating clinically relevant and supportive
responses","Informal counseling (e.g., emotional support conversation)",GPT-2 family,No users involved,"","","","","","","","",Y,B,L,"",N,"","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
103,Shiwakoti 2025,Addressing the Challenges of Mental Health Conversations with Large Language Models,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",28,04,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"We use the HOPE (Mental Health cOunselling of PatiEnts) [8]
dataset for our experiments. The dataset is specifically designed
for mental health counseling and dialogue-act classification tasks.
The HOPE dataset includes approximately 12,800 utterances ex-
tracted from 212 mental health counseling sessions, sourced from
publicly available YouTube videos. ",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,"The authors adapted ChatMGL, a GPT-2–based model originally trained on STEM Q&A with supervised fine-tuning and PPO, for mental health dialogue. They dropped PPO, focusing instead on supervised fine-tuning with Hugging Face’s SFTT and Delta Tuning (a parameter-efficient method that updates only part of the weights). ","Unspecified, might include formal therapy methods",BERT family; GPT-2 family,No users involved,"","","","","","","","",Y,B,L,"METEOR and ROUGE tested, but Strawman models",Y,B,L,"",Y,B,L,"",N,"","","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",""
102,,Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",10,3,2025,USA,Empirical research involving an LLM,Therapist-facing application,Multi-turn chatbot,"","",Utterance suggestions,No dataset used for development or evaluation,"","","","","","","",Only prompting,"Prompts were comprised of (1) context for the request, (2) how the model was meant to respond, 3) the specific request, and (4) the output format ",Other CBT techniques,GPT-4 / GPT-4o family,Yes,"",N,N,Y,Y,"","Two staff psychologists (EB, AJ) each reviewed all 
ChatGPT responses and evaluated them along the 
following dimensions: (1) task completion and (2 
degree to which input information was incorporated in the output.","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,W,H,human therapists,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
102,,Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",10,3,2025,USA,Empirical research involving an LLM,Therapist-facing application,"",No clients/patients involved,"",Other: OCD exposure hierarchy generation,No dataset used for development or evaluation,"","","","","","","",Only prompting,"",Other CBT techniques,GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,w,h,benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
102,,Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",10,3,2025,USA,Empirical research involving an LLM,Therapist-facing application,Other: ,"","",Other: OCD exposure hierarchy generation (therapy material generation),No dataset used for development or evaluation,"","","","","","","",Only prompting,"Prompts were comprised of (1) context for the request, (2) how the model was meant to respond, 3) the specific request, and (4) the output format ",Other CBT techniques,GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,W,H,benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
101,Scholich 2025,A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",21,05,2025,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,Used scripted prompts to elicit chatbot interactions and compared with therapists; coded with MULTI; non-naturalistic brief interactions. — Quote: “We also only examined brief interactions with scripted prompts… The interaction logs … were coded using the Multitheoretical List of Therapeutic Interventions” (Position: p. 12; PubMed Methods).,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified; Other: Pi, and Replika",No,"","","","","","","","","","","","","","","","",Y,-,H,“coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” ,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"significant gaps in chatbots’ crisis management abilities, including the absence of risk assessment and failure to refer users to lifesaving crisis hotlines” (Position: p. 12).","","","","","","","","","","","","","","","","","","","","",""
101,Scholich 2025,A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",21,05,2025,USA,Empirical research involving an LLM,Client-facing application,"",Other: Therapists,17,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Each chatbot was prompted in the most up-to-date
version...","Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Other: Chatbots Pi & Replika, Modell behind unclear ",No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,-,-,-,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.",Y,"See e.g., ""Strong Framing of Opinions and Suggestions""",N,"",N,"",Y,Table 1,N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
101,Scholich 2025,A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",21,05,2025,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved; Other: Therapists,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,Used scripted prompts to elicit chatbot interactions and compared with therapists; coded with MULTI; non-naturalistic brief interactions. — Quote: “We also only examined brief interactions with scripted prompts… The interaction logs … were coded using the Multitheoretical List of Therapeutic Interventions” (Position: p. 12; PubMed Methods).,"Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Other: Pi, Replika",No,"","","","","","","","",N,"","","",N,"","","",Y,-,H,“coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” ,N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.",Y,"See e.g., ""Strong Framing of Opinions and Suggestions""",N,"",N,"",Y,Table 1,N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
100,Luo 2025,"""Shaping ChatGPT into my Digital Therapist"": A thematic analysis of social media discourse on using generative artificial intelligence for mental health",Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10,7,2025,USA,Other: thematic analyses of social media data,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,"",Internet data -- mental health forum,English,No,No,Unknown,Unknown,"","","","",No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
100,Luo 2025,"""Shaping ChatGPT into my Digital Therapist"": A thematic analysis of social media discourse on using generative artificial intelligence for mental health",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10,7,2025,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,87 Reddit posts,"",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",n,y,n,y,"","Qualitative analysis:
This approach involves six main steps: getting familiar with the data; generating initial codes; searching for themes; reviewing themes; defining and naming themes; and producing the report. The analysis process was iterative and recursive, allowing the researchers to move back and forth between these steps as needed to ensure thoroughness and validity. ","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
100,Luo 2025,"""Shaping ChatGPT into my Digital Therapist"": A thematic analysis of social media discourse on using generative artificial intelligence for mental health",Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10,7,2025,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,87 Reddit posts,"",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,"","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",n,y,n,y,"","Qualitative analysis:
This approach involves six main steps: getting familiar with the data; generating initial codes; searching for themes; reviewing themes; defining and naming themes; and producing the report. The analysis process was iterative and recursive, allowing the researchers to move back and forth between these steps as needed to ensure thoroughness and validity. ","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
99,VanMeter 2025,The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",8,1,2025,USA,Empirical research involving an LLM,Therapist-facing application,Multi-turn chatbot,"","",Patient simulations,Self-collected data,"","","","","","","",Only prompting,"","Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,B,L,older and other LLM,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
99,VanMeter 2025,The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",8,1,2025,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family; Other: Microsoft Copilot,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,7 different safety questions,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",y,entire study is about this,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
99,VanMeter 2025,The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",8,1,2025,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"",Other: ,No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,no benchmark,no benchmark,7 different safety questions,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",y,entire study is about this,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
98,M 2024,Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),9,10,2024,Other: Indonesia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,58,"","",data from Counsel Chat repository and Indonesian mental health articels,Emotional support dialogue -- chat logs,Other: English and Indonesian ,No,Yes,Unselected,Trained professionals,Only fine-tuning; Only prompting,"In this project, prompts were crafted based on common mental health issues such as  anxiety, stress, and academic pressure. 
Initially trained on a general mental health dataset, the model gained a basic understanding of mental health issues such as anxiety and depression. However, to address local language and cultural nuances, additional data from Indonesian sources, including articles and forums, were integrated.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,N,Y,Y,"","10 questions regarding user interface, ease of use, chatbot responsiveness, 
and overall satisfaction. might include standard instrument, but not described.","","","","","","","","","",Y,"","",Accuracy,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"",N,"",N,"",Y,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
98,M 2024,Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9,10,2024,Other: Indonesia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,58,"",External data set,"Additionally, local data from Indonesian mental health articles and discussion forums fine-tuned the model.","Other: ""mental health articles and discussion forums""",Other: Indonesian,Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of GPT-4 + RAG (see Fig. 1) and user interface,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",n,n,y,y,"","A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.","",n,"","","",n,"","","",y,no benchmark,no benchmark,"""training accuracy"" of generative LLM. Not sure what this accurac is measuring.",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",User experience rating,no benchmark,no benchmark,"A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
98,M 2024,Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9,10,2024,Other: Indonesia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,58,"",External data set,"Additionally, local data from Indonesian mental health articles and discussion forums fine-tuned the model.","Other: ""mental health articles and discussion forums""",Other: Indonesian,No,No,Unknown,Unknown,Fine-tuning + other modules; Prompting + other modules,Fine-tuning of GPT-4 + RAG (see Fig. 1) and user interface,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",n,n,y,y,"","10 questions regarding user interface, ease of use, chatbot responsiveness, 
and overall satisfaction. might include standard instrument, but not described.","",n,"","","",n,"","","",y,no benchmark,no benchmark,"""training accuracy"" of generative LLM. Not sure what this accurac is measuring.",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",User experience rating,no benchmark,no benchmark,"A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
97,Y 2024,A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",08,08,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,16,Utterance suggestions,Self-collected data,"collected a substantial dataset comprising 21,924 records
Fig. 2. Volunteer Screening Chart
through five distinct avenues: instances, media, literature,
guidelines, and databases. To ensure the integrity and rele-
vance of our data, we undertook a meticulous cleaning pro-
cess spearheaded by three experienced quality controllers.
Each controller boasts broad expertise in mental health
counseling. Initially, we eliminated duplicates and linguistic
inaccuracies. Subsequently, we focused on screening data
for key terms such as “sleep”, “dream”, “evening”, “night”,
“bed” and related expressions. This step aimed to sift out
conversational data irrelevant to our study’s objectives, en-
suring that the textual content was pertinent to the context
of potential sleep disorders. We further conducted dialog
integrity screening. Finally, through manual inspection on a
case-by-case basis, we excluded dialogue data that contra-
dicted universal core values. This rigorous curation process
resulted in the selection of 764 dialogues",Emotional support dialogue -- chat logs,Other: unknown,No,No,Unknown,Unknown,Fine-tuning + other modules,"ChatGLM model with LoRA fine-tuning, optimized using AdamW for 450 epochs. — Quote: “We optimized the model parameters using the AdamW optimizer… training process spanned 450 epochs.” (Position: Model Construction)","Unspecified, might include formal therapy methods",Other:  ChatGLM-LoRA,Yes,"",N,N,Y,Y,"",High satisfaction (>80% smooth design); moderate effectiveness (~50% improved sleep); high acceptance (75% continued use). — Quote: “More than 80% of the volunteers think that our app is well designed and smooth to use” (Position: Application Utilization),"",Y,B,L,metrics… including BLEU-4 and ROUGE” (Position: Model Evaluation),"","","","",Y,B,L,evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation),"","","","","",N,"","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","",N,"","","",Y,ChatGLM is open-source/self-hostable,"","","","","","","","","","","","","","","","","","","Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one."
97,Y 2024,A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8,8,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,16,"","External data set, modified","Between November 28 and December 14, 2023, our study collected a substantial dataset comprising 21,924 records through five distinct avenues: instances, media, literature,
guidelines, and database.",Other: Mixed,Other: unknown,No,No,Unknown,Unknown,Only fine-tuning,LoRA,"Unspecified, might include formal therapy methods",Other: ChatGLM,Yes,"",N,N,Y,Y,"",See Application Utilization,"",Y,-,-,-,N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",Y,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
97,Y 2024,A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",08,08,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,16,Other: ,Self-collected data,"collected a substantial dataset comprising 21,924 records
Fig. 2. Volunteer Screening Chart
through five distinct avenues: instances, media, literature,
guidelines, and databases. To ensure the integrity and rele-
vance of our data, we undertook a meticulous cleaning pro-
cess spearheaded by three experienced quality controllers.
Each controller boasts broad expertise in mental health
counseling. Initially, we eliminated duplicates and linguistic
inaccuracies. Subsequently, we focused on screening data
for key terms such as “sleep”, “dream”, “evening”, “night”,
“bed” and related expressions. This step aimed to sift out
conversational data irrelevant to our study’s objectives, en-
suring that the textual content was pertinent to the context
of potential sleep disorders. We further conducted dialog
integrity screening. Finally, through manual inspection on a
case-by-case basis, we excluded dialogue data that contra-
dicted universal core values. This rigorous curation process
resulted in the selection of 764 dialogues",Other: Mixed,Other: unknown,No,No,Unknown,Unknown,Only fine-tuning,LoRA,"Unspecified, might include formal therapy methods",Other: ChatGLM,Yes,"",N,N,Y,Y,"",High satisfaction (>80% smooth design); moderate effectiveness (~50% improved sleep); high acceptance (75% continued use). — Quote: “More than 80% of the volunteers think that our app is well designed and smooth to use” (Position: Application Utilization),"",Y,-,-,-,N,"","","",Y,B,L,evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation),N,"","","","",N,"","","",N,"","","","",N,"","","","","","","","","","","","","","","","","","","","","",N,"",N,"",Y,ChatGLM is open-source/self-hostable,N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one."
96,,Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12,12,24,Other: Slovenia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: Convenience sample,42,"",Self-collected data,"First, we introduce a novel, golden standard dataset, comprising panel 
data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires 
and qualitative daily diary entries. ",Other: ,Other: unknown,No,No,Unknown,Unknown,Fine-tuning + other modules,"The system uses a cognitive architecture that combines NLP, a Theory of Mind module with user modeling and forecasting, CBT-based expert knowledge, strategy control, and natural language generation.","Other CBT techniques; Unspecified, might include formal therapy methods; Other: A strategy is selected and adapted according to the metrics, determined in the previous part. The text serves to mitigate the user’s mental health problems based mostly on CBT, and, if the forecasted trend is negative, to try to break 
that trend. To ensure that the user follows the selected strategy, the text on CBT is wrapped in a persuasion strategy.","Other: Long-short term memory 
network (LSTM)?",Yes,"",Y,N,Y,Y,"One of the ICAs on the 
7-point Likert scale using two measures from UEQ"," “Our system” (M = 5.368) found this work’s system statistically significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).","",N,"","","",N,"","","",Y,B,L,Woebot,Y,B,L,Woebot,"",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.",Y,"Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair",N,"",N,"",N,"",""
96,,Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12,12,2024,Other: Slovenia,Empirical research involving an LLM,"",Multi-turn chatbot,General population,42,"",Self-collected data,"495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores … and qualitative daily diary entries",Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) ,Other: unknown,No,No,Other: convenience sample,Unknown,Fine-tuning + other modules,"“cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies” (Position: Methods)","Unspecified, might include formal therapy methods","Other: highest accuracy: 91.41% using k-nearest neighbors (kNN),highest accuracy of other systems: 84% using … long-short term memory network (LSTM)",Yes,"",Y,N,Y,Y,UEQ," “Our system” (M = 5.368) found this work’s system statistically 
significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the 
measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).","",N,"","","",N,"","","",Y,B,L,"Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation)",Y,B,L,"Table 6 Accuracy of machine learning models in forecasting 7-day 
SAD (stress, anxiety, and depression) levels based on single text 
entries against extrapolation","",N,"","","","","","","","",N,"","","","","","","","","Stress, Anxiety, Depression",B,L ,Woebot,"","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,comparison with Woebot — Quote: “system outperformed Woebot … in reducing stress … and anxiety levels” (Position: Results),"","","","",""
96,,Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12,12,2024,Other: Slovenia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,42,"",Self-collected data,"First, we introduce a novel, golden standard dataset, comprising panel 
data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires 
and qualitative daily diary entries. ",Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) ,Other: unknown,No,No,Unknown,Other: n.a.,Fine-tuning + other modules,"“cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies” (Position: Methods)","Other CBT techniques; Unspecified, might include formal therapy methods","GPT-3 family; Other: GPT-Neo, GPT-J, AI21 Jurassic-1",Yes,"",Y,N,Y,Y,"One of the ICAs on the 
7-point Likert scale using two measures from UEQ"," “Our system” (M = 5.368) found this work’s system statistically significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).","",N,"","","",N,"","","",Y,B,L,"Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation)",Y,B,L,"Table 6 Accuracy of machine learning models in forecasting 7-day 
SAD (stress, anxiety, and depression) levels based on single text 
entries against extrapolation","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.",Y,"Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.","","","","","","","","","","","","",Y,"Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair",Y,"","","","","",""
94,Meyer 2025,"LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour",Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",30,4,2025,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,"","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules",CBT: Motivational interviewing,GPT-4 / GPT-4o family,Yes,"",n,y,y,y,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Client Evaluation of Motivational Interviewing Scale (CEMI) (Madson et al., 2013, 2015, 2016)",S,L,without MI fine-tuning,"Working Alliance Inventory - short
revised (WAI-SR) ",S,L,without MI fine-tuning,𝛥 Readiness to Change,S,L,without MI fine-tuning,"",Y,"We identify harmful outputs in two ways: First, we take participant’s
ratings of generated turns into account, specifically with respect to
turns rated as offensive/harmful. Secondly, we draw on MI-literature as
well as empathy research and ethical academic discussions about LLMs
outlined below to identify behaviours which are unsuited specifically
in the context of LLM-delivered MI-conversations. Based on this, we
create an annotation scheme to be applied to the logged conversations after data collection. This annotation scheme goes beyond the
identification of MI-Inadherent behaviours defined in the MISC (listed
in Table 1). While such behaviours are not suited for MI interactions
in general, we lay special emphasis on such actions that would be
suitable in MI-interactions with a human counsellor, but should be
avoided when responses are generated by LLMs. We focus specifically
on significant concerns around anthropomorphism and the unreliable
factual correctness of LLM outputs",y,"To test the validity of the annotation scheme, the first author
annotated each GPT-4 generated turn collected in the user study along
the 4 dimensions of undesired behaviour outlined above, allowing
multiple labels per turn. Based on the annotation scheme, GPT-outputs
containing advice, factual information, sharing personal information, or
revealing emotions, as well as outputs with directive or confrontational
wording were marked as harmful. Following this, a second annotator
(not an author) was employed to annotate a share of the turns along
the four axes. 50 randomly sampled turns, along with the guidelines,
were given to the annotator for annotation in a training round.",n,"",y,in the schema,n,"",n,"",n,"",n,"",n,"",y,"either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle",n,"",n,"",""
94,Meyer 2025,"LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour",Richard Gaus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),30,4,2025,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,"","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.",CBT: Motivational interviewing,GPT-4 / GPT-4o family,Yes,"",y,y,y,y,German version of the Working Alliance Inventory - short revised (WAI-SR),"Also measuring undesirable output via annotation.
For qualitative results, see 4.3.3.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","Client Evaluation of Motivational Interviewing Scale (CEMI), Readiness to Change delta",s,l,Measures MI adherence of clients. Benchmark: GPT-4 out of the box,Number of harmful outputs (user-rated),b,l,Benchmark: GPT-4 out of the box,German version of the Working Alliance Inventory - short revised (WAI-SR),s,l,Benchmark: GPT-4 out of the box,"",n,"",n,"",n,"",n,"",y,"",n,"",n,"",n,"",y,"",y,Control: GPT-4 out-of-the-box,n,"",n,"",""
94,Meyer 2025,"LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour",Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),30,4,2025,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,159,"",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.

MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules",CBT: Motivational interviewing,GPT-4 / GPT-4o family,Yes,"",n,y,y,y,"","Also measuring undesirable output via annotation.
For qualitative results, see 4.3.3.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","Client Evaluation of Motivational Interviewing Scale (CEMI) (Madson et al., 2013, 2015, 2016); Readiness to Change delta",S,L,Measures MI adherence of clients. Benchmark: GPT-4 out of the box without MI fine-tuning,Number of harmful outputs (user-rated),b,l,Benchmark: GPT-4 out of the box without MI fine-tuning,German version of the Working Alliance Inventory - short revised (WAI-SR),S,L,Benchmark: GPT-4 out of the box without MI fine-tuning,"",n,"",n,"",n,"",n,"",y,"",n,"",n,"",n,"",n,WAI-SR but is neither symptom nor function scale,y,"either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle

Control: GPT-4 out-of-the-box",n,"",n,"",""
93,N 2025,Early Detection and Personalized Intervention in Mental Health,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16,04,2025,India,Empirical research involving an LLM,Analysis of conversation transcripts,Multi-turn chatbot,No clients/patients involved,"","",External data set,epsilon3/cbt-cognitive-distortions-analysis,Psychotherapy -- chat logs,English,Yes,Yes,Unknown,Other: ,Only fine-tuning,"",Other CBT techniques,GPT-2 family,No users involved,"","","","","","","","",Y,unsure,L,Bleu score,"","","","",Y,unsure,L,"","","","","","","","","","","","","","","",Y,W,L,"","","","","","",Empathy Score,B,L,"",Human Helpfulness,B,L,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
93,N 2025,Early Detection and Personalized Intervention in Mental Health,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14,2,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"epsilon3/cbt-cognitive-distortions-analysis dataset
https://huggingface.co/datasets/epsilon3/cbt-cognitive-distortions-analysis
",Internet data -- mental health Q&A,English,Yes,Yes,Unknown,Other: not applicable,Fine-tuning + other modules,Fine-tuning of GPT-2 + other modules such as BERT-based cognitive distortion classification,CBT: Cognitive restructuring,BERT family; GPT-2 family,No users involved,"","","","","","",Only quantitative scoring of ,"",y,b,l,BLEU against human responses. Benchmark: non fine-tuned GPT-2,n,"","","",n,"","",Performance of cognitive distortion classification of non-generative BERT model is given (but it's non-generative),n,"","","","",n,"","","",n,"","","","",y,b,l,Benchmark: non fine-tuned GPT-2,n,"","","","",User rating,b,l,"User scores of ""helpfulness"" and ""empathy"". Benchmark: non fine-tuned GPT-2.","","","","","","","","","",y,"The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.",n,"",y,GPT-2,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
93,N 2025,Early Detection and Personalized Intervention in Mental Health,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14,2,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"epsilon3/cbt-cognitive-distortions-analysis dataset
https://huggingface.co/datasets/epsilon3/cbt-cognitive-distortions-analysis
",Internet data -- mental health Q&A,English,Yes,Yes,Unknown,Other: not applicable,Fine-tuning + other modules,Fine-tuning of GPT-2 + other modules such as BERT-based cognitive distortion classification,CBT: Cognitive restructuring,BERT family; GPT-2 family,No users involved,"","","","","","",Only quantitative scoring of ,"",Y,b,l,BLEU against human responses. Benchmark: non fine-tuned GPT-2,"","","","",Y,no benchmark,no benchmark,Performance of cognitive distortion classification of non-generative BERT model is given (but it's non-generative),"","","","","",n,"","","",n,"","","","",Y,b,l,Benchmark: non fine-tuned GPT-2,n,"","","","",User rating,b,l,"User scores of ""helpfulness"" and ""empathy"". Benchmark: non fine-tuned GPT-2.","","","","","","","","","",y,"The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.",n,"",y,GPT-2,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
92,,Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",02,07,2025,USA,Population survey,Client-facing application,Multi-turn chatbot,No clients/patients involved,0,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Evaluated existing GPT-based chatbots via persona-based simulated interactions, scored with CAPE framework. ","Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",N,N,Y,"",??CAPE Framework — Quote: “The CAPE framework lays a foundation for future quality assessments” (Position: Discussion),"Strengths = accessibility, safety in most cases; Weaknesses = occasional advertising, reading level too high, missing referral for severe depression. ","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y?,"(suicidality referral present, severe depression detection failed) — Quote: “the ability of the chatbots to always recommend connecting to another person for users expressing suicidality … frequent failure to connect personas exhibiting severe depression to a human",Y,we did not find any unsafe recommendations for managing depression or wellness,"","","","","","","","","","","","","","","","","","","","","The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots."
92,,Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",02,07,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: Researchers,2,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.",Y,"Chatbots mostly preserved privacy and avoided harmful
content. However",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
92,,Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",02,07,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Evaluated existing GPT-based chatbots via persona-based simulated interactions, scored with CAPE framework. ","Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,no benchmark,no benchmark,"CAPE category ratings, no benchmark",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.",Y,"Chatbots mostly preserved privacy and avoided harmful
content. ",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots."
91,Song 2025,“Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26,04,2025,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),30,"",No dataset used for development or evaluation,"","","","","","","",Fine-tuning + other modules,"Used fine-tuning and modular architecture (context, expression, feedback generators) to simulate different therapeutic styles.
Three key technical components were employed: the Context Generator, the Expression Expander, and the Feedback Generator","Informal counseling (e.g., emotional support conversation)","Other: MiniMax 6.5s (245K), Doubao Function Call model (32K) ",Yes,"",Y,Y,Y,Y,SAM-Scale,"emotional valence and dominance improved; deliberate style increased happiness, rapid style increased surprise. ","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y?,Sam Scale?,"","","","","","",""
91,Song 2025,“Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26,4,2025,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,30,"",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"Despited being labeled as fine-tuning, I suppose it is more about promtpting and other modules (Context Generator, Expression Expander, and the Feedback Generator)?","Unspecified, might include formal therapy methods",Other:  MiniMax 6.5s(245K),No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,Self-Assessment Manikin Scale + heart rate variability,N,"",N,"",N,"",""
91,Song 2025,“Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26,4,2025,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires),30,"",No dataset used for development or evaluation,"","","","","","","",Fine-tuning + other modules,"Used fine-tuning and modular architecture (context, expression, feedback generators) to simulate different therapeutic styles.
Three key technical components were employed: the Context Generator, the Expression Expander, and the Feedback Generator.

Not clear if they really fine-tuned.","Unspecified, might include formal therapy methods","Other: MiniMax 6.5s (245K), Doubao Function Call model (32K) ",No,"","","","","",SAM-Scale,"emotional valence and dominance improved; deliberate style increased happiness, rapid style increased surprise. ","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,(Self-Assessment Manikin Scale)+ Heart Rate variability,N,"",N,"",N,"",""
90,Jafari 2025,"Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19,01,2025,Other: Iran,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients with disorder explicitly based on ICD or DSM,9,"",External data set,"ArmanEmo dataset, a Persian emotion detection dataset",Emotional support dialogue -- speech transcripts,Other: Persian,No,Yes,Unknown,Unknown,Prompting + other modules,"the proposed system integrates several modules: emotion detection, disorder identification, and language model validation see figure 1 ","Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods",BERT family,No,"",N,"","","","","","",N,"","","","","","","",Y?,"","","","","","","","",N,"","","","","","","","",N,"","","","","","","","",N,"","","","","","","","","","","","",Y,see 4.1,Y,"Responses are validated for non-toxicity before being delivered to
the user.",Y,"ParsBERT, XLM-R are open-source","","","","","","","","","","","","","","","","","","","This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy."
90,Jafari 2025,"Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19,01,2025,Other: Iran,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,9,"",Other: External data set + a modified (translated) data set,ArmanEmo & Jigsaw Toxic Comment Classification translated into persian,Other: Emotion detection dataset and classification dataset,Other: Persian,No,Yes,Unknown,Other: No responders,Only fine-tuning; Prompting + other modules,Trained on ArmanEmo dataset & Jigsaw Toxic Comment Classification translated into persian + user message validiation module. ,"Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family,Yes,"",-,Y,Y,N,"","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users",N,"",N,"",N,"",N,"",N,"",N,"",Y,"",N,"",N,"",N,"",""
90,Jafari 2025,"Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19,01,2025,Other: Iran,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),9,"",External data set,ArmanEmo,Internet data -- mental health forum,Other: Persian,No,Yes,Unknown,Lay people,Only fine-tuning; Prompting + other modules,Trained on ArmanEmo dataset & Jigsaw Toxic Comment Classification translated into persian + user message validiation module. ,"Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family,Yes,"",-,Y,Y,N,"","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,see 4.1,Y,"In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users",N,"",N,"",N,"",N,"",N,"",N,"",Y,PHQ-9,N,"",N,"",N,"","This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy."
89,S 2024,Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21,06,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Designed prompt phrases, iteratively refined; three prompting regimes (zero-shot, few-shot, CoT); modeled counselor reasoning style; two LLM families evaluated (ChatGLM, ERNIE Bot; also Qianwen)","Unspecified, might include formal therapy methods","Qwen family; Other: ChatGLM, ERNIE Bot, Qianwen (as model and as judge)",No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,B,L,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),"","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.)","","","","","","","","","","","","","","","","","","",""
89,S 2024,Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21,6,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"",Patient simulations,Self-collected data,"We gathered doctor-
patient conversations from online medical consultation web-
sites and identified representative psychological issues from
the experiences of people around us and public platforms
such as Weibo and Zhihu. Our dataset encompasses the most
common mental health concerns and A substantial portion of
the themes is centered on stress and interpersonal relationships,
as these two are important contributing factors to mental
disorders. We also considered the perspectives of the LGBTQ
community and diverse cultural groups.Utilizing this data, we
crafted 31 unique questions.",Other: ,Chinese,No,No,Unselected,Unknown,Only prompting,"Three different prompt methods: Zero-Shot, Few-Shot, Chain of Thought (CoT)","Informal counseling (e.g., emotional support conversation)","Other: ERNIE Bot, Qianwen, and ChatGLM",No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","",Y,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
89,S 2024,Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21,06,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"",Patient simulations,No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Only prompting,"Designed prompt phrases, iteratively refined; three prompting regimes (zero-shot, few-shot, CoT); modeled counselor reasoning style; two LLM families evaluated (ChatGLM, ERNIE Bot; also Qianwen)","Unspecified, might include formal therapy methods","Qwen family; Other: ChatGLM, ERNIE Bot, Qianwen (as model and as judge)",No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","",Y,B,L,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),"","","","","","","","","","",llm-as-a-judge emotional resonance and understanding,b,l,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),llm-as-a-judge professionalism etc.,b,l,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),"","","","","","","","","",Y,"	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.)","","","","","","","","","","","","","","","","","","",""
88,Barzkar 2025,The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",26,5,2025,Other: Iran,Other: Case study,Client-facing application,Multi-turn chatbot,Patients with disorder explicitly based on ICD or DSM,1,"",No dataset used for development or evaluation,"","","","","","","",Other: None,None,"Unspecified, might include formal therapy methods",Other: unspecified,No,"",N,N,N,N,"","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
88,Barzkar 2025,The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",26,5,2025,Other: Iran,Other: Case study,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),1,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",n,y,n,y,"","This article is a case study with loose, qualitative descriptions","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
88,Barzkar 2025,The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",26,5,2025,Other: Iran,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),1,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",n,y,n,y,"","This article is a case study with loose, qualitative descriptions","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
87,,Evaluating Language Models for Assessing Counselor Reflections,Reviewer Two,"","",30,12,2024,USA,Empirical research involving an LLM,Client-facing application,"","","",Utterance suggestions,Other: both self-collected and external data set ,"",Psychotherapy -- speech transcripts,English,Yes,No,Unknown,Trained professionals,Fine-tuning + other modules; Prompting + other modules,"",CBT: Motivational interviewing,BERT family; GPT-3.5 family; Mistral family; Other: Flan ,No users involved,"","","","","","","","","","","","","","","","",Y,B,L,Recall,Y,S,L,"Pearson, Spearman, Kendall’s Tau","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
87,,Evaluating Language Models for Assessing Counselor Reflections,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,12,2024,USA,Empirical research involving an LLM,Therapist-facing application,"","","",Treatment fidelity feedback,"External data set, modified","Original dataset: Perez-Rosas motivational interviewing dataset.
Modification: Added custom annnotations
Also extended with their own MI prompts.
We thus obtained 4,365 client prompt-counselor reflection pairs, including 2,429 prompt-CR and 1,636 prompt-SR pairs.

public under https://lit.eecs.umich.edu/downloads.html#PAIR",Other: MI reflections (client prompt - counselor reflection pairs),English,Yes,Yes,Unknown,Trained professionals,Only prompting,"They fine-tune RoBERTa but only prompt the generative models (Flan-t5, Mistral, GPT-3.5) that they use",CBT: Motivational interviewing,BERT family; T5 family; GPT-3.5 family; Mistral family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","",yes for pair but this is not genAI,y,w,l,"Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark)","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
87,,Evaluating Language Models for Assessing Counselor Reflections,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,12,2024,USA,Empirical research involving an LLM,Therapist-facing application,"","","",Treatment fidelity feedback,"External data set, modified","Original dataset: Perez-Rosas motivational interviewing dataset.
Modification: Added custom annnotations
Also extended with their own MI prompts.
We thus obtained 4,365 client prompt-counselor reflection pairs, including 2,429 prompt-CR and 1,636 prompt-SR pairs.

public under https://lit.eecs.umich.edu/downloads.html#PAIR",Other: MI reflections (client prompt - counselor reflection pairs),English,Yes,No,Unknown,Trained professionals,Only prompting,"They fine-tune RoBERTa but only prompt the generative models (Flan-t5, Mistral, GPT-3.5) that they use",CBT: Motivational interviewing,BERT family; T5 family; GPT-3.5 family; Mistral family,No users involved,"","","","","","","","",n,"","","",n,"","","",Y,w,l,Recall@1 (Tables 3 and 4),y,w,l,"Pearson, Spearman, Kendall’s Tau. Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark)","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
85,Gabriel 2024,Can AI relate: Testing large language model response for mental health support,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),12,11,2024,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,not mentioned ,"",External data set," Reddit with of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.",Other: Reddit Posts,English,No,No,Unknown,Unknown,Only prompting,"","Peer support conversation; Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","",Y,S,H,"","","","","","","","","","","","","","","",EPIT-ONE framework,B,H,"",MITI,W,H,"",Empathy metrics ,S,H,"","","","","","","","","","","","","","","","","","","","","","","","","","","used metrics: EPIT-ONE framework, MITI, RoVERTa"
85,Gabriel 2024,Can AI relate: Testing large language model response for mental health support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,11,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"No specific name: ""We use the Reddit API to collect
a new dataset of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.""

Available under https://github.com/skgabriel/mh-eval",Internet data -- mental health forum,English,No,Yes,Unselected,Lay people,Only prompting,"","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,b,h,"Measures: Likert empathy rating, MITI global score. Benchmark: human peer supporter from Reddit post.",n,"","","","",n,"","","",n,"","","","",Empathy rating unequality,no benchmark,no benchmark,no benchmark. Measure: difference in automated empathy ratings between different ethnic groups,"","","","","","","","","",n,"",n,"",n,"",n,"",y,Appendix Table 2,y,"",n,"",n,"",n,"",n,"",n,"",n,"",""
85,Gabriel 2024,Can AI relate: Testing large language model response for mental health support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12,11,2024,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"No specific name: ""We use the Reddit API to collect
a new dataset of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.""

Available under https://github.com/skgabriel/mh-eval",Internet data -- mental health forum,English,No,Yes,Unselected,Lay people,Only prompting,"","Peer support conversation; Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,b,h,"Measures: Likert empathy rating, MITI global score. Benchmark: human peer supporter from Reddit post.",n,"","","","",n,"","","",n,"","","","",Empathy rating unequality,unknown,l,"Table 1: mechanical turk worker ratings (""Human"") vs. GPT-4","","","","","","","","","",n,"",n,"",n,"",n,"",y,Appendix Table 2,y,"",n,"",n,"",n,"",n,"",n,"",n,"","used metrics: EPIT-ONE framework, MITI, RoVERTa"
83,M 2025,Multimodal Framework for Therapeutic Consultations,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
83,M 2025,Multimodal Framework for Therapeutic Consultations,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
83,M 2025,Multimodal Framework for Therapeutic Consultations,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
82,Eryilmaz 2024,Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",20,11,2024,Other: Turkey,Empirical research involving an LLM,Client-facing application,Other: Emotional reflection generation,Other: unknown recruitment criteria,200,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"The model was trained using one-shot 
learning, meaning that it was given a client statement and 
a counselor’s emotional reflection response as input, and 
then used that information to generate its own emotional 
reflection response.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,w,h,likert-scale appropriateness rating of emotional reflection responses. compared GPT-4 against human counselor responses.,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
82,Eryilmaz 2024,Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",20,11,2024,Other: Turkey,Empirical research involving an LLM,Analysis of conversation transcripts,One-turn chatbot (usually Q&A),No clients/patients involved,"","",Other: emotional expressions from psychological counseling sessions,"",Psychotherapy -- speech transcripts,Other: ,No,No,Unknown,Trained professionals,Only prompting,"no development, only prompting ","Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Average rating of emotional reflections 
",W,H,human therapists ,"","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
82,Eryilmaz 2024,Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",20,11,2024,Other: Turkey,Empirical research involving an LLM,Client-facing application,Other: Emotional reflection generation,No clients/patients involved,"","",Self-collected data,"In this study, 200 real counseling ses
sions were recorded and transcribed to capture authentic 
client interactions. From these, 50 client statements were 
identified that best represented a range of emotions such as 
sadness, guilt, anxiety, determination, and anger, based on 
the PANAS Scale, along with helplessness, love, trust, lone
liness, and doubt from existing literature, aiming to ensure

emotional expressions from psychological counseling sessions",Psychotherapy -- speech transcripts,Other: unknown,No,No,Other: unknown,Trained professionals,Only prompting,"The model was trained using one-shot 
learning, meaning that it was given a client statement and 
a counselor’s emotional reflection response as input, and 
then used that information to generate its own emotional 
reflection response.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,w,h,likert-scale appropriateness rating of emotional reflection responses. compared GPT-4 against human counselor responses.,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
81,A 2024,Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,Richard Gaus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3,12,2024,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,"Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
81,A 2024,Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,Reviewer Two,Conference paper,"",3,6,2024,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,Not specified,"",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"Employs an ""Empathetic Meta-Chain (EMC) learning method"" that combines meta-learning, Chain of Thought prompting, and counseling strategies. Multiple approaches were compared including zero-shot, few-shot, CoT, meta-learning, and EMC.","Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,"","","We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
81,A 2024,Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,Consensus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3,12,2024,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,no benchmark,no benchmark,"We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance

Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
80,P 2023,Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*,Reviewer Two,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",7,12,2023,Other: Spain,Empirical research involving an LLM,"Other: question-answering system based on
retrieval augmented generation — this approach allows the
system to generate answers based on a corpus of documents
curated by psychologists and psychiatrists",One-turn chatbot (usually Q&A),No clients/patients involved,"","",Self-collected data,"",Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists,Other: spanish,No,No,"","",Fine-tuning + other modules,Retrieval Augmented Generation (RAG) kombiniert mit prompting,"Unspecified, might include formal therapy methods",BERT family,No,"",N,N,N,N,"","","","","","","",Y,B,L,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
80,P 2023,Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",7,12,2023,Other: Spain,Empirical research involving an LLM,"","","","","",External data set,"The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG",Other: Mental health education documents,Other: Spanish,No,No,Unknown,Unknown,"","","",BERT family; GPT-2 family; Other: BLOOMZ,No users involved,"","","","","","","","",n,"","","",y,no benchmark,no benchmark,Distance between embeddings of reference and output.,y,no benchmark,no benchmark,"F1 score, not quiet clear what of.",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
80,P 2023,Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*,Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",7,12,2023,Other: Spain,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists

The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG",Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists,Other: Spanish,No,No,Other: not applicable,Other: not applicable,Prompting + other modules,Retrieval Augmented Generation (RAG) kombiniert mit prompting,"Unspecified, might include formal therapy methods",BERT family; GPT-2 family; Other: BLOOMZ,No users involved,"","","","","","","","",n,"","","",y,no benchmark,no benchmark,Distance between embeddings of reference and output.,y,no benchmark,no benchmark,"F1 score, not quiet clear what of.",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
75,Kulkarni 2024,Conversational ai for mental health support,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",25,04,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,data from Kaggle,Emotional support dialogue -- chat logs,Other: Unsure,No,Yes,Unknown,Unknown,Only fine-tuning,"","Unspecified, might include formal therapy methods",BERT family; Other: LTSM,No users involved,"",N,N,N,N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"",""
75,Kulkarni 2024,Conversational ai for mental health support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25,4,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"Data from various sources (""Simple conversations, psychological questions, information on classic therapy sessions, and general advice for people with anxiety and depression. This data can be used to train a chatbot model that can act as a therapist to assist patients with anxiety and depression. The dataset includes intents. An ""intent"" is the purpose of a user's message. There are a total of 89 such intents, some of which are “greeting, sad, stressed, worthless, depressed, happy, anxious, sleep, scared, hate, problem” etc."").

Seemingly for intent classification.",Other: unknown,Other: unknown,Other: unknown,No,Unknown,Unknown,Only fine-tuning,"Fine-tuning of BERT, likely for intent classification. Not clear how responses are generated -- via LSTM?","Unspecified, might include formal therapy methods",BERT family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,Confusion matrices for the BERT and LSTM models (Fig. 7 and 8),n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
75,Kulkarni 2024,Conversational ai for mental health support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25,4,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"Data from various sources (""Simple conversations, psychological questions, information on classic therapy sessions, and general advice for people with anxiety and depression. This data can be used to train a chatbot model that can act as a therapist to assist patients with anxiety and depression. The dataset includes intents. An ""intent"" is the purpose of a user's message. There are a total of 89 such intents, some of which are “greeting, sad, stressed, worthless, depressed, happy, anxious, sleep, scared, hate, problem” etc."").

Seemingly for intent classification.","","","","",Unknown,Unknown,Only fine-tuning,"","Unspecified, might include formal therapy methods",BERT family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
72,Chan 2025,AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",28,02,2025,China,Other: Qualitative Study ,Client-facing application,Multi-turn chatbot,General population,69,"",Self-collected data,qualitative data from 69 students,Other: qualitative interviews,Chinese,No,No,Unselected,"","","","Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,"",N,Y,N,N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
72,Chan 2025,AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28,2,2025,China,Population survey,Client-facing application,Multi-turn chatbot,General population,69,"",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods",Other: various GenAI chatbots,Yes,"",n,y,n,y,"","This study employed a deductive thematic analysis, using the conceptual framework of Grodniewicz and Hohol’s (2023) three challenges in AI psychotherapy as a general guide","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
72,Chan 2025,AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28,2,2025,China,Population survey,Client-facing application,Multi-turn chatbot,General population,69,"",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,"","","","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Other: various GenAI chatbots,Yes,"",N,Y,N,y,"","This study employed a deductive thematic analysis, using the conceptual framework of Grodniewicz and Hohol’s (2023) three challenges in AI psychotherapy as a general guide","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
71,Barabas 2025,Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6,6,2025,Other: Switzerland,Empirical research involving an LLM,Therapist-facing application,Multi-turn chatbot,"",20,Patient simulations,No dataset used for development or evaluation,"","","","","",Unselected,Trained professionals,Only prompting,prompted to simulate a psychiatric first-responder chatbot,"Unspecified, might include formal therapy methods",GPT-3.5 family,Yes,"",N,N,Y,Y,"","10-point Likert scale rating the overall experience, the pleasantness of the chat, the appropriateness of the responses, the realism of the chatbot, and the helpfulness of the advice
Yes/No/ dont know:
1. It would be used by patients. (Hypothesized patients’ subjective norm)
2. It would be recommended by outpatient psychiatrists (Subjective norm)
3. It would relieve the psychiatric emergency department. (Anticipated
benefit)
4. It would be helpful for patients in
a crisis situation. (Expectancy of
usefulness)","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,B,L,compared to chatbots in custumer service,N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",Y,"Table 1 Study population characteristics
Male 
Total 5 
Age (26–30 years) 4;
(31–35 years) 1
Years of training 2.6 years (range:
1–3 years)
Swiss nationality/other nationality (EU) 0/5 
Psychiatry/other specialty/psychologist 5/0/0 
Female
Total 15
(26–30 years) 1; (31–35 years) 8;
(36–40 years) 5; (> 40 years) 1
2.4 years (range: 1–7 years)
Swiss nationality/other nationality (EU) 7/8
Psychiatry/other specialty/psychologist 4/4/7",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Primarily, as text-based systems, LLMs
cannot interpret crucial nonverbal cues
and they lack cultural sensitivity, which
is essential for a comprehensive mental
health assessment [2]. Secondly, research
shows that users tend to over-rely on the
accuracy of information delivered by LLMs,
which may discourage patients from seeking timely professional mental health assistance rather than facilitating it [5, 24].
Thirdly, technical challenges, such as AI
hallucination and insufficient technological literacy among users or connectivity,
further compromise the accessibility and
reliability of these systems. Fourthly, given
thelimited regulationof AI, the storageand
potential use of sensitive mental health
data for LLM training pose significant data
privacy concerns, underscoring the need
for robust legal and ethical frameworks. Finally, LLMs lack real-time safety protocols,
such as direct emergency service contact,
presenting a critical limitation when users
are in immediate danger.",""
71,Barabas 2025,Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6,6,2025,Other: Switzerland,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,no benchmark,n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
71,Barabas 2025,Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6,6,2025,Other: Switzerland,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"",Other: ,No dataset used for development or evaluation,"","","","","",Other: ,Other: ,Only prompting,prompted to simulate a psychiatric first-responder chatbot,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",y,no benchmark,no benchmark,no benchmark,n,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",n,"",N,"",N,"",N,"",N,"",N,"",N,"",n,"",""
69,P 2024,Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",4,11,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"","","",Self-collected data,"",Internet data -- mental health forum,English,No,No,Unknown,Unknown,Fine-tuning + other modules,fine tuning + sentiment analysis module,CBT: Cognitive restructuring,GPT-3.5 family,Yes,"",n,n,y,y,"","","",n,"","","",n,"","","",y,B,L,"between the rule-based sentiment analysis module and the
sentiment analysis capabilities of the GPT-powered model",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",y,"In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.",y," Conducted thorough testing of the
chatbot’s responses (Fig. 6) to various user queries and
scenarios to assess its ability to uphold ethical standards
and protect user privacy. This testing involved simulating
user interactions with the chatbot and evaluating its
responses for any instances of inappropriate or unethical behavior, such as providing inaccurate information,
breaching confidentiality, or engaging in discriminatory
practices. Any identified issues were promptly addressed
and remediated to ensure that the chatbot consistently
adheres to ethical guidelines and respects user privacy.",n,"",y," Conducted thorough testing of the
chatbot’s responses (Fig. 6) to various user queries and
scenarios to assess its ability to uphold ethical standards
and protect user privacy. This testing involved simulating
user interactions with the chatbot and evaluating its
responses for any instances of inappropriate or unethical behavior, such as providing inaccurate information,
breaching confidentiality, or engaging in discriminatory
practices. Any identified issues were promptly addressed
and remediated to ensure that the chatbot consistently
adheres to ethical guidelines and respects user privacy.",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
69,P 2024,Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",4,11,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","Data Preparation: The sentiment analysis model is
trained on an annotated text classification dataset after
undergoing extensive cleaning, normalization, and tok-
enization processes to ensure the quality and integrity of
the data. The datasets used are compiled from a multitude
of sources, including a text classification repository of so-
cial media tweets, restaurant reviews, and airline reviews.
Data was also extracted from the Reddit API across vari-
ous forums such as those focused on CBT, mental health,
therapy, anxiety, depression, and CPTSD. Initially, the
dataset comprised 3,000 data points, with a text column
serving as the input and a target column providing labeled
sentiment. Recognizing the importance of data volume
in enhancing the accuracy and robustness of machine
learning models, we expand the dataset (TABLE I) to encompass 12,221 data points

for RAG",Other: Mixed,English,Other: Unknown,No,Other: n.a.,Other: n.a.,Fine-tuning + other modules,It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot,Other CBT techniques,GPT-3.5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",y,no benchmark,no benchmark,Performance of the sentiment classification component.,n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
69,P 2024,Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",4,11,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,unknown,"","External data set, modified","Data Preparation: The sentiment analysis model is
trained on an annotated text classification dataset after
undergoing extensive cleaning, normalization, and tok-
enization processes to ensure the quality and integrity of
the data. The datasets used are compiled from a multitude
of sources, including a text classification repository of so-
cial media tweets, restaurant reviews, and airline reviews.
Data was also extracted from the Reddit API across vari-
ous forums such as those focused on CBT, mental health,
therapy, anxiety, depression, and CPTSD. Initially, the
dataset comprised 3,000 data points, with a text column
serving as the input and a target column providing labeled
sentiment. Recognizing the importance of data volume
in enhancing the accuracy and robustness of machine
learning models, we expand the dataset (TABLE I) to encompass 12,221 data points

for RAG","Other: Other: statements, labeled with sentiments (sentiment analysis data)",English,No,No,Other: n.a.,Other: n.a.,Fine-tuning + other modules,"It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot

fine tuning + sentiment analysis module",CBT: Cognitive restructuring,GPT-3.5 family,Yes,"",n,n,y,y,"",See V.B.,"",n,"","","",n,"","","",y,B,L,"between the rule-based sentiment analysis module and the
sentiment analysis capabilities of the GPT-powered model",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",y,"In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
68,Golden 2024,Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",18,10,2024,USA,Empirical research involving an LLM,Client-facing application,Other: Development of new Framework,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Other: Assessment of OCD-Coach,FAITA-mental health chatbot was designed to classify OCD-Coach's responses,Other CBT techniques,"ChatGPT, model unspecified",Yes,"",Y,Y,Y,Y,FAITA-Mental Health,"Readability high (Flesch-Kincaid 10.8), messages long; insufficient cultural tailoring; transparency reactive not proactive; crisis responses generic (no local numbers) — Quote: “10.8 on the Flesch-Kincaid …”; “did not make an attempt to ask Sam about her cultural identity variables”; “grant … a score of 1 … provides details upon request, [but] does not offer full, immediate … disclosure”; “no specific numbers or links were provided … score of 1 … general recommendations but no local resources” (Position: p. 8–9)","",N,"","","","","","","","","","","","","","","","",N,"","","","","","","","",N,"","","","","","","","",N,"","","","","","","","","","","","",Y,"“After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9)",Y,“OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9),"","","","","","","","","","","","","","","","","","",Y,"Future research should explore how this tool can be integrated into the decision-making processes of mental health professionals, health care organizations, and technology developers.” (Position: p. 10)",""
68,Golden 2024,Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",18,10,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,Study describes FAITA Mental Health Framework and evaluates OCD Coach as an existing LLM-based Tool,"","","","","","",Other: N/A,"","Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"With regard to crisis response, when Sam reported suicidal
ideation, OCD Coach directed her to emergency services or a crisis hotline “in [her] area” (Subdomain 2: Interactivity Quality)",Y,"Subdomain 2: Evidence-Based Content
",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
68,Golden 2024,Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",18,10,2024,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,Study describes FAITA Mental Health Framework and evaluates OCD Coach as an existing LLM-based Tool,"","","","","","",Only prompting,FAITA-mental health chatbot was designed to classify OCD-Coach's responses,"Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"“After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9)",Y,“OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9),N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
66,H 2024,Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
66,H 2024,Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
66,H 2024,Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
64,Hasei 2025,"Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study",Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",25,02,2025,Other: japan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,5,"","","","","","","","","",Only prompting,"two age-appropriate AI chatbots, leveraging GPT-4, were developed to provide natural, empathetic conversations","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,somewhat,somewhat,y,"","All participants noted improved treatment motivation … 80% disclosing personal concerns … 24/7 availability particularly benefited patients
Four out of five participants reported significant reductions in anxiety and stress levels post-intervention” (Results, p. 1)","",n,"","","","","","","","","","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","","","","","","","","","","",n,"","","","","","","","","","","","","","","","","","","","","",n,"",""
64,Hasei 2025,"Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",25,2,2025,Other: Japan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"Other: Pediatric and adolescent and
young adult (AYA) patients with cancer",5,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,Y,Y,Y,"","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.",Y,"Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.",N,"",N,"",Y,Table 2,N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
64,Hasei 2025,"Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study",Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",25,02,2025,Other: Japan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,5,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"two age-appropriate AI chatbots, leveraging GPT-4, were developed to provide natural, empathetic conversations","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,Y,Y,y,"","All participants noted improved treatment motivation … 80% disclosing personal concerns … 24/7 availability particularly benefited patients
Four out of five participants reported significant reductions in anxiety and stress levels post-intervention” (Results, p. 1)","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.",Y,"Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.",N,"",N,"",Y,Table 2,N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
63,Milligan 2024,Developing a single-session outcome measure using natural language processing on digital mental health transcripts,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",30,5,2024,UK,Empirical research involving an LLM,"","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
63,Milligan 2024,Developing a single-session outcome measure using natural language processing on digital mental health transcripts,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",30,5,2024,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"The data were provided and 
anonymised by Qwell",Emotional support dialogue -- chat logs,English,No,No,Unknown,Other: Mixed,Other: None of these ,They used a pre-trained RoBERTa model via Hugging Face to analyze text patterns without mentioning any fine-tuning or prompt engineering. The model was applied directly to classify and interpret data.,"Unspecified, might include formal therapy methods",BERT family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",Content validity index,-,-,-,"","","","","","","","","",N,"",N,"",Y,RoBERTa should be on-premise-capable,N,"",N,"",N,"",N,"",N,"",N,"",N,"",y,content validity with clinicians and SUs to improve the relevance and clarity of the items,N,"",""
63,Milligan 2024,Developing a single-session outcome measure using natural language processing on digital mental health transcripts,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",30,5,2024,UK,Empirical research involving an LLM,"","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
62,Gu 2024,Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories,Richard Gaus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24,7,2024,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","","External data set, modified","1,000 QA pairs from the PsyQA dataset.  two human annotators inspected and labeled the responses of professional supporters in the PsyQA dataset with the corresponding professional psychological counseling theory categories (Cognitive-Behavioral Therapy labeled as 1, Dialectical Behavior Therapy as 2, Person-Centered Therapy as 3, and Reality Therapy as 4). The original data had quantities of 189, 317, 196, and 298 for categories 1, 2, 3, and 4, respectively.",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Other: Mix of trained and lay,Fine-tuning + other modules,"Prompting of general purpose LLMs (stage 1, probably ChatGPT) + fine-tuning of BERT (stage 2) + some type of RAG based on SimCSE (stage 3)",Mix of formal therapy methods,"BERT family; ChatGPT, model unspecified",No users involved,"","","","","","","","",y,b,l,"Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.",y,b,l,"PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.",n,"","","",n,"","","","",y,b,l,"Fluency, helpfulness, relevance, empathy, professionalism, evaluated by psychology graduate students on a 5-Likert scale. Benchmark are other LLM-based methods.",n,"","","","",n,"","","",y,s,l,Dist-2. Benchmark are other LLM-based methods.,"","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
62,Gu 2024,Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24,7,2024,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"PsyQA dataset (Sun et al., 2021). ",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Other: Well-trained volunteers or professional counselors = mixed,Fine-tuning + other modules; Prompting + other modules,Prompting for emotional analysis + fine-tuning BERT for therapy classification + semantic retrieval with SimCSE for relevant case examples.,"Unspecified, might include formal therapy methods","BERT family; ChatGPT, model unspecified",No users involved,"","","","","","","","",Y,B,L,"Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",Y,B,L,"Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",N,"","","",N,"","","","",Y,B,H,"To evaluate the quality of generated supportive responses, we
also conduct a human evaluation. We recruit 12 graduate stu-
dents, major in psychology, to annotate the responses.",N,"","","","",N,"","","",Y,"B
",L,"Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
62,Gu 2024,Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories,Consensus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24,7,2024,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","","External data set, modified","1,000 QA pairs from the PsyQA dataset.  two human annotators inspected and labeled the responses of professional supporters in the PsyQA dataset with the corresponding professional psychological counseling theory categories (Cognitive-Behavioral Therapy labeled as 1, Dialectical Behavior Therapy as 2, Person-Centered Therapy as 3, and Reality Therapy as 4). The original data had quantities of 189, 317, 196, and 298 for categories 1, 2, 3, and 4, respectively.",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Other: Mix of trained and lay,Fine-tuning + other modules; Prompting + other modules,"Prompting of general purpose LLMs (stage 1, probably ChatGPT) + fine-tuning of BERT (stage 2) + some type of RAG based on SimCSE (stage 3)",Mix of formal therapy methods,"BERT family; ChatGPT, model unspecified",No users involved,"","","","","","","","",Y,B,L,"Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",Y,B,L,"PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",N,"","","",N,"","","","",Y,B,l,"Fluency, helpfulness, relevance, empathy, professionalism, evaluated by psychology graduate students on a 5-Likert scale. Benchmark are other LLM-based methods.",N,"","","","",N,"","","",Y,s,L,"Dist-2. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
60,J 2023,Conversational Agents for Dementia using Large Language Models,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,09,2023,Other: mexico,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Informal counseling (e.g., emotional support conversation)",Other: unclear,No,"",n,"","","","","","",n,"","","","","","","","","","","","","","","","",n,"","","","","","","","",n,"","","","","","","","",n,"","","","","","","","","","","","",n,"","","","","","","","","","","","","","","","","","","","","","","",""
60,J 2023,Conversational Agents for Dementia using Large Language Models,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,9,2023,Other: Mexico,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,Prompt engineering to shape ChatGPT’s dialogue,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
60,J 2023,Conversational Agents for Dementia using Large Language Models,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,09,2023,Other: Mexico,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,Prompt engineering to shape ChatGPT’s dialogue,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
58,,Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",24,10,2024,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"","","","","Other: Digitial Standardized Patients- no collection of data, only creation via GPT 3.5","By using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
These characteristics encompassed a spectrum of ages (40 and
80 years old), genders (male, female, transgender male, and
transgender female), races (non-Hispanic black, non-Hispanic
white, and Hispanic), and annual income ($25,000, $50,000,
$500,000, and $1,000,000). We also generated a control group
without assigned identities. Each DSP represented a unique
mix of these demographic characteristics.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses",Psychotherapy -- chat logs,English,Yes,No,Psychopathology,Trained professionals,Only prompting,"o systematically test
the conversational agent’s responses for potential bias, we
developed an experimental setup where the conversational
agent interacted with digital standardized patients (DSPs)
exhibiting symptoms of anxiety or depression. DSPs were
GPT-3.5-enabled chatbots prompted to emulate a patient
with depression and anxiety seeking professional help from
a therapist. This was achieved by prompting GPT-3.5 to:
“Pretend that you are [first_name], a human client speaking to
a therapist. You are NOT the therapist; you are the clienBy using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses",Other CBT techniques,GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Average Tone,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
58,,Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",24,10,2024,USA,Empirical research involving an LLM,Client-facing application,Other: Spoken dialog system,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"Their intervention: The AI therapist tested in this study is an LLM-based pro- gram previously developed and tested at Cedars-Sinai Medical Center, with the goal of offering AI-enabled, self-
administered, mental health support within immersive virtual
reality environments.18 The user interacts with an AI conver-
sational agent designed for history-taking and therapeutic sup-
port for anxiety and depression. The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
The agent facilitates turn-based conversations, summarizing
and concluding each session, and was found to be an accepta-
ble and feasible support modality for patients with mild-to-
moderate anxiety or depression.18 Here, we focus on whether
there is evidence of any sociodemographic bias exhibited by
the conversational agent.",Other CBT techniques,GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",LIWC score domains and linguistic marker scores,no benchmark,no benchmark,no benchmark,"","","","","","","","","",y,"from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.",n,"",n,"",y,"The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).",y,"yes, in simulated DSPs (digital standardized patient)",y,"yes, in simulated DSPs (digital standardized patient)",n,"",n,"",n,"",n,"",n,"",n,"",""
58,,Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24,10,2024,USA,Empirical research involving an LLM,Client-facing application,Other: Spoken dialog system,No clients/patients involved,"","",No dataset used for development or evaluation,"By using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
These characteristics encompassed a spectrum of ages (40 and
80 years old), genders (male, female, transgender male, and
transgender female), races (non-Hispanic black, non-Hispanic
white, and Hispanic), and annual income ($25,000, $50,000,
$500,000, and $1,000,000). We also generated a control group
without assigned identities. Each DSP represented a unique
mix of these demographic characteristics.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Prompting + other modules,"Their intervention: The AI therapist tested in this study is an LLM-based pro- gram previously developed and tested at Cedars-Sinai Medical Center, with the goal of offering AI-enabled, self-
administered, mental health support within immersive virtual
reality environments.18 The user interacts with an AI conver-
sational agent designed for history-taking and therapeutic sup-
port for anxiety and depression. The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
The agent facilitates turn-based conversations, summarizing
and concluding each session, and was found to be an accepta-
ble and feasible support modality for patients with mild-to-
moderate anxiety or depression.18 Here, we focus on whether
there is evidence of any sociodemographic bias exhibited by
the conversational agent.",Other CBT techniques,GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",LIWC score domains and linguistic marker scores,no benchmark,no benchmark,no benchmark,average tone (sentiment score),no benchmark,no benchmark,no benchmark,"","","","","",y,"from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.",y,"The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses",n,"",y,"The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).",y,"yes, in simulated DSPs (digital standardized patient)",y,"yes, in simulated DSPs (digital standardized patient)",n,"",n,"",n,"",n,"",n,"",n,"",""
57,Jeong 2025,AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28,03,2025,Other: korea,Empirical research involving an LLM,Therapist-facing application,"","","",Other: self efficacy enhancement,Self-collected data,"","",Other: ,No,No,Unselected,Lay people,Only prompting,ChatGPT played the role of a client and optionally provided AI-generated feedback to trainees,"Informal counseling (e.g., emotional support conversation); Other: counselor-training simulation",GPT-4 / GPT-4o family,Yes,"",N,Y,Y,Y,"",Self-efficacy improved across sessions; anxiety reduced mainly between Session 1 and 2. — Quote: same as quantitative assessment line,"",n,"","","","","","","","","","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","","","","","",n,"","","","",n,"","","","","","","","","","","","","","","","","","","","","",n,"",""
57,Jeong 2025,AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28,3,2025,Other: Korea,Empirical research involving an LLM,Therapist-facing application,"","","",Treatment fidelity feedback,No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",Y,Y,Y,Y,CSQ and PU,"Qualitative assessment only implicitly mentioned in the discussion section, where user experiences are reported.","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"Although the risk was minimal in the present study because
the client was fictional, it may not be completely credible in real
situations involving real clients. Counselor educators would thus
have to be cautious about using AI to directly generate feedback
for the trainees.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,CASES & STAI,Y,"The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.",N,"",N,"",""
57,Jeong 2025,AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28,03,2025,Other: Korea,Empirical research involving an LLM,Therapist-facing application,"","","",Treatment fidelity feedback,No dataset used for development or evaluation,"","","",Other: ,Other: ,Other: ,Other: ,Only prompting,ChatGPT played the role of a client and optionally provided AI-generated feedback to trainees,"Unspecified, might include formal therapy methods; Other: counselor-training simulation",GPT-4 / GPT-4o family,Yes,"",Y,Y,Y,Y,CSQ and PU,Self-efficacy improved across sessions; anxiety reduced mainly between Session 1 and 2. — Quote: same as quantitative assessment line,"",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",n,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","",(CASES &) STAI,"","The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.",N,"",N,"",""
54,Baez 2025,"""I Don't Understand It, but Okay"": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models",Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",21,3,2025,USA,Population survey,Therapist-facing application,Multi-turn chatbot,Other: Mental health practitioners,21,Other: No specific application,No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",n,y,n,y,"","This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
54,Baez 2025,"""I Don't Understand It, but Okay"": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models",Reviewer Two,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),21,3,2025,USA,Population survey,Therapist-facing application,"","","",Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs,No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",N,Y,N,Y,"","Both, user experience assessment and participant attitude assessment. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview. ","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Despite it being labeled an ""empirical study"", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. "
54,Baez 2025,"""I Don't Understand It, but Okay"": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models",Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),21,3,2025,USA,Population survey,Therapist-facing application,Other: ,Other: Mental health practitioners,21,Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs. No one particular application type.,No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",N,Y,N,Y,"","Both, user experience assessment and participant attitude assessment.

This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Despite it being labeled an ""empirical study"", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. "
51,Taiwo 2025,Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",25,1,2025,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues,Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Lay people,Fine-tuning + other modules,"BERT and GPT-3.5 fine-tuned, interacting with each other","Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,"","","","","","","","",y,s,l,"BLEU, ROUGE against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.",y,s,l,BLEUScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.,y,b,l,Various classification metrics of BERT emotional distress detection. Benchmark: Bi-LSTM,n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
51,Taiwo 2025,Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25,1,2025,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"Multiple external datasets used: 
a) From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger. 
b) From Bertagnolli [53], including high-quality therapist responses to real patient´s mental health questions.","Other: a) Crowdsourced dialogues
b) Internet data -- mental health Q&A",English,No,Yes,Unknown,"Other: a) Unknown 
b) Trained professionals (""verified therapists"")",Fine-tuning + other modules; Prompting + other modules,"The system integrates two main components: a BERT-based emotional distress detection module and a fine-tuned GPT-3.5 model for Psychological First Aid (PFA) response generation.
First, BERT analyzes the user’s input to detect their emotional state and distress level. These outputs are then passed, along with the original user input, into a custom prompt designed for GPT-3.5. GPT-3.5 was fine-tuned on therapy transcripts to improve its ability to produce empathetic, context-aware PFA responses.","Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,"","","","","","","","",Y,S,L,Against chatGPT-3.5,Y,B,L,Against chatGPT-3.5,Y,B,L,Bi-LSTM for emotional distress detection,N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","Relevance, personalisation, PFA (advice and guidance)professional referral, and validation and empathy",B,?,In the text there is no statement on how or by whom this was evaluated. ,"","","","","","","","","",N,"",N,"",N,"","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
51,Taiwo 2025,Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25,1,2025,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues

From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger.",Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Lay people,Fine-tuning + other modules; Prompting + other modules,"The system integrates two main components: a BERT-based emotional distress detection module and a fine-tuned GPT-3.5 model for Psychological First Aid (PFA) response generation.
First, BERT analyzes the user’s input to detect their emotional state and distress level. These outputs are then passed, along with the original user input, into a custom prompt designed for GPT-3.5. GPT-3.5 was fine-tuned on therapy transcripts to improve its ability to produce empathetic, context-aware PFA responses.","Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,"","","","","","","","",Y,S,L,"BLEU, ROUGE against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.",Y,s,L,BERTScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.,Y,B,L,Various classification metrics of BERT emotional distress detection. Benchmark: Bi-LSTM,N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",n,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
47,Thimmanayakanapalya 2024,Digital Risk Considerations Across Generative AI-based Mental Health Apps,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),10,12,2023,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,1000000 user reviews,"",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods",Other: various LLMs,Yes,"",n,y,n,y,"","BERTopic analysis of app reviews, qualitative only","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
47,Thimmanayakanapalya 2024,Digital Risk Considerations Across Generative AI-based Mental Health Apps,Reviewer Two,Other: Workshop paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),10,12,2023,USA,Other: Observational study (user-generated reviews) with unsupervised text mining,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods",Other: unspecified,Yes,"",N,Y,N,Y,"",Qualitative analysis of user reviews focusing on risks,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
47,Thimmanayakanapalya 2024,Digital Risk Considerations Across Generative AI-based Mental Health Apps,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),10,12,2023,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,1000000 user reviews,"",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods",Other: various LLMs,Yes,"",N,Y,N,Y,"",Qualitative analysis of user reviews focusing on risks using BERTopic,"",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
46,Rasool 2025,"Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,03,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. ",Psychotherapy -- speech transcripts,English,No,Yes,Unknown,Trained professionals,Prompting + other modules,Retrieval + hierarchical fusion + attention + lexicon-enriched embeddings used to prompt LLMs.,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family,No users involved,"","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Empathy
 Coherence 
Informativeness 
Fluency",B,L,"","","","","","","","","","","","","","",Y (some),Llama and DeepSeek are,"","","","","","","","","","","","","","","","","","",""
46,Rasool 2025,"Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,3,2025,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"",No clients/patients involved,"","",External data set,"The primary dataset used in this study consists of psychotherapy transcripts from
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. ",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,"''We introduce Emotion-Aware Embedding Fusion, a novel framework integrating hierarchical fusion and attention mechanisms'' ... ","Unspecified, might include formal therapy methods",T5 family; GPT-4 / GPT-4o family; Llama 2 family; Other: DeepSeek-R1,No,"",N,N,N,N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
46,Rasool 2025,"Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13,03,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"The primary dataset used in this study consists of psychotherapy transcripts from
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. ",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,Retrieval + hierarchical fusion + attention + lexicon-enriched embeddings used to prompt LLMs.,"Unspecified, might include formal therapy methods",T5 family; GPT-4 / GPT-4o family; Llama 2 family; Other: DeepSeek-R1,No users involved,"","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",automatic empathy rating,B,L,they study integration of empathy lexicon into different LLMs. benchmark are those LLMs without empathy lexicon integration.,"coherence, informativeness, fluency",w,l,they study integration of empathy lexicon into different LLMs. benchmark are those LLMs without empathy lexicon integration.,"","","","","","","","","",Y,Llama and DeepSeek are,"","","","","","","","","","","","","","","","","","",""
45,P 2024,Expert Patient Interaction Language Model (EPILM),Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16,12,2024,India,Empirical research involving an LLM,Analysis of conversation transcripts,Other: Unknown but I assume both multiturn and one turn for the forum answers.,General population,"","",External data set,"[COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. 

WebMD, Mayo Clinic and Heatlhline.com 
- One text exchange between a patient and their therapist is included in this dataset. The dataset was assembled from online FAQs, WebMD, Mayo Clinic, and HealthLine, among other well-known healthcare blogs",Other: Emotional support dialogue and Internet data,English,No,Other: Unknown as the datasets are not fully described,Unknown,Trained professionals,Fine-tuning + other modules,"The proposed method involves enhancing the 
Falcon-7B model through the application of quantization low rank adaptation (Q-LoRA). ","Unspecified, might include formal therapy methods",BERT family,No users involved,"","","","","","","","","","","","",Y,B,L,"BERT Score, Sentence Transformer score","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
45,P 2024,Expert Patient Interaction Language Model (EPILM),Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16,12,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"","","",External data set,CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only fine-tuning,Q-LoRA fine-tuning,"Unspecified, might include formal therapy methods",Other: Falcon-7B,No users involved,"","","","","","","","",n,"","","",y,b,l,benchmark is GPT-3.5. measure is BERTScore and sentence transformer score with dataset expert responses as reference,n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,falcon,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
45,P 2024,Expert Patient Interaction Language Model (EPILM),Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16,12,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat

[COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. ",Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only fine-tuning,Q-LoRA fine-tuning,"Unspecified, might include formal therapy methods",Other: Falcon-7B,No users involved,"","","","","","","","",n,"","","",Y,b,l,benchmark is GPT-3.5. measure is BERTScore and sentence transformer score with dataset expert responses as reference,n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,falcon,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
43,Mohssen 2024,Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",27,11,2024,Other: Iraq,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,Alzheimer's Q&A dataset https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of GPT-2 + memory mechanism (storing previous embeddings in an array),"Unspecified, might include formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",y,no benchmark,no benchmark,BLEU with reference,n,"","","",n,"","","",y,no benchmark,no benchmark,Train loss,"",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
43,Mohssen 2024,Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model,Reviewer Two,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",27,11,2024,Other: Iraq,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately! ",Internet data -- mental health Q&A,English,No,"Other: probably yes, but name not specified. ",Unknown,Other: Probably mixed,Fine-tuning + other modules; Prompting + other modules,"Developed by fine-tuning a GPT-2 model on a domain-specific dataset and integrating it into a larger architecture that included array-based context storage for multi-turn dialogue, Top-K/Top-P sampling for controlled text generation, and response ranking modules using BLEU scores and cosine similarity to select the most relevant and coherent reply.","Unspecified, might include formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",Y,-,-,-,Y,-,-,-,N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",Computational performance/ Training metrics,-,-,-,"","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N ,"",""
43,Mohssen 2024,Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",27,11,2024,Other: Iraq,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately!

But user has other dataset that fits description: https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Fine-tuning + other modules,"Developed by fine-tuning a GPT-2 model on a domain-specific dataset and integrating it into a larger architecture that included array-based context storage for multi-turn dialogue, Top-K/Top-P sampling for controlled text generation, and response ranking modules using BLEU scores and cosine similarity to select the most relevant and coherent reply.","Unspecified, might include formal therapy methods",GPT-2 family,No users involved,"","","","","","","","",Y,no benchmark,no benchmark,BLEU with reference,n,"","","",N,"","","",y,no benchmark,no benchmark,Train loss,"",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",y,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N ,"",""
42,Haber 2025,"The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4,2,25,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,1,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,"","",N,Y,N,Y,"","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc",Y,"This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
42,Haber 2025,"The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis",Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",04,02,2025,India,"Other: theoretical,clinical, and ethical analysis",Client-facing application,Other: image generator,No clients/patients involved,"","","","","","","","","","",Only prompting,"The research team, consisting of an AI product manager and
prompt engineer (OP), several expert psychologists (YH, KG, ZE,
and DHS), and a music therapist (TA), jointly developed the two
AI tools from December 2023 to January 2024. Initially, a
primary prompt was created by YH. This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests
and added operational instructions to ensure safety and ethics.
Subsequently, group members with clinical backgrounds (YH,
KG, ZE, TA) tested the tool on themselves and shared their
experiences and suggestions for improvements. Throughout this
process, the team realized that due to the tools’ depth and their
ability to reflect on unconscious internal parts, the presence of a
mental health care professional would be essential. An expert in
social psychology, DHS, addressed cultural issues, which were
also incorporated into the prompt. Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications.","Psychodynamic psychotherapy; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,Y,N,Y,"","Results reported: Yes (case-level outcomes) — Quote: “the AI successfully managed to … become more experientially aligned with the patient” (Position: Clinical section, p. 11)

Empathic failures, uncanny feelings, corrective alignment described — Quote: “empathic failure occurred … however, the ability to overcome empathic failures … holds significant therapeutic value” (Position: Clinical section, p. 11)","",N,"","","","","","","","","","","",N,"","","","",N,"","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",Y,"Y (SAFE-AI protocol includes screening, monitoring, and evaluation) — Quote: “SAFE-AI protocol (Screening, Alignment, Facilitation, and Evaluation of AI-Enhanced Interventions) provides systematic guidelines … including consent, alliance monitoring, empathic failures” (Position: Section 4.7, p. 13)",Y,"(recommend therapist-side implementation) — Quote: “Therefore, these tools should be used on the therapist’s computer … ensuring anonymity for the patient.” ",Y,"Quote: “Patients must be fully informed about how their data will be used and must provide explicit consent … robust data protection measures are paramount” (Position: Section 4.2, p. 12)","","","","","","","","","","","","",somewhat," Quote: “therapist, patient, and AI agent … proof-of-concept in this article illustrates … therapeutic value” (Position: Clinical section, p. 11)",somewhat,"“SAFE-AI protocol … provides systematic guidelines for implementing AI-based externalization techniques … while ensuring cultural sensitivity, gender representation, and therapeutic authenticity.”",""
42,Haber 2025,"The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis",Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",04,02,2025,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,1,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"The research team, consisting of an AI product manager and
prompt engineer (OP), several expert psychologists (YH, KG, ZE,
and DHS), and a music therapist (TA), jointly developed the two
AI tools from December 2023 to January 2024. Initially, a
primary prompt was created by YH. This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests
and added operational instructions to ensure safety and ethics.
Subsequently, group members with clinical backgrounds (YH,
KG, ZE, TA) tested the tool on themselves and shared their
experiences and suggestions for improvements. Throughout this
process, the team realized that due to the tools’ depth and their
ability to reflect on unconscious internal parts, the presence of a
mental health care professional would be essential. An expert in
social psychology, DHS, addressed cultural issues, which were
also incorporated into the prompt. Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications.","Psychodynamic psychotherapy; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,Y,N,Y,"","Results reported: Yes (case-level outcomes) — Quote: “the AI successfully managed to … become more experientially aligned with the patient” (Position: Clinical section, p. 11)

Empathic failures, uncanny feelings, corrective alignment described — Quote: “empathic failure occurred … however, the ability to overcome empathic failures … holds significant therapeutic value” (Position: Clinical section, p. 11)","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","","","",Y,"This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
40,K 2024,"Multimodal Integration, Fine Tuning of Large Language Model for Autism Support",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)","","","",India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,"The research begins by curating a specialized User and
Counsellor Interaction CSV dataset by querying an autism
counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source
for reducing bias and adding a human touch. Alongside the
primary dataset, the additional data points ensure a compre-
hensive understanding of the domain.",Other: unclear,"","","","","",Fine-tuning + other modules,Fine-tuned LLM + integration of other data modalities (vision),"Other: ""Autism counselling""",Other: Falcon-7B,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
40,K 2024,"Multimodal Integration, Fine Tuning of Large Language Model for Autism Support",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,1,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,"","Other: The research begins by curating a specialized User and Counsellor Interaction CSV dataset by querying an autism counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source for reducing bias and adding a human touch. ","Other: unknown, probably english or hindi",No,No,Unknown,Trained professionals,Only fine-tuning,Fine-tuned with PEFT-LoRA,"Unspecified, might include formal therapy methods",Other: Falcon7B,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",Y,Falcon7B ,N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
40,K 2024,"Multimodal Integration, Fine Tuning of Large Language Model for Autism Support",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18,1,2024,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,"The research begins by curating a specialized User and
Counsellor Interaction CSV dataset by querying an autism
counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source
for reducing bias and adding a human touch. Alongside the
primary dataset, the additional data points ensure a compre-
hensive understanding of the domain.",Other: unclear,"Other: unknown, probably english or hindi",Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuned LLM + integration of other data modalities (vision),"Unspecified, might include formal therapy methods",Other: Falcon-7B,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",Y,Falcon7B ,N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
39,Cross 2024,Use of AI in mental health care: Community and mental health professionals survey,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",11,10,2024,Other: Australia,Population survey,Other: Client-facing AND therapist-facing,Other: No specific subtype (AI use in general),General population,107 community members AND 86 mental health providers,Other: No specific subtype (AI use in general),No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods","Other: AI in general, mostly ChatGPT",Yes,"",y,y,y,y,AI attitude scale (AIAS-4): doi: 10.3389/fpsyg.2023.1191628,"","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
39,Cross 2024,Use of AI in mental health care: Community and mental health professionals survey,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)","","","",Other: Australia,Population survey,Other: survey,"",General population,193 — Quote: “final sample consisted of 107 CMs and 86 MHPs”,"",Self-collected data,"","",English,No,No,Unselected,Other: both lay and pros,"","",Other: no intervention,Other: none,Yes,"",N,Y,Y,Y,"","See section: Technology Comfort, AI Attitudes, and AI
Use Intention","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",see UX,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"",Y,"",""
39,Cross 2024,Use of AI in mental health care: Community and mental health professionals survey,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",11,10,2024,Other: Australia,Population survey,Other: Client-facing AND therapist-facing,Other: No specific subtype (AI use in general),General population,193 — Quote: “final sample consisted of 107 CMs and 86 MHPs”,Other: No specific subtype (AI use in general),No dataset used for development or evaluation,"","",Other: ,Other: ,Other: ,Other: ,Other: ,"","","Unspecified, might include formal therapy methods","Other: AI in general, mostly ChatGPT",Yes,"",y,y,y,y,AI attitude scale (AIAS-4): doi: 10.3389/fpsyg.2023.1191628,"See section: Technology Comfort, AI Attitudes, and AI
Use Intention","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,both CMs and MHPs were surveyed,"","",""
34,Q 2025,Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21,03,2025,China,Empirical research involving an LLM,Analysis of conversation transcripts,Multi-turn chatbot,No clients/patients involved,"","",External data set,EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed),Psychotherapy -- speech transcripts,Chinese,"",Yes,"",Trained professionals,Any reinforcement learning; Other: proposed a dynamic adversarial test method based on cross-variation” (Position: Abstract in excerpt),"Cross-mutation/crossover to swap context & emotion while fixing behavior; compute BSI & CR; chi-square tests — Quote: “behavior-anchored crossover-mutation… fixes the behavior dimension, swaps context and emotion… Core metrics: Behavior Sensitivity Index (BSI)… Consistency Rate (CR)… Chi-square test”","Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Claude family; Other: selected four widely-used LLMs … DeepSeek, GPT-4, Claude, Wenxin-Yiyan",No users involved,"","","","","","","","","","","","","","","","",Y,B,L,Models classified cases requiring intervention; BSI>1 and higher CR indicate better behavior sensitivity/consistency; DeepSeek/Wenxin/Claude > GPT-4. — Quote: “BSI and CR … DeepSeek 1.0662 / 0.8985 … GPT-4 1.0415 / 0.8250,"","","","","","","","","",Y,W,L,Each LLM’s own “intervention decision” used to compute metrics. — Quote: “intervention decisions recorded and results labeled … quantitative analysis uses BSI and CR”,"","","","","","","","","","","","","","","","","","","","","","","",Y,detect the ability of LLMs to recognize extreme behaviors … behaviors covering high-risk actions,Y,evaluate … ability to intervene ethically … provide data-driven insights for optimizing safety mechanisms,"","","","","","","","","","","","","","","","","","","","",""
34,Q 2025,Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21,03,2025,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified",selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.,Other: Roleplay and sharing/microblogging data,Chinese,No,No,Unknown,Unknown,Only prompting,"The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice.",Other,GPT-4 / GPT-4o family; Claude family; Other: Deepseek & Wenxin - Yiyan,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",Behavior Sensitivity Index,-,L,No Indexmodel; comparison of multiple LLMs,Consistency Ratio,-,L,No Indexmodel; comparison of multiple LLMs,"","","","","",N,"",N,"",Y & N ,"Some are, others don´t",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
34,Q 2025,Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21,03,2025,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.

EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed)","Other: test cases for ""psychological counseling scenarios"" involving extreme behaviors",Chinese,No,No,Unselected,Unknown,Only prompting,"The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice. 

Fig. 1","Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Claude family; Other: DeepSeek, ERNIE Bot",No users involved,"","","","","","","","",N,"","","",N,"","","",Y,B,L,Models classified cases requiring intervention; BSI>1 and higher CR indicate better behavior sensitivity/consistency; DeepSeek/Wenxin/Claude > GPT-4. — Quote: “BSI and CR … DeepSeek 1.0662 / 0.8985 … GPT-4 1.0415 / 0.8250,N,"","","","","","","","",N,"","","","",N,"","","",N,"","","","",Behavior Sensitivity Index,no benchmark,no benchmark,No Indexmodel; comparison of multiple LLMs,Consistency Ratio,no benchmark,no benchmark,No Indexmodel; comparison of multiple LLMs,"","","","","",N,"",N,"",Y,"Some are, others don´t",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
32,Heinz 2025,Randomized trial of a generative AI chatbot for mental health treatment,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27,3,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients with disorder explicitly based on ICD or DSM,210,"",Self-collected data,"The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dialogues. The dialogues were developed by our research team, including a board-certified psychiatrist and a clinical psychologist, and peer-reviewed using evidence-based (primarily CBT) modalities.",Psychotherapy -- chat logs,English,No,No,Unknown,Unknown,Fine-tuning + other modules,Main LLM was fine-tuned but there are also other modules (e.g. safety classification),Other CBT techniques,Llama 2 family; Other: Falcon-7B,Yes,"",y,n,y,y,(Working Alliance Inventory — Short Revised [WAI-SR]),Results of survey in Fig 5,"",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","Patient Health Questionnaire 927 (PHQ-9), the Generalized Anxiety Disorder Questionnaire for the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM-IV) (GAD-Q-IV), and the Weight Concerns Scale (WCS) within the Stanford–Washington University Eating Disorder (SWED)",b,l,Waitlist control,engagement with Therabot (number of messages sent),no benchmark,no benchmark,no benchmark,"","","","","",y,"Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model.",n,"All responses from Therabot were supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from Therabot (e.g., providing medical advice), we contacted the participant to provide correction.

However, no automatic screening",y,"Llama 2, Falcon-7B",n,"usage of Amazon AWS, no discussion of user privacy",y,See Table 2,n,"",y,Figures 3 and 6,y,Figure 6,y,various measures,y,Waitlist control,n,"",n,"",""
32,Heinz 2025,Randomized trial of a generative AI chatbot for mental health treatment,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",27,3,2025,Other: Lebanon,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),210,"",Self-collected data,"",Other: expertly written therapist–patient dialogues based on third-wave CBT,Other: unknown,No,No,Unknown,Trained professionals,Fine-tuning + other modules,"Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).","Unspecified, might include formal therapy methods","Other: Not specified ""The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dia-
logues""",Yes,"",N,N,Y,Y,"","Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX. ","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model",Y,"All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.",N,"",N,"",Y,Table 2,N,"",N,"",N,"",Y,"Patient Health 
Questionnaire 9(PHQ-9), the Generalized Anxiety 
Disorder Questionnaire for the Diagnostic and Statistical 
Manual of Mental Disorders, Fourth Edition (DSM-IV)
(GAD-Q-IV), and the Weight Concerns Scale (WCS) within 
the Stanford–Washington University Eating Disorder
(SWED), as measures of depression, anxiety, and weight 
concerns, respectively. Therapeutic alliance 
(Working Alliance Inventory — Short Revised [WAI-SR]).",Y,"To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.",N,"",N,"",""
32,Heinz 2025,Randomized trial of a generative AI chatbot for mental health treatment,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27,3,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),210,"",Self-collected data,"The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dialogues. The dialogues were developed by our research team, including a board-certified psychiatrist and a clinical psychologist, and peer-reviewed using evidence-based (primarily CBT) modalities.",Psychotherapy -- chat logs,English,Yes,No,Unknown,Trained professionals,Fine-tuning + other modules,"Main LLM was fine-tuned but there are also other modules (e.g. safety classification)

Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).",Other CBT techniques,Llama 2 family; Other: Falcon-7B,Yes,"",N,N,Y,Y,"","Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX.

Results in Fig 5","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","Patient Health Questionnaire 927 (PHQ-9), the Generalized Anxiety Disorder Questionnaire for the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM-IV) (GAD-Q-IV), and the Weight Concerns Scale (WCS) within the Stanford–Washington University Eating Disorder (SWED)",b,l,Waitlist control,"","","","","","","","","",Y,"In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model",n,"All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.

However, no automatic screening pre-transmission",y,"Llama 2, Falcon-7B",N,"usage of Amazon AWS, no discussion of user privacy",Y,Table 2,N,"",N,"",N,"",Y,"Patient Health 
Questionnaire 9(PHQ-9), the Generalized Anxiety 
Disorder Questionnaire for the Diagnostic and Statistical 
Manual of Mental Disorders, Fourth Edition (DSM-IV)
(GAD-Q-IV), and the Weight Concerns Scale (WCS) within 
the Stanford–Washington University Eating Disorder
(SWED), as measures of depression, anxiety, and weight 
concerns, respectively. Therapeutic alliance 
(Working Alliance Inventory — Short Revised [WAI-SR]).",Y,"To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.",N,"",N,"",""
31,R 2024,Artificial Intelligence in Mental Health Care: The T5 Chatbot Project,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9,12,2024,Other: United Arab Emirates,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Only fine-tuning,PEFT (qLoRA) fine-tuning of T5,"Unspecified, might include formal therapy methods",T5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",y,no benchmark,no benchmark,Metric: training loss. No benchmark,"",n,"","","",n,"","","","",n,"","","",y,no benchmark,no benchmark,Metrics: Dist-1 and Dist-2 on chatbot responses. No benchmark,"","","","","","","","","","","","","","",n,"",n,"",y,"",y,"Privacy Concerns: Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
31,R 2024,Artificial Intelligence in Mental Health Care: The T5 Chatbot Project,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9,12,2024,Other: UAE,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Other: Multiple external data sets,Table 1,Other: Multiple types (compare results data loading and preparation),English,No,Yes,Unknown,Other: ,Only fine-tuning,PEFT + QLORA,"Unspecified, might include formal therapy methods",T5 family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","Training Metrics (training runtime, samples processed per second, steps per second, and Training loss)",-,-,-,Diversity Metrics (richness and variability; Dist-1 and Dist-2),-,-,-,"","","","","",N,"",N,"",Y,T5 can be used on-premise and via cloud (not in the text). ,Y,"Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Ethical Considerations: Which users were informed?
31,R 2024,Artificial Intelligence in Mental Health Care: The T5 Chatbot Project,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9,12,2024,Other: United Arab Emirates,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Only fine-tuning,PEFT (qLoRA) fine-tuning of T5,"Unspecified, might include formal therapy methods",T5 family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",y,no benchmark,no benchmark,Metric: training loss. No benchmark,"",N,"","","",N,"","","","",N,"","","",y,no benchmark,no benchmark,Metrics: Dist-1 and Dist-2 on chatbot responses. No benchmark,"","","","","","","","","","","","","","",N,"",N,"",Y,T5 can be used on-premise and via cloud (not in the text). ,Y,"Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Ethical Considerations: Which users were informed?
30,Abroms 2025,Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,1,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting; Other: RAG,"Instructions to act like a counselor etc. implemented; This model also allowed us to use retrieval augmented generation technology, which allows the model to be provided with a corpus of knowledge to reduce hallucinations
and misinformation [8]. Both the provision of instructions and source materials can serve as guardrails that keep the chatbot in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",Y,S,L,Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).,Y,W,H,"Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).An index was developed to code the responses and measure adherence to leading smoking cessation guidelines and counseling practices. The items in the index were developed to
reflect leading guidance as captured in USPSTF public health guidelines for quitting smoking and Clearing the Air: Quit
Smoking Today [12,13] and common counseling practices [14].",N,"","","","",Y,W,H,"Each chatbot response was independently categorized by 2
coders for their adherence on each item of the index.",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
30,Abroms 2025,Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,01,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,bots were queried with a fixed set of 12 prompts derived from real search queries; responses were coded against a guideline derived index,"Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,W,H,"dherence to an index developed
from the US Preventive Services Task Force public health guidelines for quitting smoking and counseling principles. Enough?","","","","","",N,"","","",N,"","","","",maybe the adherence index here.,"","","","","","","","","","","","","","",Y,some types of misinformation were present in 22% of responses,"","","","","","","","","","","","","","","","","","",maybe?,"somehow, since the benchmark were clinical guidelines:

Responses were analyzed for their adherence to an index developed from the US Preventive Services Task Force public health guidelines for quitting smoking and counseling principles.",""
30,Abroms 2025,Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30,01,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"Instructions to act like a counselor etc. implemented; This model also allowed us to use retrieval augmented generation technology, which allows the model to be provided with a corpus of knowledge to reduce hallucinations
and misinformation [8]. Both the provision of instructions and source materials can serve as guardrails that keep the chatbot in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",Y,S,L,Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).,Y,W,H,"Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).An index was developed to code the responses and measure adherence to leading smoking cessation guidelines and counseling practices. The items in the index were developed to
reflect leading guidance as captured in USPSTF public health guidelines for quitting smoking and Clearing the Air: Quit
Smoking Today [12,13] and common counseling practices [14].",N,"","","","",Y,W,H,"Each chatbot response was independently categorized by 2
coders for their adherence on each item of the index.",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
29,Hatch 2025,When ELIZA meets therapists: A Turing test for the heart and mind,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",12,02,2025,USA,Other: experimental vignette study with panel raters,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved; General population,830,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. ","Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,"","",N,N,N,N,"","","","","","","","","","","",N ,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","",N,"",""
29,Hatch 2025,When ELIZA meets therapists: A Turing test for the heart and mind,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12,2,2025,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. Like the therapists’ instructions, the prompt did not place any limits on the length of the response, nor was ChatGPT shown the therapists’ responses.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",Sentiment and part-of-speech,- (no B/S/W classification possible),H,Therapist responses,"","","","","","","","","",N,"",N,"",N,"",N,"",Y,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"", 
29,Hatch 2025,When ELIZA meets therapists: A Turing test for the heart and mind,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",12,2,2025,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,830,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. Like the therapists’ instructions, the prompt did not place any limits on the length of the response, nor was ChatGPT shown the therapists’ responses.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",Sentiment and part-of-speech,unclear,H,Therapist responses,Difference between responses (detection),S,H,Therapist responses,"","","","","",N,"",N,"",N,"",N,"",Y,"In the current study, participants (N = 830) were 45.17 years old on average (SD = 16.56),
59.88% mentioned being in a current romantic relationship, and 18.07% of the sample
reported having ever engaged in couple therapy. Most participants identified as a woman
(50.60%), slightly fewer identified as a man (47.95%), and the remaining individuals identified
as non-binary or third-gender (0.24%), 0.12% preferred not to say, and 0.07% of the sample
did not answer. A majority of the sample identified as straight (83.25%), 7.83% of the sample
identified as bisexual, 2.65% as gay, 1.81% as asexual, 1.45% as lesbian, 0.72% as queer, and
0.60% preferred to not disclose. When considering race and ethnicity, most participants iden-
tified as non-Hispanic White (49.40%), followed by Black (18.80%), White Hispanic (16.87%),
Asian (5. %), Black Hispanic (0.84%), American Indian or Alaskan Native (0.12%), and the
remaining sample identified as other (8.43%), or preferred not to disclose (0.12%).",N,"",N,"",N,"",N,"",N,"",N,"",N,"", 
28,K 2023,Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,12,2023,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Emotional Support Conversation dataset (Liu 2021 Towards emotional support dialog systems),Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Lay people,Fine-tuning + other modules,Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family; Llama 2 family; Other: SeqGPT,No users involved,"","","","","","","","",y,b,l,"BLEU, ROUGE, METEOR. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",n,"","","",n,"","","",n,"","","","",y,b,l,"Human interactive evaluation (fluency, comforting, etc.), comparison with other models. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5).","","","","","",y,b,l,"Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)","","","","","","","","","","","","","","","","","","",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
28,K 2023,Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,12,2023,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","Emotional Support Conversation dataset. Comprises 1,053 multi-turn dialogues, amounting to 31,410 utterances. ""We have augmented the dataset with additional annotations to signify the current state of the dialogue.""",Emotional support dialogue -- chat logs,English,No,Yes,Unknown,Other: Crowdworkers,Fine-tuning + other modules,"Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.","Unspecified, might include formal therapy methods",Llama 2 family; Other: Azure GPT-3.5,No users involved,"","","","","","","","",Y,B,L,Other LLMs,N,"","","",N,"","","",N,"","","","",Y,B,L,Other LLMs,N,"","","","",Y,B,L,Other LLMs,N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
28,K 2023,Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,12,2023,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","Emotional Support Conversation dataset. Comprises 1,053 multi-turn dialogues, amounting to 31,410 utterances. ""We have augmented the dataset with additional annotations to signify the current state of the dialogue.""",Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Lay people,Fine-tuning + other modules,"Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework

Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.","Unspecified, might include formal therapy methods",GPT-3.5 family; Llama 2 family; Other: SeqGPT,No users involved,"","","","","","","","",Y,B,L,"BLEU, ROUGE, METEOR. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",N,"","","",N,"","","",N,"","","","",Y,B,L,"Human interactive evaluation (fluency, comforting, etc.), comparison with other models. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5).",N,"","","","",Y,B,L,"Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
26,Frisone 2025,SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",5,5,2025,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population; Other: working professionals,8,"",Self-collected data,"",Emotional support dialogue -- chat logs,Other: unknown. Italian?,No,No,Unknown,Other: Unsure,Only fine-tuning,"Socrates was subjected to rigorous fine-tuning with pains-takingly detailed instructions designed to optimize therapeutic interactions. The model was specifically trained to recognize and appropriately respond to a wide spectrum of emotional states expressed by users, while respecting moments of  silence or reluctance to engage in conversation. It was calibrated to avoid overly intrusive questioning that might compromise user comfort and trust and instead detect nuanced linguistic patterns that might indicate various psychological states. Throughout its development, emphasis was placed on maintaining an appropriate balance between providing support and encouraging self-reflection",Peer support conversation,"ChatGPT, model unspecified",Yes,"",Y,N,Y,N,"System Usability Scale, SUS
User Experience Scale, UES 
Client Satisfaction Questionnaire, CSQ
Likert-scale","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",Y,"Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.",Y,see previous one,N,"",Z,"Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"Integrating Socrates into environ-
ments such as hospitals or drug rehabilitation centers could
therefore provide substantial benefits. Patients could gain
additional time for psychological reflection beyond what current staffing constraints allow. Moreover, some individuals
might find it easier to discuss sensitive issues with Socrates
rather than in face-to-face interactions, potentially experienc-
ing less perceived judgment and accelerating psychological
change processes. Health care providers might also benefit
from this technology through more streamlined workflows
and reduced burnout risk.
",""
26,Frisone 2025,SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",5,5,2025,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,8,"",Other: yes but unclear which,"","","","","","","",Only fine-tuning,"",Other CBT techniques,"ChatGPT, model unspecified",Yes,"",y,y,y,y,"system usability (System Usability Scale, SUS), user engagement
(User Experience Scale, UES), and satisfaction with the psy-
chological service (Client Satisfaction Questionnaire, CSQ).","Most participants reported feeling strongly understood by
the chatbot, with the majority indicating a high degree of
perceived emotional attunement. Participants described the
experience as intuitive to navigate, personally meaningful,
and contextually relevant to their situations. A general sense
of satisfaction was expressed across the participant group,
with favorable ratings on the satisfaction measures.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","Subjective Units of Distress Scale, SUDS",no benchmark,no benchmark,pre post measurement,"Positive and Negative Affect Schedule, PANAS",no benchmark,no benchmark,pre post measurement,"","","","","",y,"Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.",n,"",n,"",n,"",n,"",n,"",n,"",n,"",y,"",n,"",n,"",n,"",""
26,Frisone 2025,SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",5,5,2025,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,8,"","Other: some dataset was used, but completely unknown characteristics","",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Only fine-tuning,"Socrates was subjected to rigorous fine-tuning with pains-takingly detailed instructions designed to optimize therapeutic interactions. The model was specifically trained to recognize and appropriately respond to a wide spectrum of emotional states expressed by users, while respecting moments of  silence or reluctance to engage in conversation. It was calibrated to avoid overly intrusive questioning that might compromise user comfort and trust and instead detect nuanced linguistic patterns that might indicate various psychological states. Throughout its development, emphasis was placed on maintaining an appropriate balance between providing support and encouraging self-reflection","CBT: Cognitive restructuring; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",Y,N,Y,N,"System Usability Scale, SUS
User Experience Scale, UES 
Client Satisfaction Questionnaire, CSQ
Likert-scale","Most participants reported feeling strongly understood by
the chatbot, with the majority indicating a high degree of
perceived emotional attunement. Participants described the
experience as intuitive to navigate, personally meaningful,
and contextually relevant to their situations. A general sense
of satisfaction was expressed across the participant group,
with favorable ratings on the satisfaction measures.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","Subjective Units of Distress Scale, SUDS",no benchmark,no benchmark,pre post measurement,"Positive and Negative Affect Schedule, PANAS",no benchmark,no benchmark,pre post measurement,"","","","","",y,"Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.",n,"",n,"",y,"Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.",n,"",n,"",n,"",n,"",y,"",n,"",n,"",Y,"Integrating Socrates into environ-
ments such as hospitals or drug rehabilitation centers could
therefore provide substantial benefits. Patients could gain
additional time for psychological reflection beyond what current staffing constraints allow. Moreover, some individuals
might find it easier to discuss sensitive issues with Socrates
rather than in face-to-face interactions, potentially experienc-
ing less perceived judgment and accelerating psychological
change processes. Health care providers might also benefit
from this technology through more streamlined workflows
and reduced burnout risk.
",""
25,Habicht 2025,Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10,03,2025,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,244,"","","","","","","","","",Prompting + other modules,-,CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques,GPT-4 / GPT-4o family,Yes,"",n,y,y,y,"","higher reliable improvement, recovery, and reliable recovery rates” 
users perceived the AI-enabled therapy support tool as most useful for discussing their problems to gain awareness and clarity … learning how to apply coping skills”","",n,"","","","","","","","","","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","","","","","","","","","","","","","","","","","","","","","","",y,"Patients using the AI-enabled therapy support tool exhibited … fewer dropouts from treatment.” (Position: Results, p. 1)","","",Y,"higher reliable improvement, recovery, and reliable recovery rates” (Position: Results, p. 1)",Y,comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (,"","",Y,"eal-world observational study … in 5 of the United Kingdom’s National Health Service Talking Therapies services” (Position: Methods, p. 1)",""
25,Habicht 2025,Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10,3,2025,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"Other: Probably patients with disorders, but not specified","","",No dataset used for development or evaluation,"","","","","","","",Other: unknown,"","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,Y,N,Y,"","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",Y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",N,"",N,"",Y,Table 1,N,"",N,"",N,"",Y,"Reliable improvement refers to a clinically significant
improvement in symptoms following a course of treatment and
is calculated as the score difference between the first and the
last validated clinical questionnaire completed. The types of
questionnaires patients complete are tailored to their specific
condition. For example, the Patient Health Questionnaire-9
(PHQ-9) [30] is used to measure depression symptom severity,
and the Generalized Anxiety Disorder-7 (GAD-7) [31] is used
to measure anxiety symptom severity. A clinically significant
improvement in symptoms is considered a change score ≥6 for
PHQ-9 or ≥4 for GAD-7 [26]",Y,"We compared the clinical outcomes of individuals who signed
up to use the AI-enabled therapy support tool (the intervention
group) with those of individuals who did not (the control group):",N,"",N,"",""
25,Habicht 2025,Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10,03,2025,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,244,"",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,-,"CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,Y,y,Y,"","higher reliable improvement, recovery, and reliable recovery rates” 
users perceived the AI-enabled therapy support tool as most useful for discussing their problems to gain awareness and clarity … learning how to apply coping skills”","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",Y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",N,"",N,"",Y,Table 1,N,"",N,"",N,"",Y,"Reliable improvement refers to a clinically significant
improvement in symptoms following a course of treatment and
is calculated as the score difference between the first and the
last validated clinical questionnaire completed. The types of
questionnaires patients complete are tailored to their specific
condition. For example, the Patient Health Questionnaire-9
(PHQ-9) [30] is used to measure depression symptom severity,
and the Generalized Anxiety Disorder-7 (GAD-7) [31] is used
to measure anxiety symptom severity. A clinically significant
improvement in symptoms is considered a change score ≥6 for
PHQ-9 or ≥4 for GAD-7 [26]",Y,comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (,N,"",N,"",""
24,B 2024,Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues,Richard Gaus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3,12,2024,China,Empirical research involving an LLM,"","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
24,B 2024,Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3,12,2024,China,Empirical research involving an LLM,Analysis of conversation transcripts,"","","","",External data set,MDRDC and Dialogue Safety,Other: Twitter (X),Chinese,No,No,Unselected,Lay people,Only fine-tuning,"","","BERT family; T5 family; GPT-3.5 family; ChatGPT, model unspecified",No,"","","","","","","","","","","","",Y,B,L,BERT; BERT- CRF; BERT-MCRF; ROBERTA,Y,B,L,"Precision, Recall, Macro-F1","","","","","",N,"","","",Y,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
24,B 2024,Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues,Consensus,Conference paper,"",3,12,2024,China,Empirical research involving an LLM,"","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
22,Hornstein 2025,Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study,Richard Gaus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
22,Hornstein 2025,Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study,Reviewer Two,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
22,Hornstein 2025,Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study,Consensus,"","","","","","","","","","","","","","","","","","","","","","","",BERT family,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
20,K 2025,"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5,5,2025,Other: New Zealand,Population survey,Client-facing application,"",General population,306,"",Self-collected data,"",Other: Rating,English,No,No,Unknown,Unknown,"","","Unspecified, might include formal therapy methods",Other: AI in general,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
20,K 2025,"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5,5,2025,Other: New Zealand,Population survey,Client-facing application,Multi-turn chatbot,General population,348,"",No dataset used for development or evaluation,"","","","","","","","","","Informal counseling (e.g., emotional support conversation)","Other: just general ""generative AI application""",Yes,"",y,n,y,y,"Human Computer Trust Scale S. S. Siddharth Gulati and D. Lamas, “Design, development and evaluation of a human-computer trust scale,” Behaviour & Information Technology","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",-,"",-,"",-,"",-,"",y,"articipants were largely Caucasian Amer-
ican (57.3%), followed by African American (23.5%), Asian
American (12.6%) and others/did not specify (6.6%)",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
20,K 2025,"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5,5,2025,Other: New Zealand,Population survey,Client-facing application,Multi-turn chatbot,General population,306,"",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,"","","Unspecified, might include formal therapy methods","Other: just general ""generative AI application""",Yes,"",y,n,y,y,"Human Computer Trust Scale S. S. Siddharth Gulati and D. Lamas, “Design, development and evaluation of a human-computer trust scale,” Behaviour & Information Technology",No user experience assessment but participant attitude survey,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
18,A 2025,Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,01,2025,Other: Egypt,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","Bilingual (English & Arabic) mental health text corpus, origin not found",Other: ,"Other: English, Arabic",No,Yes,Unknown,Unknown,Only fine-tuning,Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses,"Informal counseling (e.g., emotional support conversation)",Other: Jais-13B,No users involved,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",cross-entropy loss,"","",compared to data set 70.58 %,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility"
18,A 2025,Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,1,2025,Other: Egypt,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,Web-scraped data from diverse sources,"Other: Internet data, various types and sources (mixed)",Other: Arabic,No,No,"Other: Not applicable, data is not on users","Other: Not applicable, no interaction data",Only fine-tuning,Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B,"Unspecified, might include formal therapy methods",Other: Jais-13B,"","","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",cross-entropy loss,no benchmark,no benchmark,no benchmark,"","","","","","","","","",n,"",n,"",y,Jais-13B is on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
18,A 2025,Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15,1,2025,Other: Egypt,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","Web-scraped data from diverse sources

Bilingual (English & Arabic) mental health text corpus, origin not found","Other: Internet data, various types and sources (mixed)",Other: Arabic,No,No,"Other: Not applicable, data is not on users","Other: Not applicable, no interaction data",Only fine-tuning,"Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B
Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses","Unspecified, might include formal therapy methods",Other: Jais-13B,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",cross-entropy loss,no benchmark,no benchmark,compared to data set 70.58 %,"","","","","","","","","",n,"",n,"",y,Jais-13B is on-premise,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility"
17,A 2025,A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),23,05,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified",mental health counseling conversations from huggingface; fine-tuned with unsloth and ollama,Emotional support dialogue -- chat logs,English,No,Yes,Unknown,Lay people,Fine-tuning + other modules,"Fine-tuned LLaMA 3.2 (3B parameters) using Unsloth and Ollama; used RAG for retrieval, LangChain memory for conversation history, and FAISS for relevant material recall","Informal counseling (e.g., emotional support conversation)",Other: Llama 3.2,No,"","","","","","",technical proof-of-concept; no real users assessed,"",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","",N,"","","","","","","","",Y,Llama is ? so are unsloth/ollama,"","","","","","","","","","","","","","","","","","",""
17,A 2025,A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),23,5,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"Huggingface: Mental Health Counseling Conversations
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations",Internet data -- mental health Q&A,English,No,Yes,Unselected,Unknown,Fine-tuning + other modules,Fine-tuning via LoRA + RAG pipeline,"Unspecified, might include formal therapy methods",Other: Llama 3.2 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","unclear metrics: relevance, empathy, conciseness, context",no benchmark,no benchmark,Unclear what these metrics are (human ratings? automatic?). No benchmarks used.,"","","","","","","","","",n,"",n,"",y,Llama 3.2 used,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
17,A 2025,A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),23,5,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"Huggingface: Mental Health Counseling Conversations
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Fine-tuning + other modules,"Fine-tuned LLaMA 3.2 (3B parameters) using Unsloth and Ollama; used RAG for retrieval, LangChain memory for conversation history, and FAISS for relevant material recall","Unspecified, might include formal therapy methods",Other: Llama 3.2 family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","unclear metrics: relevance, empathy, conciseness, context",no benchmark,no benchmark,Unclear what these metrics are (human ratings? automatic?). No benchmarks used.,N,"","","",N,"","","","",n,"",n,"",Y,Llama 3.2 used,n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
15,S 2025,MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9,4,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"Other: Unclear, probably general population","","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules; Other: RAG,Prompt Engineering + RAG + generic LLMs,"Other: The chatbot's architecture is built on a 
robust knowledge base encompassing evidence-based therapeutic techniques, including cognitive-behavioral therapy (CBT) principles and motivational interviewing 
strategies.",BERT family,Yes,"",N,N,Y,Y,"","Not described, how many users and rather vague representation of results in Fig. 7.","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"The chatbot's functionality extends beyond mere conversation, integrating mood tracking features, 
personalized coping strategy recommendations, and crisis 
detection algorithms with built-in escalation protocols for 
high-risk situations",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
15,S 2025,MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",09,11,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,assumes prior knowledge from mental health-related soruces or web-scraped materials,"",English,"","","","",Prompting + other modules,"uses NLP and probabilistic models to classify symptoms, map user queries to mental health knowledge base, and personalize therapeutic suggestions",Other CBT techniques,BERT family,No,"","","","","","",descpritive only,"",N,"","","","","","","","","","","","","","","","",N,"","","","","","","","",N,"","","","","","","","",N,"","","","","","","","","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","","Ambitious platform vision, but lacks any form of quantitative, clinical, or user-centered evaluation"
15,S 2025,MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",09,4,2025,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"No clients/patients involved; Other: User data shown in Fig. 7 but sample nowhere futher described. Hence unclear, how data was retrieved. ","","",No dataset used for development or evaluation,"","",Other: ,"","","","",Prompting + other modules; Other: RAG,"uses NLP and probabilistic models to classify symptoms, map user queries to mental health knowledge base, and personalize therapeutic suggestions","CBT: Motivational interviewing; Other CBT techniques; Other: The chatbot's architecture is built on a 
robust knowledge base encompassing evidence-based therapeutic techniques, including cognitive-behavioral therapy (CBT) principles and motivational interviewing 
strategies.",BERT family,Yes,"",N,N,Y,Y,"","Not described, how many users and rather vague representation of results in Fig. 7.","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"The chatbot's functionality extends beyond mere conversation, integrating mood tracking features, 
personalized coping strategy recommendations, and crisis 
detection algorithms with built-in escalation protocols for 
high-risk situations","","","","","","","","","","","","","","","","","","","","","","","Ambitious platform vision, but lacks any form of quantitative, clinical, or user-centered evaluation"
11,F 2024,LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",08,12,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","mental health and healthcare-related texts; sources not fully specified, combined domain corpora and simulated patient records",Other: mix,English,Yes,Yes,Unknown,Unknown,Prompting + other modules,"","Informal counseling (e.g., emotional support conversation)",Other: not specif.,No,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz"
11,F 2024,LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8,12,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","In order to effectively function and provide personalized
response to a user, data preparation plays a vital role. The
data comes from multiple sources, such as books, articles,
news, media, etc

We started experimenting with fine-tuning-based approach.
To generate the training dataset, we collected various mental
health related books, articles, and news stories and prepared
almost 500 pairs of instructions in the format of ”system”,
”user”, and ”assistant”. What we found is that this approach
is good for an LLM-based system that is already trained
on certain things","Other: different types (books, articles, etc.), not further specified",Other: unknown,No,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning (according to Fig. 3 at least) and RAG,"Unspecified, might include formal therapy methods",GPT-3.5 family; Other: all-MiniLM-L6-v2 (for embedding),No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",Relevancy,b,l,"Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",User Satisfaction Score,b,l,"Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
11,F 2024,LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8,12,2024,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","External data set, modified","mental health and healthcare-related texts; sources not fully specified, combined domain corpora and simulated patient records","Other: different types (books, articles, etc.), not further specified",Other: unknown,No,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning (according to Fig. 3 at least) and RAG,"Unspecified, might include formal therapy methods",GPT-3.5 family; Other: all-MiniLM-L6-v2 (for embedding),No users involved,"","","","","","","","",n,"","","",y,b,l,"relevancy score. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",User Satisfaction Score,b,l,"Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz"
10,McBain 2025,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",5,3,2025,USA,Empirical research involving an LLM,Analysis of conversation transcripts,"","","","",Self-collected data,SIRI-2 with added expert suicidologist ratings,Other: SIRI-2 test data with ratings,English,No,No,Other: n.a.,Other: n.a.,Only prompting,"","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",y,no benchmark,no benchmark,Correlation between LLM suicide response ratings and expert ratings,"",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",y,"widespread use of LLM
technology—including new companies already drawing on
LLM technology for mental health care [29]—could reach a
much wider audience of individuals coping with depression and
suicidal thoughts. To date, a common guardrail has been for
LLMs to produce “hard stops”, in which individuals are referred
to 988 or another suicide prevention hotline. While such referrals
may be beneficial, they also artificially circumscribed
interactions in a way that could be taken as a missed opportunity.",""
10,McBain 2025,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",5,3,2025,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"Responses by expert suicidologists on a previously-published standardized scale: the Suicide Intervention Response Inventory (SIRI-2)
[15].",Other: Standardized clinical instrument with hypothetical scenarios and predefined responses,English,No,Yes,Other: Not applicable; Artificial cases.,Trained professionals,Only prompting,"Research team members prompted LLMs with the original instructions for the SIRI-2, as well as with one of the SIRI2-2’s 24 items. We did not prompt LLMs with any additional text. ","Other: LLMs did not provide support but instead compared to clinician responses using the SIRI-2 scoring system (= ""rating assessment"").",GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","",Bias,W,H, Compared to the ratings of expert suicidologist.,Overall Performance,S/B,H,"Compared to the ratings of expert suicidologist.
","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
10,McBain 2025,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",5,3,2025,USA,Empirical research involving an LLM,Analysis of conversation transcripts,Other: ,"","","",External data set,"Responses by expert suicidologists on a previously-published standardized scale: the Suicide Intervention Response Inventory (SIRI-2)
[15].",Other: Standardized clinical instrument with hypothetical scenarios and predefined responses,English,Other: Yes (hypothetical cases created by experts),Yes,Other: n.a.,Trained professionals,Only prompting,"Research team members prompted LLMs with the original instructions for the SIRI-2, as well as with one of the SIRI2-2’s 24 items. We did not prompt LLMs with any additional text. ","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",y,no benchmark,no benchmark,Correlation between LLM suicide response ratings and expert ratings,"",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","",siri-2 z-score,no benchmark,no benchmark,"Compared to the ratings of expert suicidologist.
","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
9,Kang 2025,Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3,1,2025,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,20,"",Self-collected data,"",Other: Rating,Other: Korean,No,No,Unselected,Lay people,"","","Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; ChatGPT, model unspecified; Other: Conflicting information (?)

The HoMemeTown chatbot, powered by ChatGPT 4.0

The chatbot relies on the GPT API, a general-purpose language model provided by OpenAI, instead of a domain-specific model
trained for mental health counseling. The GPT API offers a range of models, such as Davinci, GPT-3.5, and GPT-4, which can be selected based on desired performance and cost
consideration

Another limitation is the potential inconsistency in comparing Dr CareSam, built on the ChatGPT 4.0 API,",Yes,"",N,Y,Y,Y,"","Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. ","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",Y,"The risk detection system in HoMemeTown was developed
based on established clinical guidelines [19] and validated
screening tools [20], implementing a sophisticated approach to
identifying and responding to potential mental health concerns.
The system continuously monitors user interactions for primary
risk indicators, including expressions of suicidal ideation, severe
depression symptoms, and anxiety crisis signals, while also
tracking secondary indicators such as sleep disturbance patterns
and social withdrawal signs.
When potential risks are detected, the system implements a
graduated response protocol that has been carefully designed
to provide appropriate levels of support while avoiding
unnecessary escalation. For mild risk situations, the system
offers empathetic acknowledgment and self-help resources,
drawing from evidence-based interventions [21]. In cases of
moderate risk, the response includes more direct expressions
of concern and specific mental health resources, while severe
risk triggers an immediate crisis response protocol with direct
connections to professional support services. To address the challenge of potential false positives in risk
detection, we implemented a sophisticated validation system
that examines multiple contextual factors before triggering
interventions. This system uses NLP techniques to analyze the
broader context of user communications, helping to distinguish
between casual expressions and genuine indicators of distress.
Regular professional review of high-risk cases ensures the
ongoing refinement of detection algorithms and response
protocols, maintaining a balance between sensitivity and
specificity in risk assessment.",N,"",N,"",N,"",Y,"The study included 20 participants aged 18 to 27 (mean 23.3,
SD 1.96) years with 60% (12/20) female and 40% (8/20) male.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",Comparative Analysis unclear in methodological aspects to me. 
9,Kang 2025,Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3,1,2025,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,20,"",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,Elaborate server architecture around OpenAI API (see Fig. 1),"Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,"",n,y,y,y,"","The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","quantitative ux assessment (""overall satisfaction"")",b,l,"benchmarks are other mental health chatbots and general LLMs: Woebot, Happify, GPT-3.5, Google Bard","","","","","","","","","",y,"We incorporated a risk detection function to identify potential
mental health crises and provide appropriate resources when
necessary",y,"In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.",n,"",y,"Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"Only: Integration with health care systems
• Investigate secure ways to integrate chatbot data with electronic health records, while maintaining user privacy.",""
9,Kang 2025,Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3,1,2025,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,20,"",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,Prompting + other modules,Elaborate server architecture around OpenAI API (see Fig. 1),"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,"",N,Y,Y,Y,"","The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.

Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. ","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","quantitative ux assessment (""overall satisfaction"")",b,l,"benchmarks are other mental health chatbots and general LLMs: Woebot, Happify, GPT-3.5, Google Bard","","","","","","","","","",Y,"The risk detection system in HoMemeTown was developed
based on established clinical guidelines [19] and validated
screening tools [20], implementing a sophisticated approach to
identifying and responding to potential mental health concerns.
The system continuously monitors user interactions for primary
risk indicators, including expressions of suicidal ideation, severe
depression symptoms, and anxiety crisis signals, while also
tracking secondary indicators such as sleep disturbance patterns
and social withdrawal signs.
When potential risks are detected, the system implements a
graduated response protocol that has been carefully designed
to provide appropriate levels of support while avoiding
unnecessary escalation. For mild risk situations, the system
offers empathetic acknowledgment and self-help resources,
drawing from evidence-based interventions [21]. In cases of
moderate risk, the response includes more direct expressions
of concern and specific mental health resources, while severe
risk triggers an immediate crisis response protocol with direct
connections to professional support services. To address the challenge of potential false positives in risk
detection, we implemented a sophisticated validation system
that examines multiple contextual factors before triggering
interventions. This system uses NLP techniques to analyze the
broader context of user communications, helping to distinguish
between casual expressions and genuine indicators of distress.
Regular professional review of high-risk cases ensures the
ongoing refinement of detection algorithms and response
protocols, maintaining a balance between sensitivity and
specificity in risk assessment.",y,"In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.",N,"",N,"Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].",n,"The study included 20 participants aged 18 to 27 (mean 23.3,
SD 1.96) years with 60% (12/20) female and 40% (8/20) male.",N,"",N,"",N,"",N,"",N,"",N,"",N,"Only: Integration with health care systems
• Investigate secure ways to integrate chatbot data with electronic health records, while maintaining user privacy.",Comparative Analysis unclear in methodological aspects to me. 
8,Schaefer 2025,"Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study",Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",13,06,2025,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,527,"",Self-collected data,"","","","","","","","","",CBT: Cognitive restructuring; Other CBT techniques,Other,Yes,"",Y; working alliance inventory-short report? ,"",Y,Y,"","users expressed positive attitudes toward digital mental health solutions, with key motives including avoiding embarrassment (36%) and concerns about appearance in face-to-face consultations (35%). Expectations focused on emotional support (35%) and expressing feelings (32%)","",N,"","","","","","","","","","","","","","","","",N,"","","","","","","","",N,"","","","","","","","",N,"","","","","","","","","","","","","","","","","","",Y,"Yes → Adheres to data protection principles, anonymized use","","","","","","","","","","","","","","","","",""
8,Schaefer 2025,"Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",13,6,2025,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,21,"",No dataset used for development or evaluation,"","","","","","","",Fine-tuning + other modules,"Clare is a standalone tool, based on fine-tuned LLMs plus other modules voice inputs, safety checks",CBT: Cognitive restructuring; Other CBT techniques,Other: Clare (R) by clare&me GmbH,Yes,"",y,y,n,y,User experience questionnaire (UEQ),"","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",WAI-SR working alliance,no benchmark,no benchmark,no benchmark,"","","","","","","","","",y,"If suicidality or severe distress is detected, users are directed to
psychological support hotlines and blocked from further use to
ensure safety.",y,"A custom moderation application programming interface (API)
filters inputs and outputs, flagging inappropriate content before
LLM processing. Key safety features include the following.",y,"Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.",n,did not describe how claire aligns with data protection regulations,y,Table 3,n,"",y,See 3.2 Engagement patterns,n,"",y,"",n,"",n,"",n,"",""
8,Schaefer 2025,"Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study",Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",13,6,2025,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,21,"",No dataset used for development or evaluation,"","","","","","","",Fine-tuning + other modules,"Clare is a standalone tool, based on fine-tuned LLMs plus other modules voice inputs, safety checks",CBT: Cognitive restructuring; Other CBT techniques,Other: Clare (R) by clare&me GmbH,Yes,"",y,n,Y,y,User experience questionnaire (UEQ),"users expressed positive attitudes toward digital mental health solutions, with key motives including avoiding embarrassment (36%) and concerns about appearance in face-to-face consultations (35%). Expectations focused on emotional support (35%) and expressing feelings (32%)","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",WAI-SR working alliance,no benchmark,no benchmark,no benchmark,"","","","","","","","","",y,"If suicidality or severe distress is detected, users are directed to
psychological support hotlines and blocked from further use to
ensure safety.",y,"A custom moderation application programming interface (API)
filters inputs and outputs, flagging inappropriate content before
LLM processing. Key safety features include the following.",n,"Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.
But not on-premise capable!",Y,"Yes → Adheres to data protection principles, anonymized use
https://www.clareandme.com/post/what-happens-with-your-data-at-clare-me",y,Table 3,n,"",y,See 3.2 Engagement patterns,n,"",y,"UCLA, PHQ-D, PHQ-4, SWLS, Mini-SPIN",n,"",n,"",n,"",""
6,Herbert 2025,Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12,02,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","","","","",English,No,No,Psychopathology,Trained professionals,Only prompting,"","CBT: Motivational interviewing; Informal counseling (e.g., emotional support conversation); Other: “if possible,” obtain a commitment to start the treatment medication, buprenorphine. ",GPT-4 / GPT-4o family,Yes,"",N,Y,N,N,"","","",N,"","","",N,"","","",N,"","","",N,"","","","","","","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"Question ""Is this infomation safe?""",N,"",N,"",N,"",N,"",N,"",N,"",Y,DSM 5,N,"",N,"",N,"",Unsure about the metrics. We can discuss during the consensus. See table 4
6,Herbert 2025,Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12,2,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",No dataset used for development or evaluation,"","","","","","","",Only prompting,"Prompt engineering on GPT-4. Authors describe the model is being ""tuned"" or ""fine-tuned"" but in fact they only iteratively adjust the prompt without any weight adjustment.",CBT: Motivational interviewing,GPT-4 / GPT-4o family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,no benchmark,no benchmark,Binary scoring rubrics for GPT-4 responses (see Table 4),n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",""
6,Herbert 2025,Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12,2,2025,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",Self-collected data,see Generating Data for Evaluation,Emotional support dialogue -- chat logs,English,Yes,No,Other: not applicable,Other: not applicable,Only prompting,"Prompt engineering on GPT-4. Authors describe the model is being ""tuned"" or ""fine-tuned"" but in fact they only iteratively adjust the prompt without any weight adjustment.","CBT: Motivational interviewing; Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",y,no benchmark,no benchmark,Binary scoring rubrics for GPT-4 responses (see Table 4),N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",n,"",N,"",N,"",N,"",N,"",N,"",N,"",n,"",N,"",N,"",N,"",Unsure about the metrics. We can discuss during the consensus. See table 4
5,Huang 2025,Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3,4,2025,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"The study uses data from YiXinli Community, a major Chinese online counseling platform (https://www.xinli001.com) frequented by nearly 40 million users worldwide for psychological support. On this platform, 
users can confidentially express their psychological concerns and receive advice from human counselors in the open Q&A section (https://www.xinli001.com/qa). We utilized a web data crawler called houyicaiji (https://www.houyicaiji.com/) to gather this Q&A data, which encompasses 10,903 distinct questions and 19,682 human counselor responses collected from November 3, 2022, to 
March 30, 2023.",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Trained professionals,Prompting + other modules,"","Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",Y,W,H,"","",Perceived information quality (unpromted),W,H,"",Perceived information quality (promted),B/S/W (depending on risk situation),H,"","","","","","",Y,"In this study, the ethical risks associated with directly applying LLMs to individuals seeking mental 
health support were mitigated by utilizing pre-existing real-world data. ",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",All comparisons between AI and human counselors based on ML Models. 
5,Huang 2025,Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3,4,2025,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"Data crawled from YiXinli Community (xinli001.com/qa). They crawled this data themselved ->  10,903 distinct questions and 19,682 human counselor responses",Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Unknown,Only prompting,The help-seeking questions were posted to the GPT-3.5-turbo model by OpenAI API to get responses from ChatGPT,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,"","","","","","","","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","Automatic ""perceived information quality"" (PIQ) rating via ML model",b,h,benchmark are real human responses from the Q&A dataset,Various linguistic factors,mixed (Table 9),h,benchmark are real human responses from the Q&A dataset,"","","","","",n,"",n,"",n,"",y,"First, although direct application risks were 
avoided, it is crucial to ensure that the real-world data used is properly protected and compliant with privacy regulations during both 
collection and processing (
). In this study, the data used was publicly available online, and all analysis was con­
ducted in a manner that safeguarded personal privacy. ",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"","This study also was a ""transcript analysis"" via BERT"
5,Huang 2025,Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3,4,2025,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,"","",External data set,"The study uses data from YiXinli Community, a major Chinese online counseling platform (https://www.xinli001.com) frequented by nearly 40 million users worldwide for psychological support. On this platform, 
users can confidentially express their psychological concerns and receive advice from human counselors in the open Q&A section (https://www.xinli001.com/qa). We utilized a web data crawler called houyicaiji (https://www.houyicaiji.com/) to gather this Q&A data, which encompasses 10,903 distinct questions and 19,682 human counselor responses collected from November 3, 2022, to 
March 30, 2023.",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Unknown,Only prompting,The help-seeking questions were posted to the GPT-3.5-turbo model by OpenAI API to get responses from ChatGPT,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",Y,W,H,"","","Automatic ""perceived information quality"" (PIQ) rating via ML model (prompted ChatGPT)",b,H,benchmark are real human responses from the Q&A dataset,Various linguistic factors,mixed (Table 9),h,benchmark are real human responses from the Q&A dataset,"","","","","",n,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","All comparisons between AI and human counselors based on ML Models. 

This study also was a ""transcript analysis"" via BERT"
4,Liu 2025,Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots,Richard Gaus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),8,1,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,48,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,Only sub-study 3 employed an LLM,Other: positive psychology intervention,GPT-3.5 family,Yes,"",n,y,n,y,"","Before the study’s completion, we conducted unstructured interviews to delve deeper into the participants’ experiences with the chatbots. Many participants expressed that the chatbot’s responses evoked feelings of surprise and novelty. They emphasized the importance of feeling understood as pivotal in enhancing their acceptance and engagement with the chatbot-based intervention. Since GPT-based chatbots can generate text in a human-like manner, mimicking the interaction between users and psychotherapists, five participants indicated that the anthropomorphic responses felt so authentic that they found it difficult to distinguish whether they were AI-generated. These qualitative insights reinforce our quantitative findings and underscore the critical importance of interaction in the design of chatbots.","",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","",PHQ-9,b,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ","Scales of Psychological Well-being (PWB), Satisfaction With Life Scale (SWLS)",b,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ","GAD-7, PANAS-P, PANAS-N",s,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ","",n,"",n,"",n,"",n,"",n,"",n,"",n,"",n,"",y,"",y,"",n,"",n,"",""
4,Liu 2025,Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots,Reviewer Two,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),8,1,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,"Substudy 1
ITT-Sample: 207
CC-Sample: 154

Substudy 2
ITT-Sample: 70
CC-Sample 53

Substudy 3
ITT-Sample: 72
CC-Sample: 48","",No dataset used for development or evaluation,"","","","","","","",Prompting + other modules,"Pre-trained ChatGPT-3.5-Turbo API without fine-tuning but prompt engineering, integrating it with other system components.","Unspecified, might include formal therapy methods",GPT-3.5 family,Yes,"",N,Y,N,Y,"",Only partialy described in the discussion section.,"",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",Y,"our exhaustive analysis of all chatbot- 
user dialogues recorded in this study did not reveal any inappropriate responses. This result, however, is not defini-
tive, and further research is required to ascertain the safety of chatbots in psychological research",N,"",N,"",N,"",N,"",N,"",N,"",Y,"PHQ-9, GAD-7, PANAS-P, PANAS-N, SWLS, SVS",Y,"Evident in whole study (e.g., Finally, it is worth noting that our choice of an active control group (that is, a non-trivial control group)...)",N,"",N,"",""
4,Liu 2025,Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots,Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),8,1,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,48,"",No dataset used for development or evaluation,"","","","","","","",Only prompting,Only sub-study 3 employed an LLM,Other: positive psychology intervention,GPT-3.5 family,Yes,"",N,Y,N,Y,"","Before the study’s completion, we conducted unstructured interviews to delve deeper into the participants’ experiences with the chatbots. Many participants expressed that the chatbot’s responses evoked feelings of surprise and novelty. They emphasized the importance of feeling understood as pivotal in enhancing their acceptance and engagement with the chatbot-based intervention. Since GPT-based chatbots can generate text in a human-like manner, mimicking the interaction between users and psychotherapists, five participants indicated that the anthropomorphic responses felt so authentic that they found it difficult to distinguish whether they were AI-generated. These qualitative insights reinforce our quantitative findings and underscore the critical importance of interaction in the design of chatbots.","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","PHQ-9, PANAS-P, Satisfaction With Life Scale (SWLS)",b,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",Scales of Psychological Well-being (PWB),unknown,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ","GAD-7, PANAS-N",s,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ","",N,"",n,"",N,"",N,"",N,"",N,"",N,"",N,"",Y,"PHQ-9, GAD-7, PANAS-P, PANAS-N, SWLS, SVS",Y,"Evident in whole study (e.g., Finally, it is worth noting that our choice of an active control group (that is, a non-trivial control group)...)",N,"",N,"",""
3,Zhang 2024,VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",26,11,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),49,"",Self-collected data,zhang 2024 dataset,Other: Descriptions of mental disorders in the DSM-5,Other: unknown,Other: unknown,No,Other: n.a.,Other: n.a.,Fine-tuning + other modules,Fine-tuning + knowledge retrieval,"Unspecified, might include formal therapy methods",Other: ChatGLM2,Yes,"",y,n,y,y,"Counselor rating form‑short (CRF‑S), Client satisfaction scale (CSS)","","",n,"","","",n,"","","",n,"","","",n,"","","","",y,s,l,"We invited three counselors to evaluate 
these conversation cases. Benchmark is ""LLM-Counselor Support System""",n,"","","","",n,"","","",n,"","","","",Client satisfaction scale (CSS),b,l,Benchmars are other LLM-based models,Counselor rating form‑short (CRF‑S),b,l,Benchmars are other LLM-based models,Sentiment score,b,l,Benchmars are other LLM-based models,"",n,"",n,"",y,"",n,"",n,"",n,"",n,"",n,"",n,"",y,"",n,"",n,"",""
3,Zhang 2024,VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",26,11,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: Patients with psychological problems and having sought professional help,49,"",Self-collected data,"","Other: Synthetic, self-created counseling dialogues for fine-tuning.",Other: Either Chines (most likely) or English,Yes,No,Unknown,Unknown,Fine-tuning + other modules; Prompting + other modules,"A base LLM was fine-tuned on 80 structured psychological counseling cases to adapt it to clinical contexts. Additionally, the model uses dynamic prompting with a structured DSM-5-based knowledge base. This is integrated into a larger pipeline involving text summarization, keyword extraction, and similarity analysis, alongside multimodal outputs (e.g., facial expressions, voice, and a visual avatar) to enable affective interaction.","Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods",Other: ChatGLM2-6B ,Yes,"",Y,N,Y,Y,Client satisfaction scale (CSS),"","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,S/B,L,The LLM-Counselor Support System,N,"","","","",N,"","","",N ,"","","","",Counselor rating form-short (CRF-S),B,L,Other LLMs/Models,"","","","","","","","","",N,"",Y ,"A professional counselor with three years of experience 
monitored the entire conversation to ensure the safety of the 
counseling process",Y,GLM2-6B model ,N,"",Y,"Finally, 49 participants were selected to participate in 
this study for the experiment. The age range of participants 
is 15–40 years old. Among them, 29 (59.18%) were male 
and 20 (40.82%) were female; 15 (30.61%) were middle 
school students, 6 (12.24%) were undergraduate students, 
12 (24.49%) were graduate students, and 16 (32.65%) were 
already working; 12 (24.49%) were 15–18 years old, 21 
(42.86%) were 18–24 years old, and 16 people (32.65%) 
were over 24 years old. All participants did not have a pro-
fessional background in psychology.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
3,Zhang 2024,VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",26,11,2024,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,49,"",Self-collected data,zhang 2024 dataset,Other: Descriptions of mental disorders in the DSM-5,Other: unknown,Other: unknown,No,Other: n.a.,Other: n.a.,Fine-tuning + other modules; Prompting + other modules,"A base LLM was fine-tuned on 80 structured psychological counseling cases to adapt it to clinical contexts. Additionally, the model uses dynamic prompting with a structured DSM-5-based knowledge base. This is integrated into a larger pipeline involving text summarization, keyword extraction, and similarity analysis, alongside multimodal outputs (e.g., facial expressions, voice, and a visual avatar) to enable affective interaction.","Unspecified, might include formal therapy methods",Other: ChatGLM2-6B ,Yes,"",Y,N,Y,Y,"Counselor rating form‑short (CRF‑S), Client satisfaction scale (CSS)","","",N,"","","",N,"","","",N,"","","",N,"","","","",Y,b,l,The LLM-Counselor Support System. Benchmark: GPT-4 with zero-shot CoT,N,"","","","",N,"","","",N ,"","","","",Client satisfaction scale (CSS),b,l,Benchmars are other LLM-based models,Counselor rating form‑short (CRF‑S),b,l,Benchmars are other LLM-based models,Sentiment score,b,l,Benchmars are other LLM-based models,"",N,"",n,"A professional counselor with three years of experience 
monitored the entire conversation to ensure the safety of the 
counseling process",Y,GLM2-6B model ,N,"",Y,"Finally, 49 participants were selected to participate in 
this study for the experiment. The age range of participants 
is 15–40 years old. Among them, 29 (59.18%) were male 
and 20 (40.82%) were female; 15 (30.61%) were middle 
school students, 6 (12.24%) were undergraduate students, 
12 (24.49%) were graduate students, and 16 (32.65%) were 
already working; 12 (24.49%) were 15–18 years old, 21 
(42.86%) were 18–24 years old, and 16 people (32.65%) 
were over 24 years old. All participants did not have a pro-
fessional background in psychology.",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
2,Vo 2024,Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",20,2,2025,Other: Vietnam,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"https://huggingface.co/datasets/jkhedri/psychology-dataset; Only questions embedded, not answers.",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules; Other: RAG,"Uses RAG with prompt engineering: general LLM is guided by structured prompt templates for intent detection and empathetic response generation, while a vector-based QA database supplies domain-specific context.","Unspecified, might include formal therapy methods","Other: No specific model chosen, but as a conclusion claude-sonnet-3.5 suggested. ",No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",N,"","","","",N,"","","",N,"","","","","","","","","","","","","","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"","Three LLMs (Claude-Sonnet-3.5, gemini-1.5-pro, gpt-4o) used for evaluation of optimal base model for developed application. Then used LLM (gpt-4o) as a judge to compare reults agains each other. "
2,Vo 2024,Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,11,2024,Other: Vietnam,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"
1 https://huggingface.co/datasets/jkhedri/psychology-dataset.",Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Unknown,Prompting + other modules,"","Informal counseling (e.g., emotional support conversation)",Other: unclear,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",a little,facilitate access to professional care by linking users with nearby psychiatrists or psychology centers,""
2,Vo 2024,Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11,11,2024,Other: Vietnam,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,"","",External data set,"https://huggingface.co/datasets/jkhedri/psychology-dataset; Only questions embedded, not answers.",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,"Uses RAG with prompt engineering: general LLM is guided by structured prompt templates for intent detection and empathetic response generation, while a vector-based QA database supplies domain-specific context.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,No users involved,"","","","","","","","",N,"","","",N,"","","",N,"","","",N,"","","","",N,"","","",Y,no benchmark,no benchmark,No indexmodel but three equaly ranked models were compared ,"",N,"","","",N,"","","","",llm-as-a-judge approval and reassurance,no benchmark,no benchmark,no benchmark,llm-as-a-judge general response quality,no benchmark,no benchmark,no benchmark,"","","","","",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",N,"",""
1,Jung 2025,""" I've talked to ChatGPT about my issues last night."": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),29,04,2025,USA,Other: content analysis of SM posts,Analysis of conversation transcripts,"",General population,"","",External data set,reddit mental health conversations mentioning ChatGPT,Internet data -- mental health forum,English,No,Yes,Unselected,Lay people,"","",Other: unsure,"ChatGPT, model unspecified",Yes,"","",Y,"",Y,themtaic coding of user posts // themes: ,"","",N,"","","","","","","","","","","","","","","","",Y,"","",thematic content analysis by authors?,"","","","","",N,"","","","","","","","","","","","","","","","","","","","","","","",Y,"discuss misinformation, over-validation, and ethical risks reported by users","","",Y,Ethical use of Reddit data with disclaimers in 3.3.1,"","","","","","",Y,"Somehow: 
However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT’s overly validating nature, and privacy concerns","","","","",Y,Somehow: user-reported perceptions from Reddit,Y,"Speculative discussion on integration with clinical care, but not implemented",""
1,Jung 2025,""" I've talked to ChatGPT about my issues last night."": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),18,10,2025,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,177 Reddit posts,"",No dataset used for development or evaluation,"","","","","","","","","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",n,y,n,y,"",Thematic analysis of Reddit posts and comments,"",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","",""
1,Jung 2025,""" I've talked to ChatGPT about my issues last night."": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),18,10,2025,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,177 Reddit posts,"",No dataset used for development or evaluation,"",Other: ,Other: ,Other: ,Other: ,Other: ,Other: ,"","","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,"",n,y,n,y,"",Thematic analysis of Reddit posts and comments,"",n,"","","",n,"","","",n,"","","",n,"","","","",n,"","","",n,"","","","",n,"","","",n,"","","","","","","","","","","","","","","","","","","",Y,"discuss misinformation, over-validation, and ethical risks reported by users","","",Y,Ethical use of Reddit data with disclaimers in 3.3.1,"","","","","","",Y,"Somehow: 
However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT’s overly validating nature, and privacy concerns","","","","",Y,Somehow: user-reported perceptions from Reddit,Y,"Speculative discussion on integration with clinical care, but not implemented",""
