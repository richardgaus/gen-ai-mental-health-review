================================================================================
DATA ANALYSIS REPORT
Input File: processed/final_data.csv
Generated: 2025-10-03 16:14:07
================================================================================

DATASET OVERVIEW
----------------------------------------
Total rows: 134
Total columns: 109

COLUMN-BY-COLUMN ANALYSIS
----------------------------------------

1. covidence_id
   ===============
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 134
   Category: CATEGORICAL

   Value Counts:
     1st_search_214: 1 (0.7%)
     2nd_search_94: 1 (0.7%)
     2nd_search_68: 1 (0.7%)
     2nd_search_69: 1 (0.7%)
     2nd_search_71: 1 (0.7%)
     2nd_search_72: 1 (0.7%)
     2nd_search_80: 1 (0.7%)
     2nd_search_81: 1 (0.7%)
     2nd_search_82: 1 (0.7%)
     2nd_search_85: 1 (0.7%)
     2nd_search_87: 1 (0.7%)
     2nd_search_88: 1 (0.7%)
     2nd_search_89: 1 (0.7%)
     2nd_search_90: 1 (0.7%)
     2nd_search_92: 1 (0.7%)
     2nd_search_93: 1 (0.7%)
     2nd_search_96: 1 (0.7%)
     2nd_search_62: 1 (0.7%)
     2nd_search_97: 1 (0.7%)
     2nd_search_98: 1 (0.7%)
     2nd_search_99: 1 (0.7%)
     2nd_search_100: 1 (0.7%)
     2nd_search_101: 1 (0.7%)
     2nd_search_102: 1 (0.7%)
     2nd_search_103: 1 (0.7%)
     2nd_search_104: 1 (0.7%)
     2nd_search_106: 1 (0.7%)
     2nd_search_107: 1 (0.7%)
     2nd_search_108: 1 (0.7%)
     2nd_search_109: 1 (0.7%)
     2nd_search_110: 1 (0.7%)
     2nd_search_111: 1 (0.7%)
     2nd_search_64: 1 (0.7%)
     2nd_search_60: 1 (0.7%)
     1st_search_212: 1 (0.7%)
     2nd_search_26: 1 (0.7%)
     2nd_search_2: 1 (0.7%)
     2nd_search_3: 1 (0.7%)
     2nd_search_4: 1 (0.7%)
     2nd_search_5: 1 (0.7%)
     2nd_search_6: 1 (0.7%)
     2nd_search_8: 1 (0.7%)
     2nd_search_9: 1 (0.7%)
     2nd_search_10: 1 (0.7%)
     2nd_search_11: 1 (0.7%)
     2nd_search_12: 1 (0.7%)
     2nd_search_17: 1 (0.7%)
     2nd_search_18: 1 (0.7%)
     2nd_search_20: 1 (0.7%)
     2nd_search_25: 1 (0.7%)
     2nd_search_28: 1 (0.7%)
     2nd_search_58: 1 (0.7%)
     2nd_search_29: 1 (0.7%)
     2nd_search_30: 1 (0.7%)
     2nd_search_31: 1 (0.7%)
     2nd_search_32: 1 (0.7%)
     2nd_search_34: 1 (0.7%)
     2nd_search_40: 1 (0.7%)
     2nd_search_42: 1 (0.7%)
     2nd_search_43: 1 (0.7%)
     2nd_search_45: 1 (0.7%)
     2nd_search_46: 1 (0.7%)
     2nd_search_47: 1 (0.7%)
     2nd_search_51: 1 (0.7%)
     2nd_search_54: 1 (0.7%)
     2nd_search_57: 1 (0.7%)
     2nd_search_112: 1 (0.7%)
     2nd_search_114: 1 (0.7%)
     2nd_search_116: 1 (0.7%)
     1st_search_89: 1 (0.7%)
     1st_search_30: 1 (0.7%)
     1st_search_32: 1 (0.7%)
     1st_search_34: 1 (0.7%)
     1st_search_56: 1 (0.7%)
     1st_search_59: 1 (0.7%)
     1st_search_60: 1 (0.7%)
     1st_search_62: 1 (0.7%)
     1st_search_63: 1 (0.7%)
     1st_search_70: 1 (0.7%)
     1st_search_72: 1 (0.7%)
     1st_search_73: 1 (0.7%)
     1st_search_75: 1 (0.7%)
     1st_search_77: 1 (0.7%)
     1st_search_81: 1 (0.7%)
     1st_search_95: 1 (0.7%)
     2nd_search_117: 1 (0.7%)
     1st_search_98: 1 (0.7%)
     1st_search_101: 1 (0.7%)
     1st_search_116: 1 (0.7%)
     1st_search_119: 1 (0.7%)
     1st_search_120: 1 (0.7%)
     1st_search_124: 1 (0.7%)
     1st_search_128: 1 (0.7%)
     1st_search_134: 1 (0.7%)
     1st_search_147: 1 (0.7%)
     1st_search_194: 1 (0.7%)
     1st_search_198: 1 (0.7%)
     1st_search_200: 1 (0.7%)
     1st_search_203: 1 (0.7%)
     1st_search_204: 1 (0.7%)
     1st_search_26: 1 (0.7%)
     1st_search_17: 1 (0.7%)
     1st_search_14: 1 (0.7%)
     1st_search_13: 1 (0.7%)
     2nd_search_119: 1 (0.7%)
     2nd_search_123: 1 (0.7%)
     2nd_search_124: 1 (0.7%)
     2nd_search_125: 1 (0.7%)
     2nd_search_126: 1 (0.7%)
     2nd_search_127: 1 (0.7%)
     2nd_search_128: 1 (0.7%)
     2nd_search_129: 1 (0.7%)
     2nd_search_130: 1 (0.7%)
     2nd_search_131: 1 (0.7%)
     2nd_search_132: 1 (0.7%)
     2nd_search_133: 1 (0.7%)
     2nd_search_135: 1 (0.7%)
     2nd_search_136: 1 (0.7%)
     2nd_search_137: 1 (0.7%)
     2nd_search_138: 1 (0.7%)
     2nd_search_140: 1 (0.7%)
     2nd_search_142: 1 (0.7%)
     2nd_search_143: 1 (0.7%)
     2nd_search_144: 1 (0.7%)
     2nd_search_147: 1 (0.7%)
     2nd_search_148: 1 (0.7%)
     2nd_search_155: 1 (0.7%)
     2nd_search_156: 1 (0.7%)
     2nd_search_157: 1 (0.7%)
     2nd_search_173: 1 (0.7%)
     2nd_search_214: 1 (0.7%)
     1st_search_3: 1 (0.7%)
     1st_search_9: 1 (0.7%)
     2nd_search_1: 1 (0.7%)

   Sample values (first 20):
     1st_search_214
     1st_search_212
     1st_search_204
     1st_search_203
     1st_search_200
     1st_search_198
     1st_search_194
     1st_search_147
     1st_search_134
     1st_search_128
     1st_search_124
     1st_search_120
     1st_search_119
     1st_search_116
     1st_search_101
     1st_search_98
     1st_search_95
     1st_search_89
     1st_search_81
     1st_search_77


2. study_id
   ===========
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 134
   Category: CATEGORICAL

   Value Counts:
     1st_search_6: 1 (0.7%)
     2nd_search_169: 1 (0.7%)
     2nd_search_219: 1 (0.7%)
     2nd_search_216: 1 (0.7%)
     2nd_search_213: 1 (0.7%)
     2nd_search_210: 1 (0.7%)
     2nd_search_204: 1 (0.7%)
     2nd_search_201: 1 (0.7%)
     2nd_search_198: 1 (0.7%)
     2nd_search_192: 1 (0.7%)
     2nd_search_189: 1 (0.7%)
     2nd_search_186: 1 (0.7%)
     2nd_search_183: 1 (0.7%)
     2nd_search_180: 1 (0.7%)
     2nd_search_175: 1 (0.7%)
     2nd_search_172: 1 (0.7%)
     2nd_search_166: 1 (0.7%)
     2nd_search_231: 1 (0.7%)
     2nd_search_163: 1 (0.7%)
     2nd_search_160: 1 (0.7%)
     2nd_search_157: 1 (0.7%)
     2nd_search_154: 1 (0.7%)
     2nd_search_151: 1 (0.7%)
     2nd_search_148: 1 (0.7%)
     2nd_search_145: 1 (0.7%)
     2nd_search_142: 1 (0.7%)
     2nd_search_139: 1 (0.7%)
     2nd_search_136: 1 (0.7%)
     2nd_search_133: 1 (0.7%)
     2nd_search_130: 1 (0.7%)
     2nd_search_127: 1 (0.7%)
     2nd_search_124: 1 (0.7%)
     2nd_search_225: 1 (0.7%)
     2nd_search_234: 1 (0.7%)
     1st_search_9: 1 (0.7%)
     2nd_search_288: 1 (0.7%)
     2nd_search_339: 1 (0.7%)
     2nd_search_336: 1 (0.7%)
     2nd_search_333: 1 (0.7%)
     2nd_search_330: 1 (0.7%)
     2nd_search_327: 1 (0.7%)
     2nd_search_324: 1 (0.7%)
     2nd_search_321: 1 (0.7%)
     2nd_search_318: 1 (0.7%)
     2nd_search_315: 1 (0.7%)
     2nd_search_312: 1 (0.7%)
     2nd_search_306: 1 (0.7%)
     2nd_search_303: 1 (0.7%)
     2nd_search_300: 1 (0.7%)
     2nd_search_291: 1 (0.7%)
     2nd_search_285: 1 (0.7%)
     2nd_search_237: 1 (0.7%)
     2nd_search_282: 1 (0.7%)
     2nd_search_279: 1 (0.7%)
     2nd_search_276: 1 (0.7%)
     2nd_search_273: 1 (0.7%)
     2nd_search_270: 1 (0.7%)
     2nd_search_264: 1 (0.7%)
     2nd_search_261: 1 (0.7%)
     2nd_search_258: 1 (0.7%)
     2nd_search_255: 1 (0.7%)
     2nd_search_252: 1 (0.7%)
     2nd_search_249: 1 (0.7%)
     2nd_search_246: 1 (0.7%)
     2nd_search_243: 1 (0.7%)
     2nd_search_240: 1 (0.7%)
     2nd_search_121: 1 (0.7%)
     2nd_search_118: 1 (0.7%)
     2nd_search_115: 1 (0.7%)
     1st_search_69: 1 (0.7%)
     1st_search_126: 1 (0.7%)
     1st_search_123: 1 (0.7%)
     1st_search_120: 1 (0.7%)
     1st_search_111: 1 (0.7%)
     1st_search_108: 1 (0.7%)
     1st_search_105: 1 (0.7%)
     1st_search_99: 1 (0.7%)
     1st_search_96: 1 (0.7%)
     1st_search_93: 1 (0.7%)
     1st_search_90: 1 (0.7%)
     1st_search_87: 1 (0.7%)
     1st_search_84: 1 (0.7%)
     1st_search_81: 1 (0.7%)
     1st_search_78: 1 (0.7%)
     1st_search_57: 1 (0.7%)
     2nd_search_112: 1 (0.7%)
     1st_search_54: 1 (0.7%)
     1st_search_51: 1 (0.7%)
     1st_search_45: 1 (0.7%)
     1st_search_42: 1 (0.7%)
     1st_search_39: 1 (0.7%)
     1st_search_36: 1 (0.7%)
     1st_search_33: 1 (0.7%)
     1st_search_30: 1 (0.7%)
     1st_search_27: 1 (0.7%)
     1st_search_24: 1 (0.7%)
     1st_search_21: 1 (0.7%)
     1st_search_18: 1 (0.7%)
     1st_search_15: 1 (0.7%)
     1st_search_12: 1 (0.7%)
     1st_search_129: 1 (0.7%)
     1st_search_135: 1 (0.7%)
     1st_search_138: 1 (0.7%)
     1st_search_141: 1 (0.7%)
     2nd_search_109: 1 (0.7%)
     2nd_search_106: 1 (0.7%)
     2nd_search_103: 1 (0.7%)
     2nd_search_100: 1 (0.7%)
     2nd_search_97: 1 (0.7%)
     2nd_search_94: 1 (0.7%)
     2nd_search_91: 1 (0.7%)
     2nd_search_88: 1 (0.7%)
     2nd_search_85: 1 (0.7%)
     2nd_search_82: 1 (0.7%)
     2nd_search_79: 1 (0.7%)
     2nd_search_76: 1 (0.7%)
     2nd_search_70: 1 (0.7%)
     2nd_search_67: 1 (0.7%)
     2nd_search_64: 1 (0.7%)
     2nd_search_61: 1 (0.7%)
     2nd_search_55: 1 (0.7%)
     2nd_search_49: 1 (0.7%)
     2nd_search_46: 1 (0.7%)
     2nd_search_43: 1 (0.7%)
     2nd_search_40: 1 (0.7%)
     2nd_search_37: 1 (0.7%)
     2nd_search_25: 1 (0.7%)
     2nd_search_22: 1 (0.7%)
     2nd_search_19: 1 (0.7%)
     2nd_search_13: 1 (0.7%)
     2nd_search_5: 1 (0.7%)
     1st_search_147: 1 (0.7%)
     1st_search_144: 1 (0.7%)
     2nd_search_342: 1 (0.7%)

   Sample values (first 20):
     1st_search_6
     1st_search_9
     1st_search_12
     1st_search_15
     1st_search_18
     1st_search_21
     1st_search_24
     1st_search_27
     1st_search_30
     1st_search_33
     1st_search_36
     1st_search_39
     1st_search_42
     1st_search_45
     1st_search_51
     1st_search_54
     1st_search_57
     1st_search_69
     1st_search_78
     1st_search_81


3. title
   ========
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 134
   Category: CATEGORICAL

   Value Counts:
     Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care: 1 (0.7%)
     LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour: 1 (0.7%)
     Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.: 1 (0.7%)
     Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy: 1 (0.7%)
     Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study: 1 (0.7%)
     AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks: 1 (0.7%)
     Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*: 1 (0.7%)
     Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques: 1 (0.7%)
     Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections: 1 (0.7%)
     Can AI relate: Testing large language model response for mental health support: 1 (0.7%)
     Evaluating Language Models for Assessing Counselor Reflections: 1 (0.7%)
     The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy: 1 (0.7%)
     Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering: 1 (0.7%)
     Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery: 1 (0.7%)
     Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study: 1 (0.7%)
     Early Detection and Personalized Intervention in Mental Health: 1 (0.7%)
     Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent: 1 (0.7%)
     Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories: 1 (0.7%)
     A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy: 1 (0.7%)
     Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates: 1 (0.7%)
     The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries: 1 (0.7%)
     "Shaping ChatGPT into my Digital Therapist": A thematic analysis of social media discourse on using generative artificial intelligence for mental health: 1 (0.7%)
     A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study: 1 (0.7%)
     Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder: 1 (0.7%)
     Addressing the Challenges of Mental Health Conversations with Large Language Models: 1 (0.7%)
     Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings: 1 (0.7%)
     Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot: 1 (0.7%)
     Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model: 1 (0.7%)
     Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT: 1 (0.7%)
     Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management: 1 (0.7%)
     Classifying Anxiety and Depression through LLMs Virtual Interactions: A Case Study with ChatGPT: 1 (0.7%)
     Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis: 1 (0.7%)
     Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study: 1 (0.7%)
     Conversational Agents for Dementia using Large Language Models: 1 (0.7%)
     Feasibility of combining spatial computing and AI for mental health support in anxiety and depression: 1 (0.7%)
     SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention: 1 (0.7%)
     Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach: 1 (0.7%)
     VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model: 1 (0.7%)
     Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots: 1 (0.7%)
     Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A: 1 (0.7%)
     Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.: 1 (0.7%)
     Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study: 1 (0.7%)
     Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users: 1 (0.7%)
     Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study: 1 (0.7%)
     LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant: 1 (0.7%)
     A Bi-Lingual Counselling Chatbot Application for Support of Gender Based Violence Victims in Kenya: 1 (0.7%)
     A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot: 1 (0.7%)
     Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots: 1 (0.7%)
     Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent: 1 (0.7%)
     Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study: 1 (0.7%)
     Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks: 1 (0.7%)
     Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support: 1 (0.7%)
     When ELIZA meets therapists: A Turing test for the heart and mind: 1 (0.7%)
     Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis: 1 (0.7%)
     Artificial Intelligence in Mental Health Care: The T5 Chatbot Project: 1 (0.7%)
     Randomized trial of a generative AI chatbot for mental health treatment: 1 (0.7%)
     Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios: 1 (0.7%)
     Multimodal Integration, Fine Tuning of Large Language Model for Autism Support: 1 (0.7%)
     The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis: 1 (0.7%)
     Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model: 1 (0.7%)
     Expert Patient Interaction Language Model (EPILM): 1 (0.7%)
     Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation: 1 (0.7%)
     Digital Risk Considerations Across Generative AI-based Mental Health Apps: 1 (0.7%)
     Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support: 1 (0.7%)
     "I Don't Understand It, but Okay": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models: 1 (0.7%)
     AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction: 1 (0.7%)
     Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT: 1 (0.7%)
     Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms: 1 (0.7%)
     Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.: 1 (0.7%)
     Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues: 1 (0.7%)
     ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane: 1 (0.7%)
     Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach: 1 (0.7%)
     Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change: 1 (0.7%)
     Leveraging ChatGPT to optimize depression intervention through explainable deep learning: 1 (0.7%)
     Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety: 1 (0.7%)
     Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring: 1 (0.7%)
     Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support: 1 (0.7%)
     A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study: 1 (0.7%)
     Towards Motivational and Empathetic Response Generation in Online Mental Health Support: 1 (0.7%)
     Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment: 1 (0.7%)
     Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context: 1 (0.7%)
     Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study: 1 (0.7%)
     Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study: 1 (0.7%)
     Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example): 1 (0.7%)
     Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study: 1 (0.7%)
     Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.: 1 (0.7%)
     Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation: 1 (0.7%)
     Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement: 1 (0.7%)
     A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II: 1 (0.7%)
     Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023: 1 (0.7%)
     Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I: 1 (0.7%)
     Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings: 1 (0.7%)
     Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support: 1 (0.7%)
     CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering: 1 (0.7%)
     Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models: 1 (0.7%)
     Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference: 1 (0.7%)
     Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO): 1 (0.7%)
     Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs): 1 (0.7%)
     Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments: 1 (0.7%)
     An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system: 1 (0.7%)
     Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.: 1 (0.7%)
     Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues: 1 (0.7%)
     ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.: 1 (0.7%)
     Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses: 1 (0.7%)
     The impact of prompt engineering in large language model performance: a psychiatric example: 1 (0.7%)
     SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis: 1 (0.7%)
     Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction: 1 (0.7%)
     PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling: 1 (0.7%)
     Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial: 1 (0.7%)
     Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories: 1 (0.7%)
     ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence: 1 (0.7%)
     Multi-Tiered RAG-Based Chatbot for Mental Health Support: 1 (0.7%)
     Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study: 1 (0.7%)
     Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support: 1 (0.7%)
     AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study: 1 (0.7%)
     Artificial intelligence conversational agents in mental health: Patients see potential, but prefer humans in the loop: 1 (0.7%)
     MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills: 1 (0.7%)
     Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study: 1 (0.7%)
     AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation: 1 (0.7%)
     Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults: 1 (0.7%)
     MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration: 1 (0.7%)
     Cases of Using ChatGPT as a Mental Health and Psychological Support Tool: 1 (0.7%)
     Mello: A Large Language Model for Mental Health Counselling Conversations: 1 (0.7%)
     Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot: 1 (0.7%)
     AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection: 1 (0.7%)
     Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation: 1 (0.7%)
     Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students: 1 (0.7%)
     The Role of AI Counselling in Journaling for Mental Health Improvement: 1 (0.7%)
     Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot): 1 (0.7%)
     Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model: 1 (0.7%)
     Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting: 1 (0.7%)
     Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study: 1 (0.7%)
     Safety of Large Language Models in Addressing Depression.: 1 (0.7%)
     " I've talked to ChatGPT about my issues last night.": Examining Mental Health Conversations with Large Language Models through Reddit Analysis: 1 (0.7%)

   Sample values (first 20):
     Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care
     Feasibility of combining spatial computing and AI for mental health support in anxiety and depression
     An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system
     Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments
     Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs)
     Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)
     Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference
     Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models
     CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering
     Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support
     Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings
     Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I
     Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023
     A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II
     Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement
     Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation
     Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study
     Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues
     Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example)
     Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study


4. reviewer_name
   ================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 1
   Category: CATEGORICAL

   Value Counts:
     Consensus: 134 (100.0%)


5. outlet_type
   ==============
   Data type: object
   Total values: 134
   Non-null values: 133
   Null values: 1
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     Conference paper: 68 (51.1%)
     Journal paper: 65 (48.9%)
     [NULL]: 1 (0.8%)


6. outlet_field
   ===============
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     Computer Science: 56 (41.8%)
     Digital Health: 32 (23.9%)
     HCI: 16 (11.9%)
     Medicine: 8 (6.0%)
     Other: 8 (6.0%)
     Psychiatry: 7 (5.2%)
     Psychology: 7 (5.2%)


7. day
   ======
   Data type: float64
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 31
   Category: CATEGORICAL

   Value Counts:
     17.0: 7 (5.2%)
     5.0: 7 (5.2%)
     18.0: 7 (5.2%)
     26.0: 6 (4.5%)
     2.0: 6 (4.5%)
     28.0: 6 (4.5%)
     30.0: 6 (4.5%)
     12.0: 6 (4.5%)
     11.0: 6 (4.5%)
     23.0: 6 (4.5%)
     9.0: 5 (3.7%)
     15.0: 5 (3.7%)
     3.0: 5 (3.7%)
     8.0: 5 (3.7%)
     21.0: 5 (3.7%)
     10.0: 5 (3.7%)
     27.0: 4 (3.0%)
     13.0: 4 (3.0%)
     24.0: 4 (3.0%)
     19.0: 3 (2.2%)
     6.0: 3 (2.2%)
     20.0: 3 (2.2%)
     14.0: 3 (2.2%)
     4.0: 3 (2.2%)
     25.0: 3 (2.2%)
     7.0: 3 (2.2%)
     29.0: 2 (1.5%)
     31.0: 2 (1.5%)
     16.0: 2 (1.5%)
     1.0: 1 (0.7%)
     22.0: 1 (0.7%)

   Sample values (first 20):
     9.0
     26.0
     4.0
     22.0
     14.0
     2.0
     17.0
     20.0
     28.0
     5.0
     30.0
     12.0
     19.0
     23.0
     18.0
     7.0
     1.0
     29.0
     11.0
     6.0


8. month
   ========
   Data type: float64
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 12
   Category: CATEGORICAL

   Value Counts:
     12.0: 22 (16.4%)
     1.0: 15 (11.2%)
     5.0: 15 (11.2%)
     3.0: 14 (10.4%)
     11.0: 12 (9.0%)
     10.0: 12 (9.0%)
     7.0: 11 (8.2%)
     4.0: 10 (7.5%)
     6.0: 8 (6.0%)
     2.0: 7 (5.2%)
     9.0: 6 (4.5%)
     8.0: 2 (1.5%)


9. year
   =======
   Data type: float64
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     2024.0: 63 (47.0%)
     2025.0: 48 (35.8%)
     2023.0: 18 (13.4%)
     2022.0: 3 (2.2%)
     2020.0: 1 (0.7%)
     2021.0: 1 (0.7%)


10. author_country
   ==================
   Data type: object
   Total values: 134
   Non-null values: 97
   Null values: 37
   Unique values: 29
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 37 (38.1%)
     USA: 29 (29.9%)
     China: 14 (14.4%)
     India: 13 (13.4%)
     Germany: 6 (6.2%)
     Other: Canada: 4 (4.1%)
     Other: Korea: 4 (4.1%)
     Other: Iran: 2 (2.1%)
     Other: Israel: 2 (2.1%)
     UK: 2 (2.1%)
     Other: Italy: 2 (2.1%)
     Other: Iraq: 1 (1.0%)
     Other: Mexico: 1 (1.0%)
     Other: United Arab Emirates: 1 (1.0%)
     Other: Switzerland: 1 (1.0%)
     Other: New Zealand: 1 (1.0%)
     Other: Egypt: 1 (1.0%)
     Other: Kenya: 1 (1.0%)
     Other: Japan: 1 (1.0%)
     Other: Indonesia: 1 (1.0%)
     Other: Spain: 1 (1.0%)
     Other: Turkey: 1 (1.0%)
     Other: Slovenia: 1 (1.0%)
     Other: Sri Lanka: 1 (1.0%)
     Other: Finland: 1 (1.0%)
     Other: Saudi Arabia: 1 (1.0%)
     Other: Romania: 1 (1.0%)
     Other: Pakistan: 1 (1.0%)
     Other: Philippines: 1 (1.0%)
     Other: Vietnam: 1 (1.0%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 33
     Other: Philippines: 1
     Other: Pakistan: 1
     Other: Israel: 2
     Other: Romania: 1
     Other: Saudi Arabia: 1
     Other: Canada: 4
     Other: Italy: 2
     Other: Korea: 4
     Other: Finland: 1
     Other: Sri Lanka: 1
     Other: Indonesia: 1
     Other: Slovenia: 1
     Other: Iran: 2
     Other: Turkey: 1
     Other: Spain: 1
     Other: Switzerland: 1
     Other: Japan: 1
     Other: Mexico: 1
     Other: Iraq: 1
     Other: United Arab Emirates: 1
     Other: New Zealand: 1
     Other: Egypt: 1
     Other: Kenya: 1
     Other: Vietnam: 1

   Sample values (first 20):
     India
     USA
     China
     Other: Philippines
     Other: Pakistan
     Germany
     Other: Israel
     Other: Romania
     Other: Saudi Arabia
     Other: Canada
     Other: Italy
     Other: Korea
     Other: Finland
     Other: Sri Lanka
     Other: Indonesia
     Other: Slovenia
     Other: Iran
     Other: Turkey
     Other: Spain
     Other: Switzerland


11. study_type
   ==============
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     Empirical research involving an LLM: 122 (91.0%)
     Population survey: 12 (9.0%)


12. application_type
   ====================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     Client-facing application: 117 (87.3%)
     Text-based prediction: 9 (6.7%)
     Therapist guidance application: 8 (6.0%)


13. application_subtype_client_facing
   =====================================
   Data type: object
   Total values: 134
   Non-null values: 115
   Null values: 19
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     Multi-turn chatbot: 90 (78.3%)
     [NULL]: 19 (16.5%)
     One-turn chatbot (usually Q&A): 17 (14.8%)
     Multi-modal dialogue system: 3 (2.6%)
     Spoken dialogue system: 2 (1.7%)
     Reframed thought generation: 2 (1.7%)
     Reflection generation: 1 (0.9%)


14. client_type
   ===============
   Category: MULTIPLE CHOICE (semicolon-separated)
   Total responses: 125
   Null responses: 9
   Total individual choices: 129
   Unique choice options: 9
   Average choices per response: 1.0

   Individual Choice Counts:
     No clients/patients involved: 76 (58.9%)
     General population: 29 (22.5%)
     People with some symptoms but not disorder (determined by symptom scales or questionnaires): 10 (7.8%)
     Patients recruited in hospital or outpatient treatment facility: 8 (6.2%)
     Other: unknown: 2 (1.6%)
     Patients with disorder explicitly based on ICD or DSM: 1 (0.8%)
     Other: "Child counseling experts": 1 (0.8%)
     Other: Therapists: 1 (0.8%)
     Other: Mental health practitioners: 1 (0.8%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 5
     Other: "Child counseling experts": 1
     Other: Therapists: 1
     Other: unknown: 2
     Other: Mental health practitioners: 1


15. client_count
   ================
   Data type: object
   Total values: 134
   Non-null values: 47
   Null values: 87
   Unique values: 40
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 87 (185.1%)
     1: 3 (6.4%)
     21: 2 (4.3%)
     48: 2 (4.3%)
     42: 2 (4.3%)
     11: 2 (4.3%)
     50: 2 (4.3%)
     12: 1 (2.1%)
     16: 1 (2.1%)
     9: 1 (2.1%)
     69: 1 (2.1%)
     unknown: 1 (2.1%)
     5: 1 (2.1%)
     210: 1 (2.1%)
     1000000 user reviews: 1 (2.1%)
     87 Reddit posts: 1 (2.1%)
     830: 1 (2.1%)
     8: 1 (2.1%)
     244: 1 (2.1%)
     306: 1 (2.1%)
     20: 1 (2.1%)
     49: 1 (2.1%)
     58: 1 (2.1%)
     160: 1 (2.1%)
     70: 1 (2.1%)
     399: 1 (2.1%)
     68: 1 (2.1%)
     462: 1 (2.1%)
     236: 1 (2.1%)
     349: 1 (2.1%)
     31: 1 (2.1%)
     15531: 1 (2.1%)
     111: 1 (2.1%)
     139: 1 (2.1%)
     24: 1 (2.1%)
     428: 1 (2.1%)
     7: 1 (2.1%)
     45: 1 (2.1%)
     29: 1 (2.1%)
     14: 1 (2.1%)
     177 Reddit posts: 1 (2.1%)

   Sample values (first 20):
     12
     14
     68
     462
     236
     42
     349
     31
     15531
     11
     399
     111
     24
     50
     428
     7
     45
     29
     1
     160


16. application_subtype_therapist_facing
   ========================================
   Data type: object
   Total values: 134
   Non-null values: 9
   Null values: 125
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 125 (1388.9%)
     Treatment fidelity feedback: 3 (33.3%)
     Utterance suggestions: 2 (22.2%)
     Therapy reflection tool: 1 (11.1%)
     Therapy material generation: 1 (11.1%)
     Patient simulations: 1 (11.1%)
     Various different: 1 (11.1%)


17. dataset_source
   ==================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 12
   Category: CATEGORICAL

   Value Counts:
     No dataset used for development or evaluation: 45 (33.6%)
     External data set: 42 (31.3%)
     Self-collected data: 25 (18.7%)
     External data set, modified: 10 (7.5%)
     Self-collected data but derived from external data set: 4 (3.0%)
     Other: unclear: 2 (1.5%)
     Other: Mix of self-collected and external: 1 (0.7%)
     Other: External data set + self-collected: 1 (0.7%)
     Other: External + self-collected: 1 (0.7%)
     Other: External data set, but completely unknown: 1 (0.7%)
     Other: External data set + a modified (translated) data set: 1 (0.7%)
     Other: some dataset was used, but completely unknown characteristics: 1 (0.7%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 8
     Other: Mix of self-collected and external: 1
     Other: unclear: 2
     Other: External data set + self-collected: 1
     Other: External + self-collected: 1
     Other: External data set, but completely unknown: 1
     Other: External data set + a modified (translated) data set: 1
     Other: some dataset was used, but completely unknown characteristics: 1


18. dataset_notes
   =================
   Data type: object
   Total values: 134
   Non-null values: 86
   Null values: 48
   Unique values: 84
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 48 (55.8%)
     CounselChat https://github.com/nbertagnolli/counsel-chat: 2 (2.3%)
     Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.: 2 (2.3%)
     clinical outcome data from evaluation: 1 (1.2%)
     Additionally, local data from Indonesian mental health articles and discussion forums fine-tuned the model.: 1 (1.2%)
     No specific name: "We use the Reddit API to collect
a new dataset of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits."

Available under https://github.com/skgabriel/mh-eval: 1 (1.2%)
     Original dataset: Perez-Rosas motivational interviewing dataset.
Modification: Added custom annnotations
Also extended with their own MI prompts.
We thus obtained 4,365 client prompt-counselor reflection pairs, including 2,429 prompt-CR and 1,636 prompt-SR pairs.

public under https://lit.eecs.umich.edu/downloads.html#PAIR: 1 (1.2%)
     ArmanEmo & Jigsaw Toxic Comment Classification translated into persian: 1 (1.2%)
     epsilon3/cbt-cognitive-distortions-analysis dataset
https://huggingface.co/datasets/epsilon3/cbt-cognitive-distortions-analysis
: 1 (1.2%)
     First, we introduce a novel, golden standard dataset, comprising panel 
data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires 
and qualitative daily diary entries. : 1 (1.2%)
     collected a substantial dataset comprising 21,924 records
Fig. 2. Volunteer Screening Chart
through five distinct avenues: instances, media, literature,
guidelines, and databases. To ensure the integrity and rele-
vance of our data, we undertook a meticulous cleaning pro-
cess spearheaded by three experienced quality controllers.
Each controller boasts broad expertise in mental health
counseling. Initially, we eliminated duplicates and linguistic
inaccuracies. Subsequently, we focused on screening data
for key terms such as “sleep”, “dream”, “evening”, “night”,
“bed” and related expressions. This step aimed to sift out
conversational data irrelevant to our study’s objectives, en-
suring that the textual content was pertinent to the context
of potential sleep disorders. We further conducted dialog
integrity screening. Finally, through manual inspection on a
case-by-case basis, we excluded dialogue data that contra-
dicted universal core values. This rigorous curation process
resulted in the selection of 764 dialogues: 1 (1.2%)
     The chatbot’s corpus comprises 200 manually curated context-response pairs that cover themes like stress, anxiety, depression, and motivation: 1 (1.2%)
     We use the HOPE (Mental Health cOunselling of PatiEnts) [8]
dataset for our experiments. The dataset is specifically designed
for mental health counseling and dialogue-act classification tasks.
The HOPE dataset includes approximately 12,800 utterances ex-
tracted from 212 mental health counseling sessions, sourced from
publicly available YouTube videos. : 1 (1.2%)
     a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists

The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG: 1 (1.2%)
     Primate2022 https://github.com/primate-mh/Primate2022 (but only for evaluation): 1 (1.2%)
     cognitive distortion identification dataset: 1 (1.2%)
     Alexander Street Press therapy transcripts: 1 (1.2%)
     PsyAdv Corp consisting out of Efaqa, CpsyCounD, SMILECHAT, PsyQA: 1 (1.2%)
     Using a dataset of Woebot questions and user responses (pulled from the Gen-W-MA internal testing data) labeled as being on or off topic, we fine-tuned an instance of the text-embedding-ada-002 model using the Azure OpenAI service: 1 (1.2%)
     Aditya Mental Health Counseling https://huggingface.co/datasets/Aditya149/Mental_Health_Counselling_Dataset: 1 (1.2%)
     In this study, 200 real counseling ses
sions were recorded and transcribed to capture authentic 
client interactions. From these, 50 client statements were 
identified that best represented a range of emotions such as 
sadness, guilt, anxiety, determination, and anger, based on 
the PANAS Scale, along with helplessness, love, trust, lone
liness, and doubt from existing literature, aiming to ensure

emotional expressions from psychological counseling sessions: 1 (1.2%)
     Data Preparation: The sentiment analysis model is
trained on an annotated text classification dataset after
undergoing extensive cleaning, normalization, and tok-
enization processes to ensure the quality and integrity of
the data. The datasets used are compiled from a multitude
of sources, including a text classification repository of so-
cial media tweets, restaurant reviews, and airline reviews.
Data was also extracted from the Reddit API across vari-
ous forums such as those focused on CBT, mental health,
therapy, anxiety, depression, and CPTSD. Initially, the
dataset comprised 3,000 data points, with a text column
serving as the input and a target column providing labeled
sentiment. Recognizing the importance of data volume
in enhancing the accuracy and robustness of machine
learning models, we expand the dataset (TABLE I) to encompass 12,221 data points

for RAG: 1 (1.2%)
     PsyQA, extended by "helping skills" column: 1 (1.2%)
     Emotional Support Conversation dataset. Comprises 1,053 multi-turn dialogues, amounting to 31,410 utterances. "We have augmented the dataset with additional annotations to signify the current state of the dialogue.": 1 (1.2%)
     zhang 2024 dataset: 1 (1.2%)
     The study uses data from YiXinli Community, a major Chinese online counseling platform (https://www.xinli001.com) frequented by nearly 40 million users worldwide for psychological support. On this platform, 
users can confidentially express their psychological concerns and receive advice from human counselors in the open Q&A section (https://www.xinli001.com/qa). We utilized a web data crawler called houyicaiji (https://www.houyicaiji.com/) to gather this Q&A data, which encompasses 10,903 distinct questions and 19,682 human counselor responses collected from November 3, 2022, to 
March 30, 2023.: 1 (1.2%)
     see Generating Data for Evaluation: 1 (1.2%)
     Responses by expert suicidologists on a previously-published standardized scale: the Suicide Intervention Response Inventory (SIRI-2)
[15].: 1 (1.2%)
     mental health and healthcare-related texts; sources not fully specified, combined domain corpora and simulated patient records: 1 (1.2%)
     CounselChat https://github.com/nbertagnolli/counsel-chat
Partly translated to Kiswahili: 1 (1.2%)
     Huggingface: Mental Health Counseling Conversations
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations: 1 (1.2%)
     Web-scraped data from diverse sources

Bilingual (English & Arabic) mental health text corpus, origin not found: 1 (1.2%)
     The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dialogues. The dialogues were developed by our research team, including a board-certified psychiatrist and a clinical psychologist, and peer-reviewed using evidence-based (primarily CBT) modalities.: 1 (1.2%)
     Study describes FAITA Mental Health Framework and evaluates OCD Coach as an existing LLM-based Tool: 1 (1.2%)
     selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.

EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed): 1 (1.2%)
     The research begins by curating a specialized User and
Counsellor Interaction CSV dataset by querying an autism
counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source
for reducing bias and adding a human touch. Alongside the
primary dataset, the additional data points ensure a compre-
hensive understanding of the domain.: 1 (1.2%)
     The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately!

But user has other dataset that fits description: https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset: 1 (1.2%)
     CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat

[COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. : 1 (1.2%)
     The primary dataset used in this study consists of psychotherapy transcripts from
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. : 1 (1.2%)
     EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues

From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger.: 1 (1.2%)
     By using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
These characteristics encompassed a spectrum of ages (40 and
80 years old), genders (male, female, transgender male, and
transgender female), races (non-Hispanic black, non-Hispanic
white, and Hispanic), and annual income ($25,000, $50,000,
$500,000, and $1,000,000). We also generated a control group
without assigned identities. Each DSP represented a unique
mix of these demographic characteristics.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses: 1 (1.2%)
     1,000 QA pairs from the PsyQA dataset.  two human annotators inspected and labeled the responses of professional supporters in the PsyQA dataset with the corresponding professional psychological counseling theory categories (Cognitive-Behavioral Therapy labeled as 1, Dialectical Behavior Therapy as 2, Person-Centered Therapy as 3, and Reality Therapy as 4). The original data had quantities of 189, 317, 196, and 298 for categories 1, 2, 3, and 4, respectively.: 1 (1.2%)
     fine-tuned on the 2 volumes of published counseling and psychotherapy data from
Alexander Street Press. The volumes are searchable collections of transcripts containing real counseling and therapy sessions
and first-person narratives illuminating the experience of mental illness and treatment. The 2 volumes contain 3500 session transcripts and >700,000 utterances between a counselor and a
patient.: 1 (1.2%)
     PsyQA is an authoritative Chinese dialogue dataset focused
on the field of mental health support. It covers 9 broad
topics and multiple subtopics, encompassing various aspects
of mental health. PsyQA is presented in a question-answer
format, where each example includes a question, a detailed
description of the mental issues, and keywords provided by
anonymous help-seekers. Responses are asynchronously pro-
vided by well-trained volunteers or professional counselors
and involve detailed analysis and guidance in response to the
help-seekers’ questions, aiming to offer mental health support.
Additionally, the answers have been further annotated with
seven psychological strategies based on psychological coun-
seling theory [12], : 1 (1.2%)
     Books: 
E. J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015. + D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009. + B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014. 

To make sure it was appropriate for entering into the model, the data taken from the books went through a number of preparation steps
in this research: Extraction and cleaning.: 1 (1.2%)
      FLATT (Fun to Learn to Act and Think 
through Technology) trial + HCT (Healthy Campus Trial): 1 (1.2%)
     Only evaluation data: Survey of 236 participants evaluating ChatGPT-generated vs. psychologist-generated psychological recommendations.: 1 (1.2%)
     EXTERNAL
- Data scraped from mental health subreddits
- Alexander Street Press video transcripts
	
Notes on data setSubreddits in the following categories: (a) Coping and Therapy (C-Th): 7Cup
sofTea, Existential_crisis, getting_over_it, Grief-
Support, helpmecope, hardshipmates, HereToHelp,
itgetsbetter, LostALovedOne, offmychest, MMFB,
Miscarriage, reasonstolive, SuicideBereavement,
therapy; (b) Mood Disorders (MD): depression, de-
pressed, lonely, mentalhealth; (c) Psychosis and
Anxiety (P-An): anxiety, BipolarReddit, socialanxi-
ety; and (d) Trauma and Abuse (Tr-A): abuse, sur-
vivors, Anger, emotionalabuse, PTSDcombat.
Alexander Street Press video transcripts

SELF COLLECTED
- Empathic Conversation dataset: 1 (1.2%)
     external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS

To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube: 1 (1.2%)
     Dataset was used for prompt development and final evaluation

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539]): 1 (1.2%)
     Some PDFs for configuring a custom GPT (type of PDFs not further specified): 1 (1.2%)
     We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

we crawl 2,000 blog titles from Yixinli’s QA column

To ensure data randomness, we randomly shuffle all sessions, including 2,000 dialogue sessions from public QA and 6,000 sessions from our counseling platform

Yixinli’s QA column https://www.xinli001.com/qa

self-collected: self-developed text-based counseling platform. external: Yixinli’s QA column https://www.xinli001.com/qa: 1 (1.2%)
     HOPE: 1 (1.2%)
     MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube): 1 (1.2%)
     PsyQA: 1 (1.2%)
     User comments from the r/Replika subreddit: 1 (1.2%)
     PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
: 1 (1.2%)
     from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.: 1 (1.2%)
     The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.: 1 (1.2%)
     Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website: 1 (1.2%)
     PsyQA and Crawl of various chinese social media platforms Tianya, Zhihu, Yixinli

training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.

fine-tuning: PsyQA (56,000 question-answer pairs): 1 (1.2%)
     CounselChat2,
provided through the HuggingFace Hub platform3. : 1 (1.2%)
     We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: "This site is fiction and all the characters are fictional.": 1 (1.2%)
     "Open Up" text-based online counseling service: 1 (1.2%)
     Motivational Interviewing (MI) counseling dataset from Perez-Rosas et al. (2016); Alexander Street dataset : 1 (1.2%)
     Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”: 1 (1.2%)
     42 tinitus patients as raw data + augmented dataset as test samples

The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients: 1 (1.2%)
     Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms. It is intended for training and evaluating language models that provide safer, context-aware mental-health responses.: 1 (1.2%)
     The fine-tuning process involves the 
calibration of the pre-trained Transformer model with 
specific datasets relevant to cognitive therapy. This 
includes dialogues from CBT sessions, therapeutic 
interventions, and stress management conversations.

The GPT-2 model is fine-tuned using a specialized 
therapeutic dialogue dataset where the process involves 
supervised learning and the openly available trained 
model is adapted for a wide range of therapeutic 
contexts. The goal is to improve the ability of the model 
to provide relevant and empathetic responses during 
therapy sessions. The fine-tuning dataset includes a 
variety of therapeutic dialogues which ensures the 
model can handle different therapeutic scenarios 
effectively.: 1 (1.2%)
      For example, it uses conversational 
data gained in previous interactions with the user as well as 
datasets having labeled expressions of sentiment ("emo.txt" 
and "Sentiments1.txt"): 1 (1.2%)
     We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset

Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.

https://www.kaggle.com/datasets/sagarikashreevastava/cognitive-distortion-detetction-dataset: 1 (1.2%)
     24 patients were selected to participate in the study. Before participating in an interview, the patients were thoroughly 
informed of the study’s objectives and given the opportunity to provide informed consent during their outpatient visit. 
Following approval, patients used ChatGPT at home for two weeks to seek mental-health support in their own ways. The 
participants were required to interact with ChatGPT3 for a minimum of 15 minutes per day for 14 days. Twenty to thirty 
interviews were regarded an appropriate sample size for qualitative investigations, particularly those employing inter­
views as a method of data collection.
: 1 (1.2%)
     Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown. : 1 (1.2%)
     used chatgpt as is. dataset is outcome data and not used for dev purposes: 1 (1.2%)
     Kaggle (n.d.)  This dataset encompasses 80 distinct tags, each contain
ing numerous conversational prompts and corresponding 
responses: 1 (1.2%)
     oth ChatGPT-4 and Bard responded to 20 tasks at each
stage of the study --> no new dataset created : 1 (1.2%)
     no dataset. ChatGPT is used as is.: 1 (1.2%)
     TalkLife (talklife.co) is the largest online peer-to-peer support plat-
form for mental health support. It enables conversations between
people seeking support (support seekers) and people providing sup-
port (peer supporters) in a thread-like setting. We call the post au-
thored by a support seeker as seeker post, and the response by a
peer supporter as response post. Table 1 describes the statistics of
conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number
of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question "Is the seeker talking about a mental
health related issue or situation in his/her post?". Using this anno-
tated dataset, we train a standard text classifier based on BERT [15]
(achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-
related conversations. This dataset contains 3.33M interactions from
1.48M seeker posts.: 1 (1.2%)
     only outcome data: 1 (1.2%)
     data sourced from Xinli001.com. Counsellor responses were included in the dataset. ChatGPT responses were generated by the authors.: 1 (1.2%)
     CounselChat, Brain & Behaviour Research Foundation, NHS, Wellness in Mind, White Swan Foundation: 1 (1.2%)
     External: We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].
Self-collected: In collaboration with mental
health experts (some of whom are co-authors), we manually labeled
500 thoughts and situations to identify the potential issues that
they are associated with. The result of this iterative open-ended
coding process was a set of 16 diferent issues expressed by par
ticipants. We used this dataset to fnetune a GPT-3 mod: 1 (1.2%)
     survey data from 31 participants: 1 (1.2%)
     MotiVAte, EmpatheticDialogues: 1 (1.2%)
     https://huggingface.co/datasets/jkhedri/psychology-dataset; Only questions embedded, not answers.: 1 (1.2%)

   Sample values (first 20):
     clinical outcome data from evaluation
     Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”
     We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: "This site is fiction and all the characters are fictional."
     CounselChat2,
provided through the HuggingFace Hub platform3. 
     PsyQA and Crawl of various chinese social media platforms Tianya, Zhihu, Yixinli

training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.

fine-tuning: PsyQA (56,000 question-answer pairs)
     Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website
     The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.
     from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.
     PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 

     User comments from the r/Replika subreddit
     PsyQA
     MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube)
     HOPE
     We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

we crawl 2,000 blog titles from Yixinli’s QA column

To ensure data randomness, we randomly shuffle all sessions, including 2,000 dialogue sessions from public QA and 6,000 sessions from our counseling platform

Yixinli’s QA column https://www.xinli001.com/qa

self-collected: self-developed text-based counseling platform. external: Yixinli’s QA column https://www.xinli001.com/qa
     Some PDFs for configuring a custom GPT (type of PDFs not further specified)
     Dataset was used for prompt development and final evaluation

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539])
     external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS

To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube
     EXTERNAL
- Data scraped from mental health subreddits
- Alexander Street Press video transcripts
	
Notes on data setSubreddits in the following categories: (a) Coping and Therapy (C-Th): 7Cup
sofTea, Existential_crisis, getting_over_it, Grief-
Support, helpmecope, hardshipmates, HereToHelp,
itgetsbetter, LostALovedOne, offmychest, MMFB,
Miscarriage, reasonstolive, SuicideBereavement,
therapy; (b) Mood Disorders (MD): depression, de-
pressed, lonely, mentalhealth; (c) Psychosis and
Anxiety (P-An): anxiety, BipolarReddit, socialanxi-
ety; and (d) Trauma and Abuse (Tr-A): abuse, sur-
vivors, Anger, emotionalabuse, PTSDcombat.
Alexander Street Press video transcripts

SELF COLLECTED
- Empathic Conversation dataset
     Only evaluation data: Survey of 236 participants evaluating ChatGPT-generated vs. psychologist-generated psychological recommendations.
     "Open Up" text-based online counseling service


19. dataset_type
   ================
   Data type: object
   Total values: 134
   Non-null values: 49
   Null values: 85
   Unique values: 24
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 85 (173.5%)
     Internet data -- mental health Q&A: 16 (32.7%)
     Psychotherapy -- speech transcripts: 6 (12.2%)
     Emotional support dialogue -- chat logs: 5 (10.2%)
     Internet data -- mental health forum: 2 (4.1%)
     Other: MI reflections (client prompt - counselor reflection pairs): 1 (2.0%)
     Other: Standardized clinical instrument with hypothetical scenarios and predefined responses: 1 (2.0%)
     Other: different types (books, articles, etc.), not further specified: 1 (2.0%)
     Other: Internet data, various types and sources (mixed): 1 (2.0%)
     Psychotherapy -- chat logs: 1 (2.0%)
     Other: Roleplay and sharing/microblogging data

Psychotherapy -- speech transcripts: 1 (2.0%)
     Other: unclear: 1 (2.0%)
     Other: Other: statements, labeled with sentiments (sentiment analysis data): 1 (2.0%)
     Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists: 1 (2.0%)
     Other: Mixed: 1 (2.0%)
     Other: Emotion detection dataset and classification dataset: 1 (2.0%)
     Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) : 1 (2.0%)
     Other: "mental health articles and discussion forums": 1 (2.0%)
     Other: unclear (authors call it "context-response pairs": 1 (2.0%)
     Other: Labeled examples of cognitive distortions: 1 (2.0%)
     Emotional support dialogue -- speech transcripts: 1 (2.0%)
     Other: Book data: 1 (2.0%)
     Other: unknown: 1 (2.0%)
     Other: labeled cognitive distortions: 1 (2.0%)
     Other: Descriptions of mental disorders in the DSM-5: 1 (2.0%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 18
     Other: labeled cognitive distortions: 1
     Other: unknown: 1
     Other: Book data: 1
     Other: Labeled examples of cognitive distortions: 1
     Other: unclear (authors call it "context-response pairs": 1
     Other: "mental health articles and discussion forums": 1
     Other: Mixed: 1
     Other: Psychotherapy-related (diagnostic questionnaires + daily diaries): 1
     Other: Emotion detection dataset and classification dataset: 1
     Other: MI reflections (client prompt - counselor reflection pairs): 1
     Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists: 1
     Other: Other: statements, labeled with sentiments (sentiment analysis data): 1
     Other: unclear: 1
     Other: Roleplay and sharing/microblogging data: 1
     Other: Internet data, various types and sources (mixed): 1
     Other: different types (books, articles, etc.), not further specified: 1
     Other: Standardized clinical instrument with hypothetical scenarios and predefined responses: 1
     Other: Descriptions of mental disorders in the DSM-5: 1

   Sample values (first 20):
     Psychotherapy -- speech transcripts
     Other: labeled cognitive distortions
     Internet data -- mental health Q&A
     Other: unknown
     Other: Book data
     Emotional support dialogue -- speech transcripts
     Emotional support dialogue -- chat logs
     Other: Labeled examples of cognitive distortions
     Internet data -- mental health forum
     Other: unclear (authors call it "context-response pairs"
     Other: "mental health articles and discussion forums"
     Other: Mixed
     Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) 
     Other: Emotion detection dataset and classification dataset
     Other: MI reflections (client prompt - counselor reflection pairs)
     Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists
     Other: Other: statements, labeled with sentiments (sentiment analysis data)
     Other: unclear
     Other: Roleplay and sharing/microblogging data

Psychotherapy -- speech transcripts
     Psychotherapy -- chat logs


20. dataset_language
   ====================
   Data type: object
   Total values: 134
   Non-null values: 50
   Null values: 84
   Unique values: 8
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 84 (168.0%)
     English: 30 (60.0%)
     Other: unknown: 8 (16.0%)
     Chinese: 7 (14.0%)
     Other: Indonesian: 1 (2.0%)
     Other: Persian: 1 (2.0%)
     Other: Spanish: 1 (2.0%)
     Other: unknown, probably english or hindi: 1 (2.0%)
     Other: Arabic: 1 (2.0%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 13
     Other: unknown: 8
     Other: Indonesian: 1
     Other: Persian: 1
     Other: Spanish: 1
     Other: unknown, probably english or hindi: 1
     Other: Arabic: 1


21. dataset_contains_synthetic_data
   ===================================
   Data type: object
   Total values: 134
   Non-null values: 50
   Null values: 84
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 84 (168.0%)
     No: 37 (74.0%)
     Yes: 6 (12.0%)
     Other: unknown: 4 (8.0%)
     Other: unspecified: 1 (2.0%)
     Other: Unknown: 1 (2.0%)
     Other: Yes (hypothetical cases created by experts): 1 (2.0%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 7
     Other: unspecified: 1
     Other: unknown: 4
     Other: Unknown: 1
     Other: Yes (hypothetical cases created by experts): 1


22. dataset_is_public
   =====================
   Data type: object
   Total values: 134
   Non-null values: 50
   Null values: 84
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 84 (168.0%)
     Yes: 25 (50.0%)
     No: 24 (48.0%)
     Other: unspecified: 1 (2.0%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 1
     Other: unspecified: 1


23. dataset_user_psychopathology_status
   =======================================
   Data type: object
   Total values: 134
   Non-null values: 50
   Null values: 84
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 84 (168.0%)
     Unknown: 28 (56.0%)
     Unselected: 12 (24.0%)
     Psychopathology: 3 (6.0%)
     Other: n.a.: 3 (6.0%)
     Other: not applicable: 2 (4.0%)
     Other: unknown: 1 (2.0%)
     Other: Not applicable, data is not on users: 1 (2.0%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 7
     Other: unknown: 1
     Other: not applicable: 2
     Other: n.a.: 3
     Other: Not applicable, data is not on users: 1


24. dataset_responder_type
   ==========================
   Data type: object
   Total values: 134
   Non-null values: 50
   Null values: 84
   Unique values: 11
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 84 (168.0%)
     Unknown: 22 (44.0%)
     Trained professionals: 12 (24.0%)
     Other: not applicable: 5 (10.0%)
     Lay people: 3 (6.0%)
     Other: n.a.: 2 (4.0%)
     Other: Mix of lay and trained: 1 (2.0%)
     Other: Woebot chatbot: 1 (2.0%)
     Other: not applicable (only initial posts, no responses): 1 (2.0%)
     Other: No responders: 1 (2.0%)
     Other: Mix of trained and lay: 1 (2.0%)
     Other: Not applicable, no interaction data: 1 (2.0%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 13
     Other: not applicable: 5
     Other: Mix of lay and trained: 1
     Other: Woebot chatbot: 1
     Other: not applicable (only initial posts, no responses): 1
     Other: No responders: 1
     Other: n.a.: 2
     Other: Mix of trained and lay: 1
     Other: Not applicable, no interaction data: 1


25. llm_development_approach
   ============================
   Data type: object
   Total values: 134
   Non-null values: 107
   Null values: 27
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     Only prompting: 35 (32.7%)
     [NULL]: 27 (25.2%)
     Prompting + other modules: 26 (24.3%)
     Fine-tuning + other modules: 25 (23.4%)
     Only fine-tuning: 15 (14.0%)
     Other: 6 (5.6%)


26. llm_development_approach_notes
   ==================================
   Data type: object
   Total values: 134
   Non-null values: 80
   Null values: 54
   Unique values: 80
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 54 (67.5%)
     Evaluated existing GPT-based chatbots via persona-based simulated interactions, scored with CAPE framework. : 1 (1.2%)
     The research team, consisting of an AI product manager and
prompt engineer (OP), several expert psychologists (YH, KG, ZE,
and DHS), and a music therapist (TA), jointly developed the two
AI tools from December 2023 to January 2024. Initially, a
primary prompt was created by YH. This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests
and added operational instructions to ensure safety and ethics.
Subsequently, group members with clinical backgrounds (YH,
KG, ZE, TA) tested the tool on themselves and shared their
experiences and suggestions for improvements. Throughout this
process, the team realized that due to the tools’ depth and their
ability to reflect on unconscious internal parts, the presence of a
mental health care professional would be essential. An expert in
social psychology, DHS, addressed cultural issues, which were
also incorporated into the prompt. Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications.: 1 (1.2%)
     Developed by fine-tuning a GPT-2 model on a domain-specific dataset and integrating it into a larger architecture that included array-based context storage for multi-turn dialogue, Top-K/Top-P sampling for controlled text generation, and response ranking modules using BLEU scores and cosine similarity to select the most relevant and coherent reply.: 1 (1.2%)
     Q-LoRA fine-tuning: 1 (1.2%)
     Retrieval + hierarchical fusion + attention + lexicon-enriched embeddings used to prompt LLMs.: 1 (1.2%)
     The system integrates two main components: a BERT-based emotional distress detection module and a fine-tuned GPT-3.5 model for Psychological First Aid (PFA) response generation.
First, BERT analyzes the user’s input to detect their emotional state and distress level. These outputs are then passed, along with the original user input, into a custom prompt designed for GPT-3.5. GPT-3.5 was fine-tuned on therapy transcripts to improve its ability to produce empathetic, context-aware PFA responses.: 1 (1.2%)
     ChatGPT played the role of a client and optionally provided AI-generated feedback to trainees: 1 (1.2%)
     Their intervention: The AI therapist tested in this study is an LLM-based pro- gram previously developed and tested at Cedars-Sinai Medical Center, with the goal of offering AI-enabled, self-
administered, mental health support within immersive virtual
reality environments.18 The user interacts with an AI conver-
sational agent designed for history-taking and therapeutic sup-
port for anxiety and depression. The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
The agent facilitates turn-based conversations, summarizing
and concluding each session, and was found to be an accepta-
ble and feasible support modality for patients with mild-to-
moderate anxiety or depression.18 Here, we focus on whether
there is evidence of any sociodemographic bias exhibited by
the conversational agent.: 1 (1.2%)
     Prompt engineering to shape ChatGPT’s dialogue: 1 (1.2%)
     Prompting of general purpose LLMs (stage 1, probably ChatGPT) + fine-tuning of BERT (stage 2) + some type of RAG based on SimCSE (stage 3): 1 (1.2%)
     two age-appropriate AI chatbots, leveraging GPT-4, were developed to provide natural, empathetic conversations: 1 (1.2%)
     FAITA-mental health chatbot was designed to classify OCD-Coach's responses: 1 (1.2%)
     It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot

fine tuning + sentiment analysis module: 1 (1.2%)
     prompted to simulate a psychiatric first-responder chatbot: 1 (1.2%)
     Retrieval Augmented Generation (RAG) kombiniert mit prompting: 1 (1.2%)
     The model was trained using one-shot 
learning, meaning that it was given a client statement and 
a counselor’s emotional reflection response as input, and 
then used that information to generate its own emotional 
reflection response.: 1 (1.2%)
     They fine-tune RoBERTa but only prompt the generative models (Flan-t5, Mistral, GPT-3.5) that they use: 1 (1.2%)
     Designed prompt phrases, iteratively refined; three prompting regimes (zero-shot, few-shot, CoT); modeled counselor reasoning style; two LLM families evaluated (ChatGLM, ERNIE Bot; also Qianwen): 1 (1.2%)
     Fine-tuned LLM + integration of other data modalities (vision): 1 (1.2%)
     The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice. 

Fig. 1: 1 (1.2%)
     Main LLM was fine-tuned but there are also other modules (e.g. safety classification)

Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).: 1 (1.2%)
     Fine-tuning (according to Fig. 3 at least) and RAG: 1 (1.2%)
     A base LLM was fine-tuned on 80 structured psychological counseling cases to adapt it to clinical contexts. Additionally, the model uses dynamic prompting with a structured DSM-5-based knowledge base. This is integrated into a larger pipeline involving text summarization, keyword extraction, and similarity analysis, alongside multimodal outputs (e.g., facial expressions, voice, and a visual avatar) to enable affective interaction.: 1 (1.2%)
     Only sub-study 3 employed an LLM: 1 (1.2%)
     The help-seeking questions were posted to the GPT-3.5-turbo model by OpenAI API to get responses from ChatGPT: 1 (1.2%)
     Prompt engineering on GPT-4. Authors describe the model is being "tuned" or "fine-tuned" but in fact they only iteratively adjust the prompt without any weight adjustment.: 1 (1.2%)
     Clare is a standalone tool, based on fine-tuned LLMs plus other modules voice inputs, safety checks: 1 (1.2%)
     Elaborate server architecture around OpenAI API (see Fig. 1): 1 (1.2%)
     Research team members prompted LLMs with the original instructions for the SIRI-2, as well as with one of the SIRI2-2’s 24 items. We did not prompt LLMs with any additional text. : 1 (1.2%)
     The chatbot was developed using the open-source machine learning framework Rasa, chosen for its strong support for customization. Rasa integrates two core modules: NLU for understanding user input and Core for managing dialogue flow. The development involved setting up the Rasa environment and coding in Python, including the installation of necessary components for both language understanding and dialogue management.: 1 (1.2%)
     PEFT (qLoRA) fine-tuning of T5: 1 (1.2%)
     Fine-tuned LLaMA 3.2 (3B parameters) using Unsloth and Ollama; used RAG for retrieval, LangChain memory for conversation history, and FAISS for relevant material recall: 1 (1.2%)
     Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B
Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses: 1 (1.2%)
     -: 1 (1.2%)
     Socrates was subjected to rigorous fine-tuning with pains-takingly detailed instructions designed to optimize therapeutic interactions. The model was specifically trained to recognize and appropriately respond to a wide spectrum of emotional states expressed by users, while respecting moments of  silence or reluctance to engage in conversation. It was calibrated to avoid overly intrusive questioning that might compromise user comfort and trust and instead detect nuanced linguistic patterns that might indicate various psychological states. Throughout its development, emphasis was placed on maintaining an appropriate balance between providing support and encouraging self-reflection: 1 (1.2%)
     Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework

Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.: 1 (1.2%)
     Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. Like the therapists’ instructions, the prompt did not place any limits on the length of the response, nor was ChatGPT shown the therapists’ responses.: 1 (1.2%)
     Instructions to act like a counselor etc. implemented; This model also allowed us to use retrieval augmented generation technology, which allows the model to be provided with a corpus of knowledge to reduce hallucinations
and misinformation [8]. Both the provision of instructions and source materials can serve as guardrails that keep the chatbot in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate.: 1 (1.2%)
     Trained on ArmanEmo dataset & Jigsaw Toxic Comment Classification translated into persian + user message validiation module. : 1 (1.2%)
     Fine-tuning of GPT-2 + other modules such as BERT-based cognitive distortion classification: 1 (1.2%)
     We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).: 1 (1.2%)
     GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.

MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules: 1 (1.2%)
     Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.: 1 (1.2%)
     davinci-003 + off topic classifier. davinci-003 seems not finetuned: 1 (1.2%)
     Promts: "Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis".

Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).: 1 (1.2%)
     Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).: 1 (1.2%)
     Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.: 1 (1.2%)
     Different models are simply fine-tuned on provided datasets: 1 (1.2%)
     Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization: 1 (1.2%)
     "We fine-tuned the model with an 80% to 20% train-test split" + "that is, the system is prompted
to keep generating responses"...: 1 (1.2%)
     BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.: 1 (1.2%)
     Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1): 1 (1.2%)
     We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.: 1 (1.2%)
     Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).: 1 (1.2%)
     Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier (see Fig. 1): 1 (1.2%)
     Fine-tuning of GPT-2 + voice and video interaction system: 1 (1.2%)
     Built on LangChain framework with memory and journaling integration .. Input/Templates/Prompt Instruction: 1 (1.2%)
     "A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative" .. "The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately" ... "Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition": 1 (1.2%)
     See Figure 3. All revolves around prompting of GPT-4, and then there is a "memory concept" and other stuff: 1 (1.2%)
     "LangChain for Prompting" combined with "CNN for Facial Recognition," "LSTM for Text-based Emotion Detection," "MFCC or LSTM for Audio Recognition": 1 (1.2%)
     Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis: 1 (1.2%)
     Four unique questions were asked of ChatGPT 4.0, and each question was asked five separate times. Each question asked to the model 
was performed in a separate, new conversation (10): 1 (1.2%)
     Prompt engineering + RAG: 1 (1.2%)
     “cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies” (Position: Methods): 1 (1.2%)
     LoRA: 1 (1.2%)
     Fine-tuning of GPT-4 + RAG (see Fig. 1) and user interface: 1 (1.2%)
     Used scripted prompts to elicit chatbot interactions and compared with therapists; coded with MULTI; non-naturalistic brief interactions. — Quote: “We also only examined brief interactions with scripted prompts… The interaction logs … were coded using the Multitheoretical List of Therapeutic Interventions” (Position: p. 12; PubMed Methods).: 1 (1.2%)
     Prompts were comprised of (1) context for the request, (2) how the model was meant to respond, 3) the specific request, and (4) the output format : 1 (1.2%)
     The authors adapted ChatMGL, a GPT-2–based model originally trained on STEM Q&A with supervised fine-tuning and PPO, for mental health dialogue. They dropped PPO, focusing instead on supervised fine-tuning with Hugging Face’s SFTT and Delta Tuning (a parameter-efficient method that updates only part of the weights). : 1 (1.2%)
     The proposed chatbot system aims to enhance mental health 
assistance through generative AI and embedding-based 
relevance detection. Unlike rule-based chatbots with 
predefined responses, our chatbot responds dynamically to 
diverse user inputs. The core innovations include few-shot 
learning for personalized conversation, semantic similarity 
filtering to ensure relevance, and generative response 
generation using Google Gemini API. : 1 (1.2%)
     Llama 2 is prompted, but there is also a RAG component: 1 (1.2%)
     Prompting of GPT-4o + components of a robot (speech-to-text, movement, etc.): 1 (1.2%)
     GPT-3.5 is only prompted but there is also an intent classifier and cognitive distortion identifier: 1 (1.2%)
     For the ChatGPT-generated advice, version GPT-4 was used. On October 12, 2023, 
the following question was asked on a fresh chatbot session and the response was saved:
[...] Since the study’s objective was to evaluate performance of ChatGPT-4 with-
out any refinement or customization, we deliberately refrained from using any prompt 
engineering techniques in our design and opted to use the afore mentioned one-time 
instruction to the LLM. : 1 (1.2%)
     Framework integrates ChatGPT with multimodal data (speech, facial, body movement), preprocessing via Whisper and feature extraction (librosa). Prompts include speech-derived descriptors (speed, pitch, rhythm, pauses) appended to text before input to ChatGPT. : 1 (1.2%)
     Prompting of different LLMs + other modules (speech-to-text, editable dashboard, etc.): 1 (1.2%)
     in a further chat interface (test 2), we
introduced the scope of expertise using a prompt as a pretraining tool, before proceeding
with the presentation of the case. This prompt was generated using ChatGPT, and its
reliability as a pretraining tool for the chat session was verified as a good alternative to
a longer and more complex training process in a previous study we conducted [12]: 1 (1.2%)
     The implementation of the speak-speak feature involves
specifying the models, prompts (as we are testing in a zero
learning shot environment): 1 (1.2%)
     Only prompting to evaluate capabilities of LLMs in terms of stigma and their responses to mental health symptoms.  In addition to prompting models with the stimuli without in-context examples, we also employed a novel method of prompting models with a portion of real therapy tran-
scripts from Alexander Street Press [6, 7]: 1 (1.2%)
     Uses RAG with prompt engineering: general LLM is guided by structured prompt templates for intent detection and empathetic response generation, while a vector-based QA database supplies domain-specific context.: 1 (1.2%)

   Sample values (first 20):
     We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).
     See Figure 3. All revolves around prompting of GPT-4, and then there is a "memory concept" and other stuff
     "A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative" .. "The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately" ... "Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition"
     Built on LangChain framework with memory and journaling integration .. Input/Templates/Prompt Instruction
     Fine-tuning of GPT-2 + voice and video interaction system
     Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier (see Fig. 1)
     Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).
     We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.
     Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1)
     BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.
     "We fine-tuned the model with an 80% to 20% train-test split" + "that is, the system is prompted
to keep generating responses"...
     Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization
     Different models are simply fine-tuned on provided datasets
     Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.
     Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).
     Promts: "Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis".

Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).
     davinci-003 + off topic classifier. davinci-003 seems not finetuned
     Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.
     "LangChain for Prompting" combined with "CNN for Facial Recognition," "LSTM for Text-based Emotion Detection," "MFCC or LSTM for Audio Recognition"
     Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis


27. intervention_type
   =====================
   Category: MULTIPLE CHOICE (semicolon-separated)
   Total responses: 134
   Null responses: 0
   Total individual choices: 160
   Unique choice options: 14
   Average choices per response: 1.2

   Individual Choice Counts:
     Unspecified, might include formal therapy methods: 92 (57.5%)
     Other CBT techniques: 16 (10.0%)
     CBT: Cognitive restructuring: 15 (9.4%)
     Informal counseling (e.g., emotional support conversation): 12 (7.5%)
     CBT: Motivational interviewing: 11 (6.9%)
     Mix of formal therapy methods: 3 (1.9%)
     Peer support conversation: 3 (1.9%)
     Psychoanalysis: 2 (1.2%)
     Other: Digital journaling with AI counselling: 1 (0.6%)
     Other: Bedtime stories: 1 (0.6%)
     Other: Gestalt Therapy techniques, Gestalt supervision: 1 (0.6%)
     Other: counselor-training simulation: 1 (0.6%)
     Psychodynamic psychotherapy: 1 (0.6%)
     Other: positive psychology intervention: 1 (0.6%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 5
     Other: Digital journaling with AI counselling: 1
     Other: Bedtime stories: 1
     Other: Gestalt Therapy techniques, Gestalt supervision: 1
     Other: counselor-training simulation: 1
     Other: positive psychology intervention: 1


28. models_employed
   ===================
   Category: MULTIPLE CHOICE (semicolon-separated)
   Total responses: 134
   Null responses: 0
   Total individual choices: 200
   Unique choice options: 20
   Average choices per response: 1.5

   Individual Choice Counts:
     GPT-4 / GPT-4o family: 43 (21.5%)
     Other: 33 (16.5%)
     GPT-3.5 family: 32 (16.0%)
     ChatGPT, model unspecified: 21 (10.5%)
     GPT-2 family: 18 (9.0%)
     Llama 2 family: 10 (5.0%)
     T5 family: 7 (3.5%)
     Mistral family: 5 (2.5%)
     GPT-3 family: 4 (2.0%)
     Claude family: 4 (2.0%)
     Gemini / Bard family: 4 (2.0%)
     BART family: 3 (1.5%)
     unspecified: 3 (1.5%)
     ChatGLM family: 3 (1.5%)
     Falcon family: 3 (1.5%)
     Qwen family: 2 (1.0%)
     DeepSeek family: 2 (1.0%)
     Llama 1 family: 1 (0.5%)
     Llama 3.1 family: 1 (0.5%)
     Llama 3.2 family: 1 (0.5%)


29. ux_assessment_is_present
   ============================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     No users involved: 79 (59.0%)
     Yes: 48 (35.8%)
     No: 7 (5.2%)


30. ux_uses_standard_instrument
   ===============================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     n: 121 (90.3%)
     y: 12 (9.0%)
     -: 1 (0.7%)


31. ux_uses_qualitative_assessment
   ==================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 105 (78.4%)
     y: 29 (21.6%)


32. ux_uses_quantitative_assessment
   ===================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 103 (76.9%)
     y: 31 (23.1%)


33. ux_results_reported
   =======================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 89 (66.4%)
     y: 45 (33.6%)


34. ux_assessment_instrument
   ============================
   Data type: object
   Total values: 134
   Non-null values: 13
   Null values: 121
   Unique values: 13
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 121 (930.8%)
     During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).: 1 (7.7%)
     consultation and relational empathy survey (CARE): 1 (7.7%)
     Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).: 1 (7.7%)
     Counselling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964

Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b): 1 (7.7%)
     modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al: 1 (7.7%)
     SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract): 1 (7.7%)
     Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9: 1 (7.7%)
     One of the ICAs on the 
7-point Likert scale using two measures from UEQ: 1 (7.7%)
     CSQ and PU: 1 (7.7%)
     System Usability Scale, SUS
User Experience Scale, UES 
Client Satisfaction Questionnaire, CSQ
Likert-scale: 1 (7.7%)
     Human Computer Trust Scale S. S. Siddharth Gulati and D. Lamas, “Design, development and evaluation of a human-computer trust scale,” Behaviour & Information Technology: 1 (7.7%)
     User experience questionnaire (UEQ): 1 (7.7%)
     Counselor rating form‑short (CRF‑S), Client satisfaction scale (CSS): 1 (7.7%)


35. ux_assessment_notes
   =======================
   Data type: object
   Total values: 134
   Non-null values: 46
   Null values: 88
   Unique values: 46
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 88 (191.3%)
     As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT: 1 (2.2%)
     Self-efficacy improved across sessions; anxiety reduced mainly between Session 1 and 2. — Quote: same as quantitative assessment line: 1 (2.2%)
     10 questions regarding user interface, ease of use, chatbot responsiveness, 
and overall satisfaction. might include standard instrument, but not described.: 1 (2.2%)
     High satisfaction (>80% smooth design); moderate effectiveness (~50% improved sleep); high acceptance (75% continued use). — Quote: “More than 80% of the volunteers think that our app is well designed and smooth to use” (Position: Application Utilization): 1 (2.2%)
      “Our system” (M = 5.368) found this work’s system statistically significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).: 1 (2.2%)
     Also measuring undesirable output via annotation.
For qualitative results, see 4.3.3.: 1 (2.2%)
     Only quantitative scoring of : 1 (2.2%)
     This article is a case study with loose, qualitative descriptions: 1 (2.2%)
     This study employed a deductive thematic analysis, using the conceptual framework of Grodniewicz and Hohol’s (2023) three challenges in AI psychotherapy as a general guide: 1 (2.2%)
     See V.B.: 1 (2.2%)
     All participants noted improved treatment motivation … 80% disclosing personal concerns … 24/7 availability particularly benefited patients
Four out of five participants reported significant reductions in anxiety and stress levels post-intervention” (Results, p. 1): 1 (2.2%)
     Both, user experience assessment and participant attitude assessment.

This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.: 1 (2.2%)
     72.86% rated GPT-4 advice practical … 87.94% well-explained … statistically significant” (Position: Results, p. 41–42)

GPT-4 advice perceived as more practical and clearer; effect sizes small-to-medium. — Quote: “differences … statistically significant … d ≈ 0.34 … d ≈ 0.53 …” (Position: Results, p. 41–42): 1 (2.2%)
     Qualitative analysis of user reviews focusing on risks using BERTopic: 1 (2.2%)
     Results reported: Yes (case-level outcomes) — Quote: “the AI successfully managed to … become more experientially aligned with the patient” (Position: Clinical section, p. 11)

Empathic failures, uncanny feelings, corrective alignment described — Quote: “empathic failure occurred … however, the ability to overcome empathic failures … holds significant therapeutic value” (Position: Clinical section, p. 11): 1 (2.2%)
     Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX.

Results in Fig 5: 1 (2.2%)
     Most participants reported feeling strongly understood by
the chatbot, with the majority indicating a high degree of
perceived emotional attunement. Participants described the
experience as intuitive to navigate, personally meaningful,
and contextually relevant to their situations. A general sense
of satisfaction was expressed across the participant group,
with favorable ratings on the satisfaction measures.: 1 (2.2%)
     higher reliable improvement, recovery, and reliable recovery rates” 
users perceived the AI-enabled therapy support tool as most useful for discussing their problems to gain awareness and clarity … learning how to apply coping skills”: 1 (2.2%)
     No user experience assessment but participant attitude survey: 1 (2.2%)
     The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.

Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. : 1 (2.2%)
     users expressed positive attitudes toward digital mental health solutions, with key motives including avoiding embarrassment (36%) and concerns about appearance in face-to-face consultations (35%). Expectations focused on emotional support (35%) and expressing feelings (32%): 1 (2.2%)
     Before the study’s completion, we conducted unstructured interviews to delve deeper into the participants’ experiences with the chatbots. Many participants expressed that the chatbot’s responses evoked feelings of surprise and novelty. They emphasized the importance of feeling understood as pivotal in enhancing their acceptance and engagement with the chatbot-based intervention. Since GPT-based chatbots can generate text in a human-like manner, mimicking the interaction between users and psychotherapists, five participants indicated that the anthropomorphic responses felt so authentic that they found it difficult to distinguish whether they were AI-generated. These qualitative insights reinforce our quantitative findings and underscore the critical importance of interaction in the design of chatbots.: 1 (2.2%)
     Qualitative analysis:
This approach involves six main steps: getting familiar with the data; generating initial codes; searching for themes; reviewing themes; defining and naming themes; and producing the report. The analysis process was iterative and recursive, allowing the researchers to move back and forth between these steps as needed to ensure thoroughness and validity. : 1 (2.2%)
     User experience assessment of therapists using the tool. There is a quantiative (Likert scale questionnaires) and qualitative (open-ended user questions) assessment: 1 (2.2%)
     Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).: 1 (2.2%)
     Satisfaction ratings focused on relational/emotional quality, technical/didactic, treatment support, professional orientation. — Quote: “PCA highlighted four components… relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4)” (Position: Abstract): 1 (2.2%)
     1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5): 1 (2.2%)
     Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.: 1 (2.2%)
     Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience: 1 (2.2%)
     Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions ("score", "activity", "informativeness"), while psychologist responses were rated higher in "strength".

"During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used)."​: 1 (2.2%)
     consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?: 1 (2.2%)
     Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions): 1 (2.2%)
     We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).: 1 (2.2%)
     Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.: 1 (2.2%)
     The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.

Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience: 1 (2.2%)
     Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.: 1 (2.2%)
     Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. "It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information."

semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.: 1 (2.2%)
     "User experience assessment" was content of the Reddit posts: 1 (2.2%)
     SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback): 1 (2.2%)
      The qualitative interviews were conducted remotely via Zoom by a user
experience researcher and were supported by an in-person study
coordinator. Interviews were recorded, transcribed, and
observational notes during each session were taken. The
interviews had three sequential components: 1) participants were
asked questions about their past experiences with AI conversational
agents and perceptions of AI conversational agents in mental
healthcare, 2) participants engaged with a mental health
conversational agent app to ground and standardize their
experiences, and 3) participants were asked additional general
questions related to the acceptability of AI conversational agents
in mental healthcare, and more.: 1 (2.2%)
     Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” : 1 (2.2%)
     Qualitative feedback reported in 4.4.: 1 (2.2%)
     Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)

May not be understood as the "classical" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.: 1 (2.2%)
     As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).: 1 (2.2%)
     Quantitative: Measures of the user experience with the generative and rules-based DMHIs included user engagement (number of sessions, total active days, and conversational exchanges).
: 1 (2.2%)
     Thematic analysis of Reddit posts and comments: 1 (2.2%)

   Sample values (first 20):
     As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT
     Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).
     1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5)
     Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.
     Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience
     Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions ("score", "activity", "informativeness"), while psychologist responses were rated higher in "strength".

"During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used)."​
     consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?
     Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions)
     We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
     Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.
     The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.

Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience
     Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.
     Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. "It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information."

semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.
     "User experience assessment" was content of the Reddit posts
     SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback)
      The qualitative interviews were conducted remotely via Zoom by a user
experience researcher and were supported by an in-person study
coordinator. Interviews were recorded, transcribed, and
observational notes during each session were taken. The
interviews had three sequential components: 1) participants were
asked questions about their past experiences with AI conversational
agents and perceptions of AI conversational agents in mental
healthcare, 2) participants engaged with a mental health
conversational agent app to ground and standardize their
experiences, and 3) participants were asked additional general
questions related to the acceptability of AI conversational agents
in mental healthcare, and more.
     Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” 
     Qualitative feedback reported in 4.4.
     Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)

May not be understood as the "classical" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.
     As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).


36. lexical_overlap_used
   ========================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 109 (81.3%)
     y: 25 (18.7%)


37. lexical_overlap_vs_benchmark
   ================================
   Data type: object
   Total values: 134
   Non-null values: 25
   Null values: 109
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 109 (436.0%)
     b: 14 (56.0%)
     no benchmark: 9 (36.0%)
     -: 1 (4.0%)
     s: 1 (4.0%)


38. lexical_overlap_benchmark_quality
   =====================================
   Data type: object
   Total values: 134
   Non-null values: 25
   Null values: 109
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 109 (436.0%)
     l: 15 (60.0%)
     no benchmark: 9 (36.0%)
     -: 1 (4.0%)


39. lexical_overlap_notes
   =========================
   Data type: object
   Total values: 134
   Non-null values: 24
   Null values: 110
   Unique values: 24
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 110 (458.3%)
     “We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”: 1 (4.2%)
     BLEU with reference: 1 (4.2%)
     BLEU, ROUGE against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.: 1 (4.2%)
     Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (4.2%)
     BLEU against human responses. Benchmark: non fine-tuned GPT-2: 1 (4.2%)
     -: 1 (4.2%)
     METEOR and ROUGE tested, but Strawman models: 1 (4.2%)
     BLEU, ROUGE with counselor responses as reference (CounselChat): 1 (4.2%)
     BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.: 1 (4.2%)
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Comparison against reference responses in dataset: 1 (4.2%)
     BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L: 1 (4.2%)
     BLEU SCORE: 1 (4.2%)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)

ChatGPTbased conversations
(Approach 1), fine-tuned DialoGPT transformer
conversations (Approach 2), and fine-tuned DialoGPT
transformer conversations combined with the GPT3 prompts
API (Approach 3): 1 (4.2%)
     BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations: 1 (4.2%)
     ML models (HRED, SEQ2SEQ): 1 (4.2%)
     Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.: 1 (4.2%)
     seq2seq: 1 (4.2%)
     benchmark are human responses: 1 (4.2%)
     no benchmark. metric: ROUGE-1, -2, -L: 1 (4.2%)
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR: 1 (4.2%)
     benchmark: other conversation systems: 1 (4.2%)
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.: 1 (4.2%)
     ROUGE-L - PanGu better than WenZhong: 1 (4.2%)
     BLEU, ROUGE, METEOR. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5): 1 (4.2%)

   Sample values (first 20):
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)

ChatGPTbased conversations
(Approach 1), fine-tuned DialoGPT transformer
conversations (Approach 2), and fine-tuned DialoGPT
transformer conversations combined with the GPT3 prompts
API (Approach 3)
     ROUGE-L - PanGu better than WenZhong
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.
     benchmark: other conversation systems
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR
     no benchmark. metric: ROUGE-1, -2, -L
     benchmark are human responses
     seq2seq
     Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.
     ML models (HRED, SEQ2SEQ)
     BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations
     BLEU SCORE
     “We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”
     BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Comparison against reference responses in dataset
     BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.
     BLEU, ROUGE with counselor responses as reference (CounselChat)
     METEOR and ROUGE tested, but Strawman models
     -
     BLEU against human responses. Benchmark: non fine-tuned GPT-2


40. embedding_similarity_used
   =============================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 118 (88.1%)
     y: 16 (11.9%)


41. embedding_similarity_vs_benchmark
   =====================================
   Data type: object
   Total values: 134
   Non-null values: 16
   Null values: 118
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 118 (737.5%)
     b: 9 (56.2%)
     no benchmark: 4 (25.0%)
     s: 2 (12.5%)
     w: 1 (6.2%)


42. embedding_similarity_benchmark_quality
   ==========================================
   Data type: object
   Total values: 134
   Non-null values: 16
   Null values: 118
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 118 (737.5%)
     l: 12 (75.0%)
     no benchmark: 4 (25.0%)


43. embedding_similarity_notes
   ==============================
   Data type: object
   Total values: 134
   Non-null values: 15
   Null values: 119
   Unique values: 15
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 119 (793.3%)
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.: 1 (6.7%)
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR: 1 (6.7%)
     BERTScore: 1 (6.7%)
     benchmark are human responses.: 1 (6.7%)
     seq2seq: 1 (6.7%)
     ML models (HRED, SEQ2SEQ): 1 (6.7%)
     specificity is an embedding similarity metric here: 1 (6.7%)
     Semantic Similarity via cosine distance (see Eq. 4): 1 (6.7%)
     Best model of authors compared to models of other works. : 1 (6.7%)
     on average 0.93 (±0.03) similarity in the embedding space.

Wonky measurement: embedding similarity between chatbot responses and patient inputs: 1 (6.7%)
     Distance between embeddings of reference and output.: 1 (6.7%)
     PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (6.7%)
     BERTScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.: 1 (6.7%)
     benchmark is GPT-3.5. measure is BERTScore and sentence transformer score with dataset expert responses as reference: 1 (6.7%)
     Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).: 1 (6.7%)


44. classification_used
   =======================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 108 (80.6%)
     y: 26 (19.4%)


45. classification_vs_benchmark
   ===============================
   Data type: object
   Total values: 134
   Non-null values: 26
   Null values: 108
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 108 (415.4%)
     b: 11 (42.3%)
     no benchmark: 10 (38.5%)
     w: 4 (15.4%)
     -: 1 (3.8%)


46. classification_benchmark_quality
   ====================================
   Data type: object
   Total values: 134
   Non-null values: 26
   Null values: 108
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 108 (415.4%)
     l: 14 (53.8%)
     no benchmark: 10 (38.5%)
     h: 2 (7.7%)


47. classification_notes
   ========================
   Data type: object
   Total values: 134
   Non-null values: 27
   Null values: 107
   Unique values: 27
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 107 (396.3%)
     no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.: 1 (3.7%)
     Models classified cases requiring intervention; BSI>1 and higher CR indicate better behavior sensitivity/consistency; DeepSeek/Wenxin/Claude > GPT-4. — Quote: “BSI and CR … DeepSeek 1.0662 / 0.8985 … GPT-4 1.0415 / 0.8250: 1 (3.7%)
     Various classification metrics of BERT emotional distress detection. Benchmark: Bi-LSTM: 1 (3.7%)
     between the rule-based sentiment analysis module and the
sentiment analysis capabilities of the GPT-powered model: 1 (3.7%)
     F1 score, not quiet clear what of.: 1 (3.7%)
     Recall@1 (Tables 3 and 4): 1 (3.7%)
     Performance of cognitive distortion classification of non-generative BERT model is given (but it's non-generative): 1 (3.7%)
     Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation): 1 (3.7%)
     evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation): 1 (3.7%)
     "training accuracy" of generative LLM. Not sure what this accurac is measuring.: 1 (3.7%)
     “coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” : 1 (3.7%)
     Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries)

Classification into relevant/irrelevant via all-MiniLM-L6-v2: 1 (3.7%)
     Various classification metrics (accuracy, precision, recall, f1) for checking how "relevant" the responses are. No information is given on how "relevance" is measured here.: 1 (3.7%)
     Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.: 1 (3.7%)
     Same Model (prompting scheme) but without contextual text data: 1 (3.7%)
     Accuracy of the BERT helping skill classification model: 1 (3.7%)
     The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978: 1 (3.7%)
     Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM: 1 (3.7%)
     f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.: 1 (3.7%)
     no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token: 1 (3.7%)
     accuracy, F1, precision, recall for prediction of feeling based on automatic thought (feeling-thought pairs): 1 (3.7%)
     benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative: 1 (3.7%)
     accuracy, precision, recall, F1 for classifying utterances. Task: classification of mental health dialogue turns into safe/various types of unsafe responses. benchmark: fine-tuned BERT-base and RoBERTa-large: 1 (3.7%)
     benchmark: other conversation systems. Task: emotion classification in current utterance of help-seeker: 1 (3.7%)
     accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection. Task: Cognitive distortion detection in client questions.: 1 (3.7%)
     Table 4: Benchmark is fine-tuned DistilBERT model. ChatGPT is compared against this and performs worse.
Table 2: Prompted GPT-3 model (text-davinci-003) is compared against DistilBERT.

1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not): 1 (3.7%)
     Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).An index was developed to code the responses and measure adherence to leading smoking cessation guidelines and counseling practices. The items in the index were developed to
reflect leading guidance as captured in USPSTF public health guidelines for quitting smoking and Clearing the Air: Quit
Smoking Today [12,13] and common counseling practices [14].: 1 (3.7%)

   Sample values (first 20):
     no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.
     Table 4: Benchmark is fine-tuned DistilBERT model. ChatGPT is compared against this and performs worse.
Table 2: Prompted GPT-3 model (text-davinci-003) is compared against DistilBERT.

1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)
     accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection. Task: Cognitive distortion detection in client questions.
     benchmark: other conversation systems. Task: emotion classification in current utterance of help-seeker
     accuracy, precision, recall, F1 for classifying utterances. Task: classification of mental health dialogue turns into safe/various types of unsafe responses. benchmark: fine-tuned BERT-base and RoBERTa-large
     benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative
     accuracy, F1, precision, recall for prediction of feeling based on automatic thought (feeling-thought pairs)
     no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token
     f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.
     Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM
     The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978
     Accuracy of the BERT helping skill classification model
     Same Model (prompting scheme) but without contextual text data
     Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.
     Various classification metrics (accuracy, precision, recall, f1) for checking how "relevant" the responses are. No information is given on how "relevance" is measured here.
     Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries)

Classification into relevant/irrelevant via all-MiniLM-L6-v2
     “coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” 
     "training accuracy" of generative LLM. Not sure what this accurac is measuring.
     evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation)
     Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation)


48. continuous_metrics_used
   ===========================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 126 (94.0%)
     y: 8 (6.0%)


49. continuous_metrics_vs_benchmark
   ===================================
   Data type: object
   Total values: 134
   Non-null values: 8
   Null values: 126
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 126 (1575.0%)
     no benchmark: 5 (62.5%)
     no benckmark: 1 (12.5%)
     b: 1 (12.5%)
     w: 1 (12.5%)


50. continuous_metrics_benchmark_quality
   ========================================
   Data type: object
   Total values: 134
   Non-null values: 8
   Null values: 126
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 126 (1575.0%)
     no benchmark: 6 (75.0%)
     l: 2 (25.0%)


51. continuous_metrics_notes
   ============================
   Data type: object
   Total values: 134
   Non-null values: 8
   Null values: 126
   Unique values: 8
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 126 (1575.0%)
     Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though (Table 10, 11): 1 (12.5%)
     no benchmark. loss value: 1 (12.5%)
     Comparison of chatbot-inferred PHQ-9 scores with human scorers via ICC and Cohen's kappa: 1 (12.5%)
     Table 6 Accuracy of machine learning models in forecasting 7-day 
SAD (stress, anxiety, and depression) levels based on single text 
entries against extrapolation: 1 (12.5%)
     Pearson, Spearman, Kendall’s Tau. Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark): 1 (12.5%)
     Train loss: 1 (12.5%)
     Metric: training loss. No benchmark: 1 (12.5%)
     Correlation between LLM suicide response ratings and expert ratings: 1 (12.5%)


52. expert_rating_used
   ======================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 104 (77.6%)
     y: 30 (22.4%)


53. expert_rating_vs_benchmark
   ==============================
   Data type: object
   Total values: 134
   Non-null values: 29
   Null values: 105
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 105 (362.1%)
     no benchmark: 12 (41.4%)
     b: 11 (37.9%)
     w: 5 (17.2%)
     s: 1 (3.4%)


54. expert_rating_benchmark_quality
   ===================================
   Data type: object
   Total values: 134
   Non-null values: 28
   Null values: 106
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 106 (378.6%)
     no benchmark: 11 (39.3%)
     h: 9 (32.1%)
     l: 8 (28.6%)


55. expert_rating_notes
   =======================
   Data type: object
   Total values: 134
   Non-null values: 29
   Null values: 105
   Unique values: 29
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 105 (362.1%)
     no benchmark. however, experts rated the chatbot highly in absolute terms.: 1 (3.4%)
     Binary scoring rubrics for GPT-4 responses (see Table 4): 1 (3.4%)
     Human interactive evaluation (fluency, comforting, etc.), comparison with other models. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5).: 1 (3.4%)
     Each chatbot response was independently categorized by 2
coders for their adherence on each item of the index.: 1 (3.4%)
     Fluency, helpfulness, relevance, empathy, professionalism, evaluated by psychology graduate students on a 5-Likert scale. Benchmark are other LLM-based methods.: 1 (3.4%)
     no benchmark: 1 (3.4%)
     We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance

Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.: 1 (3.4%)
     likert-scale appropriateness rating of emotional reflection responses. compared GPT-4 against human counselor responses.: 1 (3.4%)
     Measures: Likert empathy rating, MITI global score. Benchmark: human peer supporter from Reddit post.: 1 (3.4%)
     CAPE category ratings, no benchmark: 1 (3.4%)
     7 different safety questions: 1 (3.4%)
     benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating: 1 (3.4%)
     Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.: 1 (3.4%)
     Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality: 1 (3.4%)
     valuated by Gestalt psychotherapy trainees: 1 (3.4%)
     To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.: 1 (3.4%)
     benchmark: woebot non-generative/rule-based.: 1 (3.4%)
     Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale: 1 (3.4%)
     4.3 Human Evaluation Results: 1 (3.4%)
     no benchmark.: 1 (3.4%)
     Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.: 1 (3.4%)
     ML models (HRED, SEQ2SEQ): 1 (3.4%)
     benchmark is ground truth (human created reflections) and output of simple seq2seq model: 1 (3.4%)
     no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination: 1 (3.4%)
     no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.): 1 (3.4%)
     metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2: 1 (3.4%)
     Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.

Measures: Relevance, CBT structure, helpfulness: 1 (3.4%)
     psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set

Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model: 1 (3.4%)
     The LLM-Counselor Support System. Benchmark: GPT-4 with zero-shot CoT: 1 (3.4%)

   Sample values (first 20):
     no benchmark. however, experts rated the chatbot highly in absolute terms.
     psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set

Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model
     Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.

Measures: Relevance, CBT structure, helpfulness
     metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2
     no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.)
     no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination
     benchmark is ground truth (human created reflections) and output of simple seq2seq model
     ML models (HRED, SEQ2SEQ)
     Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.
     no benchmark.
     4.3 Human Evaluation Results
     Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale
     benchmark: woebot non-generative/rule-based.
     To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.
     valuated by Gestalt psychotherapy trainees
     Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality
     Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.
     benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating
     7 different safety questions
     CAPE category ratings, no benchmark


56. llm_judge_used
   ==================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 128 (95.5%)
     y: 6 (4.5%)


57. llm_judge_vs_benchmark
   ==========================
   Data type: object
   Total values: 134
   Non-null values: 6
   Null values: 128
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 128 (2133.3%)
     no benchmark: 3 (50.0%)
     b: 2 (33.3%)
     w: 1 (16.7%)


58. llm_judge_benchmark_quality
   ===============================
   Data type: object
   Total values: 134
   Non-null values: 6
   Null values: 128
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 128 (2133.3%)
     no benchmark: 3 (50.0%)
     l: 2 (33.3%)
     h: 1 (16.7%)


59. llm_judge_notes
   ===================
   Data type: object
   Total values: 134
   Non-null values: 6
   Null values: 128
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 128 (2133.3%)
     The DeepEval framework + MQG-RAG Evaluation: 1 (16.7%)
     ChatGPT-4 rated using the categories below: 1 (16.7%)
     Ragas is LLM-as-a-judge: 1 (16.7%)
     LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.: 1 (16.7%)
     Qianwen to automatically evaluate the counseling dialogue (esction: C. ): 1 (16.7%)
     No indexmodel but three equaly ranked models were compared : 1 (16.7%)


60. perplexity_used
   ===================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 130 (97.0%)
     y: 4 (3.0%)


61. perplexity_vs_benchmark
   ===========================
   Data type: object
   Total values: 134
   Non-null values: 4
   Null values: 130
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 130 (3250.0%)
     b: 2 (50.0%)
     w: 1 (25.0%)
     no benchmark: 1 (25.0%)


62. perplexity_benchmark_quality
   ================================
   Data type: object
   Total values: 134
   Non-null values: 4
   Null values: 130
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 130 (3250.0%)
     l: 3 (75.0%)
     no benchmark: 1 (25.0%)


63. perplexity_notes
   ====================
   Data type: object
   Total values: 134
   Non-null values: 4
   Null values: 130
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 130 (3250.0%)
     Best model of authors compared to models of other works. : 1 (25.0%)
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Perplexity of reference responses in dataset: 1 (25.0%)
     Benchmark: non fine-tuned GPT-2: 1 (25.0%)
     Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5): 1 (25.0%)


64. lexical_diversity_used
   ==========================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     n: 129 (96.3%)
     y: 4 (3.0%)
     n : 1 (0.7%)


65. lexical_diversity_vs_benchmark
   ==================================
   Data type: object
   Total values: 134
   Non-null values: 4
   Null values: 130
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 130 (3250.0%)
     b: 1 (25.0%)
     s: 1 (25.0%)
     no benchmark: 1 (25.0%)
     w: 1 (25.0%)


66. lexical_diversity_benchmark_quality
   =======================================
   Data type: object
   Total values: 134
   Non-null values: 4
   Null values: 130
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 130 (3250.0%)
     l: 2 (50.0%)
     no benchmark: 1 (25.0%)
     h: 1 (25.0%)


67. lexical_diversity_notes
   ===========================
   Data type: object
   Total values: 134
   Non-null values: 3
   Null values: 131
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 131 (4366.7%)
     D1 (Distinct-1) measures the richness of vocabulary in the responses: 1 (33.3%)
     Dist-2. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (33.3%)
     Metrics: Dist-1 and Dist-2 on chatbot responses. No benchmark: 1 (33.3%)


68. metric1_name
   ================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 43
   Category: CATEGORICAL

   Value Counts:
     n: 90 (67.2%)
     perplexity: 3 (2.2%)
     sentiment and part-of-speech: 1 (0.7%)
     user experience rating: 1 (0.7%)
     client evaluation of motivational interviewing scale (cemi) (madson et al., 2013, 2015, 2016); readiness to change delta: 1 (0.7%)
     user rating: 1 (0.7%)
     empathy rating unequality: 1 (0.7%)
     liwc score domains and linguistic marker scores: 1 (0.7%)
     empathy
 coherence 
informativeness 
fluency: 1 (0.7%)
     behavior sensitivity index: 1 (0.7%)
     patient health questionnaire 927 (phq-9), the generalized anxiety disorder questionnaire for the diagnostic and statistical manual of mental disorders, fourth edition (dsm-iv) (gad-q-iv), and the weight concerns scale (wcs) within the stanford–washington university eating disorder (swed): 1 (0.7%)
     whoqol-bref score change: 1 (0.7%)
     lay rating: 1 (0.7%)
     cross-entropy loss: 1 (0.7%)
     unclear metrics: relevance, empathy, conciseness, context: 1 (0.7%)
     nlu confidence: 1 (0.7%)
     relevancy: 1 (0.7%)
     quantitative ux assessment ("overall satisfaction"): 1 (0.7%)
     wai-sr working alliance: 1 (0.7%)
     automatic "perceived information quality" (piq) rating via ml model (prompted chatgpt): 1 (0.7%)
     phq-9, panas-p, satisfaction with life scale (swls): 1 (0.7%)
     subjective units of distress scale, suds: 1 (0.7%)
     avg. of stigma questions: 1 (0.7%)
     see fig. 4-9. : 1 (0.7%)
     stress test metrics: 1 (0.7%)
     lexical diversity and richness: 1 (0.7%)
     human rating, not sure by whom: 1 (0.7%)
     aaq + cds: 1 (0.7%)
     human rating (unclear if expert or not): 1 (0.7%)
     lexical diversity: 1 (0.7%)
     diversity: 1 (0.7%)
     sentiment polarity: 1 (0.7%)
     readiness ruler (patient symptom report, smoking related, validated measure): 1 (0.7%)
     reduction in emotion intensity: 1 (0.7%)
     part-of-speech (pos) analysis; dependency-syntactic-parsing (dep) analysis; semantic-dependency-parsing (sdp) analysis; sentiment analysis: 1 (0.7%)
     empathy classification (sharma et al): 1 (0.7%)
     sentiment analysis score: 1 (0.7%)
     safety (number of conversations turns until initial referral/shutdown of chatbot): 1 (0.7%)
     psychobench empathy scale: 1 (0.7%)
     reduction in anxiety symptoms (bai, gad-7): 1 (0.7%)
     posttrial guardrail review: 1 (0.7%)
     comprehensiveness, professionalism, safety: 1 (0.7%)
     client satisfaction scale (css): 1 (0.7%)

   Sample values (first 20):
     whoqol-bref score change
     n
     perplexity
     lexical diversity and richness
     human rating, not sure by whom
     aaq + cds
     human rating (unclear if expert or not)
     lexical diversity
     diversity
     sentiment polarity
     readiness ruler (patient symptom report, smoking related, validated measure)
     reduction in emotion intensity
     part-of-speech (pos) analysis; dependency-syntactic-parsing (dep) analysis; semantic-dependency-parsing (sdp) analysis; sentiment analysis
     empathy classification (sharma et al)
     sentiment analysis score
     safety (number of conversations turns until initial referral/shutdown of chatbot)
     psychobench empathy scale
     reduction in anxiety symptoms (bai, gad-7)
     posttrial guardrail review
     comprehensiveness, professionalism, safety


69. metric1_vs_benchmark
   ========================
   Data type: object
   Total values: 134
   Non-null values: 42
   Null values: 92
   Unique values: 9
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 92 (219.0%)
     no benchmark: 16 (38.1%)
     b: 15 (35.7%)
     s: 4 (9.5%)
     w: 2 (4.8%)
     no benchmark. in absolute terms favorable, with reduction in smoking, increase in confidence, importance, readiness.: 1 (2.4%)
     unclear: 1 (2.4%)
     b : 1 (2.4%)
     unknown: 1 (2.4%)
     - (no b/s/w classification possible): 1 (2.4%)


70. metric1_benchmark_quality
   =============================
   Data type: object
   Total values: 134
   Non-null values: 42
   Null values: 92
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 92 (219.0%)
     l: 17 (40.5%)
     no benchmark: 17 (40.5%)
     h: 8 (19.0%)


71. metric1_notes
   =================
   Data type: object
   Total values: 134
   Non-null values: 41
   Null values: 93
   Unique values: 35
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 93 (226.8%)
     no benchmark: 7 (17.1%)
     pre post measurement: 1 (2.4%)
     A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.: 1 (2.4%)
     Measures MI adherence of clients. Benchmark: GPT-4 out of the box without MI fine-tuning: 1 (2.4%)
     User scores of "helpfulness" and "empathy". Benchmark: non fine-tuned GPT-2.: 1 (2.4%)
     Table 1: mechanical turk worker ratings ("Human") vs. GPT-4: 1 (2.4%)
     No Indexmodel; comparison of multiple LLMs: 1 (2.4%)
     Waitlist control: 1 (2.4%)
     Therapist responses: 1 (2.4%)
     intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points): 1 (2.4%)
     Book written by a human expert.: 1 (2.4%)
     Unclear what these metrics are (human ratings? automatic?). No benchmarks used.: 1 (2.4%)
     Not clear what this metric is.: 1 (2.4%)
     Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini: 1 (2.4%)
     benchmarks are other mental health chatbots and general LLMs: Woebot, Happify, GPT-3.5, Google Bard: 1 (2.4%)
     benchmark are real human responses from the Q&A dataset: 1 (2.4%)
     Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. : 1 (2.4%)
     compared to data set 70.58 %: 1 (2.4%)
     Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. : 1 (2.4%)
     ChatGPT and GLM-4: 1 (2.4%)
     A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). : 1 (2.4%)
     PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon): 1 (2.4%)
     benchmark are responses in the kaggle dataset: 1 (2.4%)
     Other LLMs: 1 (2.4%)
     benchmark: human psychologist responses from transcripts: 1 (2.4%)
     no benchmark. only the pre-post-difference in this metric was measured.: 1 (2.4%)
     Benchmark is other NLP model: 1 (2.4%)
     seq2seq: 1 (2.4%)
     rating scale to determine quality of backward looking reflections (Textbox 5).: 1 (2.4%)
     benchmark: other conversation systems: 1 (2.4%)
     aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.: 1 (2.4%)
     different models were rated by human non-experts. no comparison with any benchmark.: 1 (2.4%)
     human psychologist responses in the CounselChat transcripts: 1 (2.4%)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts): 1 (2.4%)
     Benchmars are other LLM-based models: 1 (2.4%)

   Sample values (first 20):
     intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)
     human psychologist responses in the CounselChat transcripts
     no benchmark
     different models were rated by human non-experts. no comparison with any benchmark.
     aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.
     benchmark: other conversation systems
     rating scale to determine quality of backward looking reflections (Textbox 5).
     seq2seq
     Benchmark is other NLP model
     no benchmark. only the pre-post-difference in this metric was measured.
     benchmark: human psychologist responses from transcripts
     Other LLMs
     benchmark are responses in the kaggle dataset
     PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)
     A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). 
     ChatGPT and GLM-4
     Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. 
     Book written by a human expert.
     A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.


72. metric2_name
   ================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 24
   Category: CATEGORICAL

   Value Counts:
     n: 111 (82.8%)
     linguistic features as mentioned under 3.3: 1 (0.7%)
     scales of psychological well-being (pwb): 1 (0.7%)
     various linguistic factors: 1 (0.7%)
     overall performance: 1 (0.7%)
     user satisfaction score: 1 (0.7%)
     fuzzy matching confidence: 1 (0.7%)
     positive and negative affect schedule, panas: 1 (0.7%)
     difference between responses (detection): 1 (0.7%)
     consistency ratio: 1 (0.7%)
     average tone (sentiment score): 1 (0.7%)
     number of harmful outputs (user-rated): 1 (0.7%)
     authenticity: 1 (0.7%)
     instances of potentially concerning language detected: 1 (0.7%)
     user satisfaction rating: 1 (0.7%)
     psychobench emotional intelligence scale: 1 (0.7%)
     response quality rated by non experts: 1 (0.7%)
     perplexity, diversity, sentence coherence, edit rate: 1 (0.7%)
     shap values of words in classifier that classifies human vs. chatgpt responses: 1 (0.7%)
     ux measures: 1 (0.7%)
     change of empathy scores for the erf module: 1 (0.7%)
     average length: 1 (0.7%)
     distinct 1: 1 (0.7%)
     counselor rating form‑short (crf‑s): 1 (0.7%)

   Sample values (first 20):
     n
     linguistic features as mentioned under 3.3
     distinct 1
     average length
     change of empathy scores for the erf module
     ux measures
     shap values of words in classifier that classifies human vs. chatgpt responses
     perplexity, diversity, sentence coherence, edit rate
     response quality rated by non experts
     psychobench emotional intelligence scale
     user satisfaction rating
     instances of potentially concerning language detected
     authenticity
     number of harmful outputs (user-rated)
     average tone (sentiment score)
     consistency ratio
     difference between responses (detection)
     positive and negative affect schedule, panas
     fuzzy matching confidence
     user satisfaction score


73. metric2_vs_benchmark
   ========================
   Data type: object
   Total values: 134
   Non-null values: 23
   Null values: 111
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 111 (482.6%)
     no benchmark: 9 (39.1%)
     b: 5 (21.7%)
     unclear: 3 (13.0%)
     w: 3 (13.0%)
     s: 1 (4.3%)
     mixed (table 9): 1 (4.3%)
     unknown: 1 (4.3%)


74. metric2_benchmark_quality
   =============================
   Data type: object
   Total values: 134
   Non-null values: 23
   Null values: 111
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 111 (482.6%)
     no benchmark: 9 (39.1%)
     l: 9 (39.1%)
     h: 5 (21.7%)


75. metric2_notes
   =================
   Data type: object
   Total values: 134
   Non-null values: 22
   Null values: 112
   Unique values: 19
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 112 (509.1%)
     no benchmark: 4 (18.2%)
     No Indexmodel; comparison of multiple LLMs: 1 (4.5%)
     Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. : 1 (4.5%)
     benchmark are real human responses from the Q&A dataset: 1 (4.5%)
     Compared to the ratings of expert suicidologist.
: 1 (4.5%)
     Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini: 1 (4.5%)
     Not clear what this metric is.: 1 (4.5%)
     pre post measurement: 1 (4.5%)
     Therapist responses: 1 (4.5%)
     Benchmark: GPT-4 out of the box without MI fine-tuning: 1 (4.5%)
     it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.: 1 (4.5%)
     ChatGPT and GLM-4: 1 (4.5%)
     benchmark: woebot non-generative/rule-based.: 1 (4.5%)
     PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon): 1 (4.5%)
     benchmark are responses in the kaggle dataset, but author rated himself (vested interests???): 1 (4.5%)
     Other LLMs: 1 (4.5%)
     benchmark: human psychologist responses from transcripts: 1 (4.5%)
     Reframe Relatability, Reframe Helpfulness, Reframe Memorability, Skill Learnability: 1 (4.5%)
     Benchmars are other LLM-based models: 1 (4.5%)


76. metric3_name
   ================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 8
   Category: CATEGORICAL

   Value Counts:
     n: 127 (94.8%)
     distinct 2
: 1 (0.7%)
     expert rating 2: 1 (0.7%)
     word analysis/ count: 1 (0.7%)
     daily interaction time: 1 (0.7%)
     german version of the working alliance inventory - short revised (wai-sr): 1 (0.7%)
     gad-7, panas-n: 1 (0.7%)
     sentiment score: 1 (0.7%)


77. metric3_vs_benchmark
   ========================
   Data type: object
   Total values: 134
   Non-null values: 7
   Null values: 127
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 127 (1814.3%)
     no benchmark: 2 (28.6%)
     b: 2 (28.6%)
     s: 2 (28.6%)
     more words: 1 (14.3%)


78. metric3_benchmark_quality
   =============================
   Data type: object
   Total values: 134
   Non-null values: 7
   Null values: 127
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 127 (1814.3%)
     l: 4 (57.1%)
     no benchmark: 2 (28.6%)
     h: 1 (14.3%)


79. metric3_notes
   =================
   Data type: object
   Total values: 134
   Non-null values: 6
   Null values: 128
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 128 (2133.3%)
     no benchmark: 2 (33.3%)
     Expert judgments of empathy, fluency, specificity of PARTNER against other LLMs.: 1 (16.7%)
     Benchmark: GPT-4 out of the box without MI fine-tuning: 1 (16.7%)
     Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. : 1 (16.7%)
     Benchmars are other LLM-based models: 1 (16.7%)


80. s1_risk_detection_considered
   ================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 113 (84.3%)
     y: 21 (15.7%)


81. s1_risk_detection_notes
   ===========================
   Data type: object
   Total values: 134
   Non-null values: 22
   Null values: 112
   Unique values: 22
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 112 (509.1%)
     n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist

attending psychiatrist was checking responses: 1 (4.5%)
     Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.: 1 (4.5%)
     The risk detection system in HoMemeTown was developed
based on established clinical guidelines [19] and validated
screening tools [20], implementing a sophisticated approach to
identifying and responding to potential mental health concerns.
The system continuously monitors user interactions for primary
risk indicators, including expressions of suicidal ideation, severe
depression symptoms, and anxiety crisis signals, while also
tracking secondary indicators such as sleep disturbance patterns
and social withdrawal signs.
When potential risks are detected, the system implements a
graduated response protocol that has been carefully designed
to provide appropriate levels of support while avoiding
unnecessary escalation. For mild risk situations, the system
offers empathetic acknowledgment and self-help resources,
drawing from evidence-based interventions [21]. In cases of
moderate risk, the response includes more direct expressions
of concern and specific mental health resources, while severe
risk triggers an immediate crisis response protocol with direct
connections to professional support services. To address the challenge of potential false positives in risk
detection, we implemented a sophisticated validation system
that examines multiple contextual factors before triggering
interventions. This system uses NLP techniques to analyze the
broader context of user communications, helping to distinguish
between casual expressions and genuine indicators of distress.
Regular professional review of high-risk cases ensures the
ongoing refinement of detection algorithms and response
protocols, maintaining a balance between sensitivity and
specificity in risk assessment.: 1 (4.5%)
     All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.: 1 (4.5%)
     Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.: 1 (4.5%)
     In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model: 1 (4.5%)
     from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.: 1 (4.5%)
     The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.: 1 (4.5%)
     “After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9): 1 (4.5%)
     In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.: 1 (4.5%)
     see 4.1: 1 (4.5%)
     The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.: 1 (4.5%)
     The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.: 1 (4.5%)
     entire study is about this: 1 (4.5%)
     Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.: 1 (4.5%)
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response

4) Crisis Intervention and Ethical Safeguards: 1 (4.5%)
     Implicit in S-2: 1 (4.5%)
     Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language

Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language: 1 (4.5%)
     Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159: 1 (4.5%)
     they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values: 1 (4.5%)
     even though this is a crisis call intervention system, there is not detection of acute risk: 1 (4.5%)
     If suicidality or severe distress is detected, users are directed to
psychological support hotlines and blocked from further use to
ensure safety.: 1 (4.5%)

   Sample values (first 20):
     n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist

attending psychiatrist was checking responses
     even though this is a crisis call intervention system, there is not detection of acute risk
     they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values
     Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159
     Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language

Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language
     Implicit in S-2
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response

4) Crisis Intervention and Ethical Safeguards
     Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.
     entire study is about this
     The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.
     The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.
     Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.
     see 4.1
     In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.
     “After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9)
     The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.
     from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.
     In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model
     Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.
     All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.


82. s2_content_safety_considered
   ================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 114 (85.1%)
     y: 20 (14.9%)


83. s2_content_safety_notes
   ===========================
   Data type: object
   Total values: 134
   Non-null values: 23
   Null values: 111
   Unique values: 23
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 111 (482.6%)
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.: 1 (4.3%)
     A professional counselor with three years of experience 
monitored the entire conversation to ensure the safety of the 
counseling process: 1 (4.3%)
     A custom moderation application programming interface (API)
filters inputs and outputs, flagging inappropriate content before
LLM processing. Key safety features include the following.: 1 (4.3%)
     In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.: 1 (4.3%)
     All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.: 1 (4.3%)
     Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.: 1 (4.3%)
     All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.

However, no automatic screening pre-transmission: 1 (4.3%)
     This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc: 1 (4.3%)
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses: 1 (4.3%)
     Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.: 1 (4.3%)
     “OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9): 1 (4.3%)
     In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users: 1 (4.3%)
     Chatbots mostly preserved privacy and avoided harmful
content. : 1 (4.3%)
     Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.: 1 (4.3%)
     See e.g., "Strong Framing of Opinions and Suggestions": 1 (4.3%)
     LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11): 1 (4.3%)
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer

We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.: 1 (4.3%)
     All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results): 1 (4.3%)
     see 4.2 -> Safety considerations: 1 (4.3%)
     used reflection quality classifier: 1 (4.3%)
     During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.: 1 (4.3%)
     GPT-2 outputs are not checked: 1 (4.3%)
     discuss misinformation, over-validation, and ethical risks reported by users: 1 (4.3%)

   Sample values (first 20):
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.
     GPT-2 outputs are not checked
     During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.
     used reflection quality classifier
     see 4.2 -> Safety considerations
     All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results)
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer

We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.
     LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11)
     See e.g., "Strong Framing of Opinions and Suggestions"
     Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.
     Chatbots mostly preserved privacy and avoided harmful
content. 
     In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users
     “OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9)
     Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses
     This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc
     All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.

However, no automatic screening pre-transmission
     Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.
     All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.
     In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.


84. p1_on_premise_model_considered
   ==================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     n: 113 (84.3%)
     y: 20 (14.9%)
     y (some): 1 (0.7%)


85. p1_on_premise_model_notes
   =============================
   Data type: object
   Total values: 134
   Non-null values: 18
   Null values: 116
   Unique values: 17
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 116 (644.4%)
     GPT-2: 2 (11.1%)
     Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.
But not on-premise capable!: 1 (5.6%)
     Once the chatbot model was trained it was then 
deployed on a web page where users have access to it for interaction with the chatbot application. This was done through a command line provided by the Rasa Framework 
which directs input to and from the model interfacing with a 
web application via an endpoint provided on the Rasa endpoints file. : 1 (5.6%)
     Llama 3.2 used: 1 (5.6%)
     Jais-13B is on-premise: 1 (5.6%)
     T5 can be used on-premise and via cloud (not in the text). : 1 (5.6%)
     Llama 2, Falcon-7B: 1 (5.6%)
     Some are, others don´t: 1 (5.6%)
     Falcon7B : 1 (5.6%)
     falcon: 1 (5.6%)
     Llama and DeepSeek are: 1 (5.6%)
     	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.): 1 (5.6%)
     ChatGLM is open-source/self-hostable: 1 (5.6%)
     Gemini 1.5 Flash: 1 (5.6%)
     Partially: GPT not, Llama yes. : 1 (5.6%)
     InternLM2-7B is self-hostable: 1 (5.6%)
     GLM2-6B model : 1 (5.6%)


86. p2_privacy_awareness_considered
   ===================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 125 (93.3%)
     y: 9 (6.7%)


87. p2_privacy_awareness_notes
   ==============================
   Data type: object
   Total values: 134
   Non-null values: 11
   Null values: 123
   Unique values: 11
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 123 (1118.2%)
     
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. : 1 (9.1%)
     In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.
: 1 (9.1%)
     Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]: 1 (9.1%)
     The text data contains some private information… we… used ‘xxx’ to replace the original text: 1 (9.1%)
     The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).: 1 (9.1%)
     usage of Amazon AWS, no discussion of user privacy: 1 (9.1%)
     Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.: 1 (9.1%)
     Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.: 1 (9.1%)
     Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].: 1 (9.1%)
     Yes → Adheres to data protection principles, anonymized use
https://www.clareandme.com/post/what-happens-with-your-data-at-clare-me: 1 (9.1%)
     Ethical use of Reddit data with disclaimers in 3.3.1: 1 (9.1%)


88. e1_demographics_reporting_considered
   ========================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     n: 114 (85.1%)
     y: 19 (14.2%)
     n
: 1 (0.7%)


89. e1_demographics_reporting_notes
   ===================================
   Data type: object
   Total values: 134
   Non-null values: 20
   Null values: 114
   Unique values: 16
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 114 (570.0%)
     Table 1: 4 (20.0%)
     Table 2: 2 (10.0%)
     Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.: 1 (5.0%)
     
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.: 1 (5.0%)
     reported in multimedia appendix 3: 1 (5.0%)
     Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
: 1 (5.0%)
     f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).: 1 (5.0%)
     Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B: 1 (5.0%)
     Table-1: 1 (5.0%)
     Table 2 has just the bare minimum: 1 (5.0%)
     Appendix Table 2: 1 (5.0%)
     yes, in simulated DSPs (digital standardized patient): 1 (5.0%)
     In the current study, participants (N = 830) were 45.17 years old on average (SD = 16.56),
59.88% mentioned being in a current romantic relationship, and 18.07% of the sample
reported having ever engaged in couple therapy. Most participants identified as a woman
(50.60%), slightly fewer identified as a man (47.95%), and the remaining individuals identified
as non-binary or third-gender (0.24%), 0.12% preferred not to say, and 0.07% of the sample
did not answer. A majority of the sample identified as straight (83.25%), 7.83% of the sample
identified as bisexual, 2.65% as gay, 1.81% as asexual, 1.45% as lesbian, 0.72% as queer, and
0.60% preferred to not disclose. When considering race and ethnicity, most participants iden-
tified as non-Hispanic White (49.40%), followed by Black (18.80%), White Hispanic (16.87%),
Asian (5. %), Black Hispanic (0.84%), American Indian or Alaskan Native (0.12%), and the
remaining sample identified as other (8.43%), or preferred not to disclose (0.12%).: 1 (5.0%)
     The study included 20 participants aged 18 to 27 (mean 23.3,
SD 1.96) years with 60% (12/20) female and 40% (8/20) male.: 1 (5.0%)
     Table 3: 1 (5.0%)
     Finally, 49 participants were selected to participate in 
this study for the experiment. The age range of participants 
is 15–40 years old. Among them, 29 (59.18%) were male 
and 20 (40.82%) were female; 15 (30.61%) were middle 
school students, 6 (12.24%) were undergraduate students, 
12 (24.49%) were graduate students, and 16 (32.65%) were 
already working; 12 (24.49%) were 15–18 years old, 21 
(42.86%) were 18–24 years old, and 16 people (32.65%) 
were over 24 years old. All participants did not have a pro-
fessional background in psychology.: 1 (5.0%)


90. e2_outcomes_by_demographics_considered
   ==========================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 130 (97.0%)
     y: 4 (3.0%)


91. e2_outcomes_by_demographics_notes
   =====================================
   Data type: object
   Total values: 134
   Non-null values: 3
   Null values: 131
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 131 (4366.7%)
     see table 4!: 1 (33.3%)
     Table 3 and Table 4
section "Perceptions across demographic groups": 1 (33.3%)
     yes, in simulated DSPs (digital standardized patient): 1 (33.3%)


92. g1_early_discontinuation_considered
   =======================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 129 (96.3%)
     y: 5 (3.7%)


93. g1_early_discontinuation_notes
   ==================================
   Data type: object
   Total values: 134
   Non-null values: 5
   Null values: 129
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 129 (2580.0%)
     fig 6 participants flow: 1 (20.0%)
     Of the 654 participants who accepted and consented to the study,
105 (16.1%) did not finish the entire study. We speculate that
this dropout was caused by several factor: 1 (20.0%)
     Table 6 shows how many participants interacted only 0-15 mins daily: 1 (20.0%)
     dropouts noted: 1 (20.0%)
     See 3.2 Engagement patterns: 1 (20.0%)


94. g2_overuse_considered
   =========================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 129 (96.3%)
     y: 5 (3.7%)


95. g2_overuse_notes
   ====================
   Data type: object
   Total values: 134
   Non-null values: 5
   Null values: 129
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 129 (2580.0%)
     only a set number of sessions was administered: 1 (20.0%)
     number of messages sent is reported: 1 (20.0%)
     Table 6 shows daily interaction time over 30 mins: 1 (20.0%)
     Measures of the user experience with the generative and
rules-based DMHIs included user engagement (number of
sessions, total active days, and conversational exchanges): 1 (20.0%)
     Somehow: 
However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT’s overly validating nature, and privacy concerns: 1 (20.0%)


96. f1_validated_outcomes_considered
   ====================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 122 (91.0%)
     y: 12 (9.0%)


97. f1_validated_outcomes_notes
   ===============================
   Data type: object
   Total values: 134
   Non-null values: 15
   Null values: 119
   Unique values: 15
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 119 (793.3%)
     World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF): 1 (6.7%)
     unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures: 1 (6.7%)
     Tinnitus Handicap Inventory (THI): 1 (6.7%)
     use of the readiness ruler: 1 (6.7%)
     we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022): 1 (6.7%)
     Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7): 1 (6.7%)
     WAI-SR Bond is used, but this is not symptom or function scale: 1 (6.7%)
     Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair: 1 (6.7%)
     WAI-SR but is neither symptom nor function scale: 1 (6.7%)
     PHQ-9: 1 (6.7%)
     (CASES &) STAI: 1 (6.7%)
     Patient Health 
Questionnaire 9(PHQ-9), the Generalized Anxiety 
Disorder Questionnaire for the Diagnostic and Statistical 
Manual of Mental Disorders, Fourth Edition (DSM-IV)
(GAD-Q-IV), and the Weight Concerns Scale (WCS) within 
the Stanford–Washington University Eating Disorder
(SWED), as measures of depression, anxiety, and weight 
concerns, respectively. Therapeutic alliance 
(Working Alliance Inventory — Short Revised [WAI-SR]).: 1 (6.7%)
     Reliable improvement refers to a clinically significant
improvement in symptoms following a course of treatment and
is calculated as the score difference between the first and the
last validated clinical questionnaire completed. The types of
questionnaires patients complete are tailored to their specific
condition. For example, the Patient Health Questionnaire-9
(PHQ-9) [30] is used to measure depression symptom severity,
and the Generalized Anxiety Disorder-7 (GAD-7) [31] is used
to measure anxiety symptom severity. A clinically significant
improvement in symptoms is considered a change score ≥6 for
PHQ-9 or ≥4 for GAD-7 [26]: 1 (6.7%)
     UCLA, PHQ-D, PHQ-4, SWLS, Mini-SPIN: 1 (6.7%)
     PHQ-9, GAD-7, PANAS-P, PANAS-N, SWLS, SVS: 1 (6.7%)


98. f2_control_condition_considered
   ===================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 123 (91.8%)
     y: 11 (8.2%)


99. f2_control_condition_notes
   ==============================
   Data type: object
   Total values: 134
   Non-null values: 12
   Null values: 122
   Unique values: 12
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 122 (1016.7%)
     control group that received standard care: 1 (8.3%)
     After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. : 1 (8.3%)
     A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.: 1 (8.3%)
     We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions
BUT: only for different design decisions; no overall other intervention or no intervention as control group!: 1 (8.3%)
     Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract): 1 (8.3%)
     RCT: 1 (8.3%)
     three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract): 1 (8.3%)
     either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle

Control: GPT-4 out-of-the-box: 1 (8.3%)
     The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.: 1 (8.3%)
     To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.: 1 (8.3%)
     comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (: 1 (8.3%)
     Evident in whole study (e.g., Finally, it is worth noting that our choice of an active control group (that is, a non-trivial control group)...): 1 (8.3%)


100. i1_multilevel_feasibility_considered
   =========================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 133 (99.3%)
     y: 1 (0.7%)


101. i1_multilevel_feasibility_notes
   ====================================
   Data type: object
   Total values: 134
   Non-null values: 2
   Null values: 132
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 132 (6600.0%)
     acceptability data collected only from one stakeholder level (clinicians): 1 (50.0%)
     Somehow: user-reported perceptions from Reddit: 1 (50.0%)


102. i2_healthcare_integration_considered
   =========================================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     n: 125 (93.3%)
     y: 8 (6.0%)
     n : 1 (0.7%)


103. i2_healthcare_integration_notes
   ====================================
   Data type: object
   Total values: 134
   Non-null values: 9
   Null values: 125
   Unique values: 9
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 125 (1388.9%)
     Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions: 1 (11.1%)
     
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee: 1 (11.1%)
     Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.: 1 (11.1%)
     
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families: 1 (11.1%)
     Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].

See 6.2 and 7: 1 (11.1%)
     „Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“: 1 (11.1%)
     Integrating Socrates into environ-
ments such as hospitals or drug rehabilitation centers could
therefore provide substantial benefits. Patients could gain
additional time for psychological reflection beyond what current staffing constraints allow. Moreover, some individuals
might find it easier to discuss sensitive issues with Socrates
rather than in face-to-face interactions, potentially experienc-
ing less perceived judgment and accelerating psychological
change processes. Health care providers might also benefit
from this technology through more streamlined workflows
and reduced burnout risk.
: 1 (11.1%)
     Only: Integration with health care systems
• Investigate secure ways to integrate chatbot data with electronic health records, while maintaining user privacy.: 1 (11.1%)
     Speculative discussion on integration with clinical care, but not implemented: 1 (11.1%)


104. general_notes
   ==================
   Data type: object
   Total values: 134
   Non-null values: 22
   Null values: 112
   Unique values: 22
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 112 (509.1%)
     intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes: 1 (4.5%)
     This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy.: 1 (4.5%)
     Unsure about the metrics. We can discuss during the consensus. See table 4: 1 (4.5%)
     Comparative Analysis unclear in methodological aspects to me. : 1 (4.5%)
     Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz: 1 (4.5%)
     FuzzyWuzzy and NLU Confidence scores are provided as individual example values for illustration purposes, rather than as statistical overall evaluations or averages.

Is this study "really" suitable? Rasa itself does not include a large language model (LLM) like GPT or BERT; instead, it can integrate or be extended with LLMs or other NLP components if desired; Not described in this study. : 1 (4.5%)
     without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility: 1 (4.5%)
      : 1 (4.5%)
     Ethical Considerations: Which users were informed?: 1 (4.5%)
     Despite it being labeled an "empirical study", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. : 1 (4.5%)
     used metrics: EPIT-ONE framework, MITI, RoVERTa: 1 (4.5%)
     The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots.: 1 (4.5%)
     Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one.: 1 (4.5%)
     VR maybe as solution for the embodiment problem? : 1 (4.5%)
     Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory


Therapy type: undirected therapy resulting from interacting with ChatGPT with no prespecified initial prompt: 1 (4.5%)
     very short study : 1 (4.5%)
     No real evaluation. Only a sample conversatino is shown: 1 (4.5%)
     no alpha adjusting (table 3): 1 (4.5%)
     qualitative study - survey with 31 participants from recruited over Reddit: 1 (4.5%)
     "Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality.": 1 (4.5%)
     proof-of-concept prototype rather than a full-fledged clinical tool

No evaluation of the conversation system. Study only displayed some conversation sample "look this is good".: 1 (4.5%)
     All comparisons between AI and human counselors based on ML Models. 

This study also was a "transcript analysis" via BERT: 1 (4.5%)

   Sample values (first 20):
     intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes
     proof-of-concept prototype rather than a full-fledged clinical tool

No evaluation of the conversation system. Study only displayed some conversation sample "look this is good".
     "Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality."
     qualitative study - survey with 31 participants from recruited over Reddit
     no alpha adjusting (table 3)
     No real evaluation. Only a sample conversatino is shown
     very short study 
     Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory


Therapy type: undirected therapy resulting from interacting with ChatGPT with no prespecified initial prompt
     VR maybe as solution for the embodiment problem? 
     Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one.
     The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots.
     This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy.
     used metrics: EPIT-ONE framework, MITI, RoVERTa
     Despite it being labeled an "empirical study", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. 
     Ethical Considerations: Which users were informed?
      
     without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility
     FuzzyWuzzy and NLU Confidence scores are provided as individual example values for illustration purposes, rather than as statistical overall evaluations or averages.

Is this study "really" suitable? Rasa itself does not include a large language model (LLM) like GPT or BERT; instead, it can integrate or be extended with LLMs or other NLP components if desired; Not described in this study. 
     Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz
     Comparative Analysis unclear in methodological aspects to me. 


105. num_generative
   ===================
   Data type: int64
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     1: 106 (79.1%)
     2: 12 (9.0%)
     3: 10 (7.5%)
     4: 4 (3.0%)
     5: 1 (0.7%)
     8: 1 (0.7%)


106. num_non_generative
   =======================
   Data type: int64
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     0: 112 (83.6%)
     1: 22 (16.4%)


107. intended_intervention_type
   ===============================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 9
   Category: CATEGORICAL

   Value Counts:
     Informal support or unspecified type: 86 (64.2%)
     CBT: Cognitive restructuring: 13 (9.7%)
     CBT: Other techniques: 10 (7.5%)
     Motivational interviewing: 9 (6.7%)
     Multiple therapeutic orientations: 5 (3.7%)
     Other: 5 (3.7%)
     Peer support conversation: 3 (2.2%)
     Psychoanalysis: 2 (1.5%)
     Psychodynamic psychotherapy: 1 (0.7%)


108. inclusion_criteria
   =======================
   Data type: object
   Total values: 134
   Non-null values: 134
   Null values: 0
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     Unspecified: 89 (66.4%)
     No specific criteria: 29 (21.6%)
     People with some symptoms but not disorder (determined by symptom scales or questionnaires): 10 (7.5%)
     Recruited in in- or outpatient healthcare setting with no specific criteria: 5 (3.7%)
     Diagnosed mental disorder (ICD/DSM): 1 (0.7%)


109. primary_clinical_outcome
   =============================
   Data type: object
   Total values: 134
   Non-null values: 35
   Null values: 99
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 99 (282.9%)
     User experience assessment: 22 (62.9%)
     Validated symptom/function scale: 11 (31.4%)
     Other: 2 (5.7%)


================================================================================
END OF REPORT
================================================================================