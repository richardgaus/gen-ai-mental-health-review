================================================================================
DATA ANALYSIS REPORT
Input File: intermediate/fused_data.csv
Generated: 2025-10-03 01:20:06
================================================================================

DATASET OVERVIEW
----------------------------------------
Total rows: 488
Total columns: 111

COLUMN-BY-COLUMN ANALYSIS
----------------------------------------

1. Application Subtype
   ======================
   Data type: object
   Total values: 488
   Non-null values: 343
   Null values: 145
   Unique values: 26
   Category: CATEGORICAL

   Value Counts:
     Multi-turn chatbot: 212 (61.8%)
     [NULL]: 145 (42.3%)
     Chatbot: 55 (16.0%)
     One-turn chatbot (usually Q&A): 38 (11.1%)
     Other: : 6 (1.7%)
     Other: reframed thought generator: 4 (1.2%)
     Other: Spoken dialog system: 3 (0.9%)
     Other: one-turn recommendations generated by LLM: 2 (0.6%)
     Other: Emotional reflection generation: 2 (0.6%)
     Other: No specific subtype (AI use in general): 2 (0.6%)
     Other: conversation system with video, audio, and text input: 2 (0.6%)
     Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.: 2 (0.6%)
     Other: smartphone CBT app (Kokoro App): 1 (0.3%)
     Other: Multi-modal dialogue system: 1 (0.3%)
     Other: image generator: 1 (0.3%)
     Other: Unknown but I assume both multiturn and one turn for the forum answers.: 1 (0.3%)
     Other: Development of new Framework: 1 (0.3%)
     Other: speech-based conversation system: 1 (0.3%)
     Other: unclear: 1 (0.3%)
     Other: Spoken dialogue system: 1 (0.3%)
     Other: Multimodal dialog system: 1 (0.3%)
     Other: reflection generation: 1 (0.3%)
     Other: Journaling app with AI counselling : 1 (0.3%)
     Other: MI reflection generation: 1 (0.3%)
     Other: One-turn chatbot: 1 (0.3%)
     Other: "one turn chatbot", i.e., user inputs "feelings and confusion" and system makes analysis and outputs intervention text once. There is no turn-based interaction with the tool.: 1 (0.3%)
     Other: one-turn Q&A Chatbot: 1 (0.3%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 32
     Other: speech-based conversation system: 1
     Other: Spoken dialogue system: 1
     Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.: 2
     Other: One-turn chatbot: 1
     Other: "one turn chatbot", i.e., user inputs "feelings and confusion" and system makes analysis and outputs intervention text once. There is no turn-based interaction with the tool.: 1
     Other: conversation system with video, audio, and text input: 2
     Other: one-turn recommendations generated by LLM: 2
     Other: smartphone CBT app (Kokoro App): 1
     Other: reflection generation: 1
     Other: reframed thought generator: 4
     Other: one-turn Q&A Chatbot: 1
     Other: MI reflection generation: 1
     Other: Journaling app with AI counselling: 1
     Other: Spoken dialog system: 3
     Other: Multimodal dialog system: 1
     Other: Multi-modal dialogue system: 1
     Other: unclear: 1
     Other: Emotional reflection generation: 2
     Other: Development of new Framework: 1
     Other: Unknown but I assume both multiturn and one turn for the forum answers.: 1
     Other: image generator: 1
     Other: No specific subtype (AI use in general): 2

   Sample values (first 20):
     Chatbot
     Other: 
     Other: speech-based conversation system
     Other: Spoken dialogue system
     Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.
     Other: One-turn chatbot
     Other: "one turn chatbot", i.e., user inputs "feelings and confusion" and system makes analysis and outputs intervention text once. There is no turn-based interaction with the tool.
     Other: conversation system with video, audio, and text input
     Other: one-turn recommendations generated by LLM
     Other: smartphone CBT app (Kokoro App)
     Other: reflection generation
     Other: reframed thought generator
     Other: one-turn Q&A Chatbot
     Multi-turn chatbot
     Other: MI reflection generation
     Other: Journaling app with AI counselling 
     One-turn chatbot (usually Q&A)
     Other: Spoken dialog system
     Other: Multimodal dialog system
     Other: Multi-modal dialogue system


2. Application Subtype.1
   ========================
   Data type: object
   Total values: 488
   Non-null values: 40
   Null values: 448
   Unique values: 13
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 448 (1120.0%)
     Treatment fidelity feedback: 10 (25.0%)
     Other: : 7 (17.5%)
     Utterance suggestions: 7 (17.5%)
     Patient simulations: 5 (12.5%)
     Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition: 2 (5.0%)
     Other: No specific subtype (AI use in general): 2 (5.0%)
     Other: Probability of Advanced Tinitus Therapy success : 1 (2.5%)
     Other: OCD exposure hierarchy generation: 1 (2.5%)
     Other: OCD exposure hierarchy generation (therapy material generation): 1 (2.5%)
     Other: self efficacy enhancement: 1 (2.5%)
     Other: No specific application: 1 (2.5%)
     Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs: 1 (2.5%)
     Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs. No one particular application type.: 1 (2.5%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 11
     Other: Probability of Advanced Tinitus Therapy success: 1
     Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition: 2
     Other: OCD exposure hierarchy generation: 1
     Other: OCD exposure hierarchy generation (therapy material generation): 1
     Other: self efficacy enhancement: 1
     Other: No specific application: 1
     Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs: 1
     Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs. No one particular application type.: 1
     Other: No specific subtype (AI use in general): 2


3. Application Type
   ===================
   Data type: object
   Total values: 488
   Non-null values: 434
   Null values: 54
   Unique values: 14
   Category: CATEGORICAL

   Value Counts:
     Client-facing application: 342 (78.8%)
     Analysis of conversation transcripts: 56 (12.9%)
     [NULL]: 54 (12.4%)
     Therapist-facing application: 23 (5.3%)
     Other: Analysis of CBT diary data: 2 (0.5%)
     Other: Client-facing AND therapist-facing: 2 (0.5%)
     Other: Unsure between analysis of conversation transcripts or therapist-facing application with treatment fidelity feedback: 1 (0.2%)
     Other: Analysis of text-message conversations between clients and clinicians: 1 (0.2%)
     Other: Comparison between AI and human councellor recommendations: 1 (0.2%)
     Other: Analysis of conversation transcripts & client facing: 1 (0.2%)
     Other: Analysis of conversation transcripts; Client-facing application: 1 (0.2%)
     Other: Conversation Rewriting: 1 (0.2%)
     Other: : 1 (0.2%)
     Other: question-answering system based on
retrieval augmented generation — this approach allows the
system to generate answers based on a corpus of documents
curated by psychologists and psychiatrists: 1 (0.2%)
     Other: survey: 1 (0.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 12
     Other: Unsure between analysis of conversation transcripts or therapist-facing application with treatment fidelity feedback: 1
     Other: Analysis of CBT diary data: 2
     Other: Analysis of text-message conversations between clients and clinicians: 1
     Other: Comparison between AI and human councellor recommendations: 1
     Other: Analysis of conversation transcripts & client facing: 1
     Other: Analysis of conversation transcripts; Client-facing application: 1
     Other: Conversation Rewriting: 1
     Other: question-answering system based on: 1
     Other: Client-facing AND therapist-facing: 2
     Other: survey: 1


4. Automatic metrics:
   =====================
   Data type: float64
   Total values: 488
   Non-null values: 0
   Null values: 488
   Unique values: 0
   Category: CATEGORICAL

   Sample values (first 20):


5. Classification Benchmark quality (H/L)
   =========================================
   Data type: object
   Total values: 488
   Non-null values: 86
   Null values: 402
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 402 (467.4%)
     no benchmark: 28 (32.6%)
     L: 28 (32.6%)
     l: 15 (17.4%)
     H: 10 (11.6%)
     h: 4 (4.7%)
     Low: 1 (1.2%)


6. Classification How it compares against benchmark (B/S/W)
   ===========================================================
   Data type: object
   Total values: 488
   Non-null values: 87
   Null values: 401
   Unique values: 11
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 401 (460.9%)
     no benchmark: 28 (32.2%)
     B: 27 (31.0%)
     b: 11 (12.6%)
     W: 6 (6.9%)
     w: 5 (5.7%)
     s: 3 (3.4%)
     S: 2 (2.3%)
     -: 2 (2.3%)
     Better: 1 (1.1%)
     No benchmark: 1 (1.1%)
     unsure: 1 (1.1%)


7. Classification Notes on benchmark quality
   ============================================
   Data type: object
   Total values: 488
   Non-null values: 89
   Null values: 399
   Unique values: 67
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 399 (448.3%)
     Performance of cognitive distortion classification of non-generative BERT model is given (but it's non-generative): 2 (2.2%)
     Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM: 2 (2.2%)
     The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978: 2 (2.2%)
     benchmark is simple rule-based emotion recognition system. Task: classification of clustered dimensional values (five distinct categories): 2 (2.2%)
     Accuracy of the BERT helping skill classification model: 2 (2.2%)
     no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token: 2 (2.2%)
     Same Model (prompting scheme) but without contextual text data: 2 (2.2%)
     Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.: 2 (2.2%)
     Various classification metrics (accuracy, precision, recall, f1) for checking how "relevant" the responses are. No information is given on how "relevance" is measured here.: 2 (2.2%)
     benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative: 2 (2.2%)
     “coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” : 2 (2.2%)
     "training accuracy" of generative LLM. Not sure what this accurac is measuring.: 2 (2.2%)
     Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation): 2 (2.2%)
     evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation): 2 (2.2%)
     sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification: 2 (2.2%)
     F1 score, not quiet clear what of.: 2 (2.2%)
     Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).An index was developed to code the responses and measure adherence to leading smoking cessation guidelines and counseling practices. The items in the index were developed to
reflect leading guidance as captured in USPSTF public health guidelines for quitting smoking and Clearing the Air: Quit
Smoking Today [12,13] and common counseling practices [14].: 2 (2.2%)
     Models classified cases requiring intervention; BSI>1 and higher CR indicate better behavior sensitivity/consistency; DeepSeek/Wenxin/Claude > GPT-4. — Quote: “BSI and CR … DeepSeek 1.0662 / 0.8985 … GPT-4 1.0415 / 0.8250: 2 (2.2%)
     no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.: 2 (2.2%)
     Various classification metrics of BERT emotional distress detection. Benchmark: Bi-LSTM: 2 (2.2%)
     between the rule-based sentiment analysis module and the
sentiment analysis capabilities of the GPT-powered model: 2 (2.2%)
     f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.: 2 (2.2%)
     Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries) : 1 (1.1%)
     Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries)

Classification into relevant/irrelevant via all-MiniLM-L6-v2: 1 (1.1%)
     Classification into relevant/irrelevant via all-MiniLM-L6-v2: 1 (1.1%)
     Recall: 1 (1.1%)
     Recall@1 (Tables 3 and 4): 1 (1.1%)
     no benchmark: 1 (1.1%)
     Performance of the sentiment classification component.: 1 (1.1%)
     Accuracy: 1 (1.1%)
     Bi-LSTM for emotional distress detection: 1 (1.1%)
     yes for pair but this is not genAI: 1 (1.1%)
     Accuracy, F1 Score, Precision and Recall: 1 (1.1%)
     Woebot: 1 (1.1%)
     Precision, recall accuracy, F1 Score: 1 (1.1%)
     Confusion matrices for the BERT and LSTM models (Fig. 7 and 8): 1 (1.1%)
     benchmark are ratings by humans on MISC and Topics (see appendix eTable 2): 1 (1.1%)
     benchmark are ratings by humans on MISC and Topics (see appendix eTable 2). Task: classification of client messages into content types: 1 (1.1%)
     no benchmark. used metrics: F1, precision, recall, AUC. Task: classify mental health and other issues (mental health, suicide thoughts, family, physical health, etc.) in counseling sessions: 1 (1.1%)
     benchmark: TF-IDF-based classification via logistic regression model: 1 (1.1%)
     Benchmark are different models: 1 (1.1%)
     1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not): 1 (1.1%)
     Table 4: Benchmark is fine-tuned DistilBERT model. ChatGPT is compared against this and performs worse.
Table 2: Prompted GPT-3 model (text-davinci-003) is compared against DistilBERT.

1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not): 1 (1.1%)
     accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection.: 1 (1.1%)
     accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection. Task: Cognitive distortion detection in client questions.: 1 (1.1%)
     benchmark: other conversation systems: 1 (1.1%)
     benchmark: other conversation systems. Task: emotion classification in current utterance of help-seeker: 1 (1.1%)
     ChatGPT (GPT 3.5): 1 (1.1%)
     accuracy, precision, recall, F1 for classifying utterances: 1 (1.1%)
     accuracy, precision, recall, F1 for classifying utterances. Task: classification of mental health dialogue turns into safe/various types of unsafe responses. benchmark: fine-tuned BERT-base and RoBERTa-large: 1 (1.1%)
     Human Interrater agreement: 1 (1.1%)
     metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other).: 1 (1.1%)
     metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other). Task: classify crisis calls transcripts into 10 suicide risk labels.: 1 (1.1%)
     F1-Micro Score against human labeling: 1 (1.1%)
     they used f1 and kappa to evaluate the emotion labeling system. the benchmark was a human annotator.: 1 (1.1%)
     they used f1-micro and kappa to evaluate the emotion labeling system. the benchmark was a human annotator. Task: classify emotions in client utterances: 1 (1.1%)
     Several benchmarks used, including ones that were more recently developed. "In comparison, SPARTA-TAA obtains significant improvements over all
baselines.": 1 (1.1%)
     benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. for dialogue-act classification of conversational turns: 1 (1.1%)
     Several benchmarks used, including ones that were more recently developed. "In comparison, SPARTA-TAA obtains significant improvements over all
baselines."

benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. Task: Dialogue-act classification of conversational turns: 1 (1.1%)
     accuracy, F1, precision, recall for prediction of feeling based on automatic thought: 1 (1.1%)
     accuracy, F1, precision, recall for prediction of feeling based on automatic thought (feeling-thought pairs): 1 (1.1%)
     base BERT model with AUPRC <0.52: 1 (1.1%)
     Benchmark: BERT (no augmentation). metric: AUPRC: 1 (1.1%)
     base BERT model with AUPRC <0.52. Task: classifying cognitive distortions: 1 (1.1%)
     see below: 1 (1.1%)
     GPT4 scored 44/60 and Bard 46/60, but the study did not aim to compare those two,  : 1 (1.1%)
     Precision, Recall, Macro-F1: 1 (1.1%)

   Sample values (first 20):
     no benchmark. used metrics: F1, precision, recall, AUC. Task: classify mental health and other issues (mental health, suicide thoughts, family, physical health, etc.) in counseling sessions
     benchmark: TF-IDF-based classification via logistic regression model
     Benchmark are different models
     no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.
     1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)
     Table 4: Benchmark is fine-tuned DistilBERT model. ChatGPT is compared against this and performs worse.
Table 2: Prompted GPT-3 model (text-davinci-003) is compared against DistilBERT.

1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)
     accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection.
     accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection. Task: Cognitive distortion detection in client questions.
     benchmark: other conversation systems
     benchmark: other conversation systems. Task: emotion classification in current utterance of help-seeker
     ChatGPT (GPT 3.5)
     accuracy, precision, recall, F1 for classifying utterances
     accuracy, precision, recall, F1 for classifying utterances. Task: classification of mental health dialogue turns into safe/various types of unsafe responses. benchmark: fine-tuned BERT-base and RoBERTa-large
     sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification
     Human Interrater agreement
     metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other).
     metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other). Task: classify crisis calls transcripts into 10 suicide risk labels.
     F1-Micro Score against human labeling
     they used f1 and kappa to evaluate the emotion labeling system. the benchmark was a human annotator.
     they used f1-micro and kappa to evaluate the emotion labeling system. the benchmark was a human annotator. Task: classify emotions in client utterances


8. Classification Used (Y/N)
   ============================
   Data type: object
   Total values: 488
   Non-null values: 356
   Null values: 132
   Unique values: 8
   Category: CATEGORICAL

   Value Counts:
     n: 137 (38.5%)
     [NULL]: 132 (37.1%)
     N: 123 (34.6%)
     Y: 49 (13.8%)
     y: 42 (11.8%)
     Yes : 2 (0.6%)
     
Y: 1 (0.3%)
     Y?: 1 (0.3%)
     N : 1 (0.3%)


9. Contentual judgment:
   =======================
   Data type: float64
   Total values: 488
   Non-null values: 0
   Null values: 488
   Unique values: 0
   Category: CATEGORICAL

   Sample values (first 20):


10. Continuous Value Metrics Benchmark quality (H/L)
   ====================================================
   Data type: object
   Total values: 488
   Non-null values: 29
   Null values: 459
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 459 (1582.8%)
     no benchmark: 12 (41.4%)
     L: 7 (24.1%)
     l: 6 (20.7%)
     h: 2 (6.9%)
     No: 1 (3.4%)
     -: 1 (3.4%)


11. Continuous Value Metrics How it compares against benchmark (B/S/W)
   ======================================================================
   Data type: object
   Total values: 488
   Non-null values: 29
   Null values: 459
   Unique values: 10
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 459 (1582.8%)
     no benchmark: 10 (34.5%)
     B: 5 (17.2%)
     b: 4 (13.8%)
     w: 3 (10.3%)
     no benckmark: 2 (6.9%)
     Perplexity evaluations (B): 1 (3.4%)
     W: 1 (3.4%)
     s: 1 (3.4%)
     S: 1 (3.4%)
     -: 1 (3.4%)


12. Continuous Value Metrics Notes on benchmark quality
   =======================================================
   Data type: object
   Total values: 488
   Non-null values: 27
   Null values: 461
   Unique values: 19
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 461 (1707.4%)
     benchmark are ratings by humans on CTRS (see appendix eTable 2): 2 (7.4%)
     Metric: training loss. No benchmark: 2 (7.4%)
     Train loss: 2 (7.4%)
     Table 6 Accuracy of machine learning models in forecasting 7-day 
SAD (stress, anxiety, and depression) levels based on single text 
entries against extrapolation: 2 (7.4%)
     Comparison of chatbot-inferred PHQ-9 scores with human scorers via ICC and Cohen's kappa: 2 (7.4%)
     no benchmark. loss value: 2 (7.4%)
     benchmark is support vector regressor (lower capacity model): 2 (7.4%)
     Correlation between LLM suicide response ratings and expert ratings: 2 (7.4%)
     rule-based approach: " It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score": 1 (3.7%)
     benchmark is simple rule-based emotion recognition system.
second, better benchmark: state-of-the-art valence and arousal recognition systems from other papers (Park et al., etc.). still, this is no human. Task: rating of valence, arousal, dominance in conversations: 1 (3.7%)
     benchmark is simple rule-based emotion recognition system.
second, better benchmark: state-of-the-art valence and arousal recognition systems from other papers (Park et al., etc.). still, this is no human. Task: rating of valence, arousal, dominance in conversations

rule-based approach: " It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score": 1 (3.7%)
     ChatGPTbased conversations 
(Approach 1), fine-tuned DialoGPT transformer 
conversations (Approach 2), and fine-tuned DialoGPT 
transformer conversations combined with the GPT3 prompts 
API (Approach 3): 1 (3.7%)
     Woebot: 1 (3.7%)
     Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though (Table 10, 11): 1 (3.7%)
     Pearson, Spearman, Kendall’s Tau: 1 (3.7%)
     Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark): 1 (3.7%)
     Pearson, Spearman, Kendall’s Tau. Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark): 1 (3.7%)
     Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though.: 1 (3.7%)
     -: 1 (3.7%)


13. Continuous Value Metrics Used (Y/N)
   =======================================
   Data type: object
   Total values: 488
   Non-null values: 337
   Null values: 151
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     n: 166 (49.3%)
     [NULL]: 151 (44.8%)
     N: 142 (42.1%)
     y: 20 (5.9%)
     Y: 7 (2.1%)
     Yes: 1 (0.3%)
     Y (Perplexity): 1 (0.3%)


14. Country of first first author affiliation
   =============================================
   Data type: object
   Total values: 488
   Non-null values: 353
   Null values: 135
   Unique values: 45
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 135 (38.2%)
     USA: 97 (27.5%)
     India: 52 (14.7%)
     China: 52 (14.7%)
     Germany: 21 (5.9%)
     UK: 13 (3.7%)
     Other: Canada: 13 (3.7%)
     Other: Korea: 12 (3.4%)
     Other: Italy: 7 (2.0%)
     Other: Iran: 6 (1.7%)
     Other: Israel: 6 (1.7%)
     Other: Saudi Arabia: 5 (1.4%)
     Other: Australia: 5 (1.4%)
     Other: Turkey: 3 (0.8%)
     Other: Spain: 3 (0.8%)
     Other: Egypt: 3 (0.8%)
     Other: Switzerland: 3 (0.8%)
     Other: Iraq: 3 (0.8%)
     Other: New Zealand: 3 (0.8%)
     Other: Indonesia: 3 (0.8%)
     Other: Kenya: 3 (0.8%)
     Other: Slovenia: 3 (0.8%)
     Other: Vietnam: 3 (0.8%)
     Other: Japan: 3 (0.8%)
     Other: Romania: 3 (0.8%)
     Other: Pakistan: 3 (0.8%)
     Other: Finland: 2 (0.6%)
     Other: Sri Lanka: 2 (0.6%)
     Other: Philippines: 2 (0.6%)
     Other: United Arab Emirates: 2 (0.6%)
     Other: Mexico: 2 (0.6%)
     Other: korea: 1 (0.3%)
     Other: Kyrgyzstan: 1 (0.3%)
     Other: Russia: 1 (0.3%)
     Other: UAE: 1 (0.3%)
     Other: Lebanon: 1 (0.3%)
     Other: Poland: 1 (0.3%)
     Other: Portugal: 1 (0.3%)
     Other: mexico: 1 (0.3%)
     Other: Finnland: 1 (0.3%)
     Other: Bangladesh: 1 (0.3%)
     Other: Czech Republic: 1 (0.3%)
     Other: canada: 1 (0.3%)
     Other: Canada : 1 (0.3%)
     Other: SriLanka: 1 (0.3%)
     Other: japan: 1 (0.3%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 118
     Other: Portugal: 1
     Other: Australia: 5
     Other: Kyrgyzstan: 1
     Other: Canada: 14
     Other: Israel: 6
     Other: Russia: 1
     Other: Japan: 3
     Other: Korea: 12
     Other: Poland: 1
     Other: Bangladesh: 1
     Other: Saudi Arabia: 5
     Other: Italy: 7
     Other: Czech Republic: 1
     Other: Philippines: 2
     Other: Pakistan: 3
     Other: Romania: 3
     Other: canada: 1
     Other: Finland: 2
     Other: Finnland: 1
     Other: SriLanka: 1
     Other: Sri Lanka: 2
     Other: Indonesia: 3
     Other: Slovenia: 3
     Other: Iran: 6
     Other: Turkey: 3
     Other: Spain: 3
     Other: Switzerland: 3
     Other: japan: 1
     Other: mexico: 1
     Other: Mexico: 2
     Other: korea: 1
     Other: Iraq: 3
     Other: Lebanon: 1
     Other: United Arab Emirates: 2
     Other: UAE: 1
     Other: New Zealand: 3
     Other: Egypt: 3
     Other: Kenya: 3
     Other: Vietnam: 3

   Sample values (first 20):
     Other: Portugal
     USA
     UK
     Other: Australia
     India
     Other: Kyrgyzstan
     China
     Germany
     Other: Canada
     Other: Israel
     Other: Russia
     Other: Japan
     Other: Korea
     Other: Poland
     Other: Bangladesh
     Other: Saudi Arabia
     Other: Italy
     Other: Czech Republic
     Other: Philippines
     Other: Pakistan


15. Data Set Source
   ===================
   Data type: object
   Total values: 488
   Non-null values: 442
   Null values: 46
   Unique values: 25
   Category: CATEGORICAL

   Value Counts:
     External data set: 149 (33.7%)
     No dataset used for development or evaluation: 132 (29.9%)
     Self-collected data: 94 (21.3%)
     [NULL]: 46 (10.4%)
     External data set, modified: 28 (6.3%)
     Self-collected data but derived from external data set: 16 (3.6%)
     Other: unclear: 3 (0.7%)
     Other: External data set + a modified (translated) data set: 2 (0.5%)
     Other: External data set + Self-collected data : 1 (0.2%)
     Other: yes but unclear which: 1 (0.2%)
     Other: Multiple external data sets: 1 (0.2%)
     Other: Digitial Standardized Patients- no collection of data, only creation via GPT 3.5: 1 (0.2%)
     Other: emotional expressions from psychological counseling sessions: 1 (0.2%)
     Other: both self-collected and external data set : 1 (0.2%)
     Other: unknown: 1 (0.2%)
     Other: unspecified: 1 (0.2%)
     Other: "The mental health quiz module uses a custom dataset de-
rived from stress and anxiety indicators, consisting of 40
structured questions with multiple-choice options. Users
receive a random selection of 10 questions, ensuring
reliability and varied experience": 1 (0.2%)
     Other: reddit : 1 (0.2%)
     Other: External data set, but completely unknown: 1 (0.2%)
     Other: External + self-collected: 1 (0.2%)
     Other: External data set + self-collected: 1 (0.2%)
     Other: Unsure. Authors describe the data collection process as if they collected it themselves. But they state: "The dataset for the present study is derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com):: 1 (0.2%)
     Other: Translation of English datasets to Polish : 1 (0.2%)
     Other: Mix of self-collected and external: 1 (0.2%)
     Other: Mix of external and sel collected via data crawling: 1 (0.2%)
     Other: some dataset was used, but completely unknown characteristics: 1 (0.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 23
     Other: Mix of external and sel collected via data crawling: 1
     Other: Mix of self-collected and external: 1
     Other: Translation of English datasets to Polish: 1
     Other: unclear: 3
     Other: Unsure. Authors describe the data collection process as if they collected it themselves. But they state: "The dataset for the present study is derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com):: 1
     Other: External data set + self-collected: 1
     Other: External + self-collected: 1
     Other: "The mental health quiz module uses a custom dataset de-: 1
     Other: External data set, but completely unknown: 1
     Other: reddit: 1
     Other: External data set + Self-collected data: 1
     Other: unspecified: 1
     Other: unknown: 1
     Other: External data set + a modified (translated) data set: 2
     Other: both self-collected and external data set: 1
     Other: emotional expressions from psychological counseling sessions: 1
     Other: Digitial Standardized Patients- no collection of data, only creation via GPT 3.5: 1
     Other: Multiple external data sets: 1
     Other: yes but unclear which: 1
     Other: some dataset was used, but completely unknown characteristics: 1

   Sample values (first 20):
     External data set
     Self-collected data
     No dataset used for development or evaluation
     Self-collected data but derived from external data set
     Other: Mix of external and sel collected via data crawling
     Other: Mix of self-collected and external
     Other: Translation of English datasets to Polish 
     Other: unclear
     Other: Unsure. Authors describe the data collection process as if they collected it themselves. But they state: "The dataset for the present study is derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com):
     Other: External data set + self-collected
     Other: External + self-collected
     External data set, modified
     Other: "The mental health quiz module uses a custom dataset de-
rived from stress and anxiety indicators, consisting of 40
structured questions with multiple-choice options. Users
receive a random selection of 10 questions, ensuring
reliability and varied experience"
     Other: External data set, but completely unknown
     Other: reddit 
     Other: External data set + Self-collected data 
     Other: unspecified
     Other: unknown
     Other: External data set + a modified (translated) data set
     Other: both self-collected and external data set 


16. Day (1-31)
   ==============
   Data type: float64
   Total values: 488
   Non-null values: 454
   Null values: 34
   Unique values: 31
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 34 (7.5%)
     11.0: 24 (5.3%)
     3.0: 23 (5.1%)
     17.0: 22 (4.8%)
     18.0: 21 (4.6%)
     30.0: 21 (4.6%)
     5.0: 21 (4.6%)
     26.0: 20 (4.4%)
     28.0: 20 (4.4%)
     23.0: 20 (4.4%)
     2.0: 18 (4.0%)
     21.0: 18 (4.0%)
     10.0: 18 (4.0%)
     8.0: 17 (3.7%)
     12.0: 17 (3.7%)
     9.0: 17 (3.7%)
     13.0: 15 (3.3%)
     24.0: 15 (3.3%)
     15.0: 14 (3.1%)
     25.0: 13 (2.9%)
     19.0: 12 (2.6%)
     27.0: 11 (2.4%)
     7.0: 10 (2.2%)
     20.0: 10 (2.2%)
     16.0: 10 (2.2%)
     6.0: 9 (2.0%)
     4.0: 9 (2.0%)
     14.0: 8 (1.8%)
     31.0: 7 (1.5%)
     22.0: 6 (1.3%)
     29.0: 5 (1.1%)
     1.0: 3 (0.7%)

   Sample values (first 20):
     11.0
     9.0
     26.0
     4.0
     22.0
     14.0
     2.0
     17.0
     20.0
     28.0
     8.0
     5.0
     30.0
     12.0
     3.0
     19.0
     23.0
     16.0
     18.0
     21.0


17. Did users have actual psychopathology or were they unselected/people without mental health conditions?
   ==========================================================================================================
   Data type: object
   Total values: 488
   Non-null values: 218
   Null values: 270
   Unique values: 13
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 270 (123.9%)
     Unknown: 99 (45.4%)
     Unselected: 62 (28.4%)
     Other: : 20 (9.2%)
     Psychopathology: 17 (7.8%)
     Other: n.a.: 6 (2.8%)
     Other: not applicable: 5 (2.3%)
     Other: unknown: 2 (0.9%)
     Other: Not applicable, data is not on users: 2 (0.9%)
     Other: mixed: 1 (0.5%)
     Other: N/A: 1 (0.5%)
     Other: no users in study : 1 (0.5%)
     Other: convenience sample: 1 (0.5%)
     Other: Not applicable; Artificial cases.: 1 (0.5%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 20
     Other: not applicable: 5
     Other: unknown: 2
     Other: mixed: 1
     Other: N/A: 1
     Other: no users in study: 1
     Other: convenience sample: 1
     Other: n.a.: 6
     Other: Not applicable, data is not on users: 2
     Other: Not applicable; Artificial cases.: 1


18. Does data set contain synthetic data?
   =========================================
   Data type: object
   Total values: 488
   Non-null values: 212
   Null values: 276
   Unique values: 12
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 276 (130.2%)
     No: 147 (69.3%)
     Yes: 24 (11.3%)
     Other: : 18 (8.5%)
     Other: unknown: 11 (5.2%)
     Other: unspecified: 3 (1.4%)
     Other: Unknown: 3 (1.4%)
     Other: synthetic; human-generated (fictional): 1 (0.5%)
     Other: synthetic, human created: 1 (0.5%)
     Other: synthetic: human generated: 1 (0.5%)
     Other: AnnoMi no, self-collected yes: 1 (0.5%)
     Other: unspecified : 1 (0.5%)
     Other: Yes (hypothetical cases created by experts): 1 (0.5%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 23
     Other: synthetic; human-generated (fictional): 1
     Other: unknown: 11
     Other: synthetic, human created: 1
     Other: synthetic: human generated: 1
     Other: unspecified: 4
     Other: AnnoMi no, self-collected yes: 1
     Other: Unknown: 3
     Other: Yes (hypothetical cases created by experts): 1


19. E-1 Reporting of demographic information Considered in tool design? (Y/N)
   =============================================================================
   Data type: object
   Total values: 488
   Non-null values: 306
   Null values: 182
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 182 (59.5%)
     n: 142 (46.4%)
     N: 103 (33.7%)
     y: 30 (9.8%)
     Y: 26 (8.5%)
     N
: 2 (0.7%)
     Yes : 2 (0.7%)
     Y?: 1 (0.3%)


20. E-1 Reporting of demographic information if YES: Notes (paste text passage)
   ===============================================================================
   Data type: object
   Total values: 488
   Non-null values: 54
   Null values: 434
   Unique values: 29
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 434 (803.7%)
     Table 1: 7 (13.0%)
     Table 2: 4 (7.4%)
     Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
: 2 (3.7%)
     Table 3: 2 (3.7%)
     The study included 20 participants aged 18 to 27 (mean 23.3,
SD 1.96) years with 60% (12/20) female and 40% (8/20) male.: 2 (3.7%)
     yes, in simulated DSPs (digital standardized patient): 2 (3.7%)
     Appendix Table 2: 2 (3.7%)
     Table 2 has just the bare minimum: 2 (3.7%)
     Table-1: 2 (3.7%)
     f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).: 2 (3.7%)
     Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B: 2 (3.7%)
     Finally, 49 participants were selected to participate in 
this study for the experiment. The age range of participants 
is 15–40 years old. Among them, 29 (59.18%) were male 
and 20 (40.82%) were female; 15 (30.61%) were middle 
school students, 6 (12.24%) were undergraduate students, 
12 (24.49%) were graduate students, and 16 (32.65%) were 
already working; 12 (24.49%) were 15–18 years old, 21 
(42.86%) were 18–24 years old, and 16 people (32.65%) 
were over 24 years old. All participants did not have a pro-
fessional background in psychology.: 2 (3.7%)
     Some demographics of therapists are shown but only male/female, no ethnic etc. data: 2 (3.7%)
     Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.: 2 (3.7%)
     reported in multimedia appendix 3: 2 (3.7%)
     
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.: 2 (3.7%)
     see Table client demographic characteristics: 2 (3.7%)
     Some demographic information given: The clients were all above age 18 (Mage =
39.06, SD = 13.67, range 20–77), and most were women (58.9%).
Of the clients, 92% were native Hebrew speakers and 92% were
born in Israel. Of the clients, 53.5% had at least a bachelor’s degree;
53.5% were single and 8.9% were in a committed relationship
but unmarried; 23.2% were married and 14.2% were divorced
or widowed. : 2 (3.7%)
     Table 2 summarizes key descriptive statistics related to our
sample of task-taking therapists. Of the 978 task takers, 81.6%
(n = 798) were women. This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%). The majority of therapists had
at least three years of experience providing therapy and a
significant minority had 11 or more years of experience (n =
321, 32.8%). However, most therapists had much less
experience providing messaging therapy; 818 (83.6%) had
between zero and two years of messaging therapy experience: 1 (1.9%)
     articipants were largely Caucasian Amer-
ican (57.3%), followed by African American (23.5%), Asian
American (12.6%) and others/did not specify (6.6%): 1 (1.9%)
     In the current study, participants (N = 830) were 45.17 years old on average (SD = 16.56),
59.88% mentioned being in a current romantic relationship, and 18.07% of the sample
reported having ever engaged in couple therapy. Most participants identified as a woman
(50.60%), slightly fewer identified as a man (47.95%), and the remaining individuals identified
as non-binary or third-gender (0.24%), 0.12% preferred not to say, and 0.07% of the sample
did not answer. A majority of the sample identified as straight (83.25%), 7.83% of the sample
identified as bisexual, 2.65% as gay, 1.81% as asexual, 1.45% as lesbian, 0.72% as queer, and
0.60% preferred to not disclose. When considering race and ethnicity, most participants iden-
tified as non-Hispanic White (49.40%), followed by Black (18.80%), White Hispanic (16.87%),
Asian (5. %), Black Hispanic (0.84%), American Indian or Alaskan Native (0.12%), and the
remaining sample identified as other (8.43%), or preferred not to disclose (0.12%).: 1 (1.9%)
     See Table 2: 1 (1.9%)
     able 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
: 1 (1.9%)
     We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.: 1 (1.9%)
     see Table II: 1 (1.9%)
     See Table S1

We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.: 1 (1.9%)
     Participants were mostly male (71.1 %), White race (66.7 %), and college educated (82.2 %): 1 (1.9%)
     Table S1 and Appendix B: 1 (1.9%)
     Table 1 Study population characteristics
Male 
Total 5 
Age (26–30 years) 4;
(31–35 years) 1
Years of training 2.6 years (range:
1–3 years)
Swiss nationality/other nationality (EU) 0/5 
Psychiatry/other specialty/psychologist 5/0/0 
Female
Total 15
(26–30 years) 1; (31–35 years) 8;
(36–40 years) 5; (> 40 years) 1
2.4 years (range: 1–7 years)
Swiss nationality/other nationality (EU) 7/8
Psychiatry/other specialty/psychologist 4/4/7: 1 (1.9%)

   Sample values (first 20):
     Table-1
     Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.
     Table 2 summarizes key descriptive statistics related to our
sample of task-taking therapists. Of the 978 task takers, 81.6%
(n = 798) were women. This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%). The majority of therapists had
at least three years of experience providing therapy and a
significant minority had 11 or more years of experience (n =
321, 32.8%). However, most therapists had much less
experience providing messaging therapy; 818 (83.6%) had
between zero and two years of messaging therapy experience
     Some demographics of therapists are shown but only male/female, no ethnic etc. data
     Some demographic information given: The clients were all above age 18 (Mage =
39.06, SD = 13.67, range 20–77), and most were women (58.9%).
Of the clients, 92% were native Hebrew speakers and 92% were
born in Israel. Of the clients, 53.5% had at least a bachelor’s degree;
53.5% were single and 8.9% were in a committed relationship
but unmarried; 23.2% were married and 14.2% were divorced
or widowed. 
     We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.
     See Table S1

We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.
     
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.
     reported in multimedia appendix 3
     see Table client demographic characteristics
     able 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).

     Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).

     f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).
     Table S1 and Appendix B
     Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B
     Participants were mostly male (71.1 %), White race (66.7 %), and college educated (82.2 %)
     Table 1
     Table 2 has just the bare minimum
     see Table II
     Appendix Table 2


21. E-2 Outcomes reported by demographic subgroup Considered in tool design? (Y/N)
   ==================================================================================
   Data type: object
   Total values: 488
   Non-null values: 299
   Null values: 189
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 189 (63.2%)
     n: 161 (53.8%)
     N: 128 (42.8%)
     y: 8 (2.7%)
     Yes : 1 (0.3%)
     Y: 1 (0.3%)


22. E-2 Outcomes reported by demographic subgroup if YES: Notes (paste text passage)
   ====================================================================================
   Data type: object
   Total values: 488
   Non-null values: 7
   Null values: 481
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 481 (6871.4%)
     see table 4!: 2 (28.6%)
     yes, in simulated DSPs (digital standardized patient): 2 (28.6%)
     Table 3 and Table 4: 1 (14.3%)
     section "Perceptions across demographic groups": 1 (14.3%)
     Table 3 and Table 4
section "Perceptions across demographic groups": 1 (14.3%)


23. Embedding Similarity Benchmark quality (H/L)
   ================================================
   Data type: object
   Total values: 488
   Non-null values: 45
   Null values: 443
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 443 (984.4%)
     L: 22 (48.9%)
     l: 12 (26.7%)
     no benchmark: 8 (17.8%)
     L
: 1 (2.2%)
     H: 1 (2.2%)
     -: 1 (2.2%)


24. Embedding Similarity How it compares against benchmark (B/S/W)
   ==================================================================
   Data type: object
   Total values: 488
   Non-null values: 45
   Null values: 443
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 443 (984.4%)
     B: 19 (42.2%)
     b: 9 (20.0%)
     no benchmark: 9 (20.0%)
     s: 4 (8.9%)
     S: 2 (4.4%)
     w: 1 (2.2%)
     -: 1 (2.2%)


25. Embedding Similarity Notes on benchmark quality
   ===================================================
   Data type: object
   Total values: 488
   Non-null values: 42
   Null values: 446
   Unique values: 34
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 446 (1061.9%)
     benchmark is GPT-3.5. measure is BERTScore and sentence transformer score with dataset expert responses as reference: 2 (4.8%)
     Distance between embeddings of reference and output.: 2 (4.8%)
     benchmark are human responses.: 2 (4.8%)
     BERTScore: 2 (4.8%)
     seq2seq: 2 (4.8%)
     Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).: 2 (4.8%)
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.: 2 (4.8%)
     Best model of authors compared to models of other works. : 2 (4.8%)
     BERT Score, Sentence Transformer score: 1 (2.4%)
     -: 1 (2.4%)
     BERTScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.: 1 (2.4%)
     Wonky measurement: embedding similarity between chatbot responses and patient inputs: 1 (2.4%)
     Against chatGPT-3.5: 1 (2.4%)
     BLEUScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.: 1 (2.4%)
     PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (2.4%)
     Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (2.4%)
     PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.: 1 (2.4%)
     on average 0.93 (±0.03) similarity in the embedding space.

Wonky measurement: embedding similarity between chatbot responses and patient inputs: 1 (2.4%)
     on average 0.93 (±0.03) similarity in the embedding space.: 1 (2.4%)
     Cosine distance (see Eq. 4). no benchmark: 1 (2.4%)
     Semantic Similarity via cosine distance (see Eq. 4): 1 (2.4%)
     similar chinese language LLMs: 1 (2.4%)
     Semantic Similarity: 1 (2.4%)
     older LLM: 1 (2.4%)
     specificity is an embedding similarity metric here: 1 (2.4%)
     ML models (HRED, SEQ2SEQ): 1 (2.4%)
     Benchmark is other NLP model: 1 (2.4%)
     benchmark is a simple seq2seq model: 1 (2.4%)
     unsure, see page 8: 1 (2.4%)
     other models: 1 (2.4%)
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR: 1 (2.4%)
     metrics: BERTScore. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...): 1 (2.4%)
     DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR: 1 (2.4%)
     BERT; BERT- CRF; BERT-MCRF; ROBERTA: 1 (2.4%)

   Sample values (first 20):
     similar chinese language LLMs
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.
     DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR
     metrics: BERTScore. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...)
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR
     other models
     BERTScore
     benchmark are human responses.
     unsure, see page 8
     seq2seq
     benchmark is a simple seq2seq model
     Benchmark is other NLP model
     ML models (HRED, SEQ2SEQ)
     specificity is an embedding similarity metric here
     older LLM
     Semantic Similarity
     Cosine distance (see Eq. 4). no benchmark
     Semantic Similarity via cosine distance (see Eq. 4)
     Best model of authors compared to models of other works. 
     Wonky measurement: embedding similarity between chatbot responses and patient inputs


26. Embedding Similarity Used (Y/N)
   ===================================
   Data type: object
   Total values: 488
   Non-null values: 341
   Null values: 147
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     n: 162 (47.5%)
     [NULL]: 147 (43.1%)
     N: 132 (38.7%)
     Y: 31 (9.1%)
     y: 15 (4.4%)
     Y (Specifity): 1 (0.3%)


27. Expert Rating Benchmark quality (H/L)
   =========================================
   Data type: object
   Total values: 488
   Non-null values: 80
   Null values: 408
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 408 (510.0%)
     no benchmark: 26 (32.5%)
     h: 14 (17.5%)
     H: 14 (17.5%)
     L: 12 (15.0%)
     l: 12 (15.0%)
     -: 2 (2.5%)


28. Expert Rating How it compares against benchmark (B/S/W)
   ===========================================================
   Data type: object
   Total values: 488
   Non-null values: 82
   Null values: 406
   Unique values: 10
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 406 (495.1%)
     no benchmark: 30 (36.6%)
     B: 17 (20.7%)
     b: 13 (15.9%)
     w: 8 (9.8%)
     W: 5 (6.1%)
     s: 4 (4.9%)
     -: 2 (2.4%)
     same : 1 (1.2%)
     S: 1 (1.2%)
     S/B: 1 (1.2%)


29. Expert Rating Notes on benchmark quality
   ============================================
   Data type: object
   Total values: 488
   Non-null values: 85
   Null values: 403
   Unique values: 58
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 403 (474.1%)
     no benchmark: 6 (7.1%)
     Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.: 2 (2.4%)
     no benchmark.: 2 (2.4%)
     4.3 Human Evaluation Results: 2 (2.4%)
     To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.: 2 (2.4%)
     valuated by Gestalt psychotherapy trainees: 2 (2.4%)
     Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality: 2 (2.4%)
     Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.: 2 (2.4%)
     benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating: 2 (2.4%)
     benchmark is ground truth (human created reflections) and output of simple seq2seq model: 2 (2.4%)
     7 different safety questions: 2 (2.4%)
     Measures: Likert empathy rating, MITI global score. Benchmark: human peer supporter from Reddit post.: 2 (2.4%)
     likert-scale appropriateness rating of emotional reflection responses. compared GPT-4 against human counselor responses.: 2 (2.4%)
     Fluency, helpfulness, relevance, empathy, professionalism, evaluated by psychology graduate students on a 5-Likert scale. Benchmark are other LLM-based methods.: 2 (2.4%)
     Each chatbot response was independently categorized by 2
coders for their adherence on each item of the index.: 2 (2.4%)
     Human interactive evaluation (fluency, comforting, etc.), comparison with other models. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5).: 2 (2.4%)
     Binary scoring rubrics for GPT-4 responses (see Table 4): 2 (2.4%)
     ML models (HRED, SEQ2SEQ): 2 (2.4%)
     -: 2 (2.4%)
     metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2: 2 (2.4%)
     no benchmark. however, experts rated the chatbot highly in absolute terms.: 2 (2.4%)
     no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.): 2 (2.4%)
     no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination: 2 (2.4%)
     seq2seq: 1 (1.2%)
     psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set

Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model: 1 (1.2%)
     Relevance; CBT Structure; Helpfulness: 1 (1.2%)
     Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.: 1 (1.2%)
     We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance: 1 (1.2%)
     We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance

Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.: 1 (1.2%)
     compared to chatbots in custumer service: 1 (1.2%)
     To evaluate the quality of generated supportive responses, we
also conduct a human evaluation. We recruit 12 graduate stu-
dents, major in psychology, to annotate the responses.: 1 (1.2%)
     CAPE category ratings, no benchmark: 1 (1.2%)
     psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set: 1 (1.2%)
     dherence to an index developed
from the US Preventive Services Task Force public health guidelines for quitting smoking and counseling principles. Enough?: 1 (1.2%)
     Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model: 1 (1.2%)
     Other LLMs: 1 (1.2%)
     We invited three counselors to evaluate 
these conversation cases. Benchmark is "LLM-Counselor Support System": 1 (1.2%)
     The LLM-Counselor Support System: 1 (1.2%)
     The LLM-Counselor Support System. Benchmark: GPT-4 with zero-shot CoT: 1 (1.2%)
     Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.: 1 (1.2%)
     older and other LLM: 1 (1.2%)
     Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.

Measures: Relevance, CBT structure, helpfulness: 1 (1.2%)
     (reviewed transcripts for safety + MI fidelity) — Quote: “All authors… reviewed MICA session transcripts independently to identify any statements… inappropriate” : 1 (1.2%)
     Benchmark is other NLP model: 1 (1.2%)
     TBD - Unsure: 1 (1.2%)
     benchmark are human responses.: 1 (1.2%)
     the standard deviation
of ratings generated by DistilBERT (SD = 0.230) were much
closer to the distribution of human ratings (SD = 0.308) versus
that of the SVR (SD = 0.146: 1 (1.2%)
     human rating (read, prof, match): 1 (1.2%)
     Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale: 1 (1.2%)
     s. user experience: 1 (1.2%)
     unsure, see figure 5: 1 (1.2%)
     benchmark: woebot non-generative/rule-based.: 1 (1.2%)
     Measure: "Engagement score". Benchmark: Therapist-led sessions: 1 (1.2%)
     No relative comparisons where conducted; the experts rated in absolute terms: 1 (1.2%)
     qualified human supervisor: 1 (1.2%)
     rating scale to determine quality of backward looking reflections (Textbox 5). : 1 (1.2%)
     human therapists: 1 (1.2%)
     thematic content analysis by authors?: 1 (1.2%)

   Sample values (first 20):
     s. user experience
     no benchmark. however, experts rated the chatbot highly in absolute terms.
     Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model
     psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set
     psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set

Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model
     Relevance; CBT Structure; Helpfulness
     Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.
     Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.

Measures: Relevance, CBT structure, helpfulness
     metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2
     no benchmark
     no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.)
     rating scale to determine quality of backward looking reflections (Textbox 5). 
     No relative comparisons where conducted; the experts rated in absolute terms
     no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination
     the standard deviation
of ratings generated by DistilBERT (SD = 0.230) were much
closer to the distribution of human ratings (SD = 0.308) versus
that of the SVR (SD = 0.146
     benchmark are human responses.
     TBD - Unsure
     unsure, see figure 5
     seq2seq
     benchmark is ground truth (human created reflections) and output of simple seq2seq model


30. Expert Rating Used (Y/N)
   ============================
   Data type: object
   Total values: 488
   Non-null values: 353
   Null values: 135
   Unique values: 8
   Category: CATEGORICAL

   Value Counts:
     n: 151 (42.8%)
     [NULL]: 135 (38.2%)
     N: 107 (30.3%)
     Y: 50 (14.2%)
     y: 41 (11.6%)
     yes : 1 (0.3%)
     N
: 1 (0.3%)
     Yes : 1 (0.3%)
     N (if you consider the author him/herself an expert?): 1 (0.3%)


31. Field of Publication Outlet
   ===============================
   Data type: object
   Total values: 488
   Non-null values: 453
   Null values: 35
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings): 183 (40.4%)
     Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health): 96 (21.2%)
     Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems): 53 (11.7%)
     [NULL]: 35 (7.7%)
     Other (e.g., Humanities & Social Sciences Communications, generalist outlets): 32 (7.1%)
     Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports): 30 (6.6%)
     Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research): 30 (6.6%)
     Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering): 29 (6.4%)


32. F‑1 Validated clinical outcome measures used Considered in tool design? (Y/N)
   =================================================================================
   Data type: object
   Total values: 488
   Non-null values: 301
   Null values: 187
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 187 (62.1%)
     n: 146 (48.5%)
     N: 109 (36.2%)
     y: 25 (8.3%)
     Y: 19 (6.3%)
     Yes : 1 (0.3%)
     Y?: 1 (0.3%)


33. F‑1 Validated clinical outcome measures used if YES: Notes (paste text passage)
   ===================================================================================
   Data type: object
   Total values: 488
   Non-null values: 39
   Null values: 449
   Unique values: 26
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 449 (1151.3%)
     Tinnitus Handicap Inventory (THI): 2 (5.1%)
     Reliable improvement refers to a clinically significant
improvement in symptoms following a course of treatment and
is calculated as the score difference between the first and the
last validated clinical questionnaire completed. The types of
questionnaires patients complete are tailored to their specific
condition. For example, the Patient Health Questionnaire-9
(PHQ-9) [30] is used to measure depression symptom severity,
and the Generalized Anxiety Disorder-7 (GAD-7) [31] is used
to measure anxiety symptom severity. A clinically significant
improvement in symptoms is considered a change score ≥6 for
PHQ-9 or ≥4 for GAD-7 [26]: 2 (5.1%)
     Patient Health 
Questionnaire 9(PHQ-9), the Generalized Anxiety 
Disorder Questionnaire for the Diagnostic and Statistical 
Manual of Mental Disorders, Fourth Edition (DSM-IV)
(GAD-Q-IV), and the Weight Concerns Scale (WCS) within 
the Stanford–Washington University Eating Disorder
(SWED), as measures of depression, anxiety, and weight 
concerns, respectively. Therapeutic alliance 
(Working Alliance Inventory — Short Revised [WAI-SR]).: 2 (5.1%)
     Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair: 2 (5.1%)
     Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7): 2 (5.1%)
     PHQ-8 was measured in clients: 2 (5.1%)
     use of the readiness ruler: 2 (5.1%)
     we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022): 2 (5.1%)
     PHQ-9: 2 (5.1%)
     FIS-T is validated: 2 (5.1%)
     World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF): 2 (5.1%)
     unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures: 2 (5.1%)
     PHQ-9, GAD-7, PANAS-P, PANAS-N, SWLS, SVS: 2 (5.1%)
     The Outcome Rating Scale (ORS), Profile of Mood States: 1 (2.6%)
     We included patients aged between 18 to 65 years who
were currently undergoing psychiatric inpatient care and
had received a mental health disorder diagnosis according
to the DSM-5. Each patient
’
s assistant physician evaluated
the presence of criteria for a DSM-5 diagnosis.: 1 (2.6%)
     WAI-SR but is neither symptom nor function scale: 1 (2.6%)
     Sam Scale?: 1 (2.6%)
     measured phq-9, gad, etc. in patients but did not predict those from the transcripts: 1 (2.6%)
     CASES & STAI: 1 (2.6%)
     (CASES &) STAI: 1 (2.6%)
     various measures: 1 (2.6%)
     patients have a diagnosed anxiety disorder : 1 (2.6%)
     higher reliable improvement, recovery, and reliable recovery rates” (Position: Results, p. 1): 1 (2.6%)
     measured phq-9, gad, etc. in patients: 1 (2.6%)
     DSM 5: 1 (2.6%)
     Self-Assessment Manikin Scale + heart rate variability: 1 (2.6%)

   Sample values (first 20):
     We included patients aged between 18 to 65 years who
were currently undergoing psychiatric inpatient care and
had received a mental health disorder diagnosis according
to the DSM-5. Each patient
’
s assistant physician evaluated
the presence of criteria for a DSM-5 diagnosis.
     World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF)
     unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures
     FIS-T is validated
     The Outcome Rating Scale (ORS), Profile of Mood States
     measured phq-9, gad, etc. in patients
     measured phq-9, gad, etc. in patients but did not predict those from the transcripts
     PHQ-9
     Tinnitus Handicap Inventory (THI)
     use of the readiness ruler
     PHQ-8 was measured in clients
     patients have a diagnosed anxiety disorder 
     we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022)
     Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7)
     Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair
     WAI-SR but is neither symptom nor function scale
     Sam Scale?
     Self-Assessment Manikin Scale + heart rate variability
     CASES & STAI
     (CASES &) STAI


34. F‑2 Control condition present Considered in tool design? (Y/N)
   ==================================================================
   Data type: object
   Total values: 488
   Non-null values: 300
   Null values: 188
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 188 (62.7%)
     n: 152 (50.7%)
     N: 114 (38.0%)
     Y: 22 (7.3%)
     y: 12 (4.0%)


35. F‑2 Control condition present if YES: Notes (paste text passage)
   ====================================================================
   Data type: object
   Total values: 488
   Non-null values: 33
   Null values: 455
   Unique values: 22
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 455 (1378.8%)
     Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group).: 2 (6.1%)
     comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (: 2 (6.1%)
     To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.: 2 (6.1%)
     The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.: 2 (6.1%)
     three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract): 2 (6.1%)
     Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract): 2 (6.1%)
     RCT: 2 (6.1%)
     A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.: 2 (6.1%)
     After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. : 2 (6.1%)
     control group that received standard care: 2 (6.1%)
     Evident in whole study (e.g., Finally, it is worth noting that our choice of an active control group (that is, a non-trivial control group)...): 2 (6.1%)
     Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group): 1 (3.0%)
     We compare MENTALER with the following baselines… CoT, TPE, ReAct, Cue-CoT, Chameleon: 1 (3.0%)
     We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions
BUT: only for different design decisions; no overall other intervention or no intervention as control group!: 1 (3.0%)
     They were then randomly allocated to either the con-
trol or intervention group using the 
“
coin flip
” method: 1 (3.0%)
     We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions: 1 (3.0%)
     comparison with Woebot — Quote: “system outperformed Woebot … in reducing stress … and anxiety levels” (Position: Results): 1 (3.0%)
     either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle: 1 (3.0%)
     Control: GPT-4 out-of-the-box: 1 (3.0%)
     either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle

Control: GPT-4 out-of-the-box: 1 (3.0%)
     Waitlist control: 1 (3.0%)
     We compared the clinical outcomes of individuals who signed
up to use the AI-enabled therapy support tool (the intervention
group) with those of individuals who did not (the control group):: 1 (3.0%)

   Sample values (first 20):
     They were then randomly allocated to either the con-
trol or intervention group using the 
“
coin flip
” method
     control group that received standard care
     After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. 
     A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.
     We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions
     We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions
BUT: only for different design decisions; no overall other intervention or no intervention as control group!
     Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group)
     Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group).
     We compare MENTALER with the following baselines… CoT, TPE, ReAct, Cue-CoT, Chameleon
     Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract)
     RCT
     three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract)
     comparison with Woebot — Quote: “system outperformed Woebot … in reducing stress … and anxiety levels” (Position: Results)
     either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle
     Control: GPT-4 out-of-the-box
     either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle

Control: GPT-4 out-of-the-box
     The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.
     Waitlist control
     To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.
     comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (


36. G‑1 Early‑discontinuation data reported Considered in tool design? (Y/N)
   ============================================================================
   Data type: object
   Total values: 488
   Non-null values: 295
   Null values: 193
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 193 (65.4%)
     n: 156 (52.9%)
     N: 126 (42.7%)
     y: 11 (3.7%)
     Y: 2 (0.7%)


37. G‑1 Early‑discontinuation data reported if YES: Notes (paste text passage)
   ==============================================================================
   Data type: object
   Total values: 488
   Non-null values: 12
   Null values: 476
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 476 (3966.7%)
     fig 6 participants flow: 2 (16.7%)
     Of the 654 participants who accepted and consented to the study,
105 (16.1%) did not finish the entire study. We speculate that
this dropout was caused by several factor: 2 (16.7%)
     Table 6 shows how many participants interacted only 0-15 mins daily: 2 (16.7%)
     dropouts noted: 2 (16.7%)
     See 3.2 Engagement patterns: 2 (16.7%)
     Figures 3 and 6: 1 (8.3%)
     Patients using the AI-enabled therapy support tool exhibited … fewer dropouts from treatment.” (Position: Results, p. 1): 1 (8.3%)


38. G‑2 Over‑use reported or prevented Considered in tool design? (Y/N)
   =======================================================================
   Data type: object
   Total values: 488
   Non-null values: 296
   Null values: 192
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 192 (64.9%)
     n: 158 (53.4%)
     N: 127 (42.9%)
     y: 9 (3.0%)
     Y: 2 (0.7%)


39. G‑2 Over‑use reported or prevented if YES: Notes (paste text passage)
   =========================================================================
   Data type: object
   Total values: 488
   Non-null values: 11
   Null values: 477
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 477 (4336.4%)
     only a set number of sessions was administered: 2 (18.2%)
     number of messages sent is reported: 2 (18.2%)
     Table 6 shows daily interaction time over 30 mins: 2 (18.2%)
     Measures of the user experience with the generative and
rules-based DMHIs included user engagement (number of
sessions, total active days, and conversational exchanges): 2 (18.2%)
     Somehow: 
However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT’s overly validating nature, and privacy concerns: 2 (18.2%)
     Figure 6: 1 (9.1%)


40. If Study Type == Empirical research involving an LLM: Development Approach
   ==============================================================================
   Data type: object
   Total values: 488
   Non-null values: 368
   Null values: 120
   Unique values: 44
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 120 (32.6%)
     Only prompting: 99 (26.9%)
     Prompting + other modules: 76 (20.7%)
     Only fine-tuning: 63 (17.1%)
     Fine-tuning + other modules: 51 (13.9%)
     Fine-tuning + other modules; Prompting + other modules: 9 (2.4%)
     Other: : 7 (1.9%)
     Only fine-tuning; Only prompting: 6 (1.6%)
     Prompting + other modules; Other: RAG: 4 (1.1%)
     Other: fine-tuning + custom pipeline (see Reflection Generation Training): 2 (0.5%)
     Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier: 2 (0.5%)
     Only fine-tuning; Prompting + other modules: 2 (0.5%)
     Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers: 2 (0.5%)
     Other: RAG: 2 (0.5%)
     Other: unknown: 2 (0.5%)
     Other: Fine-tuning plus other elements like clustering: 2 (0.5%)
     Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.: 2 (0.5%)
     Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework: 2 (0.5%)
     Other: multiple different:
selection of thinking traps: fine-tuning
writing of reframes: retrieval-enhanced in-context learning: 2 (0.5%)
     Other: fine-tuning + transfer learning: 2 (0.5%)
     Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization: 2 (0.5%)
     Only prompting; Other: RAG: 2 (0.5%)
     Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response: 2 (0.5%)
     Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.: 2 (0.5%)
     Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only: 2 (0.5%)
     Other: both original training of the transformer model and subsequent fine-tuning: 2 (0.5%)
     Other: None: 1 (0.3%)
     Other: Assessment of OCD-Coach: 1 (0.3%)
     Other: N/A: 1 (0.3%)
     Any reinforcement learning; Other: proposed a dynamic adversarial test method based on cross-variation” (Position: Abstract in excerpt): 1 (0.3%)
     Other: None of these : 1 (0.3%)
     Other: logistic regression classifier: 1 (0.3%)
     Other: PARTNER: 1 (0.3%)
     Other: "we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset"
"The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output"

BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture: 1 (0.3%)
     Other: BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture: 1 (0.3%)
     Other: "we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset"
"The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output": 1 (0.3%)
     Other: Other: self-developed and trained transformer architecture: 1 (0.3%)
     Other: own: 1 (0.3%)
     Other: fine-tuning + retrieval and content expansion: 1 (0.3%)
     Other: fine-tuning + retrieval and context expansion: 1 (0.3%)
     Other: fine-tuning + wrapper.
they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.: 1 (0.3%)
     Other: they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.: 1 (0.3%)
     Other: not sure, "READER is built on transformer to jointly predict a potential dialogue-act for the
next utterance and to generate an appropriate
response": 1 (0.3%)
     Other: Fine-tuning of DialogGPT. This fine-tuned model is integrated into a custom chatbot pipeline, together with ChatGPT 3.5: 1 (0.3%)
     Other: The chatbot model was trained using supervised learning with the Rasa framework on a domain-specific counseling dataset. This does not involve fine-tuning a large pre-trained language model (LLM) or prompting, but rather training a specialized model for intent classification and dialogue management.: 1 (0.3%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 48
     Other: logistic regression classifier: 1
     Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier: 2
     Other: Fine-tuning of DialogGPT. This fine-tuned model is integrated into a custom chatbot pipeline, together with ChatGPT 3.5: 1
     Other: both original training of the transformer model and subsequent fine-tuning: 2
     Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only: 2
     Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.: 2
     Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response: 2
     Other: not sure, "READER is built on transformer to jointly predict a potential dialogue-act for the: 1
     Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization: 2
     Other: fine-tuning + transfer learning: 2
     Other: they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.: 1
     Other: fine-tuning + wrapper.: 1
     Other: fine-tuning + retrieval and context expansion: 1
     Other: fine-tuning + retrieval and content expansion: 1
     Other: Fine-tuning plus other elements like clustering: 2
     Other: own: 1
     Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework: 2
     Other: fine-tuning + custom pipeline (see Reflection Generation Training): 2
     Other: multiple different:: 2
     Other: Other: self-developed and trained transformer architecture: 1
     Other: "we developed a transformer-based model for: 2
     Other: BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture: 1
     Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers: 2
     Other: PARTNER: 1
     Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.: 2
     Other: unknown: 2
     Other: RAG: 2
     Other: None: 1
     Other: Assessment of OCD-Coach: 1
     Other: N/A: 1
     Other: None of these: 1
     Other: The chatbot model was trained using supervised learning with the Rasa framework on a domain-specific counseling dataset. This does not involve fine-tuning a large pre-trained language model (LLM) or prompting, but rather training a specialized model for intent classification and dialogue management.: 1

   Sample values (first 20):
     Other: logistic regression classifier
     Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier
     Prompting + other modules
     Other: 
     Only fine-tuning
     Other: Fine-tuning of DialogGPT. This fine-tuned model is integrated into a custom chatbot pipeline, together with ChatGPT 3.5
     Other: both original training of the transformer model and subsequent fine-tuning
     Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only
     Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.
     Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response
     Other: not sure, "READER is built on transformer to jointly predict a potential dialogue-act for the
next utterance and to generate an appropriate
response"
     Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization
     Other: fine-tuning + transfer learning
     Other: they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.
     Other: fine-tuning + wrapper.
they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.
     Other: fine-tuning + retrieval and context expansion
     Other: fine-tuning + retrieval and content expansion
     Other: Fine-tuning plus other elements like clustering
     Other: own
     Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework


41. Intervention Type
   =====================
   Data type: object
   Total values: 488
   Non-null values: 439
   Null values: 49
   Unique values: 50
   Category: CATEGORICAL

   Value Counts:
     Unspecified, might include formal therapy methods: 215 (49.0%)
     Informal counseling (e.g., emotional support conversation): 57 (13.0%)
     [NULL]: 49 (11.2%)
     Other CBT techniques: 31 (7.1%)
     CBT: Motivational interviewing: 23 (5.2%)
     CBT: Cognitive restructuring: 21 (4.8%)
     Mix of formal therapy methods: 15 (3.4%)
     CBT: Cognitive restructuring; Other CBT techniques: 10 (2.3%)
     Other CBT techniques; Unspecified, might include formal therapy methods: 8 (1.8%)
     Peer support conversation: 4 (0.9%)
     Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods: 3 (0.7%)
     Psychoanalysis: 2 (0.5%)
     Unspecified, might include formal therapy methods; Other: Gestalt Therapy techniques, Gestalt supervision: 2 (0.5%)
     Other: Digital journaling with AI counselling  : 2 (0.5%)
     CBT: Cognitive restructuring; Informal counseling (e.g., emotional support conversation): 2 (0.5%)
     Psychoanalysis; Unspecified, might include formal therapy methods: 2 (0.5%)
     Psychodynamic psychotherapy; Unspecified, might include formal therapy methods: 2 (0.5%)
     Other: Person Centered Therapy (PCT) (Carl Rogers): 2 (0.5%)
     Peer support conversation; Informal counseling (e.g., emotional support conversation): 2 (0.5%)
     Other: positive psychology intervention: 2 (0.5%)
     CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Informal counseling (e.g., emotional support conversation): 2 (0.5%)
     Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods: 2 (0.5%)
     Other: 2 (0.5%)
     Other: "Autism counselling": 1 (0.2%)
     Other: no intervention: 1 (0.2%)
     Other: LLMs did not provide support but instead compared to clinician responses using the SIRI-2 scoring system (= "rating assessment").: 1 (0.2%)
     CBT: Cognitive restructuring; Unspecified, might include formal therapy methods: 1 (0.2%)
     CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques: 1 (0.2%)
     CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Unspecified, might include formal therapy methods: 1 (0.2%)
     Other: The chatbot's architecture is built on a 
robust knowledge base encompassing evidence-based therapeutic techniques, including cognitive-behavioral therapy (CBT) principles and motivational interviewing 
strategies.: 1 (0.2%)
     CBT: Motivational interviewing; Other CBT techniques; Other: The chatbot's architecture is built on a 
robust knowledge base encompassing evidence-based therapeutic techniques, including cognitive-behavioral therapy (CBT) principles and motivational interviewing 
strategies.: 1 (0.2%)
     Informal counseling (e.g., emotional support conversation); Other: counselor-training simulation: 1 (0.2%)
     CBT: Motivational interviewing; Informal counseling (e.g., emotional support conversation); Other: “if possible,” obtain a commitment to start the treatment medication, buprenorphine. : 1 (0.2%)
     CBT: Motivational interviewing; Informal counseling (e.g., emotional support conversation): 1 (0.2%)
     Unspecified, might include formal therapy methods; Other: counselor-training simulation: 1 (0.2%)
     Other: classification model: 1 (0.2%)
     Peer support conversation; Unspecified, might include formal therapy methods: 1 (0.2%)
     Other CBT techniques; Unspecified, might include formal therapy methods; Other: A strategy is selected and adapted according to the metrics, determined in the previous part. The text serves to mitigate the user’s mental health problems based mostly on CBT, and, if the forecasted trend is negative, to try to break 
that trend. To ensure that the user follows the selected strategy, the text on CBT is wrapped in a persuasion strategy.: 1 (0.2%)
     Other: classification: 1 (0.2%)
     Other: Gestalt-Therapy: 1 (0.2%)
     Other: Bedtime stories: 1 (0.2%)
     Other CBT techniques; Informal counseling (e.g., emotional support conversation): 1 (0.2%)
     CBT: Motivational interviewing; Unspecified, might include formal therapy methods: 1 (0.2%)
     CBT: Cognitive restructuring; Psychodynamic psychotherapy; Systemic therapy; Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Other: ChatGBT as a therapist: 1 (0.2%)
     Other: Perception of AI-generated responses in mental health support: 1 (0.2%)
     Other CBT techniques; Other: ACT, ET, MBCT, DBT: 1 (0.2%)
     Other: Person-centered therapy: 1 (0.2%)
     Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Unspecified, might include formal therapy methods: 1 (0.2%)
     Psychodynamic psychotherapy: 1 (0.2%)
     Other: unclear: 1 (0.2%)
     Other: unsure: 1 (0.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 18
     Other: classification model: 1
     Other: unclear: 1
     Other: Person Centered Therapy (PCT) (Carl Rogers): 2
     Other: Person-centered therapy: 1
     Other: Perception of AI-generated responses in mental health support: 1
     Other: Digital journaling with AI counselling: 2
     Other: Bedtime stories: 1
     Other: Gestalt-Therapy: 1
     Other: classification: 1
     Other: "Autism counselling": 1
     Other: no intervention: 1
     Other: The chatbot's architecture is built on a: 1
     Other: LLMs did not provide support but instead compared to clinician responses using the SIRI-2 scoring system (= "rating assessment").: 1
     Other: positive psychology intervention: 2
     Other: unsure: 1

   Sample values (first 20):
     Other: classification model
     Unspecified, might include formal therapy methods
     Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods
     Informal counseling (e.g., emotional support conversation)
     Other CBT techniques
     CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Informal counseling (e.g., emotional support conversation)
     Mix of formal therapy methods
     Other: unclear
     CBT: Cognitive restructuring; Other CBT techniques
     CBT: Motivational interviewing
     Psychodynamic psychotherapy
     Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Unspecified, might include formal therapy methods
     CBT: Cognitive restructuring
     Peer support conversation
     Peer support conversation; Informal counseling (e.g., emotional support conversation)
     Other: Person Centered Therapy (PCT) (Carl Rogers)
     Other: Person-centered therapy
     Other CBT techniques; Other: ACT, ET, MBCT, DBT
     Other: Perception of AI-generated responses in mental health support
     CBT: Cognitive restructuring; Psychodynamic psychotherapy; Systemic therapy; Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Other: ChatGBT as a therapist


42. Is data set public?
   =======================
   Data type: object
   Total values: 488
   Non-null values: 215
   Null values: 273
   Unique values: 9
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 273 (127.0%)
     No: 102 (47.4%)
     Yes: 87 (40.5%)
     Other: : 18 (8.4%)
     Other: unspecified: 3 (1.4%)
     Other: synthetic human-generated: 1 (0.5%)
     Other: AnnoMi yes, self-collected no: 1 (0.5%)
     Other: unspecified : 1 (0.5%)
     Other: Unknown as the datasets are not fully described: 1 (0.5%)
     Other: probably yes, but name not specified. : 1 (0.5%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 8
     Other: synthetic human-generated: 1
     Other: unspecified: 4
     Other: AnnoMi yes, self-collected no: 1
     Other: Unknown as the datasets are not fully described: 1
     Other: probably yes, but name not specified.: 1


43. I‑1 Multilevel feasibility/acceptability data collected Considered in tool design? (Y/N)
   ============================================================================================
   Data type: object
   Total values: 488
   Non-null values: 305
   Null values: 183
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 183 (60.0%)
     n: 170 (55.7%)
     N: 124 (40.7%)
     Y: 8 (2.6%)
     y: 2 (0.7%)
     somewhat: 1 (0.3%)


44. I‑1 Multilevel feasibility/acceptability data collected if YES: Notes (paste text passage)
   ==============================================================================================
   Data type: object
   Total values: 488
   Non-null values: 10
   Null values: 478
   Unique values: 8
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 478 (4780.0%)
     acceptability data collected only from one stakeholder level (clinicians): 2 (20.0%)
     Somehow: user-reported perceptions from Reddit: 2 (20.0%)
     Human evaluations relied on two groups: 
mental healthcare professionals and researchers who believe they are suffering mental 
health issue. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness and satisfaction of the 
chatbot, its conversation quality, and potential areas of 
improvement. : 1 (10.0%)
     rated by both patients and professionals: 1 (10.0%)
     Yes (trainee evaluations of AI vs. human) — Quote: “evaluated by Gestalt psychotherapy trainees using a Likert scale rating” (Position: Abstract): 1 (10.0%)
     content validity with clinicians and SUs to improve the relevance and clarity of the items: 1 (10.0%)
      Quote: “therapist, patient, and AI agent … proof-of-concept in this article illustrates … therapeutic value” (Position: Clinical section, p. 11): 1 (10.0%)
     both CMs and MHPs were surveyed: 1 (10.0%)


45. I‑2 Healthcare‑integration considerations addressed Considered in tool design? (Y/N)
   ========================================================================================
   Data type: object
   Total values: 488
   Non-null values: 312
   Null values: 176
   Unique values: 9
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 176 (56.4%)
     n: 161 (51.6%)
     N: 111 (35.6%)
     Y: 25 (8.0%)
     y: 9 (2.9%)
     N : 2 (0.6%)
     Yes : 1 (0.3%)
     somewhat: 1 (0.3%)
     maybe?: 1 (0.3%)
     a little: 1 (0.3%)


46. I‑2 Healthcare‑integration considerations addressed if YES: Notes (paste text passage)
   ==========================================================================================
   Data type: object
   Total values: 488
   Non-null values: 41
   Null values: 447
   Unique values: 30
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 447 (1090.2%)
     
"Automation of time-consuming tasks in iCBT could, through a positive lens, lead to
improved cost-effectiveness, which is an important point in often over-encumbered and
underfinanced psychiatry treatment and care contexts."
"
Fully automated iCBT, including the prediction of emotional states coupled with a CA
in charge of the iCBT with no human therapist involvement, would be both unwanted
and unethical. For legal reasons, having a clinical professional involved and ultimately
responsible for treatment is mandatory today and unlikely to change in the foreseeable
future. Only hybrid solutions of man-machine co-involvement are therefore further
discussed here. One such hybrid scenario would be the sole automation of emotion
recognition. This scenario starts with initial machine recognition of emotional states
derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional
states can then be fed to a human clinician as decision support. In theory, this could
render an improved understanding of a patient’s emotional state and also change of
state across time during iCBT. This could ultimately improve treatment tailoring and
effectiveness through the patient perceiving the therapist as more empathic, strengthening
the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy
work modules to better suit the patient’s emotional state"
" An interesting
but largely untested scenario would be extended automation of emotional recognition
coupled with therapist-supported CA treatment. This would involve not only the potential
benefit of emotion recognition discussed above but also cost-effective semi-automated
treatment. One such implementation would be that the emotionally informed CA drafts
empathically written therapy responses to the patient’s messages and the human therapist
then scrutinizes the responses and signs off on them with or without making prior changes.
A major portion of iCBT costs come from therapists spending time drafting responses to
patients in the treatment portal, unlocking a major potential for cost-saving strategies. An
additional downside risk with this scenario would be that the human therapist—due to
stress or other human factors—signs off on written responses of lower therapeutic quality.
Proper training and structured follow-up of therapists are likely required in this scenario,
which in turn may offset some of the cost-effectiveness of the approach. That stated since
a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored
CA seems in accordance with that overarching aim of iCBT.": 2 (4.9%)
     Only: Integration with health care systems
• Investigate secure ways to integrate chatbot data with electronic health records, while maintaining user privacy.: 2 (4.9%)
     Integrating Socrates into environ-
ments such as hospitals or drug rehabilitation centers could
therefore provide substantial benefits. Patients could gain
additional time for psychological reflection beyond what current staffing constraints allow. Moreover, some individuals
might find it easier to discuss sensitive issues with Socrates
rather than in face-to-face interactions, potentially experienc-
ing less perceived judgment and accelerating psychological
change processes. Health care providers might also benefit
from this technology through more streamlined workflows
and reduced burnout risk.
: 2 (4.9%)
     „Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“: 2 (4.9%)
     
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families: 2 (4.9%)
     
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee: 2 (4.9%)
     Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions: 2 (4.9%)
     Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.: 2 (4.9%)
     Speculative discussion on integration with clinical care, but not implemented: 2 (4.9%)
     this is all: Automatic emotion recognition models can be integrated into existing feedback systems to provide an indication of the levels of
emotional coherence in psychotherapy sessions and allow therapists
to modify their interventions accordingly.: 2 (4.9%)
     "Specific applications of machine learning–based evaluation may go beyond after-action call-level summaries of statement-level evaluations that cannot affect the quality of care received by the caller in the moment. Specifically, real-time evaluation may also be possible. For example, machine-learning–enabled call center software could label interventions as they occur, providing just-in-time feedback and suggestions to call takers when specific interventions are not provided." (...) "..stakeholders include call takers who can use this information to learn from previous calls, supervisors and administrators who may better identify and then direct resources to call takers who are struggling with their performance, and funders who may begin to include population-level data on the quality of conversations - complementing existing metrics on answer rates and wait times." : 2 (4.9%)
     Primarily, as text-based systems, LLMs
cannot interpret crucial nonverbal cues
and they lack cultural sensitivity, which
is essential for a comprehensive mental
health assessment [2]. Secondly, research
shows that users tend to over-rely on the
accuracy of information delivered by LLMs,
which may discourage patients from seeking timely professional mental health assistance rather than facilitating it [5, 24].
Thirdly, technical challenges, such as AI
hallucination and insufficient technological literacy among users or connectivity,
further compromise the accessibility and
reliability of these systems. Fourthly, given
thelimited regulationof AI, the storageand
potential use of sensitive mental health
data for LLM training pose significant data
privacy concerns, underscoring the need
for robust legal and ethical frameworks. Finally, LLMs lack real-time safety protocols,
such as direct emergency service contact,
presenting a critical limitation when users
are in immediate danger.: 1 (2.4%)
     facilitate access to professional care by linking users with nearby psychiatrists or psychology centers: 1 (2.4%)
     We want to emphasize, though, that our chatbot, while 
a step forward, is not a replacement for human therapists. 
Instead, we envision it as an auxiliary resource that can 
provide support in scenarios where human resources are 
stretched thin, or as an additional tool to complement 
traditional therapeutic processes.

 Integration with clinical systems: future research 
could look into integrating the chatbot with chatbot to provide more personalized and context-
aware support. It could also facilitate better 
coordination with healthcare professionals, alerting 
them when the chatbot identifies potential serious 
concerns.
existing clinical systems. This would allow the : 1 (2.4%)
     widespread use of LLM
technology—including new companies already drawing on
LLM technology for mental health care [29]—could reach a
much wider audience of individuals coping with depression and
suicidal thoughts. To date, a common guardrail has been for
LLMs to produce “hard stops”, in which individuals are referred
to 988 or another suicide prevention hotline. While such referrals
may be beneficial, they also artificially circumscribed
interactions in a way that could be taken as a missed opportunity.: 1 (2.4%)
     eal-world observational study … in 5 of the United Kingdom’s National Health Service Talking Therapies services” (Position: Methods, p. 1): 1 (2.4%)
     e.g. " The suggestion to integrate structured therapy 
techniques and conduct research on the efficacy 
of chatbots in mental health support indicates the 
potential for collaboration between the chatbot 
application and professionals in the field. Future 
policies could encourage partnerships between 
developers and professionals in psychology and 
counseling to enhance the chatbots effectiveness 
and provide a well-rounded approach to mental 
healthcare (23).": 1 (2.4%)
     somehow, since the benchmark were clinical guidelines:

Responses were analyzed for their adherence to an index developed from the US Preventive Services Task Force public health guidelines for quitting smoking and counseling principles.: 1 (2.4%)
     “SAFE-AI protocol … provides systematic guidelines for implementing AI-based externalization techniques … while ensuring cultural sensitivity, gender representation, and therapeutic authenticity.”: 1 (2.4%)
     Future research should explore how this tool can be integrated into the decision-making processes of mental health professionals, health care organizations, and technology developers.” (Position: p. 10): 1 (2.4%)
     some description in following paragraph but too superficial:

The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).: 1 (2.4%)
     The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).: 1 (2.4%)
     Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].

See 6.2 and 7: 1 (2.4%)
     See 6.2 and 7: 1 (2.4%)
     Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].: 1 (2.4%)
     Based on the analysis of the results, we will consider that implementing a custom
ChatGPT in a robot to support ADHD therapies presents considerable potential. Its advan-
tages include personalization, where ChatGPT can tailor interactions to each patient’s unique
needs and responses, thus potentially enhancing the therapeutic experience. Consistency is
another benefit, as a ChatGPT-equipped robot can offer stable support, which is crucial in
ADHD therapies where routine and predictability play vital roles. Additionally, ChatGPT’s
capability to understand and generate natural language can significantly increase the en-
gagement and interactivity of therapy sessions for children with ADHD, thus making them
more dynamic and effective. However, we also found a range of complex challenges and
considerations. Key among these is the need for emotional intelligence.
Significantly, ChatGPT and its use by a robotic assistant should complement, not
replace, human therapists (as also mentioned by the studies [14,21]), as the human element
is critical, especially for children with ADHD. : 1 (2.4%)
     n conclusion, our pilot study suggests that AI chatbots,
such as ChatGPT
, 
can positively impact the quality of life
of psychiatric inpatients while being well-received. Despite
the limitations inherent in a pilot study
, 
such as a small
sample size and the use of convenience sampling, our find-
ings provide valuable insights into the potential role of AI
in psychiatric care: 1 (2.4%)
     The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in need.: 1 (2.4%)
     Fully automated iCBT, including the prediction of emotional states coupled with a CA in charge of the iCBT with no human therapist involvement, would be both unwanted and unethical. For legal reasons, having a clinical professional involved and ultimately responsible for treatment is mandatory today and unlikely to change in the foreseeable future. Only hybrid solutions of man-machine co-involvement are therefore further discussed here. One such hybrid scenario would be the sole automation of emotion recognition. This scenario starts with initial machine recognition of emotional states derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional states can then be fed to a human clinician as decision support. In theory, this could render an improved understanding of a patient’s emotional state and also change of state across time during iCBT. This could ultimately improve treatment tailoring and effectiveness through the patient perceiving the therapist as more empathic, strengthening the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy work modules to better suit the patient’s emotional state. A potential risk with this approach would be the drift of the therapist’s own emotional assessment influenced by the machine’s estimated emotional state of the patient which may be wrong or biased. An interesting but largely untested scenario would be extended automation of emotional recognition coupled with therapist-supported CA treatment. This would involve not only the potential benefit of emotion recognition discussed above but also cost-effective semi-automated treatment. One such implementation would be that the emotionally informed CA drafts empathically written therapy responses to the patient’s messages and the human therapist then scrutinizes the responses and signs off on them with or without making prior changes. A major portion of iCBT costs come from therapists spending time drafting responses to patients in the treatment portal, unlocking a major potential for cost-saving strategies. An additional downside risk with this scenario would be that the human therapist—due to stress or other human factors—signs off on written responses of lower therapeutic quality. Proper training and structured follow-up of therapists are likely required in this scenario, which in turn may offset some of the cost-effectiveness of the approach. That stated since a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored CA seems in accordance with that overarching aim of iCBT.: 1 (2.4%)
     Authors mention that model is designed as a "solution that could be used on common 
hardware by users without having the knowledge and 
technical proficiency regarding large language models": 1 (2.4%)

   Sample values (first 20):
     n conclusion, our pilot study suggests that AI chatbots,
such as ChatGPT
, 
can positively impact the quality of life
of psychiatric inpatients while being well-received. Despite
the limitations inherent in a pilot study
, 
such as a small
sample size and the use of convenience sampling, our find-
ings provide valuable insights into the potential role of AI
in psychiatric care
     We want to emphasize, though, that our chatbot, while 
a step forward, is not a replacement for human therapists. 
Instead, we envision it as an auxiliary resource that can 
provide support in scenarios where human resources are 
stretched thin, or as an additional tool to complement 
traditional therapeutic processes.

 Integration with clinical systems: future research 
could look into integrating the chatbot with chatbot to provide more personalized and context-
aware support. It could also facilitate better 
coordination with healthcare professionals, alerting 
them when the chatbot identifies potential serious 
concerns.
existing clinical systems. This would allow the 
     e.g. " The suggestion to integrate structured therapy 
techniques and conduct research on the efficacy 
of chatbots in mental health support indicates the 
potential for collaboration between the chatbot 
application and professionals in the field. Future 
policies could encourage partnerships between 
developers and professionals in psychology and 
counseling to enhance the chatbots effectiveness 
and provide a well-rounded approach to mental 
healthcare (23)."
     Authors mention that model is designed as a "solution that could be used on common 
hardware by users without having the knowledge and 
technical proficiency regarding large language models"
     Based on the analysis of the results, we will consider that implementing a custom
ChatGPT in a robot to support ADHD therapies presents considerable potential. Its advan-
tages include personalization, where ChatGPT can tailor interactions to each patient’s unique
needs and responses, thus potentially enhancing the therapeutic experience. Consistency is
another benefit, as a ChatGPT-equipped robot can offer stable support, which is crucial in
ADHD therapies where routine and predictability play vital roles. Additionally, ChatGPT’s
capability to understand and generate natural language can significantly increase the en-
gagement and interactivity of therapy sessions for children with ADHD, thus making them
more dynamic and effective. However, we also found a range of complex challenges and
considerations. Key among these is the need for emotional intelligence.
Significantly, ChatGPT and its use by a robotic assistant should complement, not
replace, human therapists (as also mentioned by the studies [14,21]), as the human element
is critical, especially for children with ADHD. 
     "Specific applications of machine learning–based evaluation may go beyond after-action call-level summaries of statement-level evaluations that cannot affect the quality of care received by the caller in the moment. Specifically, real-time evaluation may also be possible. For example, machine-learning–enabled call center software could label interventions as they occur, providing just-in-time feedback and suggestions to call takers when specific interventions are not provided." (...) "..stakeholders include call takers who can use this information to learn from previous calls, supervisors and administrators who may better identify and then direct resources to call takers who are struggling with their performance, and funders who may begin to include population-level data on the quality of conversations - complementing existing metrics on answer rates and wait times." 
     this is all: Automatic emotion recognition models can be integrated into existing feedback systems to provide an indication of the levels of
emotional coherence in psychotherapy sessions and allow therapists
to modify their interventions accordingly.
     The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).
     some description in following paragraph but too superficial:

The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).
     
"Automation of time-consuming tasks in iCBT could, through a positive lens, lead to
improved cost-effectiveness, which is an important point in often over-encumbered and
underfinanced psychiatry treatment and care contexts."
"
Fully automated iCBT, including the prediction of emotional states coupled with a CA
in charge of the iCBT with no human therapist involvement, would be both unwanted
and unethical. For legal reasons, having a clinical professional involved and ultimately
responsible for treatment is mandatory today and unlikely to change in the foreseeable
future. Only hybrid solutions of man-machine co-involvement are therefore further
discussed here. One such hybrid scenario would be the sole automation of emotion
recognition. This scenario starts with initial machine recognition of emotional states
derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional
states can then be fed to a human clinician as decision support. In theory, this could
render an improved understanding of a patient’s emotional state and also change of
state across time during iCBT. This could ultimately improve treatment tailoring and
effectiveness through the patient perceiving the therapist as more empathic, strengthening
the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy
work modules to better suit the patient’s emotional state"
" An interesting
but largely untested scenario would be extended automation of emotional recognition
coupled with therapist-supported CA treatment. This would involve not only the potential
benefit of emotion recognition discussed above but also cost-effective semi-automated
treatment. One such implementation would be that the emotionally informed CA drafts
empathically written therapy responses to the patient’s messages and the human therapist
then scrutinizes the responses and signs off on them with or without making prior changes.
A major portion of iCBT costs come from therapists spending time drafting responses to
patients in the treatment portal, unlocking a major potential for cost-saving strategies. An
additional downside risk with this scenario would be that the human therapist—due to
stress or other human factors—signs off on written responses of lower therapeutic quality.
Proper training and structured follow-up of therapists are likely required in this scenario,
which in turn may offset some of the cost-effectiveness of the approach. That stated since
a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored
CA seems in accordance with that overarching aim of iCBT."
     Fully automated iCBT, including the prediction of emotional states coupled with a CA in charge of the iCBT with no human therapist involvement, would be both unwanted and unethical. For legal reasons, having a clinical professional involved and ultimately responsible for treatment is mandatory today and unlikely to change in the foreseeable future. Only hybrid solutions of man-machine co-involvement are therefore further discussed here. One such hybrid scenario would be the sole automation of emotion recognition. This scenario starts with initial machine recognition of emotional states derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional states can then be fed to a human clinician as decision support. In theory, this could render an improved understanding of a patient’s emotional state and also change of state across time during iCBT. This could ultimately improve treatment tailoring and effectiveness through the patient perceiving the therapist as more empathic, strengthening the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy work modules to better suit the patient’s emotional state. A potential risk with this approach would be the drift of the therapist’s own emotional assessment influenced by the machine’s estimated emotional state of the patient which may be wrong or biased. An interesting but largely untested scenario would be extended automation of emotional recognition coupled with therapist-supported CA treatment. This would involve not only the potential benefit of emotion recognition discussed above but also cost-effective semi-automated treatment. One such implementation would be that the emotionally informed CA drafts empathically written therapy responses to the patient’s messages and the human therapist then scrutinizes the responses and signs off on them with or without making prior changes. A major portion of iCBT costs come from therapists spending time drafting responses to patients in the treatment portal, unlocking a major potential for cost-saving strategies. An additional downside risk with this scenario would be that the human therapist—due to stress or other human factors—signs off on written responses of lower therapeutic quality. Proper training and structured follow-up of therapists are likely required in this scenario, which in turn may offset some of the cost-effectiveness of the approach. That stated since a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored CA seems in accordance with that overarching aim of iCBT.
     Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions
     
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee
     The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in need.
     Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.
     
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families
     Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].
     See 6.2 and 7
     Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].

See 6.2 and 7
     „Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“


47. LLM as a judge Benchmark quality (H/L)
   ==========================================
   Data type: object
   Total values: 488
   Non-null values: 17
   Null values: 471
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 471 (2770.6%)
     L: 8 (47.1%)
     no benchmark: 4 (23.5%)
     h: 2 (11.8%)
     l: 1 (5.9%)
     H: 1 (5.9%)
     -: 1 (5.9%)


48. LLM as a judge How it compares against benchmark (B/S/W)
   ============================================================
   Data type: object
   Total values: 488
   Non-null values: 16
   Null values: 472
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 472 (2950.0%)
     no benchmark: 4 (25.0%)
     B: 4 (25.0%)
     b: 2 (12.5%)
     -: 2 (12.5%)
     w: 2 (12.5%)
     B/S: 1 (6.2%)
     W: 1 (6.2%)


49. LLM as a judge Notes on benchmark quality
   =============================================
   Data type: object
   Total values: 488
   Non-null values: 18
   Null values: 470
   Unique values: 13
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 470 (2611.1%)
     The DeepEval framework + MQG-RAG Evaluation: 2 (11.1%)
     ChatGPT-4 rated using the categories below: 2 (11.1%)
     Ragas is LLM-as-a-judge: 2 (11.1%)
     LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.: 2 (11.1%)
     Qianwen to automatically evaluate the counseling dialogue (esction: C. ): 2 (11.1%)
     TBD - Unsure: 1 (5.6%)
     GPT 4 better than GPT3.5: 1 (5.6%)
     BERT-based empathy scoring model.
benchmarks are other LLMs, not fine tuned to empathy tasks: 1 (5.6%)
     no benchmark: 1 (5.6%)
     human responses from existing literature: 1 (5.6%)
     No clear indexmodel, so not necessarly a benchmark (?): 1 (5.6%)
     Each LLM’s own “intervention decision” used to compute metrics. — Quote: “intervention decisions recorded and results labeled … quantitative analysis uses BSI and CR”: 1 (5.6%)
     No indexmodel but three equaly ranked models were compared : 1 (5.6%)


50. LLM as a judge Used (Y/N)
   =============================
   Data type: object
   Total values: 488
   Non-null values: 333
   Null values: 155
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     n: 181 (54.4%)
     [NULL]: 155 (46.5%)
     N: 132 (39.6%)
     Y: 14 (4.2%)
     y: 6 (1.8%)


51. Language of data set
   ========================
   Data type: object
   Total values: 488
   Non-null values: 214
   Null values: 274
   Unique values: 22
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 274 (128.0%)
     English: 117 (54.7%)
     Chinese: 29 (13.6%)
     Other: unknown: 22 (10.3%)
     Other: : 20 (9.3%)
     Other: Korean: 3 (1.4%)
     Other: Persian: 3 (1.4%)
     Other: Arabic: 2 (0.9%)
     Other: unknown, probably english or hindi: 2 (0.9%)
     Other: Spanish: 2 (0.9%)
     Other: Indonesian: 2 (0.9%)
     Other: spanish: 1 (0.5%)
     Other: English, Arabic: 1 (0.5%)
     Other: unknown. Italian?: 1 (0.5%)
     Other: Unsure: 1 (0.5%)
     Other: indian: 1 (0.5%)
     Other: English and Indonesian : 1 (0.5%)
     Other: Italien: 1 (0.5%)
     Other: german, english : 1 (0.5%)
     Other: unspecified: 1 (0.5%)
     Japanese: 1 (0.5%)
     Other: German: 1 (0.5%)
     Other: Either Chines (most likely) or English: 1 (0.5%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 47
     Other: unknown: 22
     Other: German: 1
     Other: Korean: 3
     Other: unspecified: 1
     Other: german, english: 1
     Other: Italien: 1
     Other: indian: 1
     Other: English and Indonesian: 1
     Other: Indonesian: 2
     Other: Persian: 3
     Other: spanish: 1
     Other: Spanish: 2
     Other: Unsure: 1
     Other: unknown, probably english or hindi: 2
     Other: unknown. Italian?: 1
     Other: English, Arabic: 1
     Other: Arabic: 2
     Other: Either Chines (most likely) or English: 1

   Sample values (first 20):
     English
     Chinese
     Other: unknown
     Other: German
     Japanese
     Other: Korean
     Other: 
     Other: unspecified
     Other: german, english 
     Other: Italien
     Other: indian
     Other: English and Indonesian 
     Other: Indonesian
     Other: Persian
     Other: spanish
     Other: Spanish
     Other: Unsure
     Other: unknown, probably english or hindi
     Other: unknown. Italian?
     Other: English, Arabic


52. Lexical Overlap Benchmark quality (H/L)
   ===========================================
   Data type: object
   Total values: 488
   Non-null values: 70
   Null values: 418
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 418 (597.1%)
     L: 29 (41.4%)
     no benchmark: 19 (27.1%)
     l: 17 (24.3%)
     -: 3 (4.3%)
     L

: 1 (1.4%)
     H: 1 (1.4%)


53. Lexical Overlap How it compares against benchmark (B/S/W)
   =============================================================
   Data type: object
   Total values: 488
   Non-null values: 71
   Null values: 417
   Unique values: 9
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 417 (587.3%)
     B: 23 (32.4%)
     no benchmark: 20 (28.2%)
     b: 18 (25.4%)
     -: 3 (4.2%)
     s: 2 (2.8%)
     S: 2 (2.8%)
     BLEU score (B): 1 (1.4%)
     N : 1 (1.4%)
     unsure: 1 (1.4%)


54. Lexical Overlap Notes on benchmark quality
   ==============================================
   Data type: object
   Total values: 488
   Non-null values: 68
   Null values: 420
   Unique values: 48
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 420 (617.6%)
     -: 3 (4.4%)
     BLEU, ROUGE, METEOR. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5): 2 (2.9%)
     no benchmark. metric: ROUGE-1, -2, -L: 2 (2.9%)
     “We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”: 2 (2.9%)
     BLEU SCORE: 2 (2.9%)
     BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.: 2 (2.9%)
     BLEU, ROUGE with counselor responses as reference (CounselChat): 2 (2.9%)
     BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations: 2 (2.9%)
     METEOR and ROUGE tested, but Strawman models: 2 (2.9%)
     ML models (HRED, SEQ2SEQ): 2 (2.9%)
     Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.: 2 (2.9%)
     BLEU with reference: 2 (2.9%)
     seq2seq: 2 (2.9%)
     BLEU against human responses. Benchmark: non fine-tuned GPT-2: 2 (2.9%)
     benchmark: other conversation systems: 2 (2.9%)
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.: 2 (2.9%)
     BLEU, ROUGE against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.: 2 (2.9%)
     ROUGE-L - PanGu better than WenZhong: 2 (2.9%)
     BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L: 2 (2.9%)
     Bleu score: 1 (1.5%)
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets: 1 (1.5%)
     metrics… including BLEU-4 and ROUGE” (Position: Model Evaluation): 1 (1.5%)
     Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.: 1 (1.5%)
     Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (1.5%)
     Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (1.5%)
      BLEU [57] and ROUGE
[58] metrics: 1 (1.5%)
     Against chatGPT-3.5: 1 (1.5%)
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Comparison against reference responses in dataset: 1 (1.5%)
     Comparison against reference responses in dataset: 1 (1.5%)
     older LLM: 1 (1.5%)
     0.85 cosine similarity score: 1 (1.5%)
     BLEU score: 1 (1.5%)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts): 1 (1.5%)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)

ChatGPTbased conversations
(Approach 1), fine-tuned DialoGPT transformer
conversations (Approach 2), and fine-tuned DialoGPT
transformer conversations combined with the GPT3 prompts
API (Approach 3): 1 (1.5%)
     ROUGE-L. no benchmark: 1 (1.5%)
     similar chinese language LLMs: 1 (1.5%)
     DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR: 1 (1.5%)
     metrics: ROUGE, METEOR. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...): 1 (1.5%)
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR: 1 (1.5%)
     other models: 1 (1.5%)
     benchmark are human responses.: 1 (1.5%)
     benchmark are human responses: 1 (1.5%)
     benchmark is a simple seq2seq model: 1 (1.5%)
     Benchmark is other NLP model: 1 (1.5%)
     Lexical Overlap within comparative linguistic analysis: 1 (1.5%)
     Rouge comparison to Alexander Street Press reference. No benchmark.: 1 (1.5%)
     ChatGPTbased conversations 
(Approach 1), fine-tuned DialoGPT transformer 
conversations (Approach 2), and fine-tuned DialoGPT 
transformer conversations combined with the GPT3 prompts 
API (Approach 3): 1 (1.5%)
     Other LLMs: 1 (1.5%)

   Sample values (first 20):
     ChatGPTbased conversations 
(Approach 1), fine-tuned DialoGPT transformer 
conversations (Approach 2), and fine-tuned DialoGPT 
transformer conversations combined with the GPT3 prompts 
API (Approach 3)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)

ChatGPTbased conversations
(Approach 1), fine-tuned DialoGPT transformer
conversations (Approach 2), and fine-tuned DialoGPT
transformer conversations combined with the GPT3 prompts
API (Approach 3)
     ROUGE-L - PanGu better than WenZhong
     ROUGE-L. no benchmark
     similar chinese language LLMs
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.
     benchmark: other conversation systems
     DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR
     metrics: ROUGE, METEOR. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...)
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR
     other models
     no benchmark. metric: ROUGE-1, -2, -L
     benchmark are human responses.
     benchmark are human responses
     seq2seq
     benchmark is a simple seq2seq model
     Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.
     ML models (HRED, SEQ2SEQ)
     Benchmark is other NLP model


55. Lexical Overlap Used (Y/N)
   ==============================
   Data type: object
   Total values: 488
   Non-null values: 369
   Null values: 119
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     n: 150 (40.7%)
     N: 144 (39.0%)
     [NULL]: 119 (32.2%)
     Y: 44 (11.9%)
     y: 30 (8.1%)
     N (all below): 1 (0.3%)


56. Lexical diversity Benchmark quality (H/L)
   =============================================
   Data type: object
   Total values: 488
   Non-null values: 9
   Null values: 479
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 479 (5322.2%)
     L: 4 (44.4%)
     no benchmark: 2 (22.2%)
     H: 2 (22.2%)
     l: 1 (11.1%)


57. Lexical diversity How it compares against benchmark (B/S/W)
   ===============================================================
   Data type: object
   Total values: 488
   Non-null values: 9
   Null values: 479
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 479 (5322.2%)
     B: 2 (22.2%)
     s: 2 (22.2%)
     no benchmark: 2 (22.2%)
     W: 2 (22.2%)
     B
: 1 (11.1%)


58. Lexical diversity Notes on benchmark quality
   ================================================
   Data type: object
   Total values: 488
   Non-null values: 7
   Null values: 481
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 481 (6871.4%)
     D1 (Distinct-1) measures the richness of vocabulary in the responses: 2 (28.6%)
     Metrics: Dist-1 and Dist-2 on chatbot responses. No benchmark: 2 (28.6%)
     Dist-2. Benchmark are other LLM-based methods.: 1 (14.3%)
     Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (14.3%)
     Dist-2. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (14.3%)


59. Lexical diversity Used (Y/N)
   ================================
   Data type: object
   Total values: 488
   Non-null values: 207
   Null values: 281
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 281 (135.7%)
     n: 109 (52.7%)
     N: 87 (42.0%)
     Y: 6 (2.9%)
     y: 3 (1.4%)
     N : 2 (1.0%)


60. Metric 1 Benchmark quality (H/L)
   ====================================
   Data type: object
   Total values: 488
   Non-null values: 114
   Null values: 374
   Unique values: 9
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 374 (328.1%)
     no benchmark: 36 (31.6%)
     l: 28 (24.6%)
     L: 19 (16.7%)
     H: 14 (12.3%)
     h: 9 (7.9%)
     -: 5 (4.4%)
     H (?): 1 (0.9%)
     L : 1 (0.9%)
     ?: 1 (0.9%)


61. Metric 1 How it compares against benchmark (B/S/W)
   ======================================================
   Data type: object
   Total values: 488
   Non-null values: 115
   Null values: 373
   Unique values: 13
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 373 (324.3%)
     no benchmark: 35 (30.4%)
     b: 25 (21.7%)
     B: 20 (17.4%)
     s: 7 (6.1%)
     W: 6 (5.2%)
     -: 6 (5.2%)
     S: 4 (3.5%)
     w: 3 (2.6%)
     no benchmark. in absolute terms favorable, with reduction in smoking, increase in confidence, importance, readiness.: 2 (1.7%)
     unclear: 2 (1.7%)
     B : 2 (1.7%)
     - (no B/S/W classification possible): 2 (1.7%)
     unknown: 1 (0.9%)


62. Metric 1 Name of metric
   ===========================
   Data type: object
   Total values: 488
   Non-null values: 146
   Null values: 342
   Unique values: 93
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 342 (234.2%)
     N: 9 (6.2%)
     perplexity: 5 (3.4%)
     n: 4 (2.7%)
     cross-entropy loss: 3 (2.1%)
     Reduction in anxiety symptoms (BAI, GAD-7): 2 (1.4%)
     Subjective Units of Distress Scale, SUDS: 2 (1.4%)
     Sentiment and part-of-speech: 2 (1.4%)
     sentiment analysis score: 2 (1.4%)
     Patient Health Questionnaire 927 (PHQ-9), the Generalized Anxiety Disorder Questionnaire for the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM-IV) (GAD-Q-IV), and the Weight Concerns Scale (WCS) within the Stanford–Washington University Eating Disorder (SWED): 2 (1.4%)
     safety (number of conversations turns until initial referral/shutdown of chatbot): 2 (1.4%)
     Behavior Sensitivity Index: 2 (1.4%)
     Empathy
 Coherence 
Informativeness 
Fluency: 2 (1.4%)
     PsychoBench Empathy Scale: 2 (1.4%)
     Posttrial guardrail review: 2 (1.4%)
     LIWC score domains and linguistic marker scores: 2 (1.4%)
     NLU confidence: 2 (1.4%)
     Comprehensiveness, Professionalism, Safety: 2 (1.4%)
     Avg. of Stigma Questions: 2 (1.4%)
     See Fig. 4-9. : 2 (1.4%)
     Lay Rating: 2 (1.4%)
     Empathy rating unequality: 2 (1.4%)
     User experience rating: 2 (1.4%)
     User rating: 2 (1.4%)
     unclear metrics: relevance, empathy, conciseness, context: 2 (1.4%)
     Empathy classification (Sharma et al): 2 (1.4%)
     quantitative ux assessment ("overall satisfaction"): 2 (1.4%)
     reduction in emotion intensity: 2 (1.4%)
     Relevancy: 2 (1.4%)
     WHOQOL-BREF score change: 2 (1.4%)
     readiness ruler (patient symptom report, smoking related, validated measure): 2 (1.4%)
     Lexical Diversity: 2 (1.4%)
     sentiment polarity: 2 (1.4%)
     Client satisfaction scale (CSS): 2 (1.4%)
     lexical diversity and richness: 2 (1.4%)
     diversity: 2 (1.4%)
     WAI-SR working alliance: 2 (1.4%)
     Perplexity: 2 (1.4%)
     Human rating, not sure by whom: 2 (1.4%)
     Construct Validity using MTMM and Hierarchical  linear models HLM: 2 (1.4%)
     AAQ + CDS: 2 (1.4%)
     PHQ-9, PANAS-P, Satisfaction With Life Scale (SWLS): 1 (0.7%)
     FuzzyWuzzy Token Set Ratio: 1 (0.7%)
     Average rating of emotional reflections 
: 1 (0.7%)
     Average Tone: 1 (0.7%)
     EPIT-ONE framework: 1 (0.7%)
     Content validity index: 1 (0.7%)
     Relevance, personalisation, PFA (advice and guidance)professional referral, and validation and empathy: 1 (0.7%)
     PHQ-9: 1 (0.7%)
     Automatic "perceived information quality" (PIQ) rating via ML model (prompted ChatGPT): 1 (0.7%)
     Automatic "perceived information quality" (PIQ) rating via ML model: 1 (0.7%)
     Perceived information quality (unpromted): 1 (0.7%)
     Computational performance/ Training metrics: 1 (0.7%)
     see UX: 1 (0.7%)
     Training Metrics (training runtime, samples processed per second, steps per second, and Training loss): 1 (0.7%)
     maybe the adherence index here.: 1 (0.7%)
     Bias: 1 (0.7%)
     Client Evaluation of Motivational Interviewing Scale (CEMI) (Madson et al., 2013, 2015, 2016); Readiness to Change delta: 1 (0.7%)
     Empathy Score: 1 (0.7%)
     Ragas evaluation metrics to calculate Faithfulness, Answer Relevancy, and Answer Correctness scores for the various
RAG models: 1 (0.7%)
     Client Evaluation of Motivational Interviewing Scale (CEMI), Readiness to Change delta: 1 (0.7%)
     Diversity: 1 (0.7%)
     User rating of Empathig Understanding (EU): 1 (0.7%)
     part-of-speech (POS) analysis; dependency-syntactic-parsing (DEP) analysis; semantic-dependency-parsing (SDP) analysis; sentiment analysis: 1 (0.7%)
     various linguistic analyses with no clear meaning (part-of-speech, sentiment, ...): 1 (0.7%)
     part-of-speech (POS) analysis : 1 (0.7%)
     
: 1 (0.7%)
     eaviness of Smoking Index (HSI) measures.: 1 (0.7%)
     reduction of negative feelings: 1 (0.7%)
     Client Evaluation of Motivational Interviewing Scale (CEMI) (Madson et al., 2013, 2015, 2016): 1 (0.7%)
     Linguistic Inquiry and word count (LIWC): 1 (0.7%)
     construct validity: 1 (0.7%)
     Macro-F1: 1 (0.7%)
     human rating (unclear if expert or not): 1 (0.7%)
     Emotion
task: 1 (0.7%)
     See Table 3: 1 (0.7%)
     expert rating 2: 1 (0.7%)
     Sentiment Analysis: 1 (0.7%)
     user experience measures (see above): 1 (0.7%)
     quantification of effect: 1 (0.7%)
     First refferal to a human: 1 (0.7%)
     Fluency: 1 (0.7%)
     Enhancing Counselling Skills and Therapeutic 
Conditions (PB-CSTC)

Enhancing Counselling Dispositions and 
Behaviours (PB-CDB)

Learning Counselling and Mental Health Topics 
(PB-LC): 1 (0.7%)
     SHAP value: 1 (0.7%)
     Emotional Intelligence Scale (EIS): 1 (0.7%)
     Anxiety Score : 1 (0.7%)
     Y: 1 (0.7%)
     HOQOL-BREF
score: 1 (0.7%)
     Integrated interaction : 1 (0.7%)
     Stress test metrics: 1 (0.7%)
     Level of stigma: 1 (0.7%)
     Response Quality Assessments (evaluating coherence,
empathy, tone) : 1 (0.7%)
     Stress, Anxiety, Depression: 1 (0.7%)
     Counselor rating form-short (CRF-S): 1 (0.7%)

   Sample values (first 20):
     HOQOL-BREF
score
     WHOQOL-BREF score change
     perplexity
     See Table 3
     lexical diversity and richness
     Perplexity
     Human rating, not sure by whom
     AAQ + CDS
     N
     Emotion
task
     human rating (unclear if expert or not)
     Lexical Diversity
     Macro-F1
     Construct Validity using MTMM and Hierarchical  linear models HLM
     construct validity
     Linguistic Inquiry and word count (LIWC)
     reduction of negative feelings
     Diversity
     diversity
     sentiment polarity


63. Metric 1 Notes on benchmark quality
   =======================================
   Data type: object
   Total values: 488
   Non-null values: 109
   Null values: 379
   Unique values: 59
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 379 (347.7%)
     no benchmark: 17 (15.6%)
     -: 4 (3.7%)
     Therapist responses: 2 (1.8%)
     A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). : 2 (1.8%)
     intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points): 2 (1.8%)
     Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. : 2 (1.8%)
     Book written by a human expert.: 2 (1.8%)
     A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.: 2 (1.8%)
     User scores of "helpfulness" and "empathy". Benchmark: non fine-tuned GPT-2.: 2 (1.8%)
     No Indexmodel; comparison of multiple LLMs: 2 (1.8%)
     Waitlist control: 2 (1.8%)
     pre post measurement: 2 (1.8%)
     benchmark are responses in the kaggle dataset: 2 (1.8%)
     compared to data set 70.58 %: 2 (1.8%)
     Unclear what these metrics are (human ratings? automatic?). No benchmarks used.: 2 (1.8%)
     Not clear what this metric is.: 2 (1.8%)
     Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini: 2 (1.8%)
     benchmarks are other mental health chatbots and general LLMs: Woebot, Happify, GPT-3.5, Google Bard: 2 (1.8%)
     benchmark are real human responses from the Q&A dataset: 2 (1.8%)
     Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. : 2 (1.8%)
     Benchmars are other LLM-based models: 2 (1.8%)
     PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon): 2 (1.8%)
     ChatGPT and GLM-4: 2 (1.8%)
     benchmark: human psychologist responses from transcripts: 2 (1.8%)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts): 2 (1.8%)
     human psychologist responses in the CounselChat transcripts: 2 (1.8%)
     seq2seq: 2 (1.8%)
     different models were rated by human non-experts. no comparison with any benchmark.: 2 (1.8%)
     Other LLMs: 2 (1.8%)
     aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.: 2 (1.8%)
     Benchmark is other NLP model: 2 (1.8%)
     no benchmark. only the pre-post-difference in this metric was measured.: 2 (1.8%)
     benchmark: other conversation systems: 2 (1.8%)
     "SPARTA-TAA obtains significant improvements over all
baselines. It reports improvements of +8.64%, +8.58%, and +6.29%
in macro-F1 (60.29), weighted-F1 (64.53), and accuracy (64.75%),
respectively, as compared to CASA": 1 (0.9%)
     worst of all: 1 (0.9%)
     No benchmark: 1 (0.9%)
     Not clear what fluency is. The reference paper measures fluency via human rating but the authors of this paper state this is an automatic assessment.: 1 (0.9%)
     rating scale to determine quality of backward looking reflections (Textbox 5).: 1 (0.9%)
     benchmark is a simple seq2seq model: 1 (0.9%)
      recognizing the utterance-level emotion of the help-seeke: 1 (0.9%)
      Compared to the ratings of expert suicidologist.: 1 (0.9%)
     PanGu better than WenZhong: 1 (0.9%)
     In the text there is no statement on how or by whom this was evaluated. : 1 (0.9%)
     Table 1: mechanical turk worker ratings ("Human") vs. GPT-4: 1 (0.9%)
     human therapists : 1 (0.9%)
     older LLM: 1 (0.9%)
     no benchmark. Measure: difference in automated empathy ratings between different ethnic groups: 1 (0.9%)
     Measures MI adherence of clients. Benchmark: GPT-4 out of the box without MI fine-tuning: 1 (0.9%)
     Measures MI adherence of clients. Benchmark: GPT-4 out of the box: 1 (0.9%)
     without MI fine-tuning: 1 (0.9%)
     Woebot: 1 (0.9%)
     Figure 1, no benchmark: 1 (0.9%)
     rule-based approach: " It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score": 1 (0.9%)
     OpenAI against Mistral with RAG modules and sentiment analysis: 1 (0.9%)
     Expert judgments of empathy, fluency, specificity of PARTNER against other LLMs.: 1 (0.9%)
     Custom metrics (Role Adherence, Answer Relevancy, Faithfulness) reported in tables across tiers of the same model; no external gold standard named: 1 (0.9%)
     (Benchamark Kaggle data set) sentiment analysis: 1 (0.9%)
     human responses from existing literature: 1 (0.9%)
     Other LLMs/Models: 1 (0.9%)

   Sample values (first 20):
     intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)
     human psychologist responses in the CounselChat transcripts
     PanGu better than WenZhong
     no benchmark
     different models were rated by human non-experts. no comparison with any benchmark.
     aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.
      recognizing the utterance-level emotion of the help-seeke
     benchmark: other conversation systems
     rating scale to determine quality of backward looking reflections (Textbox 5).
     "SPARTA-TAA obtains significant improvements over all
baselines. It reports improvements of +8.64%, +8.58%, and +6.29%
in macro-F1 (60.29), weighted-F1 (64.53), and accuracy (64.75%),
respectively, as compared to CASA"
     No benchmark
     worst of all
     seq2seq
     benchmark is a simple seq2seq model
     Benchmark is other NLP model
     no benchmark. only the pre-post-difference in this metric was measured.
     benchmark: human psychologist responses from transcripts
     rule-based approach: " It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score"
     Other LLMs


64. Metric 2 Benchmark quality (H/L)
   ====================================
   Data type: object
   Total values: 488
   Non-null values: 58
   Null values: 430
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 430 (741.4%)
     no benchmark: 17 (29.3%)
     l: 14 (24.1%)
     L: 11 (19.0%)
     H: 9 (15.5%)
     h: 5 (8.6%)
     unclear: 1 (1.7%)
     -: 1 (1.7%)


65. Metric 2 How it compares against benchmark (B/S/W)
   ======================================================
   Data type: object
   Total values: 488
   Non-null values: 58
   Null values: 430
   Unique values: 12
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 430 (741.4%)
     no benchmark: 18 (31.0%)
     b: 12 (20.7%)
     W: 6 (10.3%)
     B: 6 (10.3%)
     unclear: 5 (8.6%)
     w: 2 (3.4%)
     S: 2 (3.4%)
     -: 2 (3.4%)
     mixed (Table 9): 2 (3.4%)
     S/B: 1 (1.7%)
     B/S/W (depending on risk situation): 1 (1.7%)
     unknown: 1 (1.7%)


66. Metric 2 Name of metric
   ===========================
   Data type: object
   Total values: 488
   Non-null values: 72
   Null values: 416
   Unique values: 51
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 416 (577.8%)
     N: 5 (6.9%)
     SHAP values of words in classifier that classifies human vs. ChatGPT responses: 2 (2.8%)
     Various linguistic factors: 2 (2.8%)
     Overall Performance: 2 (2.8%)
     User Satisfaction Score: 2 (2.8%)
     Fuzzy matching confidence: 2 (2.8%)
     Positive and Negative Affect Schedule, PANAS: 2 (2.8%)
     Consistency Ratio: 2 (2.8%)
     Number of harmful outputs (user-rated): 2 (2.8%)
     Authenticity: 2 (2.8%)
     User satisfaction rating: 2 (2.8%)
     PsychoBench Emotional Intelligence Scale: 2 (2.8%)
     Counselor rating form‑short (CRF‑S): 2 (2.8%)
     change of empathy scores for the ERF module: 2 (2.8%)
     Linguistic Features as mentioned under 3.3: 2 (2.8%)
     Criterion Validity: 2 (2.8%)
     Distinct 1: 2 (2.8%)
     Average length: 2 (2.8%)
     ux measures: 1 (1.4%)
     Diversity Metrics (richness and variability; Dist-1 and Dist-2): 1 (1.4%)
     Human Helpfulness: 1 (1.4%)
     MITI: 1 (1.4%)
     average tone (sentiment score): 1 (1.4%)
     engagement with Therabot (number of messages sent): 1 (1.4%)
     Strategy task: 1 (1.4%)
     Difference between responses (detection): 1 (1.4%)
     Working Alliance Inventory - short
revised (WAI-SR) : 1 (1.4%)
     distinct-1, -2: 1 (1.4%)
     readability scores: 1 (1.4%)
     Perceived information quality (promted): 1 (1.4%)
     Scales of Psychological Well-being (PWB), Satisfaction With Life Scale (SWLS): 1 (1.4%)
     Scales of Psychological Well-being (PWB): 1 (1.4%)
     Weighted-F1: 1 (1.4%)
     User Interaction Studies (questionnaires and session 
reviews): 1 (1.4%)
     dependency-syntactic-parsing (DEP) analysis: 1 (1.4%)
     Shutdown of conversation: 1 (1.4%)
     Edit Rate: 1 (1.4%)
     Perplexity, Specificity, Diversity, Sentence coherence, edit rate: 1 (1.4%)
     Perplexity, Diversity, Sentence coherence, edit rate: 1 (1.4%)
     Response Quality: 1 (1.4%)
     non expert rating: 1 (1.4%)
     Response Quality rated by non experts: 1 (1.4%)
     LIME value: 1 (1.4%)
     Appropriateness of responses: 1 (1.4%)
     Logistic Regression: 1 (1.4%)
      patient satisfaction Likert scale : 1 (1.4%)
     criterion validity: 1 (1.4%)
     Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond): 1 (1.4%)
     Instances of potentially concerning language detected: 1 (1.4%)
     Consitency : 1 (1.4%)
     Engangement Metrics : 1 (1.4%)

   Sample values (first 20):
      patient satisfaction Likert scale 
     Linguistic Features as mentioned under 3.3
     readability scores
     Distinct 1
     distinct-1, -2
     N
     Strategy task
     Average length
     Weighted-F1
     Criterion Validity
     criterion validity
     Logistic Regression
     change of empathy scores for the ERF module
     ux measures
     dependency-syntactic-parsing (DEP) analysis
     SHAP values of words in classifier that classifies human vs. ChatGPT responses
     Edit Rate
     Perplexity, Specificity, Diversity, Sentence coherence, edit rate
     Perplexity, Diversity, Sentence coherence, edit rate
     Response Quality


67. Metric 2 Notes on benchmark quality
   =======================================
   Data type: object
   Total values: 488
   Non-null values: 51
   Null values: 437
   Unique values: 29
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 437 (856.9%)
     no benchmark: 10 (19.6%)
     PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon): 2 (3.9%)
     Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. : 2 (3.9%)
     benchmark are real human responses from the Q&A dataset: 2 (3.9%)
     Compared to the ratings of expert suicidologist.
: 2 (3.9%)
     Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini: 2 (3.9%)
     Not clear what this metric is.: 2 (3.9%)
     pre post measurement: 2 (3.9%)
     No Indexmodel; comparison of multiple LLMs: 2 (3.9%)
     it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.: 2 (3.9%)
     ChatGPT and GLM-4: 2 (3.9%)
     Benchmars are other LLM-based models: 2 (3.9%)
     benchmark: human psychologist responses from transcripts: 2 (3.9%)
     Other LLMs: 2 (3.9%)
     benchmark are responses in the kaggle dataset, but author rated himself (vested interests???): 1 (2.0%)
     PanGu better than WenZhong: 1 (2.0%)
     predicting support strategies: 1 (2.0%)
     No benchmark: 1 (2.0%)
     Reframe Relatability, Reframe Helpfulness, Reframe Memorability, Skill Learnability: 1 (2.0%)
     Therapist responses: 1 (2.0%)
     older LLM: 1 (2.0%)
     Author rated himself .. (vested interests??): 1 (2.0%)
     Benchmark: GPT-4 out of the box without MI fine-tuning: 1 (2.0%)
     Benchmark: GPT-4 out of the box: 1 (2.0%)
     without MI fine-tuning: 1 (2.0%)
     Compared to commercially-available therapy bots; no indexmodel. : 1 (2.0%)
     benchmark are responses in the kaggle dataset: 1 (2.0%)
     benchmark: woebot non-generative/rule-based.: 1 (2.0%)
     -: 1 (2.0%)

   Sample values (first 20):
     it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.
     PanGu better than WenZhong
     no benchmark
     predicting support strategies
     No benchmark
     Reframe Relatability, Reframe Helpfulness, Reframe Memorability, Skill Learnability
     benchmark: human psychologist responses from transcripts
     Other LLMs
     Author rated himself .. (vested interests??)
     benchmark are responses in the kaggle dataset
     benchmark are responses in the kaggle dataset, but author rated himself (vested interests???)
     older LLM
     PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)
     benchmark: woebot non-generative/rule-based.
     ChatGPT and GLM-4
     Compared to commercially-available therapy bots; no indexmodel. 
     without MI fine-tuning
     Benchmark: GPT-4 out of the box
     Benchmark: GPT-4 out of the box without MI fine-tuning
     No Indexmodel; comparison of multiple LLMs


68. Metric 3 Benchmark quality (H/L)
   ====================================
   Data type: object
   Total values: 488
   Non-null values: 18
   Null values: 470
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 470 (2611.1%)
     l: 8 (44.4%)
     H: 4 (22.2%)
     no benchmark: 3 (16.7%)
     L: 3 (16.7%)


69. Metric 3 How it compares against benchmark (B/S/W)
   ======================================================
   Data type: object
   Total values: 488
   Non-null values: 18
   Null values: 470
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 470 (2611.1%)
     b: 4 (22.2%)
     no benchmark: 3 (16.7%)
     B: 3 (16.7%)
     S: 3 (16.7%)
     s: 3 (16.7%)
     w: 1 (5.6%)
     more words: 1 (5.6%)


70. Metric 3 Name of metric
   ===========================
   Data type: object
   Total values: 488
   Non-null values: 26
   Null values: 462
   Unique values: 17
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 462 (1776.9%)
     N: 5 (19.2%)
     Word analysis/ Count: 2 (7.7%)
     German version of the Working Alliance Inventory - short revised (WAI-SR): 2 (7.7%)
     Daily interaction time: 2 (7.7%)
     Distinct 2
: 2 (7.7%)
     Sentiment score: 2 (7.7%)
     Accuracy: 1 (3.8%)
     Long short tem measure: 1 (3.8%)
     expert rating 2: 1 (3.8%)
     Response task: 1 (3.8%)
     various other traditional NLP metrics: 1 (3.8%)
     Safety measures: 1 (3.8%)
     𝛥 Readiness to Change: 1 (3.8%)
     Empathy metrics : 1 (3.8%)
     n: 1 (3.8%)
     GAD-7, PANAS-P, PANAS-N: 1 (3.8%)
     GAD-7, PANAS-N: 1 (3.8%)


71. Metric 3 Notes on benchmark quality
   =======================================
   Data type: object
   Total values: 488
   Non-null values: 14
   Null values: 474
   Unique values: 10
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 474 (3385.7%)
     no benchmark: 3 (21.4%)
     Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. : 2 (14.3%)
     Benchmars are other LLM-based models: 2 (14.3%)
     PanGu better than WenZhong: 1 (7.1%)
     generating supportive
response: 1 (7.1%)
     Expert judgments of empathy, fluency, specificity of PARTNER against other LLMs.: 1 (7.1%)
     Safety
was assessed by adverse events monitored during both in-app
conversational exchanges and study assessment points, instances
of concerning language detected in user free-text inputs, and
the posttrial technical guardrail assessment success rate.: 1 (7.1%)
     without MI fine-tuning: 1 (7.1%)
     Benchmark: GPT-4 out of the box: 1 (7.1%)
     Benchmark: GPT-4 out of the box without MI fine-tuning: 1 (7.1%)


72. Models Employed
   ===================
   Data type: object
   Total values: 488
   Non-null values: 478
   Null values: 10
   Unique values: 129
   Category: CATEGORICAL

   Value Counts:
     GPT-4 / GPT-4o family: 75 (15.7%)
     BERT family: 73 (15.3%)
     GPT-3.5 family: 44 (9.2%)
     ChatGPT, model unspecified: 40 (8.4%)
     GPT-2 family: 22 (4.6%)
     GPT-3.5 family; GPT-4 / GPT-4o family: 11 (2.3%)
     BERT family; GPT-2 family: 11 (2.3%)
     [NULL]: 10 (2.1%)
     GPT-3 family: 9 (1.9%)
     Mistral family: 6 (1.3%)
     Llama 2 family: 6 (1.3%)
     BERT family; GPT-3.5 family: 6 (1.3%)
     T5 family: 5 (1.0%)
     BERT family; GPT-4 / GPT-4o family: 5 (1.0%)
     Other: Falcon-7B: 4 (0.8%)
     GPT-4 / GPT-4o family; Gemini / Bard family; Claude family: 4 (0.8%)
     GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family: 4 (0.8%)
     Other: Jais-13B: 3 (0.6%)
     GPT-4 / GPT-4o family; Claude family: 3 (0.6%)
     BERT family; Llama 2 family: 3 (0.6%)
     Other: unknown: 3 (0.6%)
     BERT family; ChatGPT, model unspecified: 3 (0.6%)
     Other: unclear: 2 (0.4%)
     Other: various LLMs: 2 (0.4%)
     GPT-3.5 family; Other: NLTK VADER: 2 (0.4%)
     BERT family; GPT-3.5 family; Mistral family: 2 (0.4%)
     T5 family; GPT-4 / GPT-4o family; Llama 2 family; Other: DeepSeek-R1: 2 (0.4%)
     Other: unspecified: 2 (0.4%)
     BART family; T5 family; GPT-2 family: 2 (0.4%)
     BERT family; GPT-2 family; Other: BLOOMZ: 2 (0.4%)
     BERT family; T5 family; GPT-3.5 family; Mistral family: 2 (0.4%)
     Other: AI in general, mostly ChatGPT: 2 (0.4%)
     Qwen family; Other: ChatGLM, ERNIE Bot, Qianwen (as model and as judge): 2 (0.4%)
     BART family; ChatGPT, model unspecified: 2 (0.4%)
     GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family: 2 (0.4%)
     Llama 2 family; Other: Falcon-7B: 2 (0.4%)
     T5 family; GPT-2 family: 2 (0.4%)
     GPT-3.5 family; Llama 2 family; Other: SeqGPT: 2 (0.4%)
     BART family; T5 family; GPT-2 family; Llama 2 family; Mistral family; Other: Phi-2, GPT-J, GPT-Neo: 2 (0.4%)
     Other: ChatGLM2-6B : 2 (0.4%)
     GPT-2 family; GPT-3.5 family: 2 (0.4%)
     Other: Clare (R) by clare&me GmbH: 2 (0.4%)
     Other: ChatGLM: 2 (0.4%)
     BERT family; Gemini / Bard family: 2 (0.4%)
     GPT-3.5 family; Other: all-MiniLM-L6-v2 (for embedding): 2 (0.4%)
     Other: none: 2 (0.4%)
     Other: BlenderBot: 2 (0.4%)
     GPT-3.5 family; Other: text-embedding-3-small: 2 (0.4%)
     Other: Llama 3.2 family: 2 (0.4%)
     Other: different "AI conversational agents": 2 (0.4%)
     Other: just general "generative AI application": 2 (0.4%)
     Other: own transformer architecture: 2 (0.4%)
     Other: Long-short term memory 
network (LSTM)?: 1 (0.2%)
     Other: MiniMax 6.5s (245K), Doubao Function Call model (32K) : 1 (0.2%)
     Other: ERNIE Bot, Qianwen, and ChatGLM: 1 (0.2%)
     Other: highest accuracy: 91.41% using k-nearest neighbors (kNN),highest accuracy of other systems: 84% using … long-short term memory network (LSTM): 1 (0.2%)
     GPT-3 family; Other: GPT-Neo, GPT-J, AI21 Jurassic-1: 1 (0.2%)
     BERT family; GPT-3.5 family; Mistral family; Other: Flan : 1 (0.2%)
     Other:  ChatGLM-LoRA: 1 (0.2%)
     Other:  MiniMax 6.5s(245K): 1 (0.2%)
     Other: logistic regression mode: 1 (0.2%)
     GPT-4 / GPT-4o family; Claude family; Other: Deepseek & Wenxin - Yiyan: 1 (0.2%)
     BERT family; Other: LTSM: 1 (0.2%)
     Other: various GenAI chatbots: 1 (0.2%)
     Other: ChatGLM2: 1 (0.2%)
     Other: 1 (0.2%)
     GPT-4 / GPT-4o family; ChatGPT, model unspecified; Other: Conflicting information (?)

The HoMemeTown chatbot, powered by ChatGPT 4.0

The chatbot relies on the GPT API, a general-purpose language model provided by OpenAI, instead of a domain-specific model
trained for mental health counseling. The GPT API offers a range of models, such as Davinci, GPT-3.5, and GPT-4, which can be selected based on desired performance and cost
consideration

Another limitation is the potential inconsistency in comparing Dr CareSam, built on the ChatGPT 4.0 API,: 1 (0.2%)
     Other: not specif.: 1 (0.2%)
     Other: The chatbot was trained on a custom counselling dataset using Rasa’s (3.6.16) built-in components.: 1 (0.2%)
     Other: Llama 3.2: 1 (0.2%)
     Other: AI in general: 1 (0.2%)
     BERT family; T5 family; GPT-3.5 family; ChatGPT, model unspecified: 1 (0.2%)
     Llama 2 family; Other: Azure GPT-3.5: 1 (0.2%)
     Other: Not specified "The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dia-
logues": 1 (0.2%)
     GPT-4 / GPT-4o family; Claude family; Other: DeepSeek, ERNIE Bot: 1 (0.2%)
     GPT-4 / GPT-4o family; Claude family; Other: selected four widely-used LLMs … DeepSeek, GPT-4, Claude, Wenxin-Yiyan: 1 (0.2%)
     Other: Falcon7B: 1 (0.2%)
     GPT-4 / GPT-4o family; Llama 2 family: 1 (0.2%)
     GPT-4 / GPT-4o family; Other: various GenAI chatbots: 1 (0.2%)
     GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family; Other: Microsoft Copilot: 1 (0.2%)
     Other: InternLM2-7B-Chat with QLoRA: 1 (0.2%)
     GPT-4 / GPT-4o family; Other: Pi, Replika: 1 (0.2%)
     Other: not sure - it just says transformer model: 1 (0.2%)
     GPT-4 / GPT-4o family; Gemini / Bard family: 1 (0.2%)
     GPT-4 / GPT-4o family; Other: Bard : 1 (0.2%)
     BERT family; Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer [26] model which generates a beam of candidate responses. In the second stage a number of smaller, more specialized Transformer-based models): 1 (0.2%)
     BERT family; Other: own architecture: 1 (0.2%)
     Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer
[26] model which generates a beam of candidate responses.
In the second stage a number of smaller, more specialized
Transformer-based models): 1 (0.2%)
     BERT family; Other: Roberta, TextCNN, Text-LSTM, GPT-unknown version: 1 (0.2%)
     Llama 3.1 family; Other: own?: 1 (0.2%)
     ChatGPT, model unspecified; Other: various LLM-based chatbots: Replika, Snapchat My AI, Chai, Character.ai, Anima, Paradot, ChatGPT, Kuki: 1 (0.2%)
     ChatGPT, model unspecified; Other: various LLM-based chatbots (Replika - Snapchat My AI - Chai - Character.ai - Anima - Paradot - Kuki): 1 (0.2%)
     Other: Google T5 Transformer : 1 (0.2%)
     Other: Japanese Text-to-Text Transfer Transformer: 1 (0.2%)
     Other: "For speaker-invariant representations, we employ a pre-trained RoBERTa language model which is further fine-tuned on DAC task.": 1 (0.2%)
     GPT-2 family; Other: DialoGPT: 1 (0.2%)
     Other: "custom GPT", version not further specified: 1 (0.2%)
     GPT-3.5 family; Qwen family; Other: Baichuan-7B, Llama 1, Alpaca 1: 1 (0.2%)
     Other: Baichuan-7B: 1 (0.2%)
     BERT family; GPT-3 family; GPT-3.5 family: 1 (0.2%)
     GPT-2 family; Other: PanGu, WenZhong (based on GPT-2): 1 (0.2%)
     GPT-2 family; Other: PanGu: 1 (0.2%)
     GPT-2 family; GPT-3 family; Other: PanGu (similar to GPT-3)and WenZhong (GPT-2): 1 (0.2%)
     GPT-3 family; GPT-3.5 family: 1 (0.2%)
     Other: ML model to structure responses: 1 (0.2%)
     Other: not sure, it just says transformer model: 1 (0.2%)
     GPT-4 / GPT-4o family; Other: Chatbots Pi & Replika, Modell behind unclear : 1 (0.2%)
     Other: GPT family not specified: 1 (0.2%)
     ChatGPT, model unspecified; Other: Pi, and Replika: 1 (0.2%)
     Gemini / Bard family: 1 (0.2%)
     ChatGPT, model unspecified; Other: Whisper (speech-to-text), librosa (speech features) : 1 (0.2%)
     GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family; Other: Pi - Noni - Serena - and other commercial chatbots: 1 (0.2%)
     GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family; Other: Pi, Noni, Serena, and other commercial chatbots: 1 (0.2%)
     GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family: 1 (0.2%)
     GPT-3.5 family; Mistral family: 1 (0.2%)
     BERT family; GPT-4 / GPT-4o family; Other: additionally: CNN for Facial Recognition and LSTM for Text-based Emotion Detection: 1 (0.2%)
     BERT family; Other: InternLM2-7B-Chat with QLoRA: 1 (0.2%)
     GPT-3.5 family; Other: text-embedding-ada-002: 1 (0.2%)
     BART family; T5 family; Other: GODEL(Grounded Open Dialogue Language Model): 1 (0.2%)
     BART family: 1 (0.2%)
     GPT-3 family; GPT-3.5 family; Other: 1 (0.2%)
     Other: all types of chatbots e.g. ChatGPT: 1 (0.2%)
     Other: all types of chatbots, e.g. ChatGPT: 1 (0.2%)
     Other: not specified - using LangChain framework: 1 (0.2%)
     Other: not specified, using LangChain framework: 1 (0.2%)
     GPT-2 family; GPT-4 / GPT-4o family: 1 (0.2%)
     GPT-4 / GPT-4o family; Llama 2 family; Other: ChatGLM2-6B: 1 (0.2%)
     GPT-3.5 family; GPT-4 / GPT-4o family; Other: Vicuna: 1 (0.2%)
     Other: They say "ChatGPT". Not clear which version. Most likely 3.5 because they mention "Building on our previous exploration of GPT-3.5, one of the most advanced Natural Language Processing (NLP) technologies, this follow-up study aims to understand the perception dynamics of AI-generated responses in the realm of mental health support for young people.": 1 (0.2%)
     Other: No specific model chosen, but as a conclusion claude-sonnet-3.5 suggested. : 1 (0.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 68
     Other: logistic regression mode: 1
     Other: Baichuan-7B: 1
     Other: none: 2
     Other: BlenderBot: 2
     Other: "custom GPT", version not further specified: 1
     Other: "For speaker-invariant representations, we employ a pre-trained RoBERTa language model which is further fine-tuned on DAC task.": 1
     Other: Japanese Text-to-Text Transfer Transformer: 1
     Other: Google T5 Transformer: 1
     Other: own transformer architecture: 2
     Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer: 1
     Other: ML model to structure responses: 1
     Other: not sure - it just says transformer model: 1
     Other: not sure, it just says transformer model: 1
     Other: GPT family not specified: 1
     Other: They say "ChatGPT". Not clear which version. Most likely 3.5 because they mention "Building on our previous exploration of GPT-3.5, one of the most advanced Natural Language Processing (NLP) technologies, this follow-up study aims to understand the perception dynamics of AI-generated responses in the realm of mental health support for young people.": 1
     Other: unknown: 3
     Other: not specified, using LangChain framework: 1
     Other: not specified - using LangChain framework: 1
     Other: all types of chatbots, e.g. ChatGPT: 1
     Other: all types of chatbots e.g. ChatGPT: 1
     Other: different "AI conversational agents": 2
     Other: InternLM2-7B-Chat with QLoRA: 1
     Other: ChatGLM-LoRA: 1
     Other: ChatGLM: 2
     Other: Long-short term memory: 1
     Other: highest accuracy: 91.41% using k-nearest neighbors (kNN),highest accuracy of other systems: 84% using … long-short term memory network (LSTM): 1
     Other: MiniMax 6.5s (245K), Doubao Function Call model (32K): 1
     Other: MiniMax 6.5s(245K): 1
     Other: ERNIE Bot, Qianwen, and ChatGLM: 1
     Other: unspecified: 2
     Other: various GenAI chatbots: 1
     Other: unclear: 2
     Other: various LLMs: 2
     Other: Falcon-7B: 4
     Other: Falcon7B: 1
     Other: AI in general, mostly ChatGPT: 2
     Other: Not specified "The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dia-: 1
     Other: AI in general: 1
     Other: just general "generative AI application": 2
     Other: Jais-13B: 3
     Other: Llama 3.2: 1
     Other: Llama 3.2 family: 2
     Other: The chatbot was trained on a custom counselling dataset using Rasa’s (3.6.16) built-in components.: 1
     Other: not specif.: 1
     Other: Clare (R) by clare&me GmbH: 2
     Other: ChatGLM2: 1
     Other: ChatGLM2-6B: 2
     Other: No specific model chosen, but as a conclusion claude-sonnet-3.5 suggested.: 1

   Sample values (first 20):
     Other: logistic regression mode
     BERT family
     GPT-3.5 family
     GPT-4 / GPT-4o family
     GPT-3 family; GPT-3.5 family
     GPT-2 family; GPT-3.5 family
     Mistral family
     GPT-2 family; GPT-3 family; Other: PanGu (similar to GPT-3)and WenZhong (GPT-2)
     GPT-2 family; Other: PanGu
     GPT-2 family; Other: PanGu, WenZhong (based on GPT-2)
     Llama 2 family
     GPT-3 family
     BERT family; GPT-3 family; GPT-3.5 family
     BERT family; GPT-3.5 family
     Other: Baichuan-7B
     GPT-3.5 family; Qwen family; Other: Baichuan-7B, Llama 1, Alpaca 1
     GPT-2 family
     BERT family; GPT-2 family
     Other: none
     Other: BlenderBot


73. Month (1-12)
   ================
   Data type: float64
   Total values: 488
   Non-null values: 454
   Null values: 34
   Unique values: 12
   Category: CATEGORICAL

   Value Counts:
     12.0: 67 (14.8%)
     1.0: 54 (11.9%)
     5.0: 50 (11.0%)
     7.0: 46 (10.1%)
     3.0: 44 (9.7%)
     4.0: 39 (8.6%)
     10.0: 36 (7.9%)
     11.0: 34 (7.5%)
     [NULL]: 34 (7.5%)
     6.0: 30 (6.6%)
     2.0: 29 (6.4%)
     9.0: 17 (3.7%)
     8.0: 8 (1.8%)


74. Notes
   =========
   Data type: object
   Total values: 488
   Non-null values: 55
   Null values: 433
   Unique values: 34
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 433 (787.3%)
     intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes: 2 (3.6%)
     Unsure about the metrics. We can discuss during the consensus. See table 4: 2 (3.6%)
     Comparative Analysis unclear in methodological aspects to me. : 2 (3.6%)
     Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz: 2 (3.6%)
     FuzzyWuzzy and NLU Confidence scores are provided as individual example values for illustration purposes, rather than as statistical overall evaluations or averages.

Is this study "really" suitable? Rasa itself does not include a large language model (LLM) like GPT or BERT; instead, it can integrate or be extended with LLMs or other NLP components if desired; Not described in this study. : 2 (3.6%)
     Ambitious platform vision, but lacks any form of quantitative, clinical, or user-centered evaluation: 2 (3.6%)
     without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility: 2 (3.6%)
      : 2 (3.6%)
     Ethical Considerations: Which users were informed?: 2 (3.6%)
     Despite it being labeled an "empirical study", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. : 2 (3.6%)
     used metrics: EPIT-ONE framework, MITI, RoVERTa: 2 (3.6%)
     This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy.: 2 (3.6%)
     The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots.: 2 (3.6%)
     Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one.: 2 (3.6%)
     VR maybe as solution for the embodiment problem? : 2 (3.6%)
     the transformer model predicted items subscales from different clinical questionnaires: 2 (3.6%)
     very short study : 2 (3.6%)
     no alpha adjusting (table 3): 2 (3.6%)
     qualitative study - survey with 31 participants from recruited over Reddit: 2 (3.6%)
     This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%): 2 (3.6%)
     "Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality.": 2 (3.6%)
     This studiy does not make clear who the users are. It seems they had some users who competed a quiz: 1 (1.8%)
     No evaluation of the conversation system. Study only displayed some conversation sample "look this is good".: 1 (1.8%)
     All comparisons between AI and human counselors based on ML Models. 

This study also was a "transcript analysis" via BERT: 1 (1.8%)
     This study also was a "transcript analysis" via BERT: 1 (1.8%)
     All comparisons between AI and human counselors based on ML Models. : 1 (1.8%)
     proof-of-concept prototype rather than a full-fledged clinical tool: 1 (1.8%)
     This is one of the few studies that also included psychodynamic psychotherapy in the intervention.: 1 (1.8%)
     proof-of-concept prototype rather than a full-fledged clinical tool

No evaluation of the conversation system. Study only displayed some conversation sample "look this is good".: 1 (1.8%)
     Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory


Therapy type: undirected therapy resulting from interacting with ChatGPT with no prespecified initial prompt: 1 (1.8%)
     no evaluation was performed. Only a sample conversation was shown "look this is good".: 1 (1.8%)
     No real evaluation. Only a sample conversatino is shown: 1 (1.8%)
     Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory: 1 (1.8%)
     Three LLMs (Claude-Sonnet-3.5, gemini-1.5-pro, gpt-4o) used for evaluation of optimal base model for developed application. Then used LLM (gpt-4o) as a judge to compare reults agains each other. : 1 (1.8%)

   Sample values (first 20):
     intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes
     proof-of-concept prototype rather than a full-fledged clinical tool
     No evaluation of the conversation system. Study only displayed some conversation sample "look this is good".
     proof-of-concept prototype rather than a full-fledged clinical tool

No evaluation of the conversation system. Study only displayed some conversation sample "look this is good".
     "Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality."
     This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%)
     This is one of the few studies that also included psychodynamic psychotherapy in the intervention.
     qualitative study - survey with 31 participants from recruited over Reddit
     no alpha adjusting (table 3)
     no evaluation was performed. Only a sample conversation was shown "look this is good".
     No real evaluation. Only a sample conversatino is shown
     very short study 
     the transformer model predicted items subscales from different clinical questionnaires
     Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory
     Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory


Therapy type: undirected therapy resulting from interacting with ChatGPT with no prespecified initial prompt
     This studiy does not make clear who the users are. It seems they had some users who competed a quiz
     VR maybe as solution for the embodiment problem? 
     Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one.
     The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots.
     This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy.


75. Notes on data set
   =====================
   Data type: object
   Total values: 488
   Non-null values: 267
   Null values: 221
   Unique values: 194
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 221 (82.8%)
     CounselChat https://github.com/nbertagnolli/counsel-chat: 5 (1.9%)
     Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.: 4 (1.5%)
     PsyQA: 4 (1.5%)
     Alexander Street Press therapy transcripts: 3 (1.1%)
     Alexander Street Press, Counseling and Psychotherapy Transcripts: Volume I: 2 (0.7%)
     Kaggle (n.d.)  This dataset encompasses 80 distinct tags, each contain
ing numerous conversational prompts and corresponding 
responses: 2 (0.7%)
     Data were obtained from an online and mobile therapy app (Talkspace) for services provided
between 2014 and 2019.: 2 (0.7%)
     ArmanEmo & Jigsaw Toxic Comment Classification translated into persian: 2 (0.7%)
     oth ChatGPT-4 and Bard responded to 20 tasks at each
stage of the study --> no new dataset created : 2 (0.7%)
     Original dataset: Perez-Rosas motivational interviewing dataset.
Modification: Added custom annnotations
Also extended with their own MI prompts.
We thus obtained 4,365 client prompt-counselor reflection pairs, including 2,429 prompt-CR and 1,636 prompt-SR pairs.

public under https://lit.eecs.umich.edu/downloads.html#PAIR: 2 (0.7%)
     No specific name: "We use the Reddit API to collect
a new dataset of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits."

Available under https://github.com/skgabriel/mh-eval: 2 (0.7%)
     Data from various sources ("Simple conversations, psychological questions, information on classic therapy sessions, and general advice for people with anxiety and depression. This data can be used to train a chatbot model that can act as a therapist to assist patients with anxiety and depression. The dataset includes intents. An "intent" is the purpose of a user's message. There are a total of 89 such intents, some of which are “greeting, sad, stressed, worthless, depressed, happy, anxious, sleep, scared, hate, problem” etc.").

Seemingly for intent classification.: 2 (0.7%)
     The Emobank dataset (Buechel & Hahn, 2022)
The GoEmotions dataset (Demszky et al., 2020) 
The International Survey on Emotion Antecedents and Reactions (ISEAR) (ISEAR
dataset, https://paperswithcode.com/dataset/isear, accessed 22.08.2023
Validation: The CrowdFlower dataset (Sentiment Analysis in Text - Dataset by crowdflower,
https://data.world/crowdflower/sentiment-analysis-in-text, accessed 07.08.2022): 2 (0.7%)
     First, we introduce a novel, golden standard dataset, comprising panel 
data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires 
and qualitative daily diary entries. : 2 (0.7%)
     Data Preparation: The sentiment analysis model is
trained on an annotated text classification dataset after
undergoing extensive cleaning, normalization, and tok-
enization processes to ensure the quality and integrity of
the data. The datasets used are compiled from a multitude
of sources, including a text classification repository of so-
cial media tweets, restaurant reviews, and airline reviews.
Data was also extracted from the Reddit API across vari-
ous forums such as those focused on CBT, mental health,
therapy, anxiety, depression, and CPTSD. Initially, the
dataset comprised 3,000 data points, with a text column
serving as the input and a target column providing labeled
sentiment. Recognizing the importance of data volume
in enhancing the accuracy and robustness of machine
learning models, we expand the dataset (TABLE I) to encompass 12,221 data points

for RAG: 2 (0.7%)
     Study describes FAITA Mental Health Framework and evaluates OCD Coach as an existing LLM-based Tool: 2 (0.7%)
     1,000 QA pairs from the PsyQA dataset.  two human annotators inspected and labeled the responses of professional supporters in the PsyQA dataset with the corresponding professional psychological counseling theory categories (Cognitive-Behavioral Therapy labeled as 1, Dialectical Behavior Therapy as 2, Person-Centered Therapy as 3, and Reality Therapy as 4). The original data had quantities of 189, 317, 196, and 298 for categories 1, 2, 3, and 4, respectively.: 2 (0.7%)
     From our previous work (Tauscher et al., 2022), we utilized data from a randomized controlled trial of a community-based text-message intervention for individuals with serious mental illness (Ben- Zeev et al., 2020).

"The maximum intervention dose was three “exchanges,” wherein an exchange is defined as a cluster of thematically connected back-and-forth messages between mobile interventionist and participant (e.g. three outgoing messages and participant responses). Texting strategies included: reminders (e.g., appointments, prescription refills), information provision (e.g., psychoeducation, links to regional events and resources), cognitive challenges (e.g., restructuring dysfunctional beliefs about voices, questioning the validity of self-sabotaging automatic beliefs), self-monitoring/self-reflection (e.g., guidance on self-evaluation of affect, journaling of symptomatic experiences), relaxation techniques (e.g., diaphragmatic breathing, guided imagery), social skills training (e.g., initiating conversations, maintaining eye contact), supportive messages (e.g., affirmations, inspirational quotes), and in-vivo instruction (e.g., pre-scheduled real-time support as the patient attempted a new activity)." (from Ben-Zeev et al.)

in addition they augmented their data using different strategies (3.2 Augmentation of text data): 2 (0.7%)
     The primary dataset used in this study consists of psychotherapy transcripts from
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. : 2 (0.7%)
     Motivational Interviewing (MI) counseling dataset from Perez-Rosas et al. (2016); Alexander Street dataset : 2 (0.7%)
      FLATT (Fun to Learn to Act and Think 
through Technology) trial + HCT (Healthy Campus Trial): 2 (0.7%)
     epsilon3/cbt-cognitive-distortions-analysis dataset
https://huggingface.co/datasets/epsilon3/cbt-cognitive-distortions-analysis
: 2 (0.7%)
     collected a substantial dataset comprising 21,924 records
Fig. 2. Volunteer Screening Chart
through five distinct avenues: instances, media, literature,
guidelines, and databases. To ensure the integrity and rele-
vance of our data, we undertook a meticulous cleaning pro-
cess spearheaded by three experienced quality controllers.
Each controller boasts broad expertise in mental health
counseling. Initially, we eliminated duplicates and linguistic
inaccuracies. Subsequently, we focused on screening data
for key terms such as “sleep”, “dream”, “evening”, “night”,
“bed” and related expressions. This step aimed to sift out
conversational data irrelevant to our study’s objectives, en-
suring that the textual content was pertinent to the context
of potential sleep disorders. We further conducted dialog
integrity screening. Finally, through manual inspection on a
case-by-case basis, we excluded dialogue data that contra-
dicted universal core values. This rigorous curation process
resulted in the selection of 764 dialogues: 2 (0.7%)
     24 patients were selected to participate in the study. Before participating in an interview, the patients were thoroughly 
informed of the study’s objectives and given the opportunity to provide informed consent during their outpatient visit. 
Following approval, patients used ChatGPT at home for two weeks to seek mental-health support in their own ways. The 
participants were required to interact with ChatGPT3 for a minimum of 15 minutes per day for 14 days. Twenty to thirty 
interviews were regarded an appropriate sample size for qualitative investigations, particularly those employing inter­
views as a method of data collection.
: 2 (0.7%)
     therapy transcriptions from real patients (that they recruited themselves): 2 (0.7%)
     PsyAdv Corp consisting out of Efaqa, CpsyCounD, SMILECHAT, PsyQA: 2 (0.7%)
     Using a dataset of Woebot questions and user responses (pulled from the Gen-W-MA internal testing data) labeled as being on or off topic, we fine-tuned an instance of the text-embedding-ada-002 model using the Azure OpenAI service: 2 (0.7%)
     Aditya Mental Health Counseling https://huggingface.co/datasets/Aditya149/Mental_Health_Counselling_Dataset: 2 (0.7%)
     fine-tuned on the 2 volumes of published counseling and psychotherapy data from
Alexander Street Press. The volumes are searchable collections of transcripts containing real counseling and therapy sessions
and first-person narratives illuminating the experience of mental illness and treatment. The 2 volumes contain 3500 session transcripts and >700,000 utterances between a counselor and a
patient.: 2 (0.7%)
     PsyQA, extended by "helping skills" column: 2 (0.7%)
     Books: 
E. J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015. + D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009. + B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014. 

To make sure it was appropriate for entering into the model, the data taken from the books went through a number of preparation steps
in this research: Extraction and cleaning.: 2 (0.7%)
     cognitive distortion identification dataset: 2 (0.7%)
     PsyQA is an authoritative Chinese dialogue dataset focused
on the field of mental health support. It covers 9 broad
topics and multiple subtopics, encompassing various aspects
of mental health. PsyQA is presented in a question-answer
format, where each example includes a question, a detailed
description of the mental issues, and keywords provided by
anonymous help-seekers. Responses are asynchronously pro-
vided by well-trained volunteers or professional counselors
and involve detailed analysis and guidance in response to the
help-seekers’ questions, aiming to offer mental health support.
Additionally, the answers have been further annotated with
seven psychological strategies based on psychological coun-
seling theory [12], : 2 (0.7%)
     Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms. It is intended for training and evaluating language models that provide safer, context-aware mental-health responses.: 2 (0.7%)
     The fine-tuning process involves the 
calibration of the pre-trained Transformer model with 
specific datasets relevant to cognitive therapy. This 
includes dialogues from CBT sessions, therapeutic 
interventions, and stress management conversations.

The GPT-2 model is fine-tuned using a specialized 
therapeutic dialogue dataset where the process involves 
supervised learning and the openly available trained 
model is adapted for a wide range of therapeutic 
contexts. The goal is to improve the ability of the model 
to provide relevant and empathetic responses during 
therapy sessions. The fine-tuning dataset includes a 
variety of therapeutic dialogues which ensures the 
model can handle different therapeutic scenarios 
effectively.: 2 (0.7%)
     The chatbot’s corpus comprises 200 manually curated context-response pairs that cover themes like stress, anxiety, depression, and motivation: 2 (0.7%)
     -: 2 (0.7%)
      For example, it uses conversational 
data gained in previous interactions with the user as well as 
datasets having labeled expressions of sentiment ("emo.txt" 
and "Sentiments1.txt"): 2 (0.7%)
     We use the HOPE (Mental Health cOunselling of PatiEnts) [8]
dataset for our experiments. The dataset is specifically designed
for mental health counseling and dialogue-act classification tasks.
The HOPE dataset includes approximately 12,800 utterances ex-
tracted from 212 mental health counseling sessions, sourced from
publicly available YouTube videos. : 2 (0.7%)
     Additionally, local data from Indonesian mental health articles and discussion forums fine-tuned the model.: 2 (0.7%)
     The research begins by curating a specialized User and
Counsellor Interaction CSV dataset by querying an autism
counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source
for reducing bias and adding a human touch. Alongside the
primary dataset, the additional data points ensure a compre-
hensive understanding of the domain.: 2 (0.7%)
     . All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021: 2 (0.7%)
     DailyDialog and EmpatheticDialogues as basis; translated and merged: CORTEX; (enriched by Polish Common Crawl) : 2 (0.7%)
     Emotional Support Conversation dataset. Comprises 1,053 multi-turn dialogues, amounting to 31,410 utterances. "We have augmented the dataset with additional annotations to signify the current state of the dialogue.": 2 (0.7%)
     assumes prior knowledge from mental health-related soruces or web-scraped materials: 2 (0.7%)
     Name of new dataset: HOPE. Derived from transcripts from YouTube counseling sessions. They added self-created annotations to these data.: 2 (0.7%)
     CounselChat https://github.com/nbertagnolli/counsel-chat
Partly translated to Kiswahili: 2 (0.7%)
     mental health and healthcare-related texts; sources not fully specified, combined domain corpora and simulated patient records: 2 (0.7%)
     HOPE: 2 (0.7%)
     MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube): 2 (0.7%)
     Responses by expert suicidologists on a previously-published standardized scale: the Suicide Intervention Response Inventory (SIRI-2)
[15].: 2 (0.7%)
     PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
: 2 (0.7%)
     from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.: 2 (0.7%)
     The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.: 2 (0.7%)
     Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website: 2 (0.7%)
     The study uses data from YiXinli Community, a major Chinese online counseling platform (https://www.xinli001.com) frequented by nearly 40 million users worldwide for psychological support. On this platform, 
users can confidentially express their psychological concerns and receive advice from human counselors in the open Q&A section (https://www.xinli001.com/qa). We utilized a web data crawler called houyicaiji (https://www.houyicaiji.com/) to gather this Q&A data, which encompasses 10,903 distinct questions and 19,682 human counselor responses collected from November 3, 2022, to 
March 30, 2023.: 2 (0.7%)
     zhang 2024 dataset: 2 (0.7%)
     CounselChat2,
provided through the HuggingFace Hub platform3. : 2 (0.7%)
     We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: "This site is fiction and all the characters are fictional.": 2 (0.7%)
     https://huggingface.co/datasets/jkhedri/psychology-dataset; Only questions embedded, not answers.: 2 (0.7%)
     Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”: 2 (0.7%)
     Huggingface: Mental Health Counseling Conversations
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations: 2 (0.7%)
     By using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
These characteristics encompassed a spectrum of ages (40 and
80 years old), genders (male, female, transgender male, and
transgender female), races (non-Hispanic black, non-Hispanic
white, and Hispanic), and annual income ($25,000, $50,000,
$500,000, and $1,000,000). We also generated a control group
without assigned identities. Each DSP represented a unique
mix of these demographic characteristics.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses: 2 (0.7%)
     The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dialogues. The dialogues were developed by our research team, including a board-certified psychiatrist and a clinical psychologist, and peer-reviewed using evidence-based (primarily CBT) modalities.: 2 (0.7%)
     Random sample of 476 Protocall crisis calls: 2 (0.7%)
     Emotional Support Conversation dataset (Liu 2021 Towards emotional support dialog systems): 1 (0.4%)
     SIRI-2 with added expert suicidologist ratings: 1 (0.4%)
     
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. : 1 (0.4%)
     HOPE (Mental Health cOunselling of PatiEnts): 1 (0.4%)
     The chatbot’s corpus comprises 200 manually curated
context-response pairs that cover themes like stress, anxiety, 
depression, and motivation: 1 (0.4%)
     [COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. 

WebMD, Mayo Clinic and Heatlhline.com 
- One text exchange between a patient and their therapist is included in this dataset. The dataset was assembled from online FAQs, WebMD, Mayo Clinic, and HealthLine, among other well-known healthcare blogs: 1 (0.4%)
     CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat: 1 (0.4%)
     Primate2022 https://github.com/primate-mh/Primate2022 (but only for evaluation): 1 (0.4%)
     see Generating Data for Evaluation: 1 (0.4%)
     Evalution with Primate2022 dataset, but not for development : 1 (0.4%)
     Primate2022 https://github.com/primate-mh/Primate2022: 1 (0.4%)
     CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat

[COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. : 1 (0.4%)
     Some documents for their RAG database but no further information is given.: 1 (0.4%)
     Data crawled from YiXinli Community (xinli001.com/qa). They crawled this data themselved ->  10,903 distinct questions and 19,682 human counselor responses: 1 (0.4%)
     selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.

EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed): 1 (0.4%)
      Korean Children Voice Records from
AI-Hub: 1 (0.4%)
     Alzheimer's Q&A dataset https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset: 1 (0.4%)
     selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.: 1 (0.4%)
     Real therapy transctipts from Alexander Street Press: 1 (0.4%)
     The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately! : 1 (0.4%)
     multiple sets: existing clinical datasets - e.g. from ADHD-200 Global Competition; transcriptions of therapist-patient interactions; DHD support group discussions : 1 (0.4%)
     The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately!

But user has other dataset that fits description: https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset: 1 (0.4%)
     
1 https://huggingface.co/datasets/jkhedri/psychology-dataset.: 1 (0.4%)
     EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues

From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger.: 1 (0.4%)
     data from Counsel Chat repository and Indonesian mental health articels: 1 (0.4%)
     qualitative data from 69 students: 1 (0.4%)
     We gathered doctor-
patient conversations from online medical consultation web-
sites and identified representative psychological issues from
the experiences of people around us and public platforms
such as Weibo and Zhihu. Our dataset encompasses the most
common mental health concerns and A substantial portion of
the themes is centered on stress and interpersonal relationships,
as these two are important contributing factors to mental
disorders. We also considered the perspectives of the LGBTQ
community and diverse cultural groups.Utilizing this data, we
crafted 31 unique questions.: 1 (0.4%)
     MDRDC and Dialogue Safety: 1 (0.4%)
     data from Kaggle: 1 (0.4%)
     a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists

The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG: 1 (0.4%)
     Bilingual (English & Arabic) mental health text corpus, origin not found: 1 (0.4%)
     The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG: 1 (0.4%)
     Web-scraped data from diverse sources: 1 (0.4%)
     In this study, 200 real counseling ses
sions were recorded and transcribed to capture authentic 
client interactions. From these, 50 client statements were 
identified that best represented a range of emotions such as 
sadness, guilt, anxiety, determination, and anger, based on 
the PANAS Scale, along with helplessness, love, trust, lone
liness, and doubt from existing literature, aiming to ensure

emotional expressions from psychological counseling sessions: 1 (0.4%)
     Web-scraped data from diverse sources

Bilingual (English & Arabic) mental health text corpus, origin not found: 1 (0.4%)
      Reddit with of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.: 1 (0.4%)
     mental health counseling conversations from huggingface; fine-tuned with unsloth and ollama: 1 (0.4%)
     The data were provided and 
anonymised by Qwell: 1 (0.4%)
     In order to effectively function and provide personalized
response to a user, data preparation plays a vital role. The
data comes from multiple sources, such as books, articles,
news, media, etc

We started experimenting with fine-tuning-based approach.
To generate the training dataset, we collected various mental
health related books, articles, and news stories and prepared
almost 500 pairs of instructions in the format of ”system”,
”user”, and ”assistant”. What we found is that this approach
is good for an LLM-based system that is already trained
on certain things: 1 (0.4%)
     Table 1: 1 (0.4%)
     ArmanEmo dataset, a Persian emotion detection dataset: 1 (0.4%)
     PsyQA dataset (Sun et al., 2021). : 1 (0.4%)
     The dataset used for training and
validating the chatbot application model was obtained from an expert community website called Counselchat.com whose  platform is a link between users and verified therapists [17]. 
In this platform, verified counsellors provided responses to questions asked by users. This counselling dataset has 2,128 entries and 10 columns as shown Table I below [17].: 1 (0.4%)
     epsilon3/cbt-cognitive-distortions-analysis: 1 (0.4%)
     495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores … and qualitative daily diary entries: 1 (0.4%)
     Mental Health Counseling Conversations Dataset: 1 (0.4%)
     Between November 28 and December 14, 2023, our study collected a substantial dataset comprising 21,924 records through five distinct avenues: instances, media, literature,
guidelines, and database.: 1 (0.4%)
     EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues: 1 (0.4%)
     Multiple external datasets used: 
a) From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger. 
b) From Bertagnolli [53], including high-quality therapist responses to real patient´s mental health questions.: 1 (0.4%)
     EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed): 1 (0.4%)
     The MSP-Podcast Corpus is a well annotated dataset 
which includes a wide range of emotional expressions 
captured through spontaneous speech. The dataset's 
diversity and the quality of annotations make it an 
excellent resource for making them availed . Its 
annotations ensure reliability, making it invaluable for 
training and evaluating emotion recognition models

Video Dataset: FER+ Dataset  provides quality images with 
annotations for various facial expressions
: 1 (0.4%)
     PsyQA dataset (and PsyAdv Corp, and CPSyCounE): 1 (0.4%)
     Open Up is a free24/7 web-based, text-based counseling service in Hong Kong that enables people aged between 11 and 35 years to anonymously chat with paid staff (staff counselors or social workers) or trained volunteers. 5240 sessions with 533,609 messages (Figure 3). We stratified the 5240 sessions based on the number of messages in each session. There were a total of 131 unique message count groups among the 5240 sessions.: 1 (0.4%)
     derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com): 1 (0.4%)
     Protocall random sample of crisis calls: 1 (0.4%)
     Subreddits in the following categories: (a) Coping and Therapy (C-Th): 7Cup
sofTea, Existential_crisis, getting_over_it, Grief-
Support, helpmecope, hardshipmates, HereToHelp,
itgetsbetter, LostALovedOne, offmychest, MMFB,
Miscarriage, reasonstolive, SuicideBereavement,
therapy; (b) Mood Disorders (MD): depression, de-
pressed, lonely, mentalhealth; (c) Psychosis and
Anxiety (P-An): anxiety, BipolarReddit, socialanxi-
ety; and (d) Trauma and Abuse (Tr-A): abuse, sur-
vivors, Anger, emotionalabuse, PTSDcombat.
Alexander Street Press video transcripts: 1 (0.4%)
     self-created "Empathic Conversation dataset": 1 (0.4%)
     EXTERNAL
- Data scraped from mental health subreddits
- Alexander Street Press video transcripts
	
Notes on data setSubreddits in the following categories: (a) Coping and Therapy (C-Th): 7Cup
sofTea, Existential_crisis, getting_over_it, Grief-
Support, helpmecope, hardshipmates, HereToHelp,
itgetsbetter, LostALovedOne, offmychest, MMFB,
Miscarriage, reasonstolive, SuicideBereavement,
therapy; (b) Mood Disorders (MD): depression, de-
pressed, lonely, mentalhealth; (c) Psychosis and
Anxiety (P-An): anxiety, BipolarReddit, socialanxi-
ety; and (d) Trauma and Abuse (Tr-A): abuse, sur-
vivors, Anger, emotionalabuse, PTSDcombat.
Alexander Street Press video transcripts

SELF COLLECTED
- Empathic Conversation dataset: 1 (0.4%)
     Survey of 236 participants evaluating ChatGPT-generated vs. psychologist-generated psychological recommendations.: 1 (0.4%)
     Only evaluation data: Survey of 236 participants evaluating ChatGPT-generated vs. psychologist-generated psychological recommendations.: 1 (0.4%)
     "Open Up" text-based online counseling service: 1 (0.4%)
     external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS

To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube: 1 (0.4%)
     FLATT dataset (Fun to Learn to Act and Think through Technology): 1 (0.4%)
     Motivational Interviewing counseling dataset (Perez-Rosas 2016)

The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The dataset is derived from a collection of 284 video recordings of counseling encounters using MI. The recordings were collected from various sources, including two clinical trials, students’ counseling sessions from a graduate level MI course, wellness coaching phone calls, and demonstrations of MI strategies in brief medical encounters.: 1 (0.4%)
     42 tinitus patients as raw data + augmented dataset as test samples: 1 (0.4%)
     The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients: 1 (0.4%)
     42 tinitus patients as raw data + augmented dataset as test samples

The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients: 1 (0.4%)
     MotiVAte: 1 (0.4%)
     MotiVAte

This dataset comprises of 4k dyadic conversations between the depressed support seekers and the VA imparting appropriate suggestion, hope and motivation resulting in a total of 14,809 utterances. The conversations of the MotiVAte dataset are collected from peer-to-peer support forum and modified to represent dyadic conversations for an end-to-end online mental health support system.: 1 (0.4%)
     derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform : 1 (0.4%)
     external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS: 1 (0.4%)
     unspecified ("credible sources"): 1 (0.4%)
     User comments from the r/Replika subreddit: 1 (0.4%)
     clinical outcome data from evaluation: 1 (0.4%)
     created an own based on therapy transcript documents from different websites: 1 (0.4%)
     PsyQA and Crawl of various chinese social media platforms Tianya, Zhihu, Yixinli: 1 (0.4%)
     training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.: 1 (0.4%)
     PsyQA and Crawl of various chinese social media platforms Tianya, Zhihu, Yixinli

training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.

fine-tuning: PsyQA (56,000 question-answer pairs): 1 (0.4%)
     publically available online data resources: 1 (0.4%)
     External: PsyQA
Derived: CBT QA: 1 (0.4%)
     MESConv dataset generated from YT videos: 1 (0.4%)
     To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube: 1 (0.4%)
     HOPE
212 multi-turn English psychotherapy sessions (≈ 12.9 K utterances) between therapist and patient, transcribed from YouTube videos: 1 (0.4%)
     We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

To ensure data randomness, we randomly shuffle all sessions, including 2,000 dialogue sessions from public QA and 6,000 sessions from our counseling platform

https://github.com/qiuhuachuan/DialogueSafety: 1 (0.4%)
     We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

we crawl 2,000 blog titles from Yixinli’s QA column

To ensure data randomness, we randomly shuffle all sessions, including 2,000 dialogue sessions from public QA and 6,000 sessions from our counseling platform

Yixinli’s QA column https://www.xinli001.com/qa

self-collected: self-developed text-based counseling platform. external: Yixinli’s QA column https://www.xinli001.com/qa: 1 (0.4%)
     Name of new dataset: CORTEX. Derived from DailyDialog, EmpatheticDialogues: 1 (0.4%)
     Some PDFs for configuring a custom GPT (type of PDFs not further specified): 1 (0.4%)
     Dataset was used for evaluation only.

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539]): 1 (0.4%)
     Dataset was used for prompt development and final evaluation

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539]): 1 (0.4%)
     MotiVAte, EmpatheticDialogues: 1 (0.4%)
     In version 1 of the generator, the fine-tuning question-and-response data set came from 2 sources: the first was our prior work [40,41], and the second data source was from earlier deployments of MIBot, before the creation of MIBot v4.7. The reflections used came from a variety of sources: from previous versions of this chatbot that were deemed to be acceptable MI reflections by MI-literate researchers or actual reflections produced by MI-literate researchers or MI-expert clinicians.

To address the rate of poor reflections, we developed version 2 of the generator with 2 significant enhancements. First, a larger set of 301 fine-tuning triplets were collected over approximately 10 months of deploying the chatbot, making use of the various responses from smokers who had been recruited in a similar manner, as described in the Participant Recruitment and Screening subsection. This second data set did not include any of the data from the earlier chatbot version [40,41]. Only MI-consistent reflections were used, which were sourced from MI clinicians, MI-literate researchers, or version 1 of the generator. The labeling and selection of the MI-consistent reflections were improved by using multiple human raters and a carefully controlled decision tree to determine the validity of the reflections. The new rating scheme itself was stricter than the one used in version 1, which caused the hit rate to go down—not because the generation was worse but because of the stricter rating. The hit rate of the new generator was measured to be 55.1% (166/301) on a set of reflections.: 1 (0.4%)
     survey data from 31 participants: 1 (0.4%)
     counsel-chat: 1 (0.4%)
     Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown. : 1 (0.4%)
     "dataset" were PHQ-9 questions: 1 (0.4%)
     Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.: 1 (0.4%)
     We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset: 1 (0.4%)
     We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset

Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.

https://www.kaggle.com/datasets/sagarikashreevastava/cognitive-distortion-detetction-dataset: 1 (0.4%)
     Only questions and answers from MI chatbot reflections by Brown 2023 -- A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study.
The authors generate reflections based on these question-answer pairs using GPT-4: 1 (0.4%)
     unclear who the users of the MH quiz module are: 1 (0.4%)
     Data from messenger application LINE (line.me) from Osaka Prefectural Government: 1 (0.4%)
     Dataset collected (Ben-Zeev et al., 2020) and rated (Tauscher et al., 2022) in previous studies conducted by same institution: 1 (0.4%)
     Reddit, Twitter an Mental Health Forums as data source: 1 (0.4%)
     counsel-chat dataset, accessible on Hugging Face ( 2,700 anonymised
talks between individuals and experienced counsellors on the
website www.counselchat.com): 1 (0.4%)
     reddit r/ChatGPT: 1 (0.4%)
     Model not really trained on the dataset, but datasets used to generate suitable prompts. 

Datasets used: AnnoMI + 20 full-length simulated sessions using GPT, seeded with anonymized baseline data from young adult participants with hazardous alcohol use drawn from a recent clinical trial: 1 (0.4%)
     {17} J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015.
[18] D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009.
[19] B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014.: 1 (0.4%)
     PsyQA dataset: 1 (0.4%)
     Aditya Mental Health Counselling Dataset, Mental Health Counselling Chat, 
Counsel Chat Dataset and Amod-Mental Health Counselling Conversations: 1 (0.4%)
     Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown.: 1 (0.4%)
     used chatgpt as is. dataset is outcome data and not used for dev purposes: 1 (0.4%)
     Kaggle mental health conversational dataset https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data: 1 (0.4%)
     Talkspace data: 1 (0.4%)
     We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].

4.1 Curated Situations & Negative Thoughts
 We start by curating data sources for situations and negative thoughts.
 Thought Records Dataset (Burger et al., 2021).
 This dataset contains hypothetical and real-world situations, thoughts and emotional processes reported by crowdworkers on Amazon Mechanical Turk. We manually curate 180 pairs of diverse situations with negative thoughts from this dataset.
 Mental Health America (MHA). Situations and thoughts from crowdworkers may not reflect the broad range of mental health challenges that people face in real-life. To incorporate more real-world situations and thoughts, we ran a survey on the MHA website (screening.mhanational.org). MHA visitors (who typically use the website for screening of mental illnesses) were asked to describe any negative thoughts and the associated situations they were struggling with. We manually curate 120 pairs of self-reported situations and thoughts to ensure broad coverage of relevant topics based on high diversity and manual filtering.

https://github.com/behavioral-data/Cognitive-Reframing: 1 (0.4%)
     External: We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].
Self-collected: In collaboration with mental
health experts (some of whom are co-authors), we manually labeled
500 thoughts and situations to identify the potential issues that
they are associated with. The result of this iterative open-ended
coding process was a set of 16 diferent issues expressed by par
ticipants. We used this dataset to fnetune a GPT-3 mod: 1 (0.4%)
     CounselChat1,
the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 : 1 (0.4%)
     Due to this, data from the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 were selected. Questions
and answers are extracted, and questions are manually generated
dependent on the information available, e.g. for the NHS definition
of depression, questions such as “what is depression?" are imputed.: 1 (0.4%)
     CounselChat, Brain & Behaviour Research Foundation, NHS, Wellness in Mind, White Swan Foundation: 1 (0.4%)
     1. publicly available dataset comprising HGC, 2. AI-generated content generated by ChatGPT: 1 (0.4%)
     data sourced from Xinli001.com: 1 (0.4%)
     data sourced from Xinli001.com. Counsellor responses were included in the dataset. ChatGPT responses were generated by the authors.: 1 (0.4%)
     EmoBank, GoEmotions, ISEAR, CrowdFlower: 1 (0.4%)
     only outcome data: 1 (0.4%)
     TalkLife: 1 (0.4%)
     TalkLife (talklife.co) is the largest online peer-to-peer support platform for mental health support. It enables conversations between people seeking support (support seekers) and people providing support (peer supporters) in a thread-like setting. We call the post authored by a support seeker as seeker post, and the response by a peer supporter as response post. Table 1 describes the statistics of conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question "Is the seeker talking about a mental health related issue or situation in his/her post?". Using this annotated dataset, we train a standard text classifier based on BERT [15] (achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-related conversations. This dataset contains 3.33M interactions from 1.48M seeker posts.

https://github.com/behavioral-data/Empathy-Mental-Health/tree/master: 1 (0.4%)
     TalkLife (talklife.co) is the largest online peer-to-peer support plat-
form for mental health support. It enables conversations between
people seeking support (support seekers) and people providing sup-
port (peer supporters) in a thread-like setting. We call the post au-
thored by a support seeker as seeker post, and the response by a
peer supporter as response post. Table 1 describes the statistics of
conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number
of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question "Is the seeker talking about a mental
health related issue or situation in his/her post?". Using this anno-
tated dataset, we train a standard text classifier based on BERT [15]
(achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-
related conversations. This dataset contains 3.33M interactions from
1.48M seeker posts.: 1 (0.4%)
     no dataset. ChatGPT is used as is.: 1 (0.4%)
     Data were 20 thoughts written by two CBT therapists (each wrote 10 thoughts).: 1 (0.4%)
     reddit mental health conversations mentioning ChatGPT: 1 (0.4%)

   Sample values (first 20):
     . All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021
     Data from messenger application LINE (line.me) from Osaka Prefectural Government
     clinical outcome data from evaluation
     Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”
     created an own based on therapy transcript documents from different websites
     We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: "This site is fiction and all the characters are fictional."
     CounselChat2,
provided through the HuggingFace Hub platform3. 
     CounselChat https://github.com/nbertagnolli/counsel-chat
     PsyQA and Crawl of various chinese social media platforms Tianya, Zhihu, Yixinli
     training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.
     PsyQA and Crawl of various chinese social media platforms Tianya, Zhihu, Yixinli

training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.

fine-tuning: PsyQA (56,000 question-answer pairs)
     publically available online data resources
     Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website
     The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.
     from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.
     PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 

     External: PsyQA
Derived: CBT QA
     User comments from the r/Replika subreddit
     PsyQA
     MESConv dataset generated from YT videos


76. Notes on development approach
   =================================
   Data type: object
   Total values: 488
   Non-null values: 213
   Null values: 275
   Unique values: 141
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 275 (129.1%)
     GPT-3.5 is only prompted but there is also an intent classifier and cognitive distortion identifier: 2 (0.9%)
     Prompt engineering to shape ChatGPT’s dialogue: 2 (0.9%)
     Prompting of general purpose LLMs (stage 1, probably ChatGPT) + fine-tuning of BERT (stage 2) + some type of RAG based on SimCSE (stage 3): 2 (0.9%)
     two age-appropriate AI chatbots, leveraging GPT-4, were developed to provide natural, empathetic conversations: 2 (0.9%)
     FAITA-mental health chatbot was designed to classify OCD-Coach's responses: 2 (0.9%)
     prompted to simulate a psychiatric first-responder chatbot: 2 (0.9%)
     Retrieval Augmented Generation (RAG) kombiniert mit prompting: 2 (0.9%)
     The model was trained using one-shot 
learning, meaning that it was given a client statement and 
a counselor’s emotional reflection response as input, and 
then used that information to generate its own emotional 
reflection response.: 2 (0.9%)
     They fine-tune RoBERTa but only prompt the generative models (Flan-t5, Mistral, GPT-3.5) that they use: 2 (0.9%)
     Designed prompt phrases, iteratively refined; three prompting regimes (zero-shot, few-shot, CoT); modeled counselor reasoning style; two LLM families evaluated (ChatGLM, ERNIE Bot; also Qianwen): 2 (0.9%)
     Trained on ArmanEmo dataset & Jigsaw Toxic Comment Classification translated into persian + user message validiation module. : 2 (0.9%)
     We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).: 2 (0.9%)
     Evaluated existing GPT-based chatbots via persona-based simulated interactions, scored with CAPE framework. : 2 (0.9%)
     Fine-tuning of GPT-2 + other modules such as BERT-based cognitive distortion classification: 2 (0.9%)
     “cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies” (Position: Methods): 2 (0.9%)
     LoRA: 2 (0.9%)
     Fine-tuning of GPT-4 + RAG (see Fig. 1) and user interface: 2 (0.9%)
     Used scripted prompts to elicit chatbot interactions and compared with therapists; coded with MULTI; non-naturalistic brief interactions. — Quote: “We also only examined brief interactions with scripted prompts… The interaction logs … were coded using the Multitheoretical List of Therapeutic Interventions” (Position: p. 12; PubMed Methods).: 2 (0.9%)
     Prompts were comprised of (1) context for the request, (2) how the model was meant to respond, 3) the specific request, and (4) the output format : 2 (0.9%)
     The authors adapted ChatMGL, a GPT-2–based model originally trained on STEM Q&A with supervised fine-tuning and PPO, for mental health dialogue. They dropped PPO, focusing instead on supervised fine-tuning with Hugging Face’s SFTT and Delta Tuning (a parameter-efficient method that updates only part of the weights). : 2 (0.9%)
     The proposed chatbot system aims to enhance mental health 
assistance through generative AI and embedding-based 
relevance detection. Unlike rule-based chatbots with 
predefined responses, our chatbot responds dynamically to 
diverse user inputs. The core innovations include few-shot 
learning for personalized conversation, semantic similarity 
filtering to ensure relevance, and generative response 
generation using Google Gemini API. : 2 (0.9%)
     Prompt engineering + RAG: 2 (0.9%)
     Their intervention: The AI therapist tested in this study is an LLM-based pro- gram previously developed and tested at Cedars-Sinai Medical Center, with the goal of offering AI-enabled, self-
administered, mental health support within immersive virtual
reality environments.18 The user interacts with an AI conver-
sational agent designed for history-taking and therapeutic sup-
port for anxiety and depression. The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
The agent facilitates turn-based conversations, summarizing
and concluding each session, and was found to be an accepta-
ble and feasible support modality for patients with mild-to-
moderate anxiety or depression.18 Here, we focus on whether
there is evidence of any sociodemographic bias exhibited by
the conversational agent.: 2 (0.9%)
     ChatGPT played the role of a client and optionally provided AI-generated feedback to trainees: 2 (0.9%)
     The system integrates two main components: a BERT-based emotional distress detection module and a fine-tuned GPT-3.5 model for Psychological First Aid (PFA) response generation.
First, BERT analyzes the user’s input to detect their emotional state and distress level. These outputs are then passed, along with the original user input, into a custom prompt designed for GPT-3.5. GPT-3.5 was fine-tuned on therapy transcripts to improve its ability to produce empathetic, context-aware PFA responses.: 2 (0.9%)
     uses NLP and probabilistic models to classify symptoms, map user queries to mental health knowledge base, and personalize therapeutic suggestions: 2 (0.9%)
     A base LLM was fine-tuned on 80 structured psychological counseling cases to adapt it to clinical contexts. Additionally, the model uses dynamic prompting with a structured DSM-5-based knowledge base. This is integrated into a larger pipeline involving text summarization, keyword extraction, and similarity analysis, alongside multimodal outputs (e.g., facial expressions, voice, and a visual avatar) to enable affective interaction.: 2 (0.9%)
     Only sub-study 3 employed an LLM: 2 (0.9%)
     The help-seeking questions were posted to the GPT-3.5-turbo model by OpenAI API to get responses from ChatGPT: 2 (0.9%)
     Prompt engineering on GPT-4. Authors describe the model is being "tuned" or "fine-tuned" but in fact they only iteratively adjust the prompt without any weight adjustment.: 2 (0.9%)
     Clare is a standalone tool, based on fine-tuned LLMs plus other modules voice inputs, safety checks: 2 (0.9%)
     Elaborate server architecture around OpenAI API (see Fig. 1): 2 (0.9%)
     Research team members prompted LLMs with the original instructions for the SIRI-2, as well as with one of the SIRI2-2’s 24 items. We did not prompt LLMs with any additional text. : 2 (0.9%)
     Fine-tuning (according to Fig. 3 at least) and RAG: 2 (0.9%)
     The chatbot was developed using the open-source machine learning framework Rasa, chosen for its strong support for customization. Rasa integrates two core modules: NLU for understanding user input and Core for managing dialogue flow. The development involved setting up the Rasa environment and coding in Python, including the installation of necessary components for both language understanding and dialogue management.: 2 (0.9%)
     Fine-tuned LLaMA 3.2 (3B parameters) using Unsloth and Ollama; used RAG for retrieval, LangChain memory for conversation history, and FAISS for relevant material recall: 2 (0.9%)
     Retrieval + hierarchical fusion + attention + lexicon-enriched embeddings used to prompt LLMs.: 2 (0.9%)
     -: 2 (0.9%)
     Socrates was subjected to rigorous fine-tuning with pains-takingly detailed instructions designed to optimize therapeutic interactions. The model was specifically trained to recognize and appropriately respond to a wide spectrum of emotional states expressed by users, while respecting moments of  silence or reluctance to engage in conversation. It was calibrated to avoid overly intrusive questioning that might compromise user comfort and trust and instead detect nuanced linguistic patterns that might indicate various psychological states. Throughout its development, emphasis was placed on maintaining an appropriate balance between providing support and encouraging self-reflection: 2 (0.9%)
     Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. Like the therapists’ instructions, the prompt did not place any limits on the length of the response, nor was ChatGPT shown the therapists’ responses.: 2 (0.9%)
     Instructions to act like a counselor etc. implemented; This model also allowed us to use retrieval augmented generation technology, which allows the model to be provided with a corpus of knowledge to reduce hallucinations
and misinformation [8]. Both the provision of instructions and source materials can serve as guardrails that keep the chatbot in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate.: 2 (0.9%)
     PEFT (qLoRA) fine-tuning of T5: 2 (0.9%)
     Fine-tuned LLM + integration of other data modalities (vision): 2 (0.9%)
     The research team, consisting of an AI product manager and
prompt engineer (OP), several expert psychologists (YH, KG, ZE,
and DHS), and a music therapist (TA), jointly developed the two
AI tools from December 2023 to January 2024. Initially, a
primary prompt was created by YH. This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests
and added operational instructions to ensure safety and ethics.
Subsequently, group members with clinical backgrounds (YH,
KG, ZE, TA) tested the tool on themselves and shared their
experiences and suggestions for improvements. Throughout this
process, the team realized that due to the tools’ depth and their
ability to reflect on unconscious internal parts, the presence of a
mental health care professional would be essential. An expert in
social psychology, DHS, addressed cultural issues, which were
also incorporated into the prompt. Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications.: 2 (0.9%)
     Developed by fine-tuning a GPT-2 model on a domain-specific dataset and integrating it into a larger architecture that included array-based context storage for multi-turn dialogue, Top-K/Top-P sampling for controlled text generation, and response ranking modules using BLEU scores and cosine similarity to select the most relevant and coherent reply.: 2 (0.9%)
     Q-LoRA fine-tuning: 2 (0.9%)
     Llama 2 is prompted, but there is also a RAG component: 2 (0.9%)
     Uses RAG with prompt engineering: general LLM is guided by structured prompt templates for intent detection and empathetic response generation, while a vector-based QA database supplies domain-specific context.: 2 (0.9%)
     For the ChatGPT-generated advice, version GPT-4 was used. On October 12, 2023, 
the following question was asked on a fresh chatbot session and the response was saved:
[...] Since the study’s objective was to evaluate performance of ChatGPT-4 with-
out any refinement or customization, we deliberately refrained from using any prompt 
engineering techniques in our design and opted to use the afore mentioned one-time 
instruction to the LLM. : 2 (0.9%)
     Fine-tuning of GPT-2 + voice and video interaction system: 2 (0.9%)
     Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization: 2 (0.9%)
     Different models are simply fine-tuned on provided datasets: 2 (0.9%)
     Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.: 2 (0.9%)
     Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).: 2 (0.9%)
     Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1): 2 (0.9%)
     We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.: 2 (0.9%)
     davinci-003 + off topic classifier. davinci-003 seems not finetuned: 2 (0.9%)
     Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.: 2 (0.9%)
     Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).: 2 (0.9%)
     BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.: 2 (0.9%)
     "LangChain for Prompting" combined with "CNN for Facial Recognition," "LSTM for Text-based Emotion Detection," "MFCC or LSTM for Audio Recognition": 2 (0.9%)
     Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis: 2 (0.9%)
     Four unique questions were asked of ChatGPT 4.0, and each question was asked five separate times. Each question asked to the model 
was performed in a separate, new conversation (10): 2 (0.9%)
     Prompting of GPT-4o + components of a robot (speech-to-text, movement, etc.): 2 (0.9%)
     "We fine-tuned the model with an 80% to 20% train-test split" + "that is, the system is prompted
to keep generating responses"...: 2 (0.9%)
     See Figure 3. All revolves around prompting of GPT-4, and then there is a "memory concept" and other stuff: 2 (0.9%)
     The implementation of the speak-speak feature involves
specifying the models, prompts (as we are testing in a zero
learning shot environment): 2 (0.9%)
     Framework integrates ChatGPT with multimodal data (speech, facial, body movement), preprocessing via Whisper and feature extraction (librosa). Prompts include speech-derived descriptors (speed, pitch, rhythm, pauses) appended to text before input to ChatGPT. : 2 (0.9%)
     Only prompting to evaluate capabilities of LLMs in terms of stigma and their responses to mental health symptoms.  In addition to prompting models with the stimuli without in-context examples, we also employed a novel method of prompting models with a portion of real therapy tran-
scripts from Alexander Street Press [6, 7]: 2 (0.9%)
     in a further chat interface (test 2), we
introduced the scope of expertise using a prompt as a pretraining tool, before proceeding
with the presentation of the case. This prompt was generated using ChatGPT, and its
reliability as a pretraining tool for the chat session was verified as a good alternative to
a longer and more complex training process in a previous study we conducted [12]: 2 (0.9%)
     "A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative" .. "The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately" ... "Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition": 2 (0.9%)
     Prompting of different LLMs + other modules (speech-to-text, editable dashboard, etc.): 2 (0.9%)
     see Fig. 1 : 1 (0.5%)
     PEFT + QLORA: 1 (0.5%)
     Fine-tuning + knowledge retrieval: 1 (0.5%)
     Main LLM was fine-tuned but there are also other modules (e.g. safety classification)

Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).: 1 (0.5%)
     Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).: 1 (0.5%)
     Main LLM was fine-tuned but there are also other modules (e.g. safety classification): 1 (0.5%)
     Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. : 1 (0.5%)
     Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4): 1 (0.5%)
     The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice. 

Fig. 1: 1 (0.5%)
     The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice.: 1 (0.5%)
     Cross-mutation/crossover to swap context & emotion while fixing behavior; compute BSI & CR; chi-square tests — Quote: “behavior-anchored crossover-mutation… fixes the behavior dimension, swaps context and emotion… Core metrics: Behavior Sensitivity Index (BSI)… Consistency Rate (CR)… Chi-square test”: 1 (0.5%)
     Fine-tuned with PEFT-LoRA: 1 (0.5%)
     bots were queried with a fixed set of 12 prompts derived from real search queries; responses were coded against a guideline derived index: 1 (0.5%)
     Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework: 1 (0.5%)
     Prompt engineering improved MI fidelity (Phase II); GPT-4 via secure API; HIPAA-compliant setup. — Quote: “explicit prompting for adherence to validated MI literature and more collaborative, client-centered language enhanced the therapeutic alliance.”: 1 (0.5%)
     Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier: 1 (0.5%)
     Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.: 1 (0.5%)
     Pre-trained ChatGPT-3.5-Turbo API without fine-tuning but prompt engineering, integrating it with other system components.: 1 (0.5%)
     Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework

Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.: 1 (0.5%)
     Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.: 1 (0.5%)
     "We use ChatGPT as our base LLM and access it through
the API with prompts": 1 (0.5%)
     Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses: 1 (0.5%)
     Hybrid pipeline: NLP preprocessing → VADER sentiment scoring with thresholds → template or GPT-3.5-turbo generation → referral logic and resources → logging/monitoring; context tracking for coherence. : 1 (0.5%)
     Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B
Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses: 1 (0.5%)
     Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier (see Fig. 1): 1 (0.5%)
     Fine-tuning via LoRA + RAG pipeline: 1 (0.5%)
     Prompt Engineering + RAG + generic LLMs: 1 (0.5%)
     Built on LangChain framework with memory and journaling integration: 1 (0.5%)
     Built on LangChain framework with memory and journaling integration .. Input/Templates/Prompt Instruction: 1 (0.5%)
     Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B: 1 (0.5%)
     BERT and GPT-3.5 fine-tuned, interacting with each other: 1 (0.5%)
     The design phase mentions fine-tuning and prompt engineering, but the implemented study relied on a simple prompt with GPT-3.5. — Quote: “The design process addressed… fine-tuning the LLMs, and prompt engineering.” / “The initial study used a relatively simple prompt…”: 1 (0.5%)
     chatgbt 4 was fine-tuned with external (and internal) data sets, fine-tuned through prompt engineering and integrated into a technical framework : 1 (0.5%)
     Three different prompt methods: Zero-Shot, Few-Shot, Chain of Thought (CoT): 1 (0.5%)
     dataset fine-tuning, sentiment analysis within RAG models: 1 (0.5%)
     the proposed system integrates several modules: emotion detection, disorder identification, and language model validation see figure 1 : 1 (0.5%)
     Used fine-tuning and modular architecture (context, expression, feedback generators) to simulate different therapeutic styles.
Three key technical components were employed: the Context Generator, the Expression Expander, and the Feedback Generator: 1 (0.5%)
     GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.

MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules: 1 (0.5%)
     GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.: 1 (0.5%)
     MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules: 1 (0.5%)
     The system uses a cognitive architecture that combines NLP, a Theory of Mind module with user modeling and forecasting, CBT-based expert knowledge, strategy control, and natural language generation.: 1 (0.5%)
     Fine-tuning of GPT-2 + memory mechanism (storing previous embeddings in an array): 1 (0.5%)
     ChatGLM model with LoRA fine-tuning, optimized using AdamW for 450 epochs. — Quote: “We optimized the model parameters using the AdamW optimizer… training process spanned 450 epochs.” (Position: Model Construction): 1 (0.5%)
     Only prompting of different LLMs: 1 (0.5%)
     In this project, prompts were crafted based on common mental health issues such as  anxiety, stress, and academic pressure. 
Initially trained on a general mental health dataset, the model gained a basic understanding of mental health issues such as anxiety and depression. However, to address local language and cultural nuances, additional data from Indonesian sources, including articles and forums, were integrated.: 1 (0.5%)
     Each chatbot was prompted in the most up-to-date
version...: 1 (0.5%)
     See Fig. 1: LLMs integrated into robot to specifically adress ADHD Symptoms: 1 (0.5%)
     To this end, we propose a set of mod-
ifications: supervised fine-tuning on a carefully designed mental
health dataset and incorporating dialogue-act labels that capture
the unique structure and emotional undercurrents of therapeutic
sessions. By evaluating these adaptations, we show how ChatMGL
can be transformed into a more context-aware and empathetic
system, capable of generating clinically relevant and supportive
responses: 1 (0.5%)
     Fine-tuning of all-MiniLM-L6-v2 for relevance detection + response generation by Gemini 1.5 Flash: 1 (0.5%)
     Multimodal pipeline combining BERT embeddings (text), CNN (facial), LSTM/MFCC (audio), LangChain prompting, GPT-4 response generation; frontend in React, auth/session in Firebase. : 1 (0.5%)
     'we apply the
QLoRA [12] technique for parameter-efficient finetuning of
the generator' .. 'dense retrieval model'... ' we introduce a BERT-
based cross-encoder as a re-ranke': 1 (0.5%)
     no development, only prompting : 1 (0.5%)
     Employs an "Empathetic Meta-Chain (EMC) learning method" that combines meta-learning, Chain of Thought prompting, and counseling strategies. Multiple approaches were compared including zero-shot, few-shot, CoT, meta-learning, and EMC.: 1 (0.5%)
     The proposed method involves enhancing the 
Falcon-7B model through the application of quantization low rank adaptation (Q-LoRA). : 1 (0.5%)
     ''We introduce Emotion-Aware Embedding Fusion, a novel framework integrating hierarchical fusion and attention mechanisms'' ... : 1 (0.5%)
     They only ever refer to "prompt engineering techniques" used in their chatbot: 1 (0.5%)
     GPT-4 outputs compared with curated expert book excerpts; randomized and blinded survey design. — Quote: “participants were blinded in terms of which advice was book-based and which was ChatGPT generated.” (Position: Methods, p. 36): 1 (0.5%)
      multi-tiered chatbot leveraging Retrieval-
Augmented Generation (RAG) provides dynamic, context-aware
mental health assistance: 1 (0.5%)
     o systematically test
the conversational agent’s responses for potential bias, we
developed an experimental setup where the conversational
agent interacted with digital standardized patients (DSPs)
exhibiting symptoms of anxiety or depression. DSPs were
GPT-3.5-enabled chatbots prompted to emulate a patient
with depression and anxiety seeking professional help from
a therapist. This was achieved by prompting GPT-3.5 to:
“Pretend that you are [first_name], a human client speaking to
a therapist. You are NOT the therapist; you are the clienBy using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses: 1 (0.5%)
     Prompting for emotional analysis + fine-tuning BERT for therapy classification + semantic retrieval with SimCSE for relevant case examples.: 1 (0.5%)
     Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).: 1 (0.5%)
     They used a pre-trained RoBERTa model via Hugging Face to analyze text patterns without mentioning any fine-tuning or prompt engineering. The model was applied directly to classify and interpret data.: 1 (0.5%)
     Promts: "Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis".: 1 (0.5%)
     Promts: "Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis".

Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).: 1 (0.5%)
     It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot

fine tuning + sentiment analysis module: 1 (0.5%)
     It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot: 1 (0.5%)
     fine tuning + sentiment analysis module: 1 (0.5%)
     Fine-tuning of BERT, likely for intent classification. Not clear how responses are generated -- via LSTM?: 1 (0.5%)
     Despited being labeled as fine-tuning, I suppose it is more about promtpting and other modules (Context Generator, Expression Expander, and the Feedback Generator)?: 1 (0.5%)

   Sample values (first 20):
     We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).
     Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4)
     Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.
     See Figure 3. All revolves around prompting of GPT-4, and then there is a "memory concept" and other stuff
     Hybrid pipeline: NLP preprocessing → VADER sentiment scoring with thresholds → template or GPT-3.5-turbo generation → referral logic and resources → logging/monitoring; context tracking for coherence. 
     "A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative" .. "The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately" ... "Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition"
     Built on LangChain framework with memory and journaling integration
     Built on LangChain framework with memory and journaling integration .. Input/Templates/Prompt Instruction
     Fine-tuning of GPT-2 + voice and video interaction system
     see Fig. 1 
     Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier
     Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier (see Fig. 1)
     Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).
     "We use ChatGPT as our base LLM and access it through
the API with prompts"
     Prompt engineering improved MI fidelity (Phase II); GPT-4 via secure API; HIPAA-compliant setup. — Quote: “explicit prompting for adherence to validated MI literature and more collaborative, client-centered language enhanced the therapeutic alliance.”
     We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.
     Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1)
     BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.
     The design phase mentions fine-tuning and prompt engineering, but the implemented study relied on a simple prompt with GPT-3.5. — Quote: “The design process addressed… fine-tuning the LLMs, and prompt engineering.” / “The initial study used a relatively simple prompt…”
     "We fine-tuned the model with an 80% to 20% train-test split" + "that is, the system is prompted
to keep generating responses"...


77. Number of Clients or Patients Included
   ==========================================
   Data type: object
   Total values: 488
   Non-null values: 141
   Null values: 347
   Unique values: 61
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 347 (246.1%)
     50: 6 (4.3%)
     20: 6 (4.3%)
     42: 5 (3.5%)
     1: 5 (3.5%)
     11: 5 (3.5%)
     48: 4 (2.8%)
     21: 4 (2.8%)
     16: 3 (2.1%)
     210: 3 (2.1%)
     160: 3 (2.1%)
     49: 3 (2.1%)
     29: 3 (2.1%)
     45: 3 (2.1%)
     58: 3 (2.1%)
     428: 3 (2.1%)
     24: 3 (2.1%)
     70: 3 (2.1%)
     9: 3 (2.1%)
     69: 3 (2.1%)
     5: 3 (2.1%)
     8: 3 (2.1%)
     30: 2 (1.4%)
     306: 2 (1.4%)
     12: 2 (1.4%)
     244: 2 (1.4%)
     87 Reddit posts: 2 (1.4%)
     unknown: 2 (1.4%)
     1000000 user reviews: 2 (1.4%)
     193 — Quote: “final sample consisted of 107 CMs and 86 MHPs”: 2 (1.4%)
     830: 2 (1.4%)
     139 ("64 patients with depression" + "75 patients with anxiety"): 2 (1.4%)
     31: 2 (1.4%)
     399: 2 (1.4%)
     14: 2 (1.4%)
     10: 2 (1.4%)
     68: 2 (1.4%)
     462: 2 (1.4%)
     35: 2 (1.4%)
     236: 2 (1.4%)
     349: 2 (1.4%)
     15531: 2 (1.4%)
     177 Reddit posts: 2 (1.4%)
     111: 2 (1.4%)
     1 (case study): 2 (1.4%)
     7: 2 (1.4%)
     212 counseling sessions (not clear how many patients): 1 (0.7%)
     unclear: 1 (0.7%)
     Substudy 1
ITT-Sample: 207
CC-Sample: 154

Substudy 2
ITT-Sample: 70
CC-Sample 53

Substudy 3
ITT-Sample: 72
CC-Sample: 48: 1 (0.7%)
     527: 1 (0.7%)
     348: 1 (0.7%)
     17: 1 (0.7%)
     not mentioned : 1 (0.7%)
     200: 1 (0.7%)
     0: 1 (0.7%)
     107 community members AND 86 mental health providers: 1 (0.7%)
     1626: 1 (0.7%)
     2: 1 (0.7%)
     4: 1 (0.7%)
     111 individuals (82 females and 29 males): 1 (0.7%)
     Not specified: 1 (0.7%)
     75: 1 (0.7%)

   Sample values (first 20):
     12
     14
     10
     68
     462
     212 counseling sessions (not clear how many patients)
     35
     236
     1626
     42
     349
     31
     15531
     20
     11
     399
     111 individuals (82 females and 29 males)
     111
     24
     50


78. Other User Experience Assessment Notes
   ==========================================
   Data type: object
   Total values: 488
   Non-null values: 127
   Null values: 361
   Unique values: 90
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 361 (284.3%)
     Also measuring undesirable output via annotation.
For qualitative results, see 4.3.3.: 2 (1.6%)
      The qualitative interviews were conducted remotely via Zoom by a user
experience researcher and were supported by an in-person study
coordinator. Interviews were recorded, transcribed, and
observational notes during each session were taken. The
interviews had three sequential components: 1) participants were
asked questions about their past experiences with AI conversational
agents and perceptions of AI conversational agents in mental
healthcare, 2) participants engaged with a mental health
conversational agent app to ground and standardize their
experiences, and 3) participants were asked additional general
questions related to the acceptability of AI conversational agents
in mental healthcare, and more.: 2 (1.6%)
     Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” : 2 (1.6%)
     Qualitative feedback reported in 4.4.: 2 (1.6%)
     As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).: 2 (1.6%)
     Quantitative: Measures of the user experience with the generative and rules-based DMHIs included user engagement (number of sessions, total active days, and conversational exchanges).
: 2 (1.6%)
     Satisfaction ratings focused on relational/emotional quality, technical/didactic, treatment support, professional orientation. — Quote: “PCA highlighted four components… relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4)” (Position: Abstract): 2 (1.6%)
     72.86% rated GPT-4 advice practical … 87.94% well-explained … statistically significant” (Position: Results, p. 41–42)

GPT-4 advice perceived as more practical and clearer; effect sizes small-to-medium. — Quote: “differences … statistically significant … d ≈ 0.34 … d ≈ 0.53 …” (Position: Results, p. 41–42): 2 (1.6%)
     Qualitative analysis:
This approach involves six main steps: getting familiar with the data; generating initial codes; searching for themes; reviewing themes; defining and naming themes; and producing the report. The analysis process was iterative and recursive, allowing the researchers to move back and forth between these steps as needed to ensure thoroughness and validity. : 2 (1.6%)
     10 questions regarding user interface, ease of use, chatbot responsiveness, 
and overall satisfaction. might include standard instrument, but not described.: 2 (1.6%)
     High satisfaction (>80% smooth design); moderate effectiveness (~50% improved sleep); high acceptance (75% continued use). — Quote: “More than 80% of the volunteers think that our app is well designed and smooth to use” (Position: Application Utilization): 2 (1.6%)
      “Our system” (M = 5.368) found this work’s system statistically significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).: 2 (1.6%)
     Only quantitative scoring of : 2 (1.6%)
     "User experience assessment" was content of the Reddit posts: 2 (1.6%)
     This article is a case study with loose, qualitative descriptions: 2 (1.6%)
     This study employed a deductive thematic analysis, using the conceptual framework of Grodniewicz and Hohol’s (2023) three challenges in AI psychotherapy as a general guide: 2 (1.6%)
     All participants noted improved treatment motivation … 80% disclosing personal concerns … 24/7 availability particularly benefited patients
Four out of five participants reported significant reductions in anxiety and stress levels post-intervention” (Results, p. 1): 2 (1.6%)
     Self-efficacy improved across sessions; anxiety reduced mainly between Session 1 and 2. — Quote: same as quantitative assessment line: 2 (1.6%)
     Results reported: Yes (case-level outcomes) — Quote: “the AI successfully managed to … become more experientially aligned with the patient” (Position: Clinical section, p. 11)

Empathic failures, uncanny feelings, corrective alignment described — Quote: “empathic failure occurred … however, the ability to overcome empathic failures … holds significant therapeutic value” (Position: Clinical section, p. 11): 2 (1.6%)
     See section: Technology Comfort, AI Attitudes, and AI
Use Intention: 2 (1.6%)
     Most participants reported feeling strongly understood by
the chatbot, with the majority indicating a high degree of
perceived emotional attunement. Participants described the
experience as intuitive to navigate, personally meaningful,
and contextually relevant to their situations. A general sense
of satisfaction was expressed across the participant group,
with favorable ratings on the satisfaction measures.: 2 (1.6%)
     higher reliable improvement, recovery, and reliable recovery rates” 
users perceived the AI-enabled therapy support tool as most useful for discussing their problems to gain awareness and clarity … learning how to apply coping skills”: 2 (1.6%)
     Not described, how many users and rather vague representation of results in Fig. 7.: 2 (1.6%)
     users expressed positive attitudes toward digital mental health solutions, with key motives including avoiding embarrassment (36%) and concerns about appearance in face-to-face consultations (35%). Expectations focused on emotional support (35%) and expressing feelings (32%): 2 (1.6%)
     Before the study’s completion, we conducted unstructured interviews to delve deeper into the participants’ experiences with the chatbots. Many participants expressed that the chatbot’s responses evoked feelings of surprise and novelty. They emphasized the importance of feeling understood as pivotal in enhancing their acceptance and engagement with the chatbot-based intervention. Since GPT-based chatbots can generate text in a human-like manner, mimicking the interaction between users and psychotherapists, five participants indicated that the anthropomorphic responses felt so authentic that they found it difficult to distinguish whether they were AI-generated. These qualitative insights reinforce our quantitative findings and underscore the critical importance of interaction in the design of chatbots.: 2 (1.6%)
     SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback): 2 (1.6%)
     User experience assessment of therapists using the tool. There is a quantiative (Likert scale questionnaires) and qualitative (open-ended user questions) assessment: 2 (1.6%)
     Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions): 2 (1.6%)
     We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).: 2 (1.6%)
     As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT: 2 (1.6%)
     Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).: 2 (1.6%)
     1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5): 2 (1.6%)
     Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.: 2 (1.6%)
     Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience: 2 (1.6%)
     consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?: 2 (1.6%)
     Thematic analysis of Reddit posts and comments: 2 (1.6%)
     Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.: 2 (1.6%)
     The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.: 1 (0.8%)
     Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX.

Results in Fig 5: 1 (0.8%)
     This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.: 1 (0.8%)
     Both, user experience assessment and participant attitude assessment. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview. : 1 (0.8%)
     Both, user experience assessment and participant attitude assessment.

This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.: 1 (0.8%)
     BERTopic analysis of app reviews, qualitative only: 1 (0.8%)
     Qualitative analysis of user reviews focusing on risks: 1 (0.8%)
     Qualitative analysis of user reviews focusing on risks using BERTopic: 1 (0.8%)
     "During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used)."​: 1 (0.8%)
     Users rated AI responses more authentic, professional, and prarctical when blinded. when unblinded, the rated human responses higher (except professionalism).: 1 (0.8%)
     Results of survey in Fig 5: 1 (0.8%)
     Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX. : 1 (0.8%)
     Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale: 1 (0.8%)
     Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions ("score", "activity", "informativeness"), while psychologist responses were rated higher in "strength".: 1 (0.8%)
     To gather crucial user insights and evaluate the chatbots 
performance from a user’s perspective, we conducted 
surveys among a select group of mental health users and 
carers. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness of the 
chatbot, its conversation quality, and potential areas of 
improvement. Below, we detail the survey’s findings:
- Willingness for continued chatbot usage: when 
questioned about their willingness to engage with 
the chatbot again after the initial interaction, an 
overwhelming majority (90%) expressed a positive 
intent to reuse the service
- Rating of conversation quality (human-likeness): 
participants rated the chatbot’s conversational 
quality in terms of its human-like language on a 
scale from 1 to 5. The chatbot received an average 
score of 4.3, indicating a high degree of satisfaction 
with the chatbot’s language quality.
- Rating of conversation quality (supportiveness): 
when assessing the chatbot’s supportive nature in 
the conversation, participants gave an average score 
of 4.2 of 5, reflecting their positive experience in 
terms of perceived support.
- Overall rating of the chatbot application: when 
asked to provide an overall rating of the chatbot 
application, participants gave an average score 
of 4.6 out of 5. Notably, no participant rated the 
application lower than 4, indicating a high level of 
user satisfaction.
-  Positive feedback: participants were invited to 
share any positive feedback about their experience. 
We got 8 responses for this question and the most 
important points are the LLM-based chatbot can 
always provide useful suggestions and they feel very 
safe to talk to someone who are always available 
and talkable about their issue and sadness.
- Areas for improvement: we also encouraged 
users to suggest areas where the chatbot could 
be improved. The survey participants found the 
chatbot to be generally helpful, but suggested 
improvements such as exposing the training data 
to more diverse circumstances, enhancing the 
emotional support aspect, avoiding risky responses 
to sensitive inquiries, reducing repetition of 
examples, and focusing on more teaching sessions 
to make the interactions feel less robotic and more 
like conversing with a human friend.
- User interface (UI) suggestions: participants were 
also asked to provide suggestions for improving 
the chatbot’s UI functionalities to enhance its 
usefulness and usability. The feedback is very useful 
for us to implement further improved version. The 
suggestions include voice and image combined 
responses, able to track chat history, virtual reality 
(VR) or mixed reality innovation and realistic 
human tongues enhancement.: 1 (0.8%)
     No user experience assessment but participant attitude survey: 1 (0.8%)
     technical proof-of-concept; no real users assessed: 1 (0.8%)
     semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT 
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.: 1 (0.8%)
     descpritive only: 1 (0.8%)
     Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. : 1 (0.8%)
     The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.: 1 (0.8%)
     The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.

Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. : 1 (0.8%)
     Feedback from test users: These results suggest that the
system effectively delivers mental health insights while main-
taining user engagement.: 1 (0.8%)
     Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b): 1 (0.8%)
     Only partialy described in the discussion section.: 1 (0.8%)
     Qualitative assessment only implicitly mentioned in the discussion section, where user experiences are reported.: 1 (0.8%)
     Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions ("score", "activity", "informativeness"), while psychologist responses were rated higher in "strength".

"During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used)."​: 1 (0.8%)
     Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt): 1 (0.8%)
     Readability high (Flesch-Kincaid 10.8), messages long; insufficient cultural tailoring; transparency reactive not proactive; crisis responses generic (no local numbers) — Quote: “10.8 on the Flesch-Kincaid …”; “did not make an attempt to ask Sam about her cultural identity variables”; “grant … a score of 1 … provides details upon request, [but] does not offer full, immediate … disclosure”; “no specific numbers or links were provided … score of 1 … general recommendations but no local resources” (Position: p. 8–9): 1 (0.8%)
     May not be understood as the "classical" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.: 1 (0.8%)
     Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)

May not be understood as the "classical" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.: 1 (0.8%)
     Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience: 1 (0.8%)
     Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.: 1 (0.8%)
     Categories of qualitative feedback: helpfullness, Naturalness, Ease of Use, Engangement. : 1 (0.8%)
     Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.: 1 (0.8%)
     secondary outcome was patient satisfaction on a Likert scale : 1 (0.8%)
     Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their
goals and inquire about their interests. Notably, no users reported feeling pressured or
convinced by the prototype to change aspects of their lives against their wishes. Several
users found the prototype to be particularly valuable for creating quick and helpful
weekly plans or recipes. However, it was observed that one user found it premature
to establish concrete plans at that stage. Overall, while some users initially felt that
the prototype’s responses were not aligned with their preferences, they noted that it
was possible to clarify their intentions, and the prototype quickly adapted accordingly.
Despite this, some users felt that the assessment of their goals and life circumstances
at the beginning of the training was too superficial, leading to suggestions for a more
thorough initial assessment process.
Users highlighted several aspects of the prototype’s communication style. Most users
found the conversation to be clear and direct. In contrast, one user reported that the pro-
totype struggled to understand his goals and sometimes provided contradictory advice,
a concern not raised by others. On the positive side, one user appreciated the prototype’s
approach of asking questions rather than giving fixed instructions, which encouraged 
engagement and avoided a patronizing tone. Some users noted the positive and moti-
vating tone of the coach, particularly when asking if they were willing to continue with
suggested steps and advice. The use of a positive tone was seen as a motivating factor by
one user and was also appreciated by another who found it encouraging. Users generally
found concrete advice more helpful than generic recommendations. Additionally, one
user valued the coach’s acknowledgment of setbacks and the importance of enjoying the
process. However, some users expressed a desire for shorter, more direct conversations
that focused less on delivering general knowledge, and one user specifically requested
a more emotional and less matter-of-fact tone of voice.: 1 (0.8%)
     Two staff psychologists (EB, AJ) each reviewed all 
ChatGPT responses and evaluated them along the 
following dimensions: (1) task completion and (2 
degree to which input information was incorporated in the output.: 1 (0.8%)
     Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.

Results: Not clear/not reported: 1 (0.8%)
     Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.: 1 (0.8%)
     A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.: 1 (0.8%)
     System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.

Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based: 1 (0.8%)
     See Application Utilization: 1 (0.8%)
     Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based: 1 (0.8%)
      “Our system” (M = 5.368) found this work’s system statistically 
significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the 
measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).: 1 (0.8%)
     System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.: 1 (0.8%)
     The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.

Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience: 1 (0.8%)
     Strengths = accessibility, safety in most cases; Weaknesses = occasional advertising, reading level too high, missing referral for severe depression. : 1 (0.8%)
     emotional valence and dominance improved; deliberate style increased happiness, rapid style increased surprise. : 1 (0.8%)
     Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. "It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information."

semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.: 1 (0.8%)
     10-point Likert scale rating the overall experience, the pleasantness of the chat, the appropriateness of the responses, the realism of the chatbot, and the helpfulness of the advice
Yes/No/ dont know:
1. It would be used by patients. (Hypothesized patients’ subjective norm)
2. It would be recommended by outpatient psychiatrists (Subjective norm)
3. It would relieve the psychiatric emergency department. (Anticipated
benefit)
4. It would be helpful for patients in
a crisis situation. (Expectancy of
usefulness): 1 (0.8%)
     See V.B.: 1 (0.8%)
     Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. "It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information.": 1 (0.8%)

   Sample values (first 20):
     secondary outcome was patient satisfaction on a Likert scale 
     As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT
     Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).
     1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5)
     To gather crucial user insights and evaluate the chatbots 
performance from a user’s perspective, we conducted 
surveys among a select group of mental health users and 
carers. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness of the 
chatbot, its conversation quality, and potential areas of 
improvement. Below, we detail the survey’s findings:
- Willingness for continued chatbot usage: when 
questioned about their willingness to engage with 
the chatbot again after the initial interaction, an 
overwhelming majority (90%) expressed a positive 
intent to reuse the service
- Rating of conversation quality (human-likeness): 
participants rated the chatbot’s conversational 
quality in terms of its human-like language on a 
scale from 1 to 5. The chatbot received an average 
score of 4.3, indicating a high degree of satisfaction 
with the chatbot’s language quality.
- Rating of conversation quality (supportiveness): 
when assessing the chatbot’s supportive nature in 
the conversation, participants gave an average score 
of 4.2 of 5, reflecting their positive experience in 
terms of perceived support.
- Overall rating of the chatbot application: when 
asked to provide an overall rating of the chatbot 
application, participants gave an average score 
of 4.6 out of 5. Notably, no participant rated the 
application lower than 4, indicating a high level of 
user satisfaction.
-  Positive feedback: participants were invited to 
share any positive feedback about their experience. 
We got 8 responses for this question and the most 
important points are the LLM-based chatbot can 
always provide useful suggestions and they feel very 
safe to talk to someone who are always available 
and talkable about their issue and sadness.
- Areas for improvement: we also encouraged 
users to suggest areas where the chatbot could 
be improved. The survey participants found the 
chatbot to be generally helpful, but suggested 
improvements such as exposing the training data 
to more diverse circumstances, enhancing the 
emotional support aspect, avoiding risky responses 
to sensitive inquiries, reducing repetition of 
examples, and focusing on more teaching sessions 
to make the interactions feel less robotic and more 
like conversing with a human friend.
- User interface (UI) suggestions: participants were 
also asked to provide suggestions for improving 
the chatbot’s UI functionalities to enhance its 
usefulness and usability. The feedback is very useful 
for us to implement further improved version. The 
suggestions include voice and image combined 
responses, able to track chat history, virtual reality 
(VR) or mixed reality innovation and realistic 
human tongues enhancement.
     Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.
     Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience
     "During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used)."​
     Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions ("score", "activity", "informativeness"), while psychologist responses were rated higher in "strength".
     Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions ("score", "activity", "informativeness"), while psychologist responses were rated higher in "strength".

"During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used)."​
     consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?
     Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions)
     We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
     System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.
     Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based
     System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.

Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based
     Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.
     Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.

Results: Not clear/not reported
     Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their
goals and inquire about their interests. Notably, no users reported feeling pressured or
convinced by the prototype to change aspects of their lives against their wishes. Several
users found the prototype to be particularly valuable for creating quick and helpful
weekly plans or recipes. However, it was observed that one user found it premature
to establish concrete plans at that stage. Overall, while some users initially felt that
the prototype’s responses were not aligned with their preferences, they noted that it
was possible to clarify their intentions, and the prototype quickly adapted accordingly.
Despite this, some users felt that the assessment of their goals and life circumstances
at the beginning of the training was too superficial, leading to suggestions for a more
thorough initial assessment process.
Users highlighted several aspects of the prototype’s communication style. Most users
found the conversation to be clear and direct. In contrast, one user reported that the pro-
totype struggled to understand his goals and sometimes provided contradictory advice,
a concern not raised by others. On the positive side, one user appreciated the prototype’s
approach of asking questions rather than giving fixed instructions, which encouraged 
engagement and avoided a patronizing tone. Some users noted the positive and moti-
vating tone of the coach, particularly when asking if they were willing to continue with
suggested steps and advice. The use of a positive tone was seen as a motivating factor by
one user and was also appreciated by another who found it encouraging. Users generally
found concrete advice more helpful than generic recommendations. Additionally, one
user valued the coach’s acknowledgment of setbacks and the importance of enjoying the
process. However, some users expressed a desire for shorter, more direct conversations
that focused less on delivering general knowledge, and one user specifically requested
a more emotional and less matter-of-fact tone of voice.
     Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.


79. Other metrics:
   ==================
   Data type: float64
   Total values: 488
   Non-null values: 0
   Null values: 488
   Unique values: 0
   Category: CATEGORICAL

   Sample values (first 20):


80. P-1 On-premise-capable model Considered in tool design? (Y/N)
   =================================================================
   Data type: object
   Total values: 488
   Non-null values: 229
   Null values: 259
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 259 (113.1%)
     N: 88 (38.4%)
     n: 67 (29.3%)
     y: 47 (20.5%)
     Y: 23 (10.0%)
     Y (some): 2 (0.9%)
     Y & N : 1 (0.4%)
     -: 1 (0.4%)


81. P-1 On-premise-capable model if YES: Notes (paste text passage)
   ===================================================================
   Data type: object
   Total values: 488
   Non-null values: 44
   Null values: 444
   Unique values: 26
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 444 (1009.1%)
     GPT-2: 4 (9.1%)
     	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.): 2 (4.5%)
     Once the chatbot model was trained it was then 
deployed on a web page where users have access to it for interaction with the chatbot application. This was done through a command line provided by the Rasa Framework 
which directs input to and from the model interfacing with a 
web application via an endpoint provided on the Rasa endpoints file. : 2 (4.5%)
     Llama 3.2 used: 2 (4.5%)
     Jais-13B is on-premise: 2 (4.5%)
     T5 can be used on-premise and via cloud (not in the text). : 2 (4.5%)
     Llama 2, Falcon-7B: 2 (4.5%)
     Some are, others don´t: 2 (4.5%)
     Falcon7B : 2 (4.5%)
     falcon: 2 (4.5%)
     Llama and DeepSeek are: 2 (4.5%)
     ChatGLM is open-source/self-hostable: 2 (4.5%)
     Gemini 1.5 Flash: 2 (4.5%)
     Partially: GPT not, Llama yes. : 2 (4.5%)
     InternLM2-7B is self-hostable: 2 (4.5%)
     GLM2-6B model : 2 (4.5%)
     RoBERTa should be on-premise-capable: 1 (2.3%)
     mistral 7b: 1 (2.3%)
     ParsBERT, XLM-R are open-source: 1 (2.3%)
     (recommend therapist-side implementation) — Quote: “Therefore, these tools should be used on the therapist’s computer … ensuring anonymity for the patient.” : 1 (2.3%)
     One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4: 1 (2.3%)
     Llama is ? so are unsloth/ollama: 1 (2.3%)
     own transformer architecture -> on-premise: 1 (2.3%)
     BlenderBot: 1 (2.3%)
     Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.: 1 (2.3%)
     Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.
But not on-premise capable!: 1 (2.3%)

   Sample values (first 20):
     mistral 7b
     BlenderBot
     own transformer architecture -> on-premise
     One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4
     GPT-2
     InternLM2-7B is self-hostable
     Partially: GPT not, Llama yes. 
     Gemini 1.5 Flash
     ChatGLM is open-source/self-hostable
     ParsBERT, XLM-R are open-source
     	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.)
     RoBERTa should be on-premise-capable
     Llama and DeepSeek are
     falcon
     (recommend therapist-side implementation) — Quote: “Therefore, these tools should be used on the therapist’s computer … ensuring anonymity for the patient.” 
     Falcon7B 
     Some are, others don´t
     Llama 2, Falcon-7B
     T5 can be used on-premise and via cloud (not in the text). 
     Jais-13B is on-premise


82. P-2 Privacy/confidentiality awareness Considered in tool design? (Y/N)
   ==========================================================================
   Data type: object
   Total values: 488
   Non-null values: 224
   Null values: 264
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 264 (117.9%)
     n: 102 (45.5%)
     N: 89 (39.7%)
     Y: 18 (8.0%)
     y: 13 (5.8%)
     Z: 1 (0.4%)
     -: 1 (0.4%)


83. P-2 Privacy/confidentiality awareness if YES: Notes (paste text passage)
   ============================================================================
   Data type: object
   Total values: 488
   Non-null values: 34
   Null values: 454
   Unique values: 24
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 454 (1335.3%)
     Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]: 2 (5.9%)
     Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].: 2 (5.9%)
     Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.: 2 (5.9%)
     Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.: 2 (5.9%)
     usage of Amazon AWS, no discussion of user privacy: 2 (5.9%)
     The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).: 2 (5.9%)
     The text data contains some private information… we… used ‘xxx’ to replace the original text: 2 (5.9%)
     Ethical use of Reddit data with disclaimers in 3.3.1: 2 (5.9%)
     In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.
: 2 (5.9%)
     
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. : 2 (5.9%)
     To ensure ethical deployment, the system avoids medical 
advice, doesn’t store personal data, and complies with data 
protection standards. Sensitive inputs are not logged or 
reused. : 1 (2.9%)
     in the schema: 1 (2.9%)
     Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.: 1 (2.9%)
     Prioritize data privacy and ethical compliance: 1 (2.9%)
     Quote: “Patients must be fully informed about how their data will be used and must provide explicit consent … robust data protection measures are paramount” (Position: Section 4.2, p. 12): 1 (2.9%)
     Privacy Concerns: Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks: 1 (2.9%)
      The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.: 1 (2.9%)
     they say they release the data but where is it?: 1 (2.9%)
     Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls: 1 (2.9%)
     Yes → Adheres to data protection principles, anonymized use: 1 (2.9%)
     did not describe how claire aligns with data protection regulations: 1 (2.9%)
     Yes → Adheres to data protection principles, anonymized use
https://www.clareandme.com/post/what-happens-with-your-data-at-clare-me: 1 (2.9%)
     First, although direct application risks were 
avoided, it is crucial to ensure that the real-world data used is properly protected and compliant with privacy regulations during both 
collection and processing (
). In this study, the data used was publicly available online, and all analysis was con­
ducted in a manner that safeguarded personal privacy. : 1 (2.9%)
      Conducted thorough testing of the
chatbot’s responses (Fig. 6) to various user queries and
scenarios to assess its ability to uphold ethical standards
and protect user privacy. This testing involved simulating
user interactions with the chatbot and evaluating its
responses for any instances of inappropriate or unethical behavior, such as providing inaccurate information,
breaching confidentiality, or engaging in discriminatory
practices. Any identified issues were promptly addressed
and remediated to ensure that the chatbot consistently
adheres to ethical guidelines and respects user privacy.: 1 (2.9%)

   Sample values (first 20):
     Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.
     Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls
     they say they release the data but where is it?
      The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.
     
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. 
     Prioritize data privacy and ethical compliance
     In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.

     Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]
     The text data contains some private information… we… used ‘xxx’ to replace the original text
     To ensure ethical deployment, the system avoids medical 
advice, doesn’t store personal data, and complies with data 
protection standards. Sensitive inputs are not logged or 
reused. 
     in the schema
      Conducted thorough testing of the
chatbot’s responses (Fig. 6) to various user queries and
scenarios to assess its ability to uphold ethical standards
and protect user privacy. This testing involved simulating
user interactions with the chatbot and evaluating its
responses for any instances of inappropriate or unethical behavior, such as providing inaccurate information,
breaching confidentiality, or engaging in discriminatory
practices. Any identified issues were promptly addressed
and remediated to ensure that the chatbot consistently
adheres to ethical guidelines and respects user privacy.
     The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
     Quote: “Patients must be fully informed about how their data will be used and must provide explicit consent … robust data protection measures are paramount” (Position: Section 4.2, p. 12)
     usage of Amazon AWS, no discussion of user privacy
     Privacy Concerns: Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks
     Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.
     Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.
     Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].
     Yes → Adheres to data protection principles, anonymized use


84. Perplexity Benchmark quality (H/L)
   ======================================
   Data type: object
   Total values: 488
   Non-null values: 11
   Null values: 477
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 477 (4336.4%)
     L: 6 (54.5%)
     l: 3 (27.3%)
     no benchmark: 2 (18.2%)


85. Perplexity How it compares against benchmark (B/S/W)
   ========================================================
   Data type: object
   Total values: 488
   Non-null values: 11
   Null values: 477
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 477 (4336.4%)
     W: 3 (27.3%)
     B: 3 (27.3%)
     b: 3 (27.3%)
     no benchmark: 2 (18.2%)


86. Perplexity Notes on benchmark quality
   =========================================
   Data type: object
   Total values: 488
   Non-null values: 10
   Null values: 478
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 478 (4780.0%)
     Best model of authors compared to models of other works. : 2 (20.0%)
     Benchmark: non fine-tuned GPT-2: 2 (20.0%)
     Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5): 2 (20.0%)
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets: 1 (10.0%)
     Perplexity of reference responses in dataset: 1 (10.0%)
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Perplexity of reference responses in dataset: 1 (10.0%)
     Other LLMs: 1 (10.0%)


87. Perplexity Used (Y/N)
   =========================
   Data type: object
   Total values: 488
   Non-null values: 220
   Null values: 268
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 268 (121.8%)
     n: 108 (49.1%)
     N: 101 (45.9%)
     Y: 7 (3.2%)
     y: 4 (1.8%)


88. Publication Outlet Type
   ===========================
   Data type: object
   Total values: 488
   Non-null values: 451
   Null values: 37
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     Conference paper: 229 (50.8%)
     Journal paper: 220 (48.8%)
     [NULL]: 37 (8.2%)
     Other: Letter to the Editor : 1 (0.2%)
     Other: Workshop paper: 1 (0.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 2
     Other: Letter to the Editor: 1
     Other: Workshop paper: 1


89. READI P-1/2:
   ================
   Data type: float64
   Total values: 488
   Non-null values: 0
   Null values: 488
   Unique values: 0
   Category: CATEGORICAL

   Sample values (first 20):


90. READI categories:
   =====================
   Data type: float64
   Total values: 488
   Non-null values: 0
   Null values: 488
   Unique values: 0
   Category: CATEGORICAL

   Sample values (first 20):


91. Reference-based metrics:
   ============================
   Data type: float64
   Total values: 488
   Non-null values: 0
   Null values: 488
   Unique values: 0
   Category: CATEGORICAL

   Sample values (first 20):


92. Reviewer Name
   =================
   Data type: object
   Total values: 488
   Non-null values: 488
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     Richard Gaus: 165 (33.8%)
     Reviewer Two: 162 (33.2%)
     Consensus: 161 (33.0%)


93. S-1 Risk-detection Considered in tool design? (Y/N)
   =======================================================
   Data type: object
   Total values: 488
   Non-null values: 325
   Null values: 163
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 163 (50.2%)
     n: 146 (44.9%)
     N: 113 (34.8%)
     Y: 41 (12.6%)
     y: 23 (7.1%)
     Y?: 1 (0.3%)
     -: 1 (0.3%)


94. S-1 Risk-detection if YES: Notes (paste text passage)
   =========================================================
   Data type: object
   Total values: 488
   Non-null values: 66
   Null values: 422
   Unique values: 43
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 422 (639.4%)
     Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. : 3 (4.5%)
     Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.: 3 (4.5%)
     “After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9): 2 (3.0%)
     see 4.1: 2 (3.0%)
     Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.: 2 (3.0%)
     The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.: 2 (3.0%)
     The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.: 2 (3.0%)
     The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.: 2 (3.0%)
     entire study is about this: 2 (3.0%)
     Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.: 2 (3.0%)
     from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.: 2 (3.0%)
     Implicit in S-2: 2 (3.0%)
     In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.: 2 (3.0%)
     In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model: 2 (3.0%)
     Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159: 2 (3.0%)
     they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values: 2 (3.0%)
     All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.: 2 (3.0%)
     The chatbot's functionality extends beyond mere conversation, integrating mood tracking features, 
personalized coping strategy recommendations, and crisis 
detection algorithms with built-in escalation protocols for 
high-risk situations: 2 (3.0%)
     The risk detection system in HoMemeTown was developed
based on established clinical guidelines [19] and validated
screening tools [20], implementing a sophisticated approach to
identifying and responding to potential mental health concerns.
The system continuously monitors user interactions for primary
risk indicators, including expressions of suicidal ideation, severe
depression symptoms, and anxiety crisis signals, while also
tracking secondary indicators such as sleep disturbance patterns
and social withdrawal signs.
When potential risks are detected, the system implements a
graduated response protocol that has been carefully designed
to provide appropriate levels of support while avoiding
unnecessary escalation. For mild risk situations, the system
offers empathetic acknowledgment and self-help resources,
drawing from evidence-based interventions [21]. In cases of
moderate risk, the response includes more direct expressions
of concern and specific mental health resources, while severe
risk triggers an immediate crisis response protocol with direct
connections to professional support services. To address the challenge of potential false positives in risk
detection, we implemented a sophisticated validation system
that examines multiple contextual factors before triggering
interventions. This system uses NLP techniques to analyze the
broader context of user communications, helping to distinguish
between casual expressions and genuine indicators of distress.
Regular professional review of high-risk cases ensures the
ongoing refinement of detection algorithms and response
protocols, maintaining a balance between sensitivity and
specificity in risk assessment.: 2 (3.0%)
     even though this is a crisis call intervention system, there is not detection of acute risk: 2 (3.0%)
     If suicidality or severe distress is detected, users are directed to
psychological support hotlines and blocked from further use to
ensure safety.: 2 (3.0%)
     Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model.: 1 (1.5%)
     This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc: 1 (1.5%)
     Although the risk was minimal in the present study because
the client was fictional, it may not be completely credible in real
situations involving real clients. Counselor educators would thus
have to be cautious about using AI to directly generate feedback
for the trainees.: 1 (1.5%)
     With regard to crisis response, when Sam reported suicidal
ideation, OCD Coach directed her to emergency services or a crisis hotline “in [her] area” (Subdomain 2: Interactivity Quality): 1 (1.5%)
     We incorporated a risk detection function to identify potential
mental health crises and provide appropriate resources when
necessary: 1 (1.5%)
     detect the ability of LLMs to recognize extreme behaviors … behaviors covering high-risk actions: 1 (1.5%)
     We identify harmful outputs in two ways: First, we take participant’s
ratings of generated turns into account, specifically with respect to
turns rated as offensive/harmful. Secondly, we draw on MI-literature as
well as empathy research and ethical academic discussions about LLMs
outlined below to identify behaviours which are unsuited specifically
in the context of LLM-delivered MI-conversations. Based on this, we
create an annotation scheme to be applied to the logged conversations after data collection. This annotation scheme goes beyond the
identification of MI-Inadherent behaviours defined in the MISC (listed
in Table 1). While such behaviours are not suited for MI interactions
in general, we lay special emphasis on such actions that would be
suitable in MI-interactions with a human counsellor, but should be
avoided when responses are generated by LLMs. We focus specifically
on significant concerns around anthropomorphism and the unreliable
factual correctness of LLM outputs: 1 (1.5%)
     (suicidality referral present, severe depression detection failed) — Quote: “the ability of the chatbots to always recommend connecting to another person for users expressing suicidality … frequent failure to connect personas exhibiting severe depression to a human: 1 (1.5%)
     n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist: 1 (1.5%)
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response

4) Crisis Intervention and Ethical Safeguards: 1 (1.5%)
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response: 1 (1.5%)
     4) Crisis Intervention and Ethical Safeguards: 1 (1.5%)
     Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language

Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language: 1 (1.5%)
     Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language: 1 (1.5%)
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language: 1 (1.5%)
     ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. : 1 (1.5%)
     "With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation."

the tool itself is a suicide risk detection tool: 1 (1.5%)
     the tool itself is a suicide risk detection tool: 1 (1.5%)
     "With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation.": 1 (1.5%)
     n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist

attending psychiatrist was checking responses: 1 (1.5%)
     attending psychiatrist was checking responses: 1 (1.5%)
     In this study, the ethical risks associated with directly applying LLMs to individuals seeking mental 
health support were mitigated by utilizing pre-existing real-world data. : 1 (1.5%)

   Sample values (first 20):
     n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist
     attending psychiatrist was checking responses
     n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist

attending psychiatrist was checking responses
     even though this is a crisis call intervention system, there is not detection of acute risk
     "With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation."
     the tool itself is a suicide risk detection tool
     "With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation."

the tool itself is a suicide risk detection tool
     they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values
     Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. 
     ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. 
     Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language
     Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language
     Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language

Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language
     Implicit in S-2
     4) Crisis Intervention and Ethical Safeguards
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response

4) Crisis Intervention and Ethical Safeguards
     Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.
     entire study is about this


95. Study Type
   ==============
   Data type: object
   Total values: 488
   Non-null values: 453
   Null values: 35
   Unique values: 18
   Category: CATEGORICAL

   Value Counts:
     Empirical research involving an LLM: 397 (87.6%)
     Population survey: 38 (8.4%)
     [NULL]: 35 (7.7%)
     Conceptual or theoretical work (e.g. on ethics or safety): 2 (0.4%)
     Other: Case study: 2 (0.4%)
     Other: Propsal of a multimodal Model called "empathy AI": 1 (0.2%)
     Other: experimental vignette study with panel raters: 1 (0.2%)
     Other: theoretical,clinical, and ethical analysis: 1 (0.2%)
     Other: Observational study (user-generated reviews) with unsupervised text mining: 1 (0.2%)
     Other: Qualitative Study : 1 (0.2%)
     Other: thematic analyses of social media data: 1 (0.2%)
     Other: : 1 (0.2%)
     Other: engineering/system paper with internal evaluation (no human user study) : 1 (0.2%)
     Other: use of machine learning to evaluate clinical content (ML was used to improve quality): 1 (0.2%)
     Opinion, commentary, perspective, correspondence: 1 (0.2%)
     Other: Tool Development and Evaluation, Direct LLM performance evaluation: 1 (0.2%)
     Other: experimental research: 1 (0.2%)
     Other: Development of new taxonomy for utterances of LLM psychotherapists. Fine-tuning of BERT. Direct evaluation of ChatGPT 3.5.: 1 (0.2%)
     Other: content analysis of SM posts: 1 (0.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 14
     Other: Development of new taxonomy for utterances of LLM psychotherapists. Fine-tuning of BERT. Direct evaluation of ChatGPT 3.5.: 1
     Other: experimental research: 1
     Other: Tool Development and Evaluation, Direct LLM performance evaluation: 1
     Other: use of machine learning to evaluate clinical content (ML was used to improve quality): 1
     Other: engineering/system paper with internal evaluation (no human user study): 1
     Other: Propsal of a multimodal Model called "empathy AI": 1
     Other: thematic analyses of social media data: 1
     Other: Case study: 2
     Other: Qualitative Study: 1
     Other: Observational study (user-generated reviews) with unsupervised text mining: 1
     Other: theoretical,clinical, and ethical analysis: 1
     Other: experimental vignette study with panel raters: 1
     Other: content analysis of SM posts: 1


96. S‑2 Content‑safety evaluation conducted Considered in tool design? (Y/N)
   ============================================================================
   Data type: object
   Total values: 488
   Non-null values: 313
   Null values: 175
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 175 (55.9%)
     n: 153 (48.9%)
     N: 96 (30.7%)
     Y: 42 (13.4%)
     y: 20 (6.4%)
     -: 1 (0.3%)
     Y : 1 (0.3%)


97. S‑2 Content‑safety evaluation conducted if YES: Notes (paste text passage)
   ==============================================================================
   Data type: object
   Total values: 488
   Non-null values: 66
   Null values: 422
   Unique values: 46
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 422 (639.4%)
     LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11): 2 (3.0%)
     A professional counselor with three years of experience 
monitored the entire conversation to ensure the safety of the 
counseling process: 2 (3.0%)
     A custom moderation application programming interface (API)
filters inputs and outputs, flagging inappropriate content before
LLM processing. Key safety features include the following.: 2 (3.0%)
     In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.: 2 (3.0%)
     All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.: 2 (3.0%)
     Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.: 2 (3.0%)
     This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc: 2 (3.0%)
     Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.: 2 (3.0%)
     “OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9): 2 (3.0%)
     In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users: 2 (3.0%)
     Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.: 2 (3.0%)
     See e.g., "Strong Framing of Opinions and Suggestions": 2 (3.0%)
     discuss misinformation, over-validation, and ethical risks reported by users: 2 (3.0%)
     see 4.2 -> Safety considerations: 2 (3.0%)
     All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results): 2 (3.0%)
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.: 2 (3.0%)
     To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.: 2 (3.0%)
     GPT-2 outputs are not checked: 2 (3.0%)
     During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.: 2 (3.0%)
     used reflection quality classifier: 2 (3.0%)
     evaluate … ability to intervene ethically … provide data-driven insights for optimizing safety mechanisms: 1 (1.5%)
     All responses from Therabot were supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from Therabot (e.g., providing medical advice), we contacted the participant to provide correction.

However, no automatic screening: 1 (1.5%)
     All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.: 1 (1.5%)
     All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.

However, no automatic screening pre-transmission: 1 (1.5%)
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer

We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.: 1 (1.5%)
     Y (SAFE-AI protocol includes screening, monitoring, and evaluation) — Quote: “SAFE-AI protocol (Screening, Alignment, Facilitation, and Evaluation of AI-Enhanced Interventions) provides systematic guidelines … including consent, alliance monitoring, empathic failures” (Position: Section 4.7, p. 13): 1 (1.5%)
     see previous one: 1 (1.5%)
     study is about this exactly: 1 (1.5%)
     Question "Is this infomation safe?": 1 (1.5%)
     our exhaustive analysis of all chatbot- 
user dialogues recorded in this study did not reveal any inappropriate responses. This result, however, is not defini-
tive, and further research is required to ascertain the safety of chatbots in psychological research: 1 (1.5%)
     some types of misinformation were present in 22% of responses: 1 (1.5%)
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses: 1 (1.5%)
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer: 1 (1.5%)
     Subdomain 2: Evidence-Based Content
: 1 (1.5%)
     Although proper care was taken that no 
triggering problem was used in the survey, the participants were also 
informed about the counseling services available at our university if the 
need arose. As the responses were circulated to experts, no such 
comment which was rejected on this basis (misleading or harmful) was 
included in the study.: 1 (1.5%)
      Conducted thorough testing of the
chatbot’s responses (Fig. 6) to various user queries and
scenarios to assess its ability to uphold ethical standards
and protect user privacy. This testing involved simulating
user interactions with the chatbot and evaluating its
responses for any instances of inappropriate or unethical behavior, such as providing inaccurate information,
breaching confidentiality, or engaging in discriminatory
practices. Any identified issues were promptly addressed
and remediated to ensure that the chatbot consistently
adheres to ethical guidelines and respects user privacy.: 1 (1.5%)
     Responses are validated for non-toxicity before being delivered to
the user.: 1 (1.5%)
     attending psychiatrist was checking responses: 1 (1.5%)
     Chatbots mostly preserved privacy and avoided harmful
content. However: 1 (1.5%)
     we did not find any unsafe recommendations for managing depression or wellness: 1 (1.5%)
     To test the validity of the annotation scheme, the first author
annotated each GPT-4 generated turn collected in the user study along
the 4 dimensions of undesired behaviour outlined above, allowing
multiple labels per turn. Based on the annotation scheme, GPT-outputs
containing advice, factual information, sharing personal information, or
revealing emotions, as well as outputs with directive or confrontational
wording were marked as harmful. Following this, a second annotator
(not an author) was employed to annotate a share of the turns along
the four axes. 50 randomly sampled turns, along with the guidelines,
were given to the annotator for annotation in a training round.: 1 (1.5%)
     ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. : 1 (1.5%)
     We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.: 1 (1.5%)
     significant gaps in chatbots’ crisis management abilities, including the absence of risk assessment and failure to refer users to lifesaving crisis hotlines” (Position: p. 12).: 1 (1.5%)
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response1: 1 (1.5%)
     Chatbots mostly preserved privacy and avoided harmful
content. : 1 (1.5%)

   Sample values (first 20):
     attending psychiatrist was checking responses
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.
     GPT-2 outputs are not checked
     During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.
     study is about this exactly
     used reflection quality classifier
     see 4.2 -> Safety considerations
     To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.
     Although proper care was taken that no 
triggering problem was used in the survey, the participants were also 
informed about the counseling services available at our university if the 
need arose. As the responses were circulated to experts, no such 
comment which was rejected on this basis (misleading or harmful) was 
included in the study.
     All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results)
     ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. 
     We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer

We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.
     LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11)
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response1
     significant gaps in chatbots’ crisis management abilities, including the absence of risk assessment and failure to refer users to lifesaving crisis hotlines” (Position: p. 12).
     See e.g., "Strong Framing of Opinions and Suggestions"
     Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.
     To test the validity of the annotation scheme, the first author
annotated each GPT-4 generated turn collected in the user study along
the 4 dimensions of undesired behaviour outlined above, allowing
multiple labels per turn. Based on the annotation scheme, GPT-outputs
containing advice, factual information, sharing personal information, or
revealing emotions, as well as outputs with directive or confrontational
wording were marked as harmful. Following this, a second annotator
(not an author) was employed to annotate a share of the turns along
the four axes. 50 randomly sampled turns, along with the guidelines,
were given to the annotator for annotation in a training round.


98. Title
   =========
   Data type: object
   Total values: 488
   Non-null values: 488
   Null values: 0
   Unique values: 165
   Category: CATEGORICAL

   Value Counts:
     Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.: 3 (0.6%)
     Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates: 3 (0.6%)
     Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent: 3 (0.6%)
     LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour: 3 (0.6%)
     Early Detection and Personalized Intervention in Mental Health: 3 (0.6%)
     Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study: 3 (0.6%)
     Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery: 3 (0.6%)
     Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering: 3 (0.6%)
     The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy: 3 (0.6%)
     Evaluating Language Models for Assessing Counselor Reflections: 3 (0.6%)
     Can AI relate: Testing large language model response for mental health support: 3 (0.6%)
     Multimodal Framework for Therapeutic Consultations: 3 (0.6%)
     Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections: 3 (0.6%)
     Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques: 3 (0.6%)
     Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*: 3 (0.6%)
     Conversational ai for mental health support: 3 (0.6%)
     AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks: 3 (0.6%)
     Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study: 3 (0.6%)
     Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy: 3 (0.6%)
     A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy: 3 (0.6%)
     The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries: 3 (0.6%)
     Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care: 3 (0.6%)
     "Shaping ChatGPT into my Digital Therapist": A thematic analysis of social media discourse on using generative artificial intelligence for mental health: 3 (0.6%)
     Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction: 3 (0.6%)
     SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis: 3 (0.6%)
     The impact of prompt engineering in large language model performance: a psychiatric example: 3 (0.6%)
     Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.: 3 (0.6%)
     Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.: 3 (0.6%)
     Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms: 3 (0.6%)
     Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT: 3 (0.6%)
     Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis: 3 (0.6%)
     Classifying Anxiety and Depression through LLMs Virtual Interactions: A Case Study with ChatGPT: 3 (0.6%)
     Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management: 3 (0.6%)
     Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT: 3 (0.6%)
     Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model: 3 (0.6%)
     Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot: 3 (0.6%)
     Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings: 3 (0.6%)
     Addressing the Challenges of Mental Health Conversations with Large Language Models: 3 (0.6%)
     Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder: 3 (0.6%)
     A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study: 3 (0.6%)
     Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.: 3 (0.6%)
     Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction: 3 (0.6%)
     Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study: 3 (0.6%)
     Developing a single-session outcome measure using natural language processing on digital mental health transcripts: 3 (0.6%)
     Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study: 3 (0.6%)
     Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues: 3 (0.6%)
     Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study: 3 (0.6%)
     Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent: 3 (0.6%)
     Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots: 3 (0.6%)
     A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot: 3 (0.6%)
     MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging: 3 (0.6%)
     A Bi-Lingual Counselling Chatbot Application for Support of Gender Based Violence Victims in Kenya: 3 (0.6%)
     LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant: 3 (0.6%)
     Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study: 3 (0.6%)
     Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users: 3 (0.6%)
     Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study: 3 (0.6%)
     Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.: 3 (0.6%)
     Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A: 3 (0.6%)
     Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots: 3 (0.6%)
     VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model: 3 (0.6%)
     Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach: 3 (0.6%)
     SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention: 3 (0.6%)
     Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks: 3 (0.6%)
     When ELIZA meets therapists: A Turing test for the heart and mind: 3 (0.6%)
     Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation: 3 (0.6%)
     Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories: 3 (0.6%)
     Conversational Agents for Dementia using Large Language Models: 3 (0.6%)
     Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support: 3 (0.6%)
     AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction: 3 (0.6%)
     "I Don't Understand It, but Okay": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models: 3 (0.6%)
     Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support: 3 (0.6%)
     Digital Risk Considerations Across Generative AI-based Mental Health Apps: 3 (0.6%)
     Expert Patient Interaction Language Model (EPILM): 3 (0.6%)
     Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis: 3 (0.6%)
     Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model: 3 (0.6%)
     The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis: 3 (0.6%)
     Multimodal Integration, Fine Tuning of Large Language Model for Autism Support: 3 (0.6%)
     Use of AI in mental health care: Community and mental health professionals survey: 3 (0.6%)
     Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios: 3 (0.6%)
     Randomized trial of a generative AI chatbot for mental health treatment: 3 (0.6%)
     Artificial Intelligence in Mental Health Care: The T5 Chatbot Project: 3 (0.6%)
     PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling: 3 (0.6%)
     Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial: 3 (0.6%)
     Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories: 3 (0.6%)
     Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy: 3 (0.6%)
     Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations: 3 (0.6%)
     Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy: 3 (0.6%)
     Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example): 3 (0.6%)
     Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study: 3 (0.6%)
     Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study: 3 (0.6%)
     Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context: 3 (0.6%)
     Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment: 3 (0.6%)
     Towards Motivational and Empathetic Response Generation in Online Mental Health Support: 3 (0.6%)
     A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study: 3 (0.6%)
     Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support: 3 (0.6%)
     Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation: 3 (0.6%)
     Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring: 3 (0.6%)
     Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety: 3 (0.6%)
     Leveraging ChatGPT to optimize depression intervention through explainable deep learning: 3 (0.6%)
     Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy: 3 (0.6%)
     Deep Learning Mental Health Dialogue System: 3 (0.6%)
     Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change: 3 (0.6%)
     Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues: 3 (0.6%)
     Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.: 3 (0.6%)
     ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence: 3 (0.6%)
     Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.: 3 (0.6%)
     Feasibility of combining spatial computing and AI for mental health support in anxiety and depression: 3 (0.6%)
     An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system: 3 (0.6%)
     Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments: 3 (0.6%)
     Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs): 3 (0.6%)
     Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO): 3 (0.6%)
     Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference: 3 (0.6%)
     Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models: 3 (0.6%)
     CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering: 3 (0.6%)
     Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support: 3 (0.6%)
     Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings: 3 (0.6%)
     Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I: 3 (0.6%)
     Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023: 3 (0.6%)
     A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II: 3 (0.6%)
     Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I: 3 (0.6%)
     Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement: 3 (0.6%)
     Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation: 3 (0.6%)
     Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study: 3 (0.6%)
     Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach: 3 (0.6%)
     ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane: 3 (0.6%)
     Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.: 3 (0.6%)
     Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning: 3 (0.6%)
     AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection: 3 (0.6%)
     Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot: 3 (0.6%)
     Mello: A Large Language Model for Mental Health Counselling Conversations: 3 (0.6%)
     Cases of Using ChatGPT as a Mental Health and Psychological Support Tool: 3 (0.6%)
     Employing large language models for emotion detection in psychotherapy transcripts: 3 (0.6%)
     MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration: 3 (0.6%)
     Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback: 3 (0.6%)
     Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults: 3 (0.6%)
     AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation: 3 (0.6%)
     Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study: 3 (0.6%)
     MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills: 3 (0.6%)
     Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot: 3 (0.6%)
     Artificial intelligence conversational agents in mental health: Patients see potential, but prefer humans in the loop: 3 (0.6%)
     AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study: 3 (0.6%)
     Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support: 3 (0.6%)
     Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study: 3 (0.6%)
     Multi-Tiered RAG-Based Chatbot for Mental Health Support: 3 (0.6%)
     Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation: 3 (0.6%)
     nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care: 3 (0.6%)
     MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection: 3 (0.6%)
     Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept: 3 (0.6%)
     Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues: 3 (0.6%)
     ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.: 3 (0.6%)
     Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses: 3 (0.6%)
     Safety of Large Language Models in Addressing Depression.: 3 (0.6%)
     Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study: 3 (0.6%)
     Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting: 3 (0.6%)
     Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods: 3 (0.6%)
     Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model: 3 (0.6%)
     Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling: 3 (0.6%)
     Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot): 3 (0.6%)
     The Role of AI Counselling in Journaling for Mental Health Improvement: 3 (0.6%)
     Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students: 3 (0.6%)
     " I've talked to ChatGPT about my issues last night.": Examining Mental Health Conversations with Large Language Models through Reddit Analysis: 3 (0.6%)
     “Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles: 2 (0.4%)
     Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model: 1 (0.2%)
     Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant: 1 (0.2%)
     Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans: 1 (0.2%)

   Sample values (first 20):
     Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.
     Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care
     Feasibility of combining spatial computing and AI for mental health support in anxiety and depression
     An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system
     Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments
     Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs)
     Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)
     Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference
     Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models
     CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering
     Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support
     Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings
     Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I
     Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023
     A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II
     Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I
     Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement
     Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation
     Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study
     Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.


99. Type of Clients or Patients
   ===============================
   Data type: object
   Total values: 488
   Non-null values: 383
   Null values: 105
   Unique values: 39
   Category: CATEGORICAL

   Value Counts:
     No clients/patients involved: 218 (56.9%)
     [NULL]: 105 (27.4%)
     General population: 77 (20.1%)
     People with some symptoms but not disorder (determined by symptom scales or questionnaires): 20 (5.2%)
     Patients recruited in hospital or outpatient treatment facility: 13 (3.4%)
     Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires): 5 (1.3%)
     Other: unknown: 5 (1.3%)
     Patients with disorder explicitly based on ICD or DSM: 5 (1.3%)
     Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM: 5 (1.3%)
     Other: unclear: 3 (0.8%)
     Other: "Child counseling experts": 2 (0.5%)
     Other: Mental health practitioners: 2 (0.5%)
     Other: Researchers: 1 (0.3%)
     No clients/patients involved; Other: Therapists: 1 (0.3%)
     Other: We developed a comprehensive
recruitment strategy and publicized it widely on the campus
of Wenzhou Medical University to attract a diverse pool of
potential candidates: 1 (0.3%)
     Other: Convenience sample: 1 (0.3%)
     No clients/patients involved; Other: User data shown in Fig. 7 but sample nowhere futher described. Hence unclear, how data was retrieved. : 1 (0.3%)
     Other: unknown recruitment criteria: 1 (0.3%)
     Other: Pediatric and adolescent and
young adult (AYA) patients with cancer: 1 (0.3%)
     No clients/patients involved; General population: 1 (0.3%)
     General population; Other: working professionals: 1 (0.3%)
     Other: Probably patients with disorders, but not specified: 1 (0.3%)
     Other: Unclear, probably general population: 1 (0.3%)
     Other: Therapists: 1 (0.3%)
     Other: The author served as the sole participant: 1 (0.3%)
     No clients/patients involved; Other: Reviewers trained in developing therapeutic stories: 1 (0.3%)
     Other: Reviewers trained in developing therapeutic stories: 1 (0.3%)
     Other: The author served as the sole "participant": 1 (0.3%)
     Other: young adults, convenience sampling, online forms and in-person visits to classrooms: 1 (0.3%)
     Other: unclear (it just says "users"): 1 (0.3%)
     Other: healthy individuals without a diagnosed mental health disorder: 1 (0.3%)
     Other: Visitors to Mental Health America (MHA, large mental health website that provides mental health resources and tools to millions of users).
"Many MHA visitors are interested in mental health resources including self-guided systems.": 1 (0.3%)
     Other: chatbot usersfrom three sub-Reddits: r/Snapchat, r/Anima, and r/Parradot: 1 (0.3%)
     Patients with disorder explicitly based on ICD or DSM; General population: 1 (0.3%)
     No clients/patients involved; Other: youtube, The data collection
process provides us 12.9𝐾 utterances from 212 counseling therapy
sessions – all of them are dyadic conversations only.: 1 (0.3%)
     Other: The questions in PsyQA data set cover a variety of user groups.: 1 (0.3%)
     Other: reddit users (r/Replika): 1 (0.3%)
     Other: 68 university students recruited using convenience sampling (social media campaign, institutional newsletter): 1 (0.3%)
     People with some symptoms but not disorder (determined by symptom scales or questionnaires); Other: Human evaluations relied on two groups: volunteers who believe they are suffering mental 
health issue, mental healthcare professionals and researchers.: 1 (0.3%)
     Other: Patients with psychological problems and having sought professional help: 1 (0.3%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 32
     Other: unclear: 3
     Other: 68 university students recruited using convenience sampling (social media campaign, institutional newsletter): 1
     Other: reddit users (r/Replika): 1
     Other: The questions in PsyQA data set cover a variety of user groups.: 1
     Other: chatbot usersfrom three sub-Reddits: r/Snapchat, r/Anima, and r/Parradot: 1
     Other: Visitors to Mental Health America (MHA, large mental health website that provides mental health resources and tools to millions of users).: 1
     Other: healthy individuals without a diagnosed mental health disorder: 1
     Other: unclear (it just says "users"): 1
     Other: young adults, convenience sampling, online forms and in-person visits to classrooms: 1
     Other: The author served as the sole participant: 1
     Other: The author served as the sole "participant": 1
     Other: Reviewers trained in developing therapeutic stories: 1
     Other: "Child counseling experts": 2
     Other: Therapists: 1
     Other: unknown: 5
     Other: We developed a comprehensive: 1
     Other: Convenience sample: 1
     Other: Researchers: 1
     Other: unknown recruitment criteria: 1
     Other: Pediatric and adolescent and: 1
     Other: Mental health practitioners: 2
     Other: Probably patients with disorders, but not specified: 1
     Other: Unclear, probably general population: 1
     Other: Patients with psychological problems and having sought professional help: 1

   Sample values (first 20):
     No clients/patients involved
     Patients with disorder explicitly based on ICD or DSM
     Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM
     People with some symptoms but not disorder (determined by symptom scales or questionnaires)
     People with some symptoms but not disorder (determined by symptom scales or questionnaires); Other: Human evaluations relied on two groups: volunteers who believe they are suffering mental 
health issue, mental healthcare professionals and researchers.
     General population
     Other: unclear
     Other: 68 university students recruited using convenience sampling (social media campaign, institutional newsletter)
     Other: reddit users (r/Replika)
     Other: The questions in PsyQA data set cover a variety of user groups.
     No clients/patients involved; Other: youtube, The data collection
process provides us 12.9𝐾 utterances from 212 counseling therapy
sessions – all of them are dyadic conversations only.
     Patients recruited in hospital or outpatient treatment facility
     Patients with disorder explicitly based on ICD or DSM; General population
     Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires)
     Other: chatbot usersfrom three sub-Reddits: r/Snapchat, r/Anima, and r/Parradot
     Other: Visitors to Mental Health America (MHA, large mental health website that provides mental health resources and tools to millions of users).
"Many MHA visitors are interested in mental health resources including self-guided systems."
     Other: healthy individuals without a diagnosed mental health disorder
     Other: unclear (it just says "users")
     Other: young adults, convenience sampling, online forms and in-person visits to classrooms
     Other: The author served as the sole participant


100. Type of data set
   =====================
   Data type: object
   Total values: 488
   Non-null values: 210
   Null values: 278
   Unique values: 59
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 278 (132.4%)
     Internet data -- mental health Q&A: 43 (20.5%)
     Psychotherapy -- speech transcripts: 26 (12.4%)
     Emotional support dialogue -- chat logs: 24 (11.4%)
     Other: : 22 (10.5%)
     Internet data -- mental health forum: 9 (4.3%)
     Psychotherapy -- chat logs: 9 (4.3%)
     Emotional support dialogue -- speech transcripts: 6 (2.9%)
     Other: unknown: 3 (1.4%)
     Other: Mixed: 3 (1.4%)
     Other: Book data: 2 (1.0%)
     Other: "mental health articles and discussion forums": 2 (1.0%)
     Other: unclear: 2 (1.0%)
     Other: Labeled examples of cognitive distortions: 2 (1.0%)
     Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) : 2 (1.0%)
     Other: Emotion detection dataset and classification dataset: 2 (1.0%)
     Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists: 2 (1.0%)
     Other: MI reflections (client prompt - counselor reflection pairs): 2 (1.0%)
     Other: different types (books, articles, etc.), not further specified: 2 (1.0%)
     Other: labeled cognitive distortions: 2 (1.0%)
     Other: mixed: 2 (1.0%)
     Other: Descriptions of mental disorders in the DSM-5: 2 (1.0%)
     Other: Standardized clinical instrument with hypothetical scenarios and predefined responses: 2 (1.0%)
     Other: Rating: 2 (1.0%)
     Other: Internet data, various types and sources (mixed): 2 (1.0%)
     Other: expertly written therapist–patient dialogues based on third-wave CBT: 1 (0.5%)
     Other: SIRI-2 test data with ratings: 1 (0.5%)
     Other: Mental health education documents: 1 (0.5%)
     Other: qualitative interviews: 1 (0.5%)
     Other: Other: statements, labeled with sentiments (sentiment analysis data): 1 (0.5%)
     Other: mix: 1 (0.5%)
     Other: Multiple types (compare results data loading and preparation): 1 (0.5%)
     Other: a) Crowdsourced dialogues
b) Internet data -- mental health Q&A: 1 (0.5%)
     Other: Emotional support dialogue and Internet data: 1 (0.5%)
     Other: The research begins by curating a specialized User and Counsellor Interaction CSV dataset by querying an autism counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source for reducing bias and adding a human touch. : 1 (0.5%)
     Other: Twitter (X): 1 (0.5%)
     Other: Roleplay and sharing/microblogging data: 1 (0.5%)
     Other: Roleplay and sharing/microblogging data

Psychotherapy -- speech transcripts: 1 (0.5%)
     Other: "context-response pairs" -- what is that?: 1 (0.5%)
     Other: Reddit Posts: 1 (0.5%)
     Other: MI reflections: 1 (0.5%)
     Other: descriptions of cognitive distortions: 1 (0.5%)
     Other: Emotional support dialogue -- multimodal data: 1 (0.5%)
     Other: automatic thought-feeling pairs: 1 (0.5%)
     Other: cbt diary entries: 1 (0.5%)
     Other: Motivational interviewing reflections: 1 (0.5%)
     Other: thought records: 1 (0.5%)
     Other: automatic thought records: 1 (0.5%)
     Other: emotional experience descriptions: 1 (0.5%)
     Other: Survey responses (quantitative, self-report): 1 (0.5%)
     Other: unclear (authors call it "context-response pairs": 1 (0.5%)
     Other: A large naturalistic speech database with emotional traces

The FER+ annotations provide a set of new labels for the standard Emotion FER dataset. In FER+, each image has been labeled by 10 crowd-sourced taggers, which provide better quality ground truth for still image emotion than the original FER labels. : 1 (0.5%)
     Other: Internet data - online forum: 1 (0.5%)
     Other: Psychotherapy -- speech transcripts + Syntethic data : 1 (0.5%)
     Other: unspecified : 1 (0.5%)
     Other: unspecified: 1 (0.5%)
     Other: Psychotherapy -- Therapist Feedback: 1 (0.5%)
     Other: Speech data from children,  child
counseling service provided by the Professor Youjin Han’s
research lab: 1 (0.5%)
     Other: Self curated : 1 (0.5%)
     Other: Synthetic, self-created counseling dialogues for fine-tuning.: 1 (0.5%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 71
     Other: mixed: 2
     Other: descriptions of cognitive distortions: 1
     Other: Emotional support dialogue -- multimodal data: 1
     Other: automatic thought-feeling pairs: 1
     Other: cbt diary entries: 1
     Other: Motivational interviewing reflections: 1
     Other: thought records: 1
     Other: automatic thought records: 1
     Other: emotional experience descriptions: 1
     Other: labeled cognitive distortions: 2
     Other: MI reflections: 1
     Other: Survey responses (quantitative, self-report): 1
     Other: A large naturalistic speech database with emotional traces: 1
     Other: unknown: 3
     Other: Internet data - online forum: 1
     Other: Psychotherapy -- speech transcripts + Syntethic data: 1
     Other: Book data: 2
     Other: unspecified: 2
     Other: Psychotherapy -- Therapist Feedback: 1
     Other: Speech data from children,  child: 1
     Other: Labeled examples of cognitive distortions: 2
     Other: "context-response pairs" -- what is that?: 1
     Other: Self curated: 1
     Other: unclear (authors call it "context-response pairs": 1
     Other: "mental health articles and discussion forums": 2
     Other: Mixed: 3
     Other: Psychotherapy-related (diagnostic questionnaires + daily diaries): 2
     Other: Emotion detection dataset and classification dataset: 2
     Other: MI reflections (client prompt - counselor reflection pairs): 2
     Other: Reddit Posts: 1
     Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists: 2
     Other: Mental health education documents: 1
     Other: qualitative interviews: 1
     Other: Other: statements, labeled with sentiments (sentiment analysis data): 1
     Other: a) Crowdsourced dialogues: 1
     Other: Emotional support dialogue and Internet data: 1
     Other: unclear: 2
     Other: The research begins by curating a specialized User and Counsellor Interaction CSV dataset by querying an autism counsellor. This data encapsulates nuanced responses and: 1
     Other: Roleplay and sharing/microblogging data: 2
     Other: expertly written therapist–patient dialogues based on third-wave CBT: 1
     Other: Multiple types (compare results data loading and preparation): 1
     Other: Twitter (X): 1
     Other: Rating: 2
     Other: Internet data, various types and sources (mixed): 2
     Other: mix: 1
     Other: different types (books, articles, etc.), not further specified: 2
     Other: SIRI-2 test data with ratings: 1
     Other: Standardized clinical instrument with hypothetical scenarios and predefined responses: 2
     Other: Descriptions of mental disorders in the DSM-5: 2
     Other: Synthetic, self-created counseling dialogues for fine-tuning.: 1

   Sample values (first 20):
     Psychotherapy -- speech transcripts
     Psychotherapy -- chat logs
     Internet data -- mental health Q&A
     Other: mixed
     Other: descriptions of cognitive distortions
     Emotional support dialogue -- chat logs
     Other: Emotional support dialogue -- multimodal data
     Other: automatic thought-feeling pairs
     Other: cbt diary entries
     Other: Motivational interviewing reflections
     Other: thought records
     Internet data -- mental health forum
     Other: automatic thought records
     Other: emotional experience descriptions
     Other: labeled cognitive distortions
     Other: MI reflections
     Other: 
     Other: Survey responses (quantitative, self-report)
     Other: A large naturalistic speech database with emotional traces

The FER+ annotations provide a set of new labels for the standard Emotion FER dataset. In FER+, each image has been labeled by 10 crowd-sourced taggers, which provide better quality ground truth for still image emotion than the original FER labels. 
     Other: unknown


101. User Experience Assessment
   ===============================
   Data type: object
   Total values: 488
   Non-null values: 440
   Null values: 48
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     No users involved: 234 (53.2%)
     Yes: 151 (34.3%)
     No: 54 (12.3%)
     [NULL]: 48 (10.9%)
     Other: users were not the target group ( here: children), but educators and caregivers rated the effectiviness in potential application with children: 1 (0.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 1
     Other: users were not the target group ( here: children), but educators and caregivers rated the effectiviness in potential application with children: 1


102. User Experience Assessment Instrument
   ==========================================
   Data type: object
   Total values: 488
   Non-null values: 45
   Null values: 443
   Unique values: 31
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 443 (984.4%)
     SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract): 2 (4.4%)
     Counselor rating form‑short (CRF‑S), Client satisfaction scale (CSS): 2 (4.4%)
     User experience questionnaire (UEQ): 2 (4.4%)
     Human Computer Trust Scale S. S. Siddharth Gulati and D. Lamas, “Design, development and evaluation of a human-computer trust scale,” Behaviour & Information Technology: 2 (4.4%)
     System Usability Scale, SUS
User Experience Scale, UES 
Client Satisfaction Questionnaire, CSQ
Likert-scale: 2 (4.4%)
     AI attitude scale (AIAS-4): doi: 10.3389/fpsyg.2023.1191628: 2 (4.4%)
     CSQ and PU: 2 (4.4%)
     During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).: 2 (4.4%)
     Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9: 2 (4.4%)
     One of the ICAs on the 
7-point Likert scale using two measures from UEQ: 2 (4.4%)
     consultation and relational empathy survey (CARE): 2 (4.4%)
     modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al: 2 (4.4%)
     empathic understanding (EU), system usability (SUS), acceptance (CSQi): 2 (4.4%)
     Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).: 2 (4.4%)
     System Usability Scale (SUS) + Client Evaluation of Motivational Interviewing (CEMI): 1 (2.2%)
     Counselling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964

Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b): 1 (2.2%)
     Client satisfaction scale (CSS): 1 (2.2%)
     System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi): 1 (2.2%)
     system usability (System Usability Scale, SUS), user engagement
(User Experience Scale, UES), and satisfaction with the psy-
chological service (Client Satisfaction Questionnaire, CSQ).: 1 (2.2%)
     (Working Alliance Inventory — Short Revised [WAI-SR]): 1 (2.2%)
     Counseling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964: 1 (2.2%)
     FAITA-Mental Health: 1 (2.2%)
     Participant Feedback : 1 (2.2%)
     SAM-Scale: 1 (2.2%)
     ??CAPE Framework — Quote: “The CAPE framework lays a foundation for future quality assessments” (Position: Discussion): 1 (2.2%)
     German version of the Working Alliance Inventory - short revised (WAI-SR): 1 (2.2%)
     UEQ: 1 (2.2%)
     SEM: 1 (2.2%)
     Mental Help Seeking Attitudes Scale (MHSAS) with regard to chatbots for mental health support.
Modified version of the measure of structural barriers toward mental health service utilization as reported by Van Doren et al.: 1 (2.2%)
     Client Satisfaction Questionnaire-8 (CSQ-8): 1 (2.2%)
     themtaic coding of user posts // themes: : 1 (2.2%)

   Sample values (first 20):
     During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).
     consultation and relational empathy survey (CARE)
     System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi)
     empathic understanding (EU), system usability (SUS), acceptance (CSQi)
     Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).
     Counseling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964
     Counselling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964

Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b)
     modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al
     Mental Help Seeking Attitudes Scale (MHSAS) with regard to chatbots for mental health support.
Modified version of the measure of structural barriers toward mental health service utilization as reported by Van Doren et al.
     SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract)
     System Usability Scale (SUS) + Client Evaluation of Motivational Interviewing (CEMI)
     Participant Feedback 
     Client Satisfaction Questionnaire-8 (CSQ-8)
     Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9
     SEM
     One of the ICAs on the 
7-point Likert scale using two measures from UEQ
     UEQ
     German version of the Working Alliance Inventory - short revised (WAI-SR)
     ??CAPE Framework — Quote: “The CAPE framework lays a foundation for future quality assessments” (Position: Discussion)
     SAM-Scale


103. User Experience Assessment Methods:
   ========================================
   Data type: float64
   Total values: 488
   Non-null values: 0
   Null values: 488
   Unique values: 0
   Category: CATEGORICAL

   Sample values (first 20):


104. Uses qualitative assessment Y/N
   ====================================
   Data type: object
   Total values: 488
   Non-null values: 158
   Null values: 330
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 330 (208.9%)
     Y: 52 (32.9%)
     N: 38 (24.1%)
     y: 37 (23.4%)
     n: 30 (19.0%)
     somewhat: 1 (0.6%)


105. Uses quantitative assessment Y/N
   =====================================
   Data type: object
   Total values: 488
   Non-null values: 157
   Null values: 331
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 331 (210.8%)
     Y: 56 (35.7%)
     y: 40 (25.5%)
     N: 33 (21.0%)
     n: 27 (17.2%)
     somewhat: 1 (0.6%)


106. Uses some standard, established user experience instrument Y/N
   ===================================================================
   Data type: object
   Total values: 488
   Non-null values: 160
   Null values: 328
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 328 (205.0%)
     N: 68 (42.5%)
     n: 49 (30.6%)
     Y: 22 (13.8%)
     y: 18 (11.2%)
     -: 2 (1.2%)
     Y; working alliance inventory-short report? : 1 (0.6%)


107. Were responders trained professionals or lay people/unknown?
   =================================================================
   Data type: object
   Total values: 488
   Non-null values: 214
   Null values: 274
   Unique values: 24
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 274 (128.0%)
     Unknown: 80 (37.4%)
     Trained professionals: 55 (25.7%)
     Other: : 21 (9.8%)
     Other: not applicable: 18 (8.4%)
     Lay people: 13 (6.1%)
     Other: n.a.: 5 (2.3%)
     Other: Mix of trained and lay: 2 (0.9%)
     Other: No responders: 2 (0.9%)
     Other: Woebot chatbot: 2 (0.9%)
     Other: Not applicable, no interaction data: 2 (0.9%)
     Other: Mix of lay and trained: 1 (0.5%)
     Other: chatbot: 1 (0.5%)
     Other: experienced counsellors: 1 (0.5%)
     Other: not applicable (only initial posts, no responses): 1 (0.5%)
     Other: mixed: 1 (0.5%)
     Other: chatbot response: 1 (0.5%)
     Other: Mixed: 1 (0.5%)
     Other: unknown: 1 (0.5%)
     Other: Well-trained volunteers or professional counselors = mixed: 1 (0.5%)
     Other: a) Unknown 
b) Trained professionals ("verified therapists"): 1 (0.5%)
     Other: Probably mixed: 1 (0.5%)
     Other: both lay and pros: 1 (0.5%)
     Other: Crowdworkers: 1 (0.5%)
     Other: Unsure: 1 (0.5%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 45
     Other: not applicable: 18
     Other: unknown: 1
     Other: chatbot response: 1
     Other: mixed: 1
     Other: experienced counsellors: 1
     Other: Mix of lay and trained: 1
     Other: chatbot: 1
     Other: Woebot chatbot: 2
     Other: not applicable (only initial posts, no responses): 1
     Other: No responders: 2
     Other: n.a.: 5
     Other: Mixed: 1
     Other: Mix of trained and lay: 2
     Other: Well-trained volunteers or professional counselors = mixed: 1
     Other: a) Unknown: 1
     Other: Probably mixed: 1
     Other: both lay and pros: 1
     Other: Crowdworkers: 1
     Other: Unsure: 1
     Other: Not applicable, no interaction data: 2

   Sample values (first 20):
     Trained professionals
     Other: not applicable
     Unknown
     Other: unknown
     Other: chatbot response
     Other: mixed
     Lay people
     Other: 
     Other: experienced counsellors
     Other: Mix of lay and trained
     Other: chatbot
     Other: Woebot chatbot
     Other: not applicable (only initial posts, no responses)
     Other: No responders
     Other: n.a.
     Other: Mixed
     Other: Mix of trained and lay
     Other: Well-trained volunteers or professional counselors = mixed
     Other: a) Unknown 
b) Trained professionals ("verified therapists")
     Other: Probably mixed


108. Were results of user experience assessment reported? Y/N
   =============================================================
   Data type: object
   Total values: 488
   Non-null values: 159
   Null values: 329
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 329 (206.9%)
     Y: 69 (43.4%)
     y: 66 (41.5%)
     N: 19 (11.9%)
     n: 5 (3.1%)


109. Year (2017-2025)
   =====================
   Data type: float64
   Total values: 488
   Non-null values: 454
   Null values: 34
   Unique values: 9
   Category: CATEGORICAL

   Value Counts:
     2024.0: 211 (46.5%)
     2025.0: 152 (33.5%)
     2023.0: 60 (13.2%)
     [NULL]: 34 (7.5%)
     2022.0: 20 (4.4%)
     2020.0: 3 (0.7%)
     2021.0: 3 (0.7%)
     24.0: 2 (0.4%)
     25.0: 2 (0.4%)
     2027.0: 1 (0.2%)


110. covidence_id
   =================
   Data type: object
   Total values: 488
   Non-null values: 488
   Null values: 0
   Unique values: 165
   Category: CATEGORICAL

   Value Counts:
     1st_search_224: 3 (0.6%)
     2nd_search_98: 3 (0.6%)
     2nd_search_96: 3 (0.6%)
     2nd_search_94: 3 (0.6%)
     2nd_search_93: 3 (0.6%)
     2nd_search_92: 3 (0.6%)
     2nd_search_90: 3 (0.6%)
     2nd_search_89: 3 (0.6%)
     2nd_search_88: 3 (0.6%)
     2nd_search_87: 3 (0.6%)
     2nd_search_85: 3 (0.6%)
     2nd_search_83: 3 (0.6%)
     2nd_search_82: 3 (0.6%)
     2nd_search_81: 3 (0.6%)
     2nd_search_80: 3 (0.6%)
     2nd_search_75: 3 (0.6%)
     2nd_search_72: 3 (0.6%)
     2nd_search_71: 3 (0.6%)
     2nd_search_69: 3 (0.6%)
     2nd_search_97: 3 (0.6%)
     2nd_search_99: 3 (0.6%)
     1st_search_214: 3 (0.6%)
     2nd_search_100: 3 (0.6%)
     2nd_search_124: 3 (0.6%)
     2nd_search_123: 3 (0.6%)
     2nd_search_119: 3 (0.6%)
     2nd_search_117: 3 (0.6%)
     2nd_search_116: 3 (0.6%)
     2nd_search_114: 3 (0.6%)
     2nd_search_112: 3 (0.6%)
     2nd_search_111: 3 (0.6%)
     2nd_search_110: 3 (0.6%)
     2nd_search_109: 3 (0.6%)
     2nd_search_108: 3 (0.6%)
     2nd_search_107: 3 (0.6%)
     2nd_search_106: 3 (0.6%)
     2nd_search_104: 3 (0.6%)
     2nd_search_103: 3 (0.6%)
     2nd_search_102: 3 (0.6%)
     2nd_search_101: 3 (0.6%)
     2nd_search_68: 3 (0.6%)
     2nd_search_66: 3 (0.6%)
     2nd_search_64: 3 (0.6%)
     2nd_search_63: 3 (0.6%)
     2nd_search_25: 3 (0.6%)
     2nd_search_24: 3 (0.6%)
     2nd_search_22: 3 (0.6%)
     2nd_search_20: 3 (0.6%)
     2nd_search_18: 3 (0.6%)
     2nd_search_17: 3 (0.6%)
     2nd_search_15: 3 (0.6%)
     2nd_search_12: 3 (0.6%)
     2nd_search_11: 3 (0.6%)
     2nd_search_10: 3 (0.6%)
     2nd_search_9: 3 (0.6%)
     2nd_search_8: 3 (0.6%)
     2nd_search_6: 3 (0.6%)
     2nd_search_5: 3 (0.6%)
     2nd_search_4: 3 (0.6%)
     2nd_search_3: 3 (0.6%)
     2nd_search_2: 3 (0.6%)
     2nd_search_26: 3 (0.6%)
     2nd_search_28: 3 (0.6%)
     2nd_search_29: 3 (0.6%)
     2nd_search_46: 3 (0.6%)
     2nd_search_62: 3 (0.6%)
     2nd_search_60: 3 (0.6%)
     2nd_search_58: 3 (0.6%)
     2nd_search_57: 3 (0.6%)
     2nd_search_54: 3 (0.6%)
     2nd_search_51: 3 (0.6%)
     2nd_search_47: 3 (0.6%)
     2nd_search_45: 3 (0.6%)
     2nd_search_30: 3 (0.6%)
     2nd_search_43: 3 (0.6%)
     2nd_search_42: 3 (0.6%)
     2nd_search_40: 3 (0.6%)
     2nd_search_39: 3 (0.6%)
     2nd_search_34: 3 (0.6%)
     2nd_search_32: 3 (0.6%)
     2nd_search_31: 3 (0.6%)
     2nd_search_125: 3 (0.6%)
     2nd_search_126: 3 (0.6%)
     2nd_search_127: 3 (0.6%)
     1st_search_91: 3 (0.6%)
     1st_search_88: 3 (0.6%)
     1st_search_86: 3 (0.6%)
     1st_search_81: 3 (0.6%)
     1st_search_77: 3 (0.6%)
     1st_search_75: 3 (0.6%)
     1st_search_73: 3 (0.6%)
     1st_search_72: 3 (0.6%)
     1st_search_70: 3 (0.6%)
     1st_search_63: 3 (0.6%)
     1st_search_62: 3 (0.6%)
     1st_search_61: 3 (0.6%)
     1st_search_60: 3 (0.6%)
     1st_search_59: 3 (0.6%)
     1st_search_56: 3 (0.6%)
     1st_search_50: 3 (0.6%)
     1st_search_41: 3 (0.6%)
     1st_search_34: 3 (0.6%)
     1st_search_89: 3 (0.6%)
     1st_search_92: 3 (0.6%)
     2nd_search_128: 3 (0.6%)
     1st_search_93: 3 (0.6%)
     1st_search_212: 3 (0.6%)
     1st_search_204: 3 (0.6%)
     1st_search_203: 3 (0.6%)
     1st_search_200: 3 (0.6%)
     1st_search_198: 3 (0.6%)
     1st_search_194: 3 (0.6%)
     1st_search_147: 3 (0.6%)
     1st_search_134: 3 (0.6%)
     1st_search_128: 3 (0.6%)
     1st_search_124: 3 (0.6%)
     1st_search_120: 3 (0.6%)
     1st_search_119: 3 (0.6%)
     1st_search_116: 3 (0.6%)
     1st_search_115: 3 (0.6%)
     1st_search_101: 3 (0.6%)
     1st_search_98: 3 (0.6%)
     1st_search_95: 3 (0.6%)
     1st_search_32: 3 (0.6%)
     1st_search_30: 3 (0.6%)
     1st_search_26: 3 (0.6%)
     1st_search_18: 3 (0.6%)
     2nd_search_147: 3 (0.6%)
     2nd_search_144: 3 (0.6%)
     2nd_search_143: 3 (0.6%)
     2nd_search_142: 3 (0.6%)
     2nd_search_141: 3 (0.6%)
     2nd_search_140: 3 (0.6%)
     2nd_search_139: 3 (0.6%)
     2nd_search_138: 3 (0.6%)
     2nd_search_137: 3 (0.6%)
     2nd_search_136: 3 (0.6%)
     2nd_search_135: 3 (0.6%)
     2nd_search_134: 3 (0.6%)
     2nd_search_133: 3 (0.6%)
     2nd_search_132: 3 (0.6%)
     2nd_search_131: 3 (0.6%)
     2nd_search_130: 3 (0.6%)
     2nd_search_129: 3 (0.6%)
     2nd_search_148: 3 (0.6%)
     2nd_search_150: 3 (0.6%)
     2nd_search_152: 3 (0.6%)
     2nd_search_153: 3 (0.6%)
     1st_search_17: 3 (0.6%)
     1st_search_14: 3 (0.6%)
     1st_search_13: 3 (0.6%)
     1st_search_9: 3 (0.6%)
     1st_search_3: 3 (0.6%)
     2nd_search_214: 3 (0.6%)
     2nd_search_206: 3 (0.6%)
     2nd_search_173: 3 (0.6%)
     2nd_search_169: 3 (0.6%)
     2nd_search_157: 3 (0.6%)
     2nd_search_156: 3 (0.6%)
     2nd_search_155: 3 (0.6%)
     2nd_search_1: 3 (0.6%)
     2nd_search_91: 2 (0.4%)
     2nd_search_185: 1 (0.2%)
     2nd_search_212: 1 (0.2%)
     2nd_search_230: 1 (0.2%)

   Sample values (first 20):
     1st_search_224
     1st_search_214
     1st_search_212
     1st_search_204
     1st_search_203
     1st_search_200
     1st_search_198
     1st_search_194
     1st_search_147
     1st_search_134
     1st_search_128
     1st_search_124
     1st_search_120
     1st_search_119
     1st_search_116
     1st_search_115
     1st_search_101
     1st_search_98
     1st_search_95
     1st_search_93


111. study_id
   =============
   Data type: object
   Total values: 488
   Non-null values: 488
   Null values: 0
   Unique values: 488
   Category: CATEGORICAL

   Value Counts:
     1st_search_1: 1 (0.2%)
     2nd_search_190: 1 (0.2%)
     2nd_search_188: 1 (0.2%)
     2nd_search_187: 1 (0.2%)
     2nd_search_186: 1 (0.2%)
     2nd_search_185: 1 (0.2%)
     2nd_search_184: 1 (0.2%)
     2nd_search_183: 1 (0.2%)
     2nd_search_182: 1 (0.2%)
     2nd_search_181: 1 (0.2%)
     2nd_search_180: 1 (0.2%)
     2nd_search_179: 1 (0.2%)
     2nd_search_178: 1 (0.2%)
     2nd_search_177: 1 (0.2%)
     2nd_search_176: 1 (0.2%)
     2nd_search_175: 1 (0.2%)
     2nd_search_174: 1 (0.2%)
     2nd_search_173: 1 (0.2%)
     2nd_search_172: 1 (0.2%)
     2nd_search_171: 1 (0.2%)
     2nd_search_170: 1 (0.2%)
     2nd_search_169: 1 (0.2%)
     2nd_search_168: 1 (0.2%)
     2nd_search_167: 1 (0.2%)
     2nd_search_166: 1 (0.2%)
     2nd_search_165: 1 (0.2%)
     2nd_search_164: 1 (0.2%)
     2nd_search_163: 1 (0.2%)
     2nd_search_162: 1 (0.2%)
     2nd_search_189: 1 (0.2%)
     2nd_search_191: 1 (0.2%)
     1st_search_2: 1 (0.2%)
     2nd_search_192: 1 (0.2%)
     2nd_search_219: 1 (0.2%)
     2nd_search_218: 1 (0.2%)
     2nd_search_217: 1 (0.2%)
     2nd_search_216: 1 (0.2%)
     2nd_search_215: 1 (0.2%)
     2nd_search_214: 1 (0.2%)
     2nd_search_213: 1 (0.2%)
     2nd_search_212: 1 (0.2%)
     2nd_search_211: 1 (0.2%)
     2nd_search_210: 1 (0.2%)
     2nd_search_209: 1 (0.2%)
     2nd_search_208: 1 (0.2%)
     2nd_search_207: 1 (0.2%)
     2nd_search_206: 1 (0.2%)
     2nd_search_205: 1 (0.2%)
     2nd_search_204: 1 (0.2%)
     2nd_search_203: 1 (0.2%)
     2nd_search_202: 1 (0.2%)
     2nd_search_201: 1 (0.2%)
     2nd_search_200: 1 (0.2%)
     2nd_search_199: 1 (0.2%)
     2nd_search_198: 1 (0.2%)
     2nd_search_197: 1 (0.2%)
     2nd_search_196: 1 (0.2%)
     2nd_search_195: 1 (0.2%)
     2nd_search_194: 1 (0.2%)
     2nd_search_193: 1 (0.2%)
     2nd_search_161: 1 (0.2%)
     2nd_search_160: 1 (0.2%)
     2nd_search_159: 1 (0.2%)
     2nd_search_158: 1 (0.2%)
     2nd_search_127: 1 (0.2%)
     2nd_search_126: 1 (0.2%)
     2nd_search_125: 1 (0.2%)
     2nd_search_124: 1 (0.2%)
     2nd_search_123: 1 (0.2%)
     2nd_search_122: 1 (0.2%)
     2nd_search_121: 1 (0.2%)
     2nd_search_120: 1 (0.2%)
     2nd_search_119: 1 (0.2%)
     2nd_search_118: 1 (0.2%)
     2nd_search_117: 1 (0.2%)
     2nd_search_116: 1 (0.2%)
     2nd_search_115: 1 (0.2%)
     2nd_search_114: 1 (0.2%)
     2nd_search_113: 1 (0.2%)
     2nd_search_112: 1 (0.2%)
     2nd_search_111: 1 (0.2%)
     2nd_search_110: 1 (0.2%)
     2nd_search_109: 1 (0.2%)
     2nd_search_108: 1 (0.2%)
     2nd_search_107: 1 (0.2%)
     2nd_search_106: 1 (0.2%)
     2nd_search_105: 1 (0.2%)
     2nd_search_104: 1 (0.2%)
     2nd_search_103: 1 (0.2%)
     2nd_search_102: 1 (0.2%)
     2nd_search_101: 1 (0.2%)
     2nd_search_128: 1 (0.2%)
     2nd_search_129: 1 (0.2%)
     2nd_search_130: 1 (0.2%)
     2nd_search_145: 1 (0.2%)
     2nd_search_157: 1 (0.2%)
     2nd_search_156: 1 (0.2%)
     2nd_search_155: 1 (0.2%)
     2nd_search_154: 1 (0.2%)
     2nd_search_153: 1 (0.2%)
     2nd_search_152: 1 (0.2%)
     2nd_search_151: 1 (0.2%)
     2nd_search_150: 1 (0.2%)
     2nd_search_149: 1 (0.2%)
     2nd_search_148: 1 (0.2%)
     2nd_search_147: 1 (0.2%)
     2nd_search_146: 1 (0.2%)
     2nd_search_144: 1 (0.2%)
     2nd_search_131: 1 (0.2%)
     2nd_search_143: 1 (0.2%)
     2nd_search_142: 1 (0.2%)
     2nd_search_141: 1 (0.2%)
     2nd_search_140: 1 (0.2%)
     2nd_search_139: 1 (0.2%)
     2nd_search_138: 1 (0.2%)
     2nd_search_137: 1 (0.2%)
     2nd_search_136: 1 (0.2%)
     2nd_search_135: 1 (0.2%)
     2nd_search_134: 1 (0.2%)
     2nd_search_133: 1 (0.2%)
     2nd_search_132: 1 (0.2%)
     2nd_search_220: 1 (0.2%)
     2nd_search_221: 1 (0.2%)
     2nd_search_222: 1 (0.2%)
     2nd_search_283: 1 (0.2%)
     2nd_search_311: 1 (0.2%)
     2nd_search_310: 1 (0.2%)
     2nd_search_309: 1 (0.2%)
     2nd_search_308: 1 (0.2%)
     2nd_search_307: 1 (0.2%)
     2nd_search_306: 1 (0.2%)
     2nd_search_305: 1 (0.2%)
     2nd_search_304: 1 (0.2%)
     2nd_search_303: 1 (0.2%)
     2nd_search_302: 1 (0.2%)
     2nd_search_301: 1 (0.2%)
     2nd_search_300: 1 (0.2%)
     2nd_search_299: 1 (0.2%)
     2nd_search_298: 1 (0.2%)
     2nd_search_297: 1 (0.2%)
     2nd_search_296: 1 (0.2%)
     2nd_search_295: 1 (0.2%)
     2nd_search_294: 1 (0.2%)
     2nd_search_293: 1 (0.2%)
     2nd_search_292: 1 (0.2%)
     2nd_search_291: 1 (0.2%)
     2nd_search_290: 1 (0.2%)
     2nd_search_289: 1 (0.2%)
     2nd_search_288: 1 (0.2%)
     2nd_search_287: 1 (0.2%)
     2nd_search_286: 1 (0.2%)
     2nd_search_285: 1 (0.2%)
     2nd_search_312: 1 (0.2%)
     2nd_search_313: 1 (0.2%)
     2nd_search_314: 1 (0.2%)
     2nd_search_329: 1 (0.2%)
     2nd_search_341: 1 (0.2%)
     2nd_search_340: 1 (0.2%)
     2nd_search_339: 1 (0.2%)
     2nd_search_338: 1 (0.2%)
     2nd_search_337: 1 (0.2%)
     2nd_search_336: 1 (0.2%)
     2nd_search_335: 1 (0.2%)
     2nd_search_334: 1 (0.2%)
     2nd_search_333: 1 (0.2%)
     2nd_search_332: 1 (0.2%)
     2nd_search_331: 1 (0.2%)
     2nd_search_330: 1 (0.2%)
     2nd_search_328: 1 (0.2%)
     2nd_search_315: 1 (0.2%)
     2nd_search_327: 1 (0.2%)
     2nd_search_326: 1 (0.2%)
     2nd_search_325: 1 (0.2%)
     2nd_search_324: 1 (0.2%)
     2nd_search_323: 1 (0.2%)
     2nd_search_322: 1 (0.2%)
     2nd_search_321: 1 (0.2%)
     2nd_search_320: 1 (0.2%)
     2nd_search_319: 1 (0.2%)
     2nd_search_318: 1 (0.2%)
     2nd_search_317: 1 (0.2%)
     2nd_search_316: 1 (0.2%)
     2nd_search_284: 1 (0.2%)
     2nd_search_282: 1 (0.2%)
     2nd_search_223: 1 (0.2%)
     2nd_search_281: 1 (0.2%)
     2nd_search_250: 1 (0.2%)
     2nd_search_249: 1 (0.2%)
     2nd_search_248: 1 (0.2%)
     2nd_search_247: 1 (0.2%)
     2nd_search_246: 1 (0.2%)
     2nd_search_245: 1 (0.2%)
     2nd_search_244: 1 (0.2%)
     2nd_search_243: 1 (0.2%)
     2nd_search_242: 1 (0.2%)
     2nd_search_241: 1 (0.2%)
     2nd_search_240: 1 (0.2%)
     2nd_search_239: 1 (0.2%)
     2nd_search_238: 1 (0.2%)
     2nd_search_237: 1 (0.2%)
     2nd_search_236: 1 (0.2%)
     2nd_search_235: 1 (0.2%)
     2nd_search_234: 1 (0.2%)
     2nd_search_233: 1 (0.2%)
     2nd_search_232: 1 (0.2%)
     2nd_search_231: 1 (0.2%)
     2nd_search_230: 1 (0.2%)
     2nd_search_229: 1 (0.2%)
     2nd_search_228: 1 (0.2%)
     2nd_search_227: 1 (0.2%)
     2nd_search_226: 1 (0.2%)
     2nd_search_225: 1 (0.2%)
     2nd_search_224: 1 (0.2%)
     2nd_search_251: 1 (0.2%)
     2nd_search_252: 1 (0.2%)
     2nd_search_253: 1 (0.2%)
     2nd_search_268: 1 (0.2%)
     2nd_search_280: 1 (0.2%)
     2nd_search_279: 1 (0.2%)
     2nd_search_278: 1 (0.2%)
     2nd_search_277: 1 (0.2%)
     2nd_search_276: 1 (0.2%)
     2nd_search_275: 1 (0.2%)
     2nd_search_274: 1 (0.2%)
     2nd_search_273: 1 (0.2%)
     2nd_search_272: 1 (0.2%)
     2nd_search_271: 1 (0.2%)
     2nd_search_270: 1 (0.2%)
     2nd_search_269: 1 (0.2%)
     2nd_search_267: 1 (0.2%)
     2nd_search_254: 1 (0.2%)
     2nd_search_266: 1 (0.2%)
     2nd_search_265: 1 (0.2%)
     2nd_search_264: 1 (0.2%)
     2nd_search_263: 1 (0.2%)
     2nd_search_262: 1 (0.2%)
     2nd_search_261: 1 (0.2%)
     2nd_search_260: 1 (0.2%)
     2nd_search_259: 1 (0.2%)
     2nd_search_258: 1 (0.2%)
     2nd_search_257: 1 (0.2%)
     2nd_search_256: 1 (0.2%)
     2nd_search_255: 1 (0.2%)
     2nd_search_100: 1 (0.2%)
     2nd_search_99: 1 (0.2%)
     2nd_search_98: 1 (0.2%)
     1st_search_62: 1 (0.2%)
     1st_search_90: 1 (0.2%)
     1st_search_89: 1 (0.2%)
     1st_search_88: 1 (0.2%)
     1st_search_87: 1 (0.2%)
     1st_search_86: 1 (0.2%)
     1st_search_85: 1 (0.2%)
     1st_search_84: 1 (0.2%)
     1st_search_83: 1 (0.2%)
     1st_search_82: 1 (0.2%)
     1st_search_81: 1 (0.2%)
     1st_search_80: 1 (0.2%)
     1st_search_79: 1 (0.2%)
     1st_search_78: 1 (0.2%)
     1st_search_77: 1 (0.2%)
     1st_search_76: 1 (0.2%)
     1st_search_75: 1 (0.2%)
     1st_search_74: 1 (0.2%)
     1st_search_73: 1 (0.2%)
     1st_search_72: 1 (0.2%)
     1st_search_71: 1 (0.2%)
     1st_search_70: 1 (0.2%)
     1st_search_69: 1 (0.2%)
     1st_search_68: 1 (0.2%)
     1st_search_67: 1 (0.2%)
     1st_search_66: 1 (0.2%)
     1st_search_65: 1 (0.2%)
     1st_search_64: 1 (0.2%)
     1st_search_91: 1 (0.2%)
     1st_search_92: 1 (0.2%)
     1st_search_93: 1 (0.2%)
     1st_search_108: 1 (0.2%)
     1st_search_120: 1 (0.2%)
     1st_search_119: 1 (0.2%)
     1st_search_118: 1 (0.2%)
     1st_search_117: 1 (0.2%)
     1st_search_116: 1 (0.2%)
     1st_search_115: 1 (0.2%)
     1st_search_114: 1 (0.2%)
     1st_search_113: 1 (0.2%)
     1st_search_112: 1 (0.2%)
     1st_search_111: 1 (0.2%)
     1st_search_110: 1 (0.2%)
     1st_search_109: 1 (0.2%)
     1st_search_107: 1 (0.2%)
     1st_search_94: 1 (0.2%)
     1st_search_106: 1 (0.2%)
     1st_search_105: 1 (0.2%)
     1st_search_104: 1 (0.2%)
     1st_search_103: 1 (0.2%)
     1st_search_102: 1 (0.2%)
     1st_search_101: 1 (0.2%)
     1st_search_100: 1 (0.2%)
     1st_search_99: 1 (0.2%)
     1st_search_98: 1 (0.2%)
     1st_search_97: 1 (0.2%)
     1st_search_96: 1 (0.2%)
     1st_search_95: 1 (0.2%)
     1st_search_63: 1 (0.2%)
     1st_search_61: 1 (0.2%)
     1st_search_122: 1 (0.2%)
     1st_search_60: 1 (0.2%)
     1st_search_29: 1 (0.2%)
     1st_search_28: 1 (0.2%)
     1st_search_27: 1 (0.2%)
     1st_search_26: 1 (0.2%)
     1st_search_25: 1 (0.2%)
     1st_search_24: 1 (0.2%)
     1st_search_23: 1 (0.2%)
     1st_search_22: 1 (0.2%)
     1st_search_21: 1 (0.2%)
     1st_search_20: 1 (0.2%)
     1st_search_19: 1 (0.2%)
     1st_search_18: 1 (0.2%)
     1st_search_17: 1 (0.2%)
     1st_search_16: 1 (0.2%)
     1st_search_15: 1 (0.2%)
     1st_search_14: 1 (0.2%)
     1st_search_13: 1 (0.2%)
     1st_search_12: 1 (0.2%)
     1st_search_11: 1 (0.2%)
     1st_search_10: 1 (0.2%)
     1st_search_9: 1 (0.2%)
     1st_search_8: 1 (0.2%)
     1st_search_7: 1 (0.2%)
     1st_search_6: 1 (0.2%)
     1st_search_5: 1 (0.2%)
     1st_search_4: 1 (0.2%)
     1st_search_3: 1 (0.2%)
     1st_search_30: 1 (0.2%)
     1st_search_31: 1 (0.2%)
     1st_search_32: 1 (0.2%)
     1st_search_47: 1 (0.2%)
     1st_search_59: 1 (0.2%)
     1st_search_58: 1 (0.2%)
     1st_search_57: 1 (0.2%)
     1st_search_56: 1 (0.2%)
     1st_search_55: 1 (0.2%)
     1st_search_54: 1 (0.2%)
     1st_search_53: 1 (0.2%)
     1st_search_52: 1 (0.2%)
     1st_search_51: 1 (0.2%)
     1st_search_50: 1 (0.2%)
     1st_search_49: 1 (0.2%)
     1st_search_48: 1 (0.2%)
     1st_search_46: 1 (0.2%)
     1st_search_33: 1 (0.2%)
     1st_search_45: 1 (0.2%)
     1st_search_44: 1 (0.2%)
     1st_search_43: 1 (0.2%)
     1st_search_42: 1 (0.2%)
     1st_search_41: 1 (0.2%)
     1st_search_40: 1 (0.2%)
     1st_search_39: 1 (0.2%)
     1st_search_38: 1 (0.2%)
     1st_search_37: 1 (0.2%)
     1st_search_36: 1 (0.2%)
     1st_search_35: 1 (0.2%)
     1st_search_34: 1 (0.2%)
     1st_search_121: 1 (0.2%)
     1st_search_123: 1 (0.2%)
     2nd_search_97: 1 (0.2%)
     2nd_search_38: 1 (0.2%)
     2nd_search_66: 1 (0.2%)
     2nd_search_65: 1 (0.2%)
     2nd_search_64: 1 (0.2%)
     2nd_search_63: 1 (0.2%)
     2nd_search_62: 1 (0.2%)
     2nd_search_61: 1 (0.2%)
     2nd_search_60: 1 (0.2%)
     2nd_search_59: 1 (0.2%)
     2nd_search_58: 1 (0.2%)
     2nd_search_57: 1 (0.2%)
     2nd_search_56: 1 (0.2%)
     2nd_search_55: 1 (0.2%)
     2nd_search_54: 1 (0.2%)
     2nd_search_53: 1 (0.2%)
     2nd_search_52: 1 (0.2%)
     2nd_search_51: 1 (0.2%)
     2nd_search_50: 1 (0.2%)
     2nd_search_49: 1 (0.2%)
     2nd_search_48: 1 (0.2%)
     2nd_search_47: 1 (0.2%)
     2nd_search_46: 1 (0.2%)
     2nd_search_45: 1 (0.2%)
     2nd_search_44: 1 (0.2%)
     2nd_search_43: 1 (0.2%)
     2nd_search_42: 1 (0.2%)
     2nd_search_41: 1 (0.2%)
     2nd_search_40: 1 (0.2%)
     2nd_search_67: 1 (0.2%)
     2nd_search_68: 1 (0.2%)
     2nd_search_69: 1 (0.2%)
     2nd_search_84: 1 (0.2%)
     2nd_search_96: 1 (0.2%)
     2nd_search_95: 1 (0.2%)
     2nd_search_94: 1 (0.2%)
     2nd_search_93: 1 (0.2%)
     2nd_search_92: 1 (0.2%)
     2nd_search_91: 1 (0.2%)
     2nd_search_90: 1 (0.2%)
     2nd_search_89: 1 (0.2%)
     2nd_search_88: 1 (0.2%)
     2nd_search_87: 1 (0.2%)
     2nd_search_86: 1 (0.2%)
     2nd_search_85: 1 (0.2%)
     2nd_search_83: 1 (0.2%)
     2nd_search_70: 1 (0.2%)
     2nd_search_82: 1 (0.2%)
     2nd_search_81: 1 (0.2%)
     2nd_search_80: 1 (0.2%)
     2nd_search_79: 1 (0.2%)
     2nd_search_78: 1 (0.2%)
     2nd_search_77: 1 (0.2%)
     2nd_search_76: 1 (0.2%)
     2nd_search_75: 1 (0.2%)
     2nd_search_74: 1 (0.2%)
     2nd_search_73: 1 (0.2%)
     2nd_search_72: 1 (0.2%)
     2nd_search_71: 1 (0.2%)
     2nd_search_39: 1 (0.2%)
     2nd_search_37: 1 (0.2%)
     1st_search_124: 1 (0.2%)
     2nd_search_36: 1 (0.2%)
     2nd_search_5: 1 (0.2%)
     2nd_search_4: 1 (0.2%)
     2nd_search_3: 1 (0.2%)
     2nd_search_1: 1 (0.2%)
     1st_search_147: 1 (0.2%)
     1st_search_146: 1 (0.2%)
     1st_search_145: 1 (0.2%)
     1st_search_144: 1 (0.2%)
     1st_search_143: 1 (0.2%)
     1st_search_142: 1 (0.2%)
     1st_search_141: 1 (0.2%)
     1st_search_140: 1 (0.2%)
     1st_search_139: 1 (0.2%)
     1st_search_138: 1 (0.2%)
     1st_search_137: 1 (0.2%)
     1st_search_136: 1 (0.2%)
     1st_search_135: 1 (0.2%)
     1st_search_134: 1 (0.2%)
     1st_search_133: 1 (0.2%)
     1st_search_132: 1 (0.2%)
     1st_search_131: 1 (0.2%)
     1st_search_130: 1 (0.2%)
     1st_search_129: 1 (0.2%)
     1st_search_128: 1 (0.2%)
     1st_search_127: 1 (0.2%)
     1st_search_126: 1 (0.2%)
     1st_search_125: 1 (0.2%)
     2nd_search_6: 1 (0.2%)
     2nd_search_7: 1 (0.2%)
     2nd_search_8: 1 (0.2%)
     2nd_search_23: 1 (0.2%)
     2nd_search_35: 1 (0.2%)
     2nd_search_34: 1 (0.2%)
     2nd_search_33: 1 (0.2%)
     2nd_search_32: 1 (0.2%)
     2nd_search_31: 1 (0.2%)
     2nd_search_30: 1 (0.2%)
     2nd_search_29: 1 (0.2%)
     2nd_search_28: 1 (0.2%)
     2nd_search_27: 1 (0.2%)
     2nd_search_26: 1 (0.2%)
     2nd_search_25: 1 (0.2%)
     2nd_search_24: 1 (0.2%)
     2nd_search_22: 1 (0.2%)
     2nd_search_9: 1 (0.2%)
     2nd_search_21: 1 (0.2%)
     2nd_search_20: 1 (0.2%)
     2nd_search_19: 1 (0.2%)
     2nd_search_18: 1 (0.2%)
     2nd_search_17: 1 (0.2%)
     2nd_search_16: 1 (0.2%)
     2nd_search_15: 1 (0.2%)
     2nd_search_14: 1 (0.2%)
     2nd_search_13: 1 (0.2%)
     2nd_search_12: 1 (0.2%)
     2nd_search_11: 1 (0.2%)
     2nd_search_10: 1 (0.2%)
     2nd_search_342: 1 (0.2%)

   Sample values (first 20):
     1st_search_1
     1st_search_2
     1st_search_3
     1st_search_4
     1st_search_5
     1st_search_6
     1st_search_7
     1st_search_8
     1st_search_9
     1st_search_10
     1st_search_11
     1st_search_12
     1st_search_13
     1st_search_14
     1st_search_15
     1st_search_16
     1st_search_17
     1st_search_18
     1st_search_19
     1st_search_20


================================================================================
END OF REPORT
================================================================================