================================================================================
COHEN'S KAPPA INTERRATER AGREEMENT REPORT
Generated: 2025-10-01 18:23:38
================================================================================

METHODOLOGY AND CALCULATIONS
========================================

COLUMN TYPES ANALYZED:
  • Single-choice categorical columns: 55
  • Multi-choice categorical columns: 4
  • Numerical columns: 4 (identified but not analyzed)

STEP-BY-STEP ANALYSIS PROCESS:

1. DATA PREPARATION:
   • Loaded data with covidence_id as index
   • Identified non-consensus reviewers for comparison
   • Applied conditional logic for dependent fields

2. SINGLE-CHOICE CATEGORICAL ANALYSIS:
   • Excluded pairs where both reviewers selected 'Other:' responses
   • Applied conditional filtering (e.g., subtype fields only when main type agreed)
   • Calculated three agreement metrics for each field:
     - Percent Agreement: Simple observed agreement rate
     - Cohen's Kappa: Chance-corrected agreement coefficient
     - Gwet's AC1: Robust agreement coefficient less affected by prevalence

3. MULTI-CHOICE CATEGORICAL ANALYSIS:
   • Parsed semicolon-separated responses (e.g., 'Option A; Option B')
   • Filtered out all 'Other: xxx' responses during parsing
   • Created one-hot encoding matrices (1 = selected, 0 = not selected)
   • Pooled all binary decisions across options and studies
   • Calculated single Cohen's kappa on pooled data
   • Example: 100 studies × 5 options = 500 pooled binary decisions

4. CONDITIONAL LOGIC APPLIED:
   • application_subtype_* fields: Only when application_type agreed
   • dataset_* fields: Only when dataset_source agreed
   • ux_* fields: Only when ux_assessment_is_present agreed
   • *_vs_benchmark & *_benchmark_quality: Only when *_used agreed

5. EXCLUSION CRITERIA:
   • Pairs where both reviewers selected any 'Other:' response
   • Studies not meeting conditional requirements
   • Missing or invalid data entries

6. OVERALL METRICS CALCULATION:
   • Pooled all valid field comparisons across studies
   • Calculated overall agreement rate, AC1, and Cohen's kappa
   • Applied same exclusion rules as individual fields

================================================================================

ANALYSIS SUMMARY
----------------------------------------
Single-choice categorical columns analyzed: 55
Columns with valid kappa: 53
Columns with valid AC1: 55
Columns with valid agreement rates: 55
Multi-choice categorical columns analyzed: 4
Multi-choice columns with valid kappa: 4

OVERALL AGREEMENT ACROSS ALL FIELDS:
  Reviewers compared: Reviewer Two vs Richard Gaus
  Total field comparisons: 4,082
  Total agreements: 3,497
  Overall agreement rate: 85.7%
  Overall Gwet's AC1: 0.856
  Overall Cohen's kappa: 0.754
  Fields contributing: 55/55
  Excluded 'Other:' pairs: 41

FIELD-BY-FIELD KAPPA STATISTICS:
  Mean kappa per field: 0.334
  Median kappa per field: 0.320
  Min kappa per field: -0.200
  Max kappa per field: 0.938

FIELD-BY-FIELD AC1 STATISTICS:
  Mean AC1 per field: 0.574
  Median AC1 per field: 0.790
  Min AC1 per field: -1.000
  Max AC1 per field: 1.000

FIELD-BY-FIELD AGREEMENT STATISTICS:
  Mean agreement per field: 71.6%
  Median agreement per field: 83.2%
  Min agreement per field: 0.0%
  Max agreement per field: 100.0%


MULTI-CHOICE CATEGORICAL RESULTS
----------------------------------------
Column                                   Overall  Options  Pairs  Excl  Conditional  Status
-------------------------------------------------------------------------------------------------
models_employed                          0.751    16       113    12                 success
client_type                              0.713    5        101    1                  success
intervention_type                        0.460    10       125                       success
llm_development_approach                 0.440    5        96     4                  success

MULTI-CHOICE KAPPA STATISTICS:
  Mean overall kappa: 0.591
  Median overall kappa: 0.586
  Min overall kappa: 0.440
  Max overall kappa: 0.751

DETAILED RESULTS
----------------------------------------
Column                                   Agree%   AC1      Kappa    Pairs  Excl  Conditional  Status
---------------------------------------------------------------------------------------------------------
continuous_metrics_benchmark_quality     100.0%   1.000    N/A      2            →continuous  success
lexical_diversity_benchmark_quality      100.0%   1.000    N/A      1            →lexical_di  success
perplexity_used                          99.2%    0.985    0.853    131                       success
lexical_diversity_used                   96.9%    0.954    0.320    131                       success
outlet_type                              96.9%    0.953    0.938    128                       success
e2_outcomes_by_demographics_considered   96.2%    0.943    0.154    131                       success
i1_multilevel_feasibility_considered     96.2%    0.943    0.277    131                       success
author_country                           95.5%    0.948    0.936    66     31                 success
g1_early_discontinuation_considered      95.4%    0.908    0.232    131                       success
g2_overuse_considered                    95.4%    0.908    -0.013   131                       success
ux_uses_qualitative_assessment           95.1%    0.926    0.871    81           →ux_assessm  success
embedding_similarity_used                93.9%    0.908    0.700    131                       success
llm_judge_used                           93.9%    0.878    0.302    131                       success
lexical_overlap_used                     93.1%    0.897    0.775    131                       success
continuous_metrics_used                  93.1%    0.908    0.328    131                       success
f2_control_condition_considered          92.4%    0.847    0.459    131                       success
ux_uses_quantitative_assessment          91.4%    0.870    0.777    81           →ux_assessm  success
ux_uses_standard_instrument              90.1%    0.852    0.567    81           →ux_assessm  success
ux_results_reported                      90.1%    0.802    0.793    81           →ux_assessm  success
expert_rating_used                       89.3%    0.858    0.704    131                       success
f1_validated_outcomes_considered         88.5%    0.828    0.292    131                       success
study_type                               88.2%    0.872    0.471    127    1                  success
p2_privacy_awareness_considered          87.8%    0.837    0.272    131                       success
i2_healthcare_integration_considered     86.3%    0.840    0.169    131                       success
s1_risk_detection_considered             84.7%    0.796    0.380    131                       success
classification_used                      84.0%    0.800    0.447    131                       success
application_subtype_client_facing        83.8%    0.818    0.586    74           →applicatio  success
s2_content_safety_considered             83.2%    0.776    0.382    131                       success
e1_demographics_reporting_considered     83.2%    0.790    0.258    131                       success
application_type                         81.4%    0.776    0.407    118                       success
dataset_language                         80.0%    0.700    0.412    20     2     →dataset_so  success
dataset_contains_synthetic_data          75.0%    0.625    0.298    24           →dataset_so  success
p1_on_premise_model_considered           74.0%    0.676    0.206    131                       success
expert_rating_benchmark_quality          70.6%    0.559    0.553    17           →expert_rat  success
outlet_field                             70.5%    0.656    0.615    129                       success
dataset_is_public                        66.7%    0.556    0.360    24           →dataset_so  success
perplexity_benchmark_quality             66.7%    0.333    0.000    3            →perplexity  success
ux_assessment_is_present                 65.9%    0.545    0.467    123                       success
expert_rating_vs_benchmark               57.9%    0.474    0.397    19           →expert_rat  success
lexical_overlap_benchmark_quality        56.2%    0.417    0.026    16           →lexical_ov  success
embedding_similarity_vs_benchmark        55.6%    0.333    0.077    9            →embedding_  success
embedding_similarity_benchmark_quality   55.6%    0.333    -0.091   9            →embedding_  success
dataset_source                           53.2%    0.499    0.380    124    1                  success
dataset_responder_type                   52.4%    0.444    0.219    21           →dataset_so  success
lexical_overlap_vs_benchmark             50.0%    0.400    0.086    16           →lexical_ov  success
continuous_metrics_vs_benchmark          50.0%    0.250    0.333    2            →continuous  success
classification_benchmark_quality         42.9%    -0.143   0.000    7            →classifica  success
dataset_user_psychopathology_status      37.5%    0.271    -0.071   24           →dataset_so  success
perplexity_vs_benchmark                  33.3%    0.000    -0.200   3            →perplexity  success
classification_vs_benchmark              28.6%    -0.071   -0.129   7            →classifica  success
dataset_type                             27.8%    0.206    0.114    18     5     →dataset_so  success
application_subtype_therapist_facing     25.0%    0.062    0.000    4      1     →applicatio  success
llm_judge_vs_benchmark                   0.0%     -1.000   0.000    1            →llm_judge_  success
llm_judge_benchmark_quality              0.0%     -1.000   0.000    1            →llm_judge_  success
lexical_diversity_vs_benchmark           0.0%     -1.000   0.000    1            →lexical_di  success

CONDITIONAL LOGIC APPLIED
----------------------------------------
Some columns were analyzed with conditional logic:
Only pairs where reviewers agreed on the parent column were included.

  application_type → 2 dependent columns:
    • application_subtype_client_facing
    • application_subtype_therapist_facing
  dataset_source → 6 dependent columns:
    • dataset_type
    • dataset_language
    • dataset_contains_synthetic_data
    • dataset_is_public
    • dataset_user_psychopathology_status
    • dataset_responder_type
  ux_assessment_is_present → 4 dependent columns:
    • ux_uses_standard_instrument
    • ux_uses_qualitative_assessment
    • ux_uses_quantitative_assessment
    • ux_results_reported
  lexical_overlap_used → 2 dependent columns:
    • lexical_overlap_vs_benchmark
    • lexical_overlap_benchmark_quality
  embedding_similarity_used → 2 dependent columns:
    • embedding_similarity_vs_benchmark
    • embedding_similarity_benchmark_quality
  classification_used → 2 dependent columns:
    • classification_vs_benchmark
    • classification_benchmark_quality
  continuous_metrics_used → 2 dependent columns:
    • continuous_metrics_vs_benchmark
    • continuous_metrics_benchmark_quality
  expert_rating_used → 2 dependent columns:
    • expert_rating_vs_benchmark
    • expert_rating_benchmark_quality
  llm_judge_used → 2 dependent columns:
    • llm_judge_vs_benchmark
    • llm_judge_benchmark_quality
  perplexity_used → 2 dependent columns:
    • perplexity_vs_benchmark
    • perplexity_benchmark_quality
  lexical_diversity_used → 2 dependent columns:
    • lexical_diversity_vs_benchmark
    • lexical_diversity_benchmark_quality

INTERPRETATION GUIDE
----------------------------------------
Percent Agreement Interpretation:
  < 70%: Poor agreement
  70-79%: Fair agreement
  80-89%: Good agreement
  90-95%: Very good agreement
  > 95%: Excellent agreement

Gwet's AC1 Interpretation:
  < 0.00: Poor agreement
  0.00-0.20: Slight agreement
  0.21-0.40: Fair agreement
  0.41-0.60: Moderate agreement
  0.61-0.80: Substantial agreement
  0.81-1.00: Almost perfect agreement

Cohen's Kappa Interpretation:
  < 0.00: Poor agreement
  0.00-0.20: Slight agreement
  0.21-0.40: Fair agreement
  0.41-0.60: Moderate agreement
  0.61-0.80: Substantial agreement
  0.81-1.00: Almost perfect agreement

Notes:
- Percent agreement is often more intuitive and represents raw agreement.
- Gwet's AC1 is generally preferred over Cohen's kappa as it's less affected
  by trait prevalence and marginal probability imbalances.
- Cohen's kappa adjusts for chance agreement based on marginal probabilities.
- Gwet's AC1 adjusts for chance agreement assuming uniform distribution.
- Pairs where both reviewers selected 'Other:' responses are excluded
  from agreement calculations (shown in 'Excl' column).
- Conditional columns only include pairs where reviewers agreed on parent column.