Application Subtype,Application Subtype.1,Application Type,Automatic metrics:,Classification Benchmark quality (H/L),Classification How it compares against benchmark (B/S/W),Classification Notes on benchmark quality,Classification Used (Y/N),Contentual judgment:,Continuous Value Metrics Benchmark quality (H/L),Continuous Value Metrics How it compares against benchmark (B/S/W),Continuous Value Metrics Notes on benchmark quality,Continuous Value Metrics Used (Y/N),Country of first first author affiliation,Data Set Source,Day (1-31),Did users have actual psychopathology or were they unselected/people without mental health conditions?,Does data set contain synthetic data?,E-1 Reporting of demographic information Considered in tool design? (Y/N),E-1 Reporting of demographic information if YES: Notes (paste text passage),E-2 Outcomes reported by demographic subgroup Considered in tool design? (Y/N),E-2 Outcomes reported by demographic subgroup if YES: Notes (paste text passage),Embedding Similarity Benchmark quality (H/L),Embedding Similarity How it compares against benchmark (B/S/W),Embedding Similarity Notes on benchmark quality,Embedding Similarity Used (Y/N),Expert Rating Benchmark quality (H/L),Expert Rating How it compares against benchmark (B/S/W),Expert Rating Notes on benchmark quality,Expert Rating Used (Y/N),Field of Publication Outlet,F‑1 Validated clinical outcome measures used Considered in tool design? (Y/N),F‑1 Validated clinical outcome measures used if YES: Notes (paste text passage),F‑2 Control condition present Considered in tool design? (Y/N),F‑2 Control condition present if YES: Notes (paste text passage),G‑1 Early‑discontinuation data reported Considered in tool design? (Y/N),G‑1 Early‑discontinuation data reported if YES: Notes (paste text passage),G‑2 Over‑use reported or prevented Considered in tool design? (Y/N),G‑2 Over‑use reported or prevented if YES: Notes (paste text passage),If Study Type == Empirical research involving an LLM: Development Approach,Intervention Type,Is data set public?,I‑1 Multilevel feasibility/acceptability data collected Considered in tool design? (Y/N),I‑1 Multilevel feasibility/acceptability data collected if YES: Notes (paste text passage),I‑2 Healthcare‑integration considerations addressed Considered in tool design? (Y/N),I‑2 Healthcare‑integration considerations addressed if YES: Notes (paste text passage),LLM as a judge Benchmark quality (H/L),LLM as a judge How it compares against benchmark (B/S/W),LLM as a judge Notes on benchmark quality,LLM as a judge Used (Y/N),Language of data set,Lexical Overlap Benchmark quality (H/L),Lexical Overlap How it compares against benchmark (B/S/W),Lexical Overlap Notes on benchmark quality,Lexical Overlap Used (Y/N),Lexical diversity Benchmark quality (H/L),Lexical diversity How it compares against benchmark (B/S/W),Lexical diversity Notes on benchmark quality,Lexical diversity Used (Y/N),Metric 1 Benchmark quality (H/L),Metric 1 How it compares against benchmark (B/S/W),Metric 1 Name of metric,Metric 1 Notes on benchmark quality,Metric 2 Benchmark quality (H/L),Metric 2 How it compares against benchmark (B/S/W),Metric 2 Name of metric,Metric 2 Notes on benchmark quality,Metric 3 Benchmark quality (H/L),Metric 3 How it compares against benchmark (B/S/W),Metric 3 Name of metric,Metric 3 Notes on benchmark quality,Models Employed,Month (1-12),Notes,Notes on data set,Notes on development approach,Number of Clients or Patients Included,Other User Experience Assessment Notes,Other metrics:,P-1 On-premise-capable model Considered in tool design? (Y/N),P-1 On-premise-capable model if YES: Notes (paste text passage),P-2 Privacy/confidentiality awareness Considered in tool design? (Y/N),P-2 Privacy/confidentiality awareness if YES: Notes (paste text passage),Perplexity Benchmark quality (H/L),Perplexity How it compares against benchmark (B/S/W),Perplexity Notes on benchmark quality,Perplexity Used (Y/N),Publication Outlet Type,READI P-1/2:,READI categories:,Reference-based metrics:,Reviewer Name,S-1 Risk-detection Considered in tool design? (Y/N),S-1 Risk-detection if YES: Notes (paste text passage),Study Type,S‑2 Content‑safety evaluation conducted Considered in tool design? (Y/N),S‑2 Content‑safety evaluation conducted if YES: Notes (paste text passage),Title,Type of Clients or Patients,Type of data set,User Experience Assessment,User Experience Assessment Instrument,User Experience Assessment Methods:,Uses qualitative assessment Y/N,Uses quantitative assessment Y/N,"Uses some standard, established user experience instrument Y/N",Were responders trained professionals or lay people/unknown?,Were results of user experience assessment reported? Y/N,Year (2017-2025),covidence_id,study_id
Chatbot,,Analysis of conversation transcripts,,,,,,,,,,,Other: Japan,External data set,11.0,Unknown,No,,,,,,,,,,,,,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Other: logistic regression classifier,Other: classification model,No,,,,,,,,,Japanese,,,,,,,,,,,,,,,,,,,,,Other: logistic regression mode,7.0,,". All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021",,,,,not applicable,no LLM used,Y,"This dataset was anonymized before being provided to the authors. All efforts for this study were made with the approval of Osaka Prefecture.
",,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.,No clients/patients involved,Psychotherapy -- chat logs,No users involved,,,,,,Trained professionals,,2022.0,1st_search_224,1st_search_1
,,Analysis of conversation transcripts,,no benchmark,no benchmark,"no benchmark. used metrics: F1, precision, recall, AUC. Task: classify mental health and other issues (mental health, suicide thoughts, family, physical health, etc.) in counseling sessions",y,,,,,n,,External data set,11.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,"Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier","Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,7.0,,Data from messenger application LINE (line.me) from Osaka Prefectural Government,,,,,,,,,,,,,Conference paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.,,,No users involved,,,,,,,,2022.0,1st_search_224,1st_search_2
Other: ,,Analysis of conversation transcripts,,l,w,benchmark: TF-IDF-based classification via logistic regression model,y,,,,,n,,External data set,11.0,,,n,,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,,,,,,,"Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier","Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,7.0,,". All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021",,,,,,,,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.,,,No users involved,,,,,,,,2022.0,1st_search_224,1st_search_3
,,Client-facing application,,,,,,,,,,,Other: Portugal,No dataset used for development or evaluation,9.0,,,,,,,,,,,,,,,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",Y,"We included patients aged between 18 to 65 years who
were currently undergoing psychiatric inpatient care and
had received a mental health disorder diagnosis according
to the DSM-5. Each patient
’
s assistant physician evaluated
the presence of criteria for a DSM-5 diagnosis.",Y,"They were then randomly allocated to either the con-
trol or intervention group using the 
“
coin flip
” method",,,,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods",,,,Y,"n conclusion, our pilot study suggests that AI chatbots,
such as ChatGPT
, 
can positively impact the quality of life
of psychiatric inpatients while being well-received. Despite
the limitations inherent in a pilot study
, 
such as a small
sample size and the use of convenience sampling, our find-
ings provide valuable insights into the potential role of AI
in psychiatric care",,,,,,,,,,,,,,,B,"HOQOL-BREF
score",,,, patient satisfaction Likert scale ,,,,,,GPT-3.5 family,2.0,"intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes",,,,secondary outcome was patient satisfaction on a Likert scale ,,n,,n,,,,,,Journal paper,,,,Reviewer Two,Y,"n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist",Population survey,,,Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care,Patients with disorder explicitly based on ICD or DSM,,Yes,,,,,,,,2024.0,1st_search_214,1st_search_4
Chatbot,,Client-facing application,,,,,n,,,,,n,Other: Portugal,No dataset used for development or evaluation,9.0,,,n,,n,,,,,n,,,,n,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",y,World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF),y,control group that received standard care,n,,y,only a set number of sessions was administered,Only prompting,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,n,,,,,n,,,,,h,b,WHOQOL-BREF score change,intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points),,,,,,,,,GPT-3.5 family,2.0,,,,12,"As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT",,n,,y,"Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.",,,,,Journal paper,,,,Richard Gaus,y,attending psychiatrist was checking responses,Empirical research involving an LLM,y,attending psychiatrist was checking responses,Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,,Yes,,,n,y,n,,y,2024.0,1st_search_214,1st_search_5
Chatbot,,Client-facing application,,,,,n,,,,,n,Other: Portugal,No dataset used for development or evaluation,9.0,,,n,,n,,,,,n,,,,n,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",y,World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF),y,control group that received standard care,n,,y,only a set number of sessions was administered,Only prompting,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,n,,,,,n,,,,,h,B,WHOQOL-BREF score change,intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points),,,,,,,,,GPT-3.5 family,2.0,"intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes",,,12,"As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT",,n,,y,"Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.",,,,,Journal paper,,,,Consensus,y,"n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist

attending psychiatrist was checking responses",Empirical research involving an LLM,n,,Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,,Yes,,,n,y,n,,y,2024.0,1st_search_214,1st_search_6
Chatbot,,Client-facing application,,,,,,,,,,,USA,Self-collected data,26.0,Unknown,No,,,,,,,,,,,,,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Prompting + other modules,Other CBT techniques,,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,VR/AI therapy sessions with GPT-4 avatar in biophilic scenes; sessions recorded and transcribed; qualitative thematic analysis after single ~30-min session per participant. • Derived: Not indicated.,,,,,N,,Y,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Feasibility of combining spatial computing and AI for mental health support in anxiety and depression,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Psychotherapy -- speech transcripts,Yes,,,Y,,,Trained professionals,Y,2024.0,1st_search_212,1st_search_7
Other: speech-based conversation system,,Client-facing application,,,,,n,,,,,n,USA,Self-collected data,26.0,Unknown,No,y,Table-1,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Prompting + other modules,"CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Informal counseling (e.g., emotional support conversation)",No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,"Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”",,14,"Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).",,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,y,"The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.",Feasibility of combining spatial computing and AI for mental health support in anxiety and depression,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Psychotherapy -- speech transcripts,Yes,,,y,n,n,Trained professionals,y,2024.0,1st_search_212,1st_search_8
Other: Spoken dialogue system,,Client-facing application,,,,,n,,,,,n,USA,Self-collected data,26.0,Unknown,No,y,Table-1,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Prompting + other modules,"CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Informal counseling (e.g., emotional support conversation)",No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,"Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”",,14,"Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).",,N,,n,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,y,"The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.",Feasibility of combining spatial computing and AI for mental health support in anxiety and depression,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Psychotherapy -- speech transcripts,Yes,,,y,n,n,Trained professionals,y,2024.0,1st_search_212,1st_search_9
Chatbot,,Client-facing application,,,,,,,L,Perplexity evaluations (B),"ChatGPTbased conversations 
(Approach 1), fine-tuned DialoGPT transformer 
conversations (Approach 2), and fine-tuned DialoGPT 
transformer conversations combined with the GPT3 prompts 
API (Approach 3)",Y,UK,Self-collected data but derived from external data set,4.0,Psychopathology,No,N,,N,,,,,,,no benchmark,s. user experience,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",Other: did not find info. ,Y,"Human evaluations relied on two groups: 
mental healthcare professionals and researchers who believe they are suffering mental 
health issue. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness and satisfaction of the 
chatbot, its conversation quality, and potential areas of 
improvement. ",Y,"We want to emphasize, though, that our chatbot, while 
a step forward, is not a replacement for human therapists. 
Instead, we envision it as an auxiliary resource that can 
provide support in scenarios where human resources are 
stretched thin, or as an additional tool to complement 
traditional therapeutic processes.

 Integration with clinical systems: future research 
could look into integrating the chatbot with chatbot to provide more personalized and context-
aware support. It could also facilitate better 
coordination with healthcare professionals, alerting 
them when the chatbot identifies potential serious 
concerns.
existing clinical systems. This would allow the ",,,,,English,L,BLEU score (B),"ChatGPTbased conversations 
(Approach 1), fine-tuned DialoGPT transformer 
conversations (Approach 2), and fine-tuned DialoGPT 
transformer conversations combined with the GPT3 prompts 
API (Approach 3)",Y,,,,,,,,,,,,,,,,,GPT-3 family; GPT-3.5 family,6.0,,"created an own based on therapy transcript documents from different websites

As we could not find a suitable data set, we created one … We used real-world therapy transcript documents from websites and converted the HTML conversation texts…
",,10,"1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5)",,N,,no,"only:  such as personal details, to protect privacy and reduce computation complexities
",,,,,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system,"People with some symptoms but not disorder (determined by symptom scales or questionnaires); Other: Human evaluations relied on two groups: volunteers who believe they are suffering mental 
health issue, mental healthcare professionals and researchers.",Psychotherapy -- speech transcripts,Yes,,,Y,Y,N,Trained professionals,Y,2024.0,1st_search_204,1st_search_10
Chatbot,,Client-facing application,,,,,n,,,,,n,UK,External data set,4.0,Other: not applicable,Other: synthetic; human-generated (fictional),n,,n,,,,,n,no benchmark,no benchmark,"no benchmark. however, experts rated the chatbot highly in absolute terms.",y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,"Other: Fine-tuning of DialogGPT. This fine-tuned model is integrated into a custom chatbot pipeline, together with ChatGPT 3.5","Unspecified, might include formal therapy methods",No,y,rated by both patients and professionals,y,"e.g. "" The suggestion to integrate structured therapy 
techniques and conduct research on the efficacy 
of chatbots in mental health support indicates the 
potential for collaboration between the chatbot 
application and professionals in the field. Future 
policies could encourage partnerships between 
developers and professionals in psychology and 
counseling to enhance the chatbots effectiveness 
and provide a well-rounded approach to mental 
healthcare (23).""",,,,n,English,l,b,benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts),y,,,,,l,b,perplexity,benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts),,,,,,,,,GPT-2 family; GPT-3.5 family,6.0,,"We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: ""This site is fiction and all the characters are fictional.""",,10,"To gather crucial user insights and evaluate the chatbots 
performance from a user’s perspective, we conducted 
surveys among a select group of mental health users and 
carers. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness of the 
chatbot, its conversation quality, and potential areas of 
improvement. Below, we detail the survey’s findings:
- Willingness for continued chatbot usage: when 
questioned about their willingness to engage with 
the chatbot again after the initial interaction, an 
overwhelming majority (90%) expressed a positive 
intent to reuse the service
- Rating of conversation quality (human-likeness): 
participants rated the chatbot’s conversational 
quality in terms of its human-like language on a 
scale from 1 to 5. The chatbot received an average 
score of 4.3, indicating a high degree of satisfaction 
with the chatbot’s language quality.
- Rating of conversation quality (supportiveness): 
when assessing the chatbot’s supportive nature in 
the conversation, participants gave an average score 
of 4.2 of 5, reflecting their positive experience in 
terms of perceived support.
- Overall rating of the chatbot application: when 
asked to provide an overall rating of the chatbot 
application, participants gave an average score 
of 4.6 out of 5. Notably, no participant rated the 
application lower than 4, indicating a high level of 
user satisfaction.
-  Positive feedback: participants were invited to 
share any positive feedback about their experience. 
We got 8 responses for this question and the most 
important points are the LLM-based chatbot can 
always provide useful suggestions and they feel very 
safe to talk to someone who are always available 
and talkable about their issue and sadness.
- Areas for improvement: we also encouraged 
users to suggest areas where the chatbot could 
be improved. The survey participants found the 
chatbot to be generally helpful, but suggested 
improvements such as exposing the training data 
to more diverse circumstances, enhancing the 
emotional support aspect, avoiding risky responses 
to sensitive inquiries, reducing repetition of 
examples, and focusing on more teaching sessions 
to make the interactions feel less robotic and more 
like conversing with a human friend.
- User interface (UI) suggestions: participants were 
also asked to provide suggestions for improving 
the chatbot’s UI functionalities to enhance its 
usefulness and usability. The feedback is very useful 
for us to implement further improved version. The 
suggestions include voice and image combined 
responses, able to track chat history, virtual reality 
(VR) or mixed reality innovation and realistic 
human tongues enhancement.",,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Psychotherapy -- chat logs,Yes,,,y,y,n,Other: not applicable,y,2024.0,1st_search_204,1st_search_11
Chatbot,,Client-facing application,,,,,n,,,,,n,UK,External data set,4.0,Other: not applicable,Other: synthetic; human-generated (fictional),N,,N,,,,,n,no benchmark,no benchmark,"no benchmark. however, experts rated the chatbot highly in absolute terms.",Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,"Other: Fine-tuning of DialogGPT. This fine-tuned model is integrated into a custom chatbot pipeline, together with ChatGPT 3.5","Unspecified, might include formal therapy methods",No,Y,"Human evaluations relied on two groups: 
mental healthcare professionals and researchers who believe they are suffering mental 
health issue. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness and satisfaction of the 
chatbot, its conversation quality, and potential areas of 
improvement. ",Y,"We want to emphasize, though, that our chatbot, while 
a step forward, is not a replacement for human therapists. 
Instead, we envision it as an auxiliary resource that can 
provide support in scenarios where human resources are 
stretched thin, or as an additional tool to complement 
traditional therapeutic processes.

 Integration with clinical systems: future research 
could look into integrating the chatbot with chatbot to provide more personalized and context-
aware support. It could also facilitate better 
coordination with healthcare professionals, alerting 
them when the chatbot identifies potential serious 
concerns.
existing clinical systems. This would allow the ",,,,n,English,L,b,"benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)

ChatGPTbased conversations
(Approach 1), fine-tuned DialoGPT transformer
conversations (Approach 2), and fine-tuned DialoGPT
transformer conversations combined with the GPT3 prompts
API (Approach 3)",Y,,,,,l,b,perplexity,benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts),,,,,,,,,GPT-2 family; GPT-3.5 family,6.0,,"We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: ""This site is fiction and all the characters are fictional.""",,10,"1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5)",,N,,n,"only:  such as personal details, to protect privacy and reduce computation complexities
",,,,,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Other: not applicable,Yes,,,Y,Y,N,Other: not applicable,Y,2024.0,1st_search_204,1st_search_12
,,,,,,,,,,,,,UK,External data set,26.0,,,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,,,,,,,,,,H,W,See Table 3,,H,W,Linguistic Features as mentioned under 3.3,,,,,,Mistral family,6.0,,"CounselChat2,
provided through the HuggingFace Hub platform3. ",,,,,Y,mistral,N,,,,,,Conference paper,,,,Reviewer Two,,,,,,Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,,,No,,,,,,,,2024.0,1st_search_203,1st_search_13
Chatbot,,Client-facing application,,,,,n,,,,,n,UK,External data set,26.0,Unselected,No,n,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Only prompting,Mix of formal therapy methods,Yes,n,,n,,,,,n,English,,,,n,,,,,h,w,lexical diversity and richness,human psychologist responses in the CounselChat transcripts,unclear,unclear,readability scores,it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.,,,various other traditional NLP metrics,,Mistral family,6.0,,CounselChat https://github.com/nbertagnolli/counsel-chat,,,,,y,mistral 7b,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2024.0,1st_search_203,1st_search_14
Chatbot,,Client-facing application,,,,,n,,,,,n,UK,External data set,26.0,Unselected,No,n,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,,H,W,lexical diversity and richness,human psychologist responses in the CounselChat transcripts,H,unclear,Linguistic Features as mentioned under 3.3,it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.,,,,,Mistral family,6.0,,CounselChat https://github.com/nbertagnolli/counsel-chat,,,,,y,mistral 7b,n,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2024.0,1st_search_203,1st_search_15
,,Client-facing application,,,,,,,,,,,Other: Australia,Other: Mix of external and self collected via data crawling,22.0,Unselected,No,n,,n,,,,,,h,w,"Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model",y,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",No,n,,n,,,,,,Chinese,l,b,ROUGE-L - PanGu better than WenZhong,y,,,,,l,b,Perplexity,PanGu better than WenZhong,l,b,Distinct 1,PanGu better than WenZhong,l,b,"Distinct 2
",PanGu better than WenZhong,GPT-2 family; GPT-3 family; Other: PanGu (similar to GPT-3)and WenZhong (GPT-2),12.0,,"PsyQA and Crawl of various chinese social media platforms Tianya, Zhihu, Yixinli

PsyQA (external); web-crawled psychology corpus (unnamed). Description: Crawled Chinese psychology-forum data (Yixinli, Tianya) to pretrain PanGu-350M; then fine-tuned on 56,000 PsyQA Q&A pairs.
",,,"Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.",,N,,N,,,,,,Journal paper,,,,Reviewer Two,n,,Empirical research involving an LLM,n,,Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs),General population,Internet data -- mental health Q&A,Yes,,,n,n,n,Unknown,n,2023.0,1st_search_200,1st_search_16
Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.,,Client-facing application,,,,,n,,,,,n,Other: Australia,External data set,22.0,Unknown,No,n,,n,,,,,n,h,w,"psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set",y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Other: both original training of the transformer model and subsequent fine-tuning,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Chinese,no benchmark,no benchmark,ROUGE-L. no benchmark,y,,,,,no benchmark,no benchmark,perplexity,no benchmark,no benchmark,no benchmark,"distinct-1, -2",no benchmark,,,,,GPT-2 family; Other: PanGu,12.0,,"training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.",,,,,y,,y,"Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls",,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs),No clients/patients involved,Other: mixed,No users involved,,,,,,Unknown,,2023.0,1st_search_200,1st_search_17
Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.,,Client-facing application,,,,,n,,,,,n,Other: Australia,External data set,22.0,Unselected,No,n,,n,,,,,n,h,w,"psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set

Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model",y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Other: both original training of the transformer model and subsequent fine-tuning,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Chinese,no benchmark,no benchmark,ROUGE-L - PanGu better than WenZhong,y,,,,,no benchmark,no benchmark,Perplexity,no benchmark,no benchmark,no benchmark,Distinct 1,no benchmark,no benchmark,no benchmark,"Distinct 2
",no benchmark,"GPT-2 family; Other: PanGu, WenZhong (based on GPT-2)",12.0,,"training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.",,,"Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.",,y,,y,"Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls",,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs),No clients/patients involved,Other: mixed,No users involved,,,,,,Unknown,,2023.0,1st_search_200,1st_search_18
,,Client-facing application,,Low,Better,Benchmark are different models,Yes ,,,,,,India,External data set,11.0,,Other: unclear,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation)",Yes,,,,"Authors mention that model is designed as a ""solution that could be used on common 
hardware by users without having the knowledge and 
technical proficiency regarding large language models""",,,,,English,,,,,,,,,,,,,,,,,,,,,Llama 2 family,3.0,,"publically available online data resources;
Kaggle mental-health conversation/Q&A sets; SAMHSA SOAR sample Medical Summary Reports
",,,,,Y,Llama,Y,"User privacy and data security are paramount. The system operates under strict ethical guidelines and secure data storage protocols.”
",,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,"Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)",No clients/patients involved,Other: unclear,No users involved,,,,,,,,2024.0,1st_search_198,1st_search_19
Chatbot,,Client-facing application,,,,,,,,,,,India,External data set,14.0,Other: unknown,Other: unknown,n,,n,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,Other: unclear,No,n,,n,,,,,,Other: unknown,,,,,,,,,no benchmark,no benchmark,"Human rating, not sure by whom",different models were rated by human non-experts. no comparison with any benchmark.,,,,,,,,,Llama 2 family,3.0,,Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website,,,,,y,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,"Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)",No clients/patients involved,Other: mixed,No users involved,,,,,,Other: unknown,,2024.0,1st_search_198,1st_search_20
Chatbot,,Client-facing application,,,,,,,,,,,India,External data set,14.0,Other: unknown,Other: unknown,n,,n,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,,Other: unknown,,,,,,,,,no benchmark,no benchmark,"Human rating, not sure by whom",different models were rated by human non-experts. no comparison with any benchmark.,,,,,,,,,Llama 2 family,3.0,,Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website,,,,,Y,Llama,n,"User privacy and data security are paramount. The system operates under strict ethical guidelines and secure data storage protocols.”
",,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,"Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)",No clients/patients involved,Other: mixed,No users involved,,,,,,Other: unknown,,2024.0,1st_search_198,1st_search_21
,,Client-facing application,,,,,,,,,,,Other: Kyrgyz Republic,Self-collected data,2.0,,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,Other CBT techniques,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,GPT-3 family,11.0,,,,,,,N,,N,,,,,,,,,,Reviewer Two,,,Empirical research involving an LLM,,,Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference,Other: unclear,,No,,,,,,,,2023.0,1st_search_194,1st_search_22
Chatbot,,Client-facing application,,no benchmark,no benchmark,no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.,y,,,,,n,Other: Kyrgyzstan,Self-collected data,2.0,Unselected,"Other: synthetic, human created",n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",y,"unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures",y,"After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. ",y,fig 6 participants flow,y,number of messages sent is reported,Only fine-tuning,CBT: Cognitive restructuring; Other CBT techniques,No,n,,n,,,,,n,Other: unknown,,,,n,,,,,h,s,AAQ + CDS,aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.,,,,,,,,,GPT-3 family,11.0,,"The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.",,68,,,n,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference,"Other: 68 university students recruited using convenience sampling (social media campaign, institutional newsletter)",Other: descriptions of cognitive distortions,No,,,,,,Other: not applicable,,2023.0,1st_search_194,1st_search_23
Chatbot,,Client-facing application,,no benchmark,no benchmark,no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.,y,,,,,n,Other: Kyrgyzstan,Self-collected data,2.0,Other: not applicable,"Other: synthetic, human created",n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",y,"unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures",y,"After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. ",y,fig 6 participants flow,y,number of messages sent is reported,Only fine-tuning,CBT: Cognitive restructuring; Other CBT techniques,No,n,,n,,,,,n,Other: unknown,,,,n,,,,,h,s,AAQ + CDS,aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.,,,,,,,,,GPT-3 family,11.0,,"The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.",,68,,,N,,N,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference,General population,Other: descriptions of cognitive distortions,No,,,,,,Other: not applicable,,2023.0,1st_search_194,1st_search_24
,Treatment fidelity feedback,Analysis of conversation transcripts,,L,B,,Y,,,,,,USA,Self-collected data,17.0,Unselected,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,,"Unspecified, might include formal therapy methods",No,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,BERT family,3.0,,"Counseling/chat conversations annotated for utterance‐level features; session-level features and summaries also produced (some via prompts). Annotation: Human annotators used a codebook of counseling strategy features
",,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,No clients/patients involved,Emotional support dialogue -- chat logs,No,,,,,,Trained professionals,,2024.0,1st_search_147,1st_search_25
,,Analysis of conversation transcripts,,no benchmark,no benchmark,"1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)",y,,,,,n,USA,External data set,17.0,Unknown,No,n,"Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.",n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,BERT family; GPT-3 family; GPT-3.5 family,3.0,,from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.,,,,,n,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,,Emotional support dialogue -- chat logs,No users involved,,,,,,Trained professionals,,2024.0,1st_search_147,1st_search_26
,Other: ,Analysis of conversation transcripts,,L,W,"Table 4: Benchmark is fine-tuned DistilBERT model. ChatGPT is compared against this and performs worse.
Table 2: Prompted GPT-3 model (text-davinci-003) is compared against DistilBERT.

1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)",y,,,,,n,USA,External data set,17.0,Unknown,No,n,"Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.",n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,BERT family; GPT-3.5 family,3.0,,"from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.

Counseling/chat conversations annotated for utterance‐level features; session-level features and summaries also produced (some via prompts). Annotation: Human annotators used a codebook of counseling strategy features",,,,,N,,N,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,,Emotional support dialogue -- chat logs,No users involved,,,,,,Trained professionals,,2024.0,1st_search_147,1st_search_27
,,,,,,,N,,,,,N,Other: Australia,Self-collected data but derived from external data set,20.0,Unselected,Yes,,,,,"L
",B,similar chinese language LLMs,Y,L,B,Relevance; CBT Structure; Helpfulness,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,Other CBT techniques,No,,,,,,,,N,Chinese,"L

",B,similar chinese language LLMs,Y,,,,,,,,,,,,,,,,,GPT-3.5 family,5.0,,"PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
CBT QA dataset. Derived from: PsyQA → CBT QA (ChatGPT-generated via CBT prompt). Short description: Chinese mental-health Q&A where responses are generated with a structured CBT prompt and used to fine-tune CBT-LLM.

",,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,,Internet data -- mental health Q&A,,,,,,,"Other: LLM responses, not human responders
",,2024.0,1st_search_134,1st_search_28
Chatbot,,Client-facing application,,no benchmark,no benchmark,"accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection.",y,,,,,n,Other: Australia,Self-collected data but derived from external data set,20.0,Unselected,Yes,n,,n,,l,b,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",y,l,b,Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,Other CBT techniques,No,n,,n,,,,,n,Chinese,l,b,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",y,,,,,,,,,,,,,,,,,Other: Baichuan-7B,5.0,,"External: PsyQA
Derived: CBT QA",,,,,y,,n,they say they release the data but where is it?,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: not applicable,,2024.0,1st_search_134,1st_search_29
Chatbot,,Client-facing application,,no benchmark,no benchmark,"accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection. Task: Cognitive distortion detection in client questions.",y,,,,,N,Other: Australia,Self-collected data but derived from external data set,20.0,Unselected,Yes,n,,n,,L,B,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",Y,L,B,"Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.

Measures: Relevance, CBT structure, helpfulness",Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,Other CBT techniques,No,n,,n,,,,,N,Chinese,L,B,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",Y,,,,,,,,,,,,,,,,,"GPT-3.5 family; Qwen family; Other: Baichuan-7B, Llama 1, Alpaca 1",5.0,,"PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
CBT QA dataset. Derived from: PsyQA → CBT QA (ChatGPT-generated via CBT prompt). Short description: Chinese mental-health Q&A where responses are generated with a structured CBT prompt and used to fine-tune CBT-LLM.

",,,,,y,,N,they say they release the data but where is it?,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: not applicable,,2024.0,1st_search_134,1st_search_30
,,,,,,,,,,,,,USA,Self-collected data,28.0,,,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,,Journal paper,,,,Reviewer Two,,,Conceptual or theoretical work (e.g. on ethics or safety),,,Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support,,,,,,,,,,,2023.0,1st_search_128,1st_search_31
Chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,9.0,,,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,n,,,,,n,,,,,,,,,,,,,,,,,GPT-3 family,11.0,,,,462,"Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience",,,,,,,,,,Conference paper,,,,Richard Gaus,,,Population survey,,,Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support,Other: reddit users (r/Replika),,Yes,,,y,n,n,,y,2024.0,1st_search_128,1st_search_32
Chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,9.0,,,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,n,,,,,n,,,,,,,,,,,,,,,,,GPT-3 family,11.0,,,,462,"Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience",,,,,,,,,,Conference paper,,,,Consensus,,,Population survey,,,Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support,General population,,Yes,,,y,n,n,,y,2024.0,1st_search_128,1st_search_33
Chatbot,,Analysis of conversation transcripts,,,,,N,,,,,N,China,External data set,8.0,Unselected,No,N,,N,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,N,,N,,N,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation)",Other: unclear,N,,N,,,,,N,Chinese,,,,N,,,,,,,N,,,,N,,,,N,,GPT-2 family,11.0,proof-of-concept prototype rather than a full-fledged clinical tool,"PsyQA
he data set contains … about 22,000 pieces of psychological counseling data… Topics included… growth, emotion, love… About 8% of the answers come from national second-class psychological counselors; 35% … from volunteers…” / “The data source for the knowledge graph is PsyQA …
",,,,,N,,N,,,,,,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,"Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings",Other: The questions in PsyQA data set cover a variety of user groups.,Internet data -- mental health Q&A,No,,,,,,Other: mixed,,2024.0,1st_search_124,1st_search_34
Other: One-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,External data set,28.0,Unselected,No,n,,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,"Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.","Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,Chinese,,,,n,,,,,,,,,,,,,,,,,BERT family; GPT-2 family,10.0,"No evaluation of the conversation system. Study only displayed some conversation sample ""look this is good"".",PsyQA,,,,,y,,n,,,,,,Conference paper,,,,Richard Gaus,n,"even though this is a crisis call intervention system, there is not detection of acute risk",Empirical research involving an LLM,n,GPT-2 outputs are not checked,"Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings",No clients/patients involved,Internet data -- mental health Q&A,No,,,,,,Trained professionals,,2022.0,1st_search_124,1st_search_35
"Other: ""one turn chatbot"", i.e., user inputs ""feelings and confusion"" and system makes analysis and outputs intervention text once. There is no turn-based interaction with the tool.",,Client-facing application,,,,,n,,,,,n,China,External data set,28.0,Unselected,No,n,,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,"Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.","Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,Chinese,,,,n,,,,,,,,,,,,,,,,,BERT family; GPT-2 family,10.0,"proof-of-concept prototype rather than a full-fledged clinical tool

No evaluation of the conversation system. Study only displayed some conversation sample ""look this is good"".",PsyQA,,,,,y,,n,,,,,,Conference paper,,,,Consensus,n,"even though this is a crisis call intervention system, there is not detection of acute risk",Empirical research involving an LLM,n,GPT-2 outputs are not checked,"Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings",No clients/patients involved,Internet data -- mental health Q&A,No,,,,,,Trained professionals,,2022.0,1st_search_124,1st_search_36
,,Client-facing application,,,,,N,,,,,N,China,Self-collected data but derived from external data set,5.0,Unselected,Yes,,,,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Other: ,"Informal counseling (e.g., emotional support conversation)",No,,,,,,,,N,Other: did not find only guessed,,,,N,,,,,L,B,"Emotion
task", recognizing the utterance-level emotion of the help-seeke,L,B,Strategy task,predicting support strategies,L,B,Response task,"generating supportive
response",Other: none,11.0,,"MESConv dataset generated from YT videos

Large-scale multi-modal emotional-support dialogues with utterance-level emotion annotations and strategy labels; used for Emotion/Strategy/Response tasks.
",,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,"Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I",No clients/patients involved,Emotional support dialogue -- speech transcripts,No,,,,,N,Unknown,,2023.0,1st_search_120,1st_search_37
"Other: conversation system with video, audio, and text input",,Client-facing application,,l,b,benchmark: other conversation systems,y,,,,,n,China,External data set,5.0,Unselected,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,"Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response","Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: unknown,l,b,benchmark: other conversation systems,y,,,,,l,b,perplexity,benchmark: other conversation systems,,,,,,,,,Other: BlenderBot,11.0,,"MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube)",,,,,y,BlenderBot,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,"Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I",No clients/patients involved,Other: Emotional support dialogue -- multimodal data,No users involved,,,,,,Unknown,,2023.0,1st_search_120,1st_search_38
"Other: conversation system with video, audio, and text input",,Client-facing application,,l,b,benchmark: other conversation systems. Task: emotion classification in current utterance of help-seeker,y,,,,,N,China,Self-collected data but derived from external data set,5.0,Unselected,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,"Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response","Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: unknown,l,b,benchmark: other conversation systems,y,,,,,l,b,perplexity,benchmark: other conversation systems,,,,,,,,,Other: BlenderBot,11.0,,"MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube)

Large-scale multi-modal emotional-support dialogues with utterance-level emotion annotations and strategy labels; used for Emotion/Strategy/Response tasks.",,,,,y,BlenderBot,N,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,"Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I",No clients/patients involved,Other: Emotional support dialogue -- multimodal data,No users involved,,,,,,Unknown,,2023.0,1st_search_120,1st_search_39
,,,,,,,N,,,,,N,India,External data set,30.0,Unknown,Yes,n,,n,,L,B,"DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",Y,,no benchmark,,Y,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,"Other: not sure, ""READER is built on transformer to jointly predict a potential dialogue-act for the
next utterance and to generate an appropriate
response""","Informal counseling (e.g., emotional support conversation)",Other: unclear,n,,n,,,,,,English,L,B,"DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",Y,,,,,,,,,,,,,,,,,GPT-2 family,4.0,,"HOPE;
Switchboard Dialogue-act Corpus; (also mentions a “Dialog-act corpus”). Description: Counseling conversations with dialogue-act labels (HOPE) and a general telephone dialogue-act corpus (Switchboard) used for evaluation/generalizability. Derived: Not indicated.
",,,,,N,,Y,,,,,,Conference paper,,,,Reviewer Two,n,,Empirical research involving an LLM,n,,Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023,,Psychotherapy -- chat logs,No users involved,,,,,,Other: not applicable,,2023.0,1st_search_119,1st_search_40
Chatbot,,Client-facing application,,,,,n,,,,,n,India,External data set,30.0,Unknown,No,n,,n,,l,b,"metrics: BERTScore. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...)",y,l,b,"metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,"Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization","Unspecified, might include formal therapy methods",No,n,,n,,,,,n,English,l,b,"metrics: ROUGE, METEOR. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...)",y,,,,,,,,,,,,,,,,,GPT-2 family,4.0,,"HOPE
212 multi-turn English psychotherapy sessions (≈ 12.9 K utterances) between therapist and patient, transcribed from YouTube videos",,,,,y,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Unknown,,2023.0,1st_search_119,1st_search_41
Chatbot,,Client-facing application,,,,,N,,,,,N,India,External data set,30.0,Unknown,No,n,,n,,L,B,"metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",Y,l,b,"metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2",Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,"Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization","Unspecified, might include formal therapy methods",No,n,,n,,,,,n,English,L,B,"metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",Y,,,,,,,,,,,,,,,,,GPT-2 family,4.0,,"HOPE
212 multi-turn English psychotherapy sessions (≈ 12.9 K utterances) between therapist and patient, transcribed from YouTube videos

HOPE;
Switchboard Dialogue-act Corpus; (also mentions a “Dialog-act corpus”). Description: Counseling conversations with dialogue-act labels (HOPE) and a general telephone dialogue-act corpus (Switchboard) used for evaluation/generalizability. Derived: Not indicated.",,,,,y,,n,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Unknown,,2023.0,1st_search_119,1st_search_42
,,Analysis of conversation transcripts,,L,B,ChatGPT (GPT 3.5),Y,,,,,N,China,Self-collected data,12.0,Unselected,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",No,N,,N,,,,,N,Chinese,,,,N,,,,,,,,,,,,,,,,,BERT family,10.0,,"Multi-turn real counseling chat sessions collected on a Chinese online text-based free counseling platform, plus public QA; filtered to 7,935 sessions; labeled with an 8-category safety taxonomy; stratified split 90/10.
",,,,,N,Chat GPT 3.5 used + BERT ,N,Too little,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,Y,"During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.","A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II",,Emotional support dialogue -- chat logs,No users involved,,,,,,Unknown,,2023.0,1st_search_116,1st_search_43
,,Analysis of conversation transcripts,,no benchmark,no benchmark,"accuracy, precision, recall, F1 for classifying utterances",y,,,,,n,China,Self-collected data,12.0,Unknown,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,Chinese,,,,n,,,,,,,,,,,,,,,,,BERT family; GPT-3.5 family,10.0,,"We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

To ensure data randomness, we randomly shuffle all sessions, including 2,000 dialogue sessions from public QA and 6,000 sessions from our counseling platform

https://github.com/qiuhuachuan/DialogueSafety",,,,,n,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Other: Development of new taxonomy for utterances of LLM psychotherapists. Fine-tuning of BERT. Direct evaluation of ChatGPT 3.5.,y,study is about this exactly,"A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II",,Emotional support dialogue -- chat logs,No users involved,,,,,,Unknown,,2023.0,1st_search_116,1st_search_44
,,Analysis of conversation transcripts,,L,W,"accuracy, precision, recall, F1 for classifying utterances. Task: classification of mental health dialogue turns into safe/various types of unsafe responses. benchmark: fine-tuned BERT-base and RoBERTa-large",Y,,,,,N,China,Self-collected data,12.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Chinese,,,,N,,,,,,,,,,,,,,,,,BERT family; GPT-3.5 family,10.0,,"We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

Multi-turn real counseling chat sessions collected on a Chinese online text-based free counseling platform",,,,,N,Chat GPT 3.5 used + BERT ,N,Too little,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,Y,"During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.","A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II",,Emotional support dialogue -- chat logs,No users involved,,,,,,Unknown,,2023.0,1st_search_116,1st_search_45
,,,,,no benchmark,,Y,,,,,N,Other: Poland,Other: Translation of English datasets to Polish ,3.0,Other: not applicable,No,n,,n,,,,,N,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,Other CBT techniques,Yes,n,,,,,,,,Other: Polish,,,,N,,,,,,,,,,,,,,,,,BERT family,7.0,,"DailyDialog and EmpatheticDialogues as basis; translated and merged: CORTEX; (enriched by Polish Common Crawl);

A Polish emotion-labeled text corpus was semi-supervisedly expanded with unlabeled web data to enlarge the training set
",,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,n,,Empirical research involving an LLM,n,,"Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I",,Other: emotion-labeled text corpus,No users involved,,,,,,Other: not applicable,,2023.0,1st_search_115,1st_search_46
,,Analysis of conversation transcripts,,no benchmark,no benchmark,"sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification",y,,,,,n,,Self-collected data but derived from external data set,3.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,7.0,,"Name of new dataset: CORTEX. Derived from DailyDialog, EmpatheticDialogues",,,,,,,,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,"Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I",,,No users involved,,,,,,,,2023.0,1st_search_115,1st_search_47
,,Analysis of conversation transcripts,,no benchmark,no benchmark,"sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification",Y,,,,,N,,Self-collected data but derived from external data set,3.0,,,n,,n,,,,,N,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,,,,,,,Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,n,,,,,N,,,,,,,,,,,,,,,,,BERT family,7.0,,DailyDialog and EmpatheticDialogues as basis; translated and merged: CORTEX; (enriched by Polish Common Crawl) ,,,,,,,,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,"Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I",,,No users involved,,,,,,,,2023.0,1st_search_115,1st_search_48
,,Client-facing application,,,,,N,,,,,N,Germany,Self-collected data,19.0,,,N,,N,,,,,N,no benchmark,no benchmark,no benchmark,Y,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",N,,N,,N,,N,,,"Unspecified, might include formal therapy methods",,N,,Y,"Based on the analysis of the results, we will consider that implementing a custom
ChatGPT in a robot to support ADHD therapies presents considerable potential. Its advan-
tages include personalization, where ChatGPT can tailor interactions to each patient’s unique
needs and responses, thus potentially enhancing the therapeutic experience. Consistency is
another benefit, as a ChatGPT-equipped robot can offer stable support, which is crucial in
ADHD therapies where routine and predictability play vital roles. Additionally, ChatGPT’s
capability to understand and generate natural language can significantly increase the en-
gagement and interactivity of therapy sessions for children with ADHD, thus making them
more dynamic and effective. However, we also found a range of complex challenges and
considerations. Key among these is the need for emotional intelligence.
Significantly, ChatGPT and its use by a robotic assistant should complement, not
replace, human therapists (as also mentioned by the studies [14,21]), as the human element
is critical, especially for children with ADHD. ",,,,N,,,,,N,,,,,,,,,,,,,,,,,"Other: ""custom GPT"", version not further specified",3.0,"""Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality.""",,,,,,N,,N,,,,,,Journal paper,,,,Reviewer Two,N,,Other: experimental research,N,,Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement,No clients/patients involved,,No users involved,,,,,,,,2024.0,1st_search_101,1st_search_49
Chatbot,,Client-facing application,,,,,n,,,,,n,Germany,Self-collected data but derived from external data set,19.0,,,n,,n,,,,,n,no benchmark,no benchmark,"no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.)",y,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",3.0,,,,,,,n,,y," The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.",,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement,No clients/patients involved,,No users involved,,,,,,,,2024.0,1st_search_101,1st_search_50
Chatbot,,Client-facing application,,,,,N,,,,,N,Germany,Self-collected data,19.0,,,N,,N,,,,,N,no benchmark,no benchmark,"no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.)",Y,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,n,,,,,N,,,,,N,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",3.0,"""Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality.""",Some PDFs for configuring a custom GPT (type of PDFs not further specified),,,,,n,,N,"Not sufficient: The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.",,,,,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement,No clients/patients involved,,No users involved,,,,,,,,2024.0,1st_search_101,1st_search_51
,,Client-facing application,,,,,N,,,,,N,Other: Canada,External data set,26.0,Unknown,No,N,,N,,,,,N,no benchmark,no benchmark,no benchmark,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,CBT: Motivational interviewing,Other: unclear,N,,N,,,,,N,English,,,,N,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,9.0,,MIBot v5.1 dataset,,,,,N,,N,,,,,,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Unknown,,2024.0,1st_search_98,1st_search_52
Chatbot,,Client-facing application,,,,,n,,,,,n,Other: Canada,External data set,26.0,Unselected,Yes,n,,n,,,,,n,no benchmark,no benchmark,rating scale to determine quality of backward looking reflections (Textbox 5). ,y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,CBT: Motivational interviewing,No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,9.0,,"Dataset was used for evaluation only.

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539])",,,,,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Other: chatbot response,,2024.0,1st_search_98,1st_search_53
Chatbot,,Client-facing application,,,,,N,,,,,N,Other: Canada,External data set,26.0,Psychopathology,Yes,N,,N,,,,,N,,,,,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,CBT: Motivational interviewing,No,N,,N,,,,,,English,,,,N,,,,,no benchmark,no benchmark,human rating (unclear if expert or not),rating scale to determine quality of backward looking reflections (Textbox 5).,,,,,,,,,GPT-4 / GPT-4o family,9.0,,"Dataset was used for evaluation only.

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539])",,,,,N,,N,,,,,,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Other: not applicable,,2024.0,1st_search_98,1st_search_54
,,Analysis of conversation transcripts,,,,,,,,,,,India,External data set,23.0,Psychopathology,No,,,,,L,,other models,Y,L,,No relative comparisons where conducted; the experts rated in absolute terms,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,,,,,English,L,,other models,Y,,,,,,,,,,,,,,,,,,7.0,,"To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube",,,,,,,,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,No clients/patients involved,Psychotherapy -- speech transcripts,No,,,,,,Trained professionals,,2024.0,1st_search_95,1st_search_55
,,Analysis of conversation transcripts,,,,,n,,,,,n,India,Self-collected data but derived from external data set,23.0,Unknown,No,n,,n,,no benchmark,no benchmark,BERTScore,y,no benchmark,no benchmark,"no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination",y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",No,n,acceptability data collected only from one stakeholder level (clinicians),n,,,,,n,English,no benchmark,no benchmark,"no benchmark. metric: ROUGE-1, -2, -L",y,,,,,,,,,,,,,,,,,"BART family; T5 family; GPT-2 family; Llama 2 family; Mistral family; Other: Phi-2, GPT-J, GPT-Neo",7.0,,"external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS",,,,,y,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,,Psychotherapy -- speech transcripts,No users involved,,,,,,Unknown,,2024.0,1st_search_95,1st_search_56
,,Analysis of conversation transcripts,,,,,n,,,,,n,India,Self-collected data but derived from external data set,23.0,Unknown,No,n,,n,,no benchmark,no benchmark,BERTScore,Y,no benchmark,no benchmark,"no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination",Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",No,n,acceptability data collected only from one stakeholder level (clinicians),n,,,,,n,English,no benchmark,no benchmark,"no benchmark. metric: ROUGE-1, -2, -L",Y,,,,,,,,,,,,,,,,,"BART family; T5 family; GPT-2 family; Llama 2 family; Mistral family; Other: Phi-2, GPT-J, GPT-Neo",7.0,,"external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS

To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube",,,,,,,,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Unknown,,2024.0,1st_search_95,1st_search_57
,,Analysis of conversation transcripts,,,,,,,,,,,USA,Self-collected data,16.0,Other: Providers/ not patients,No,Y,"Table 2 summarizes key descriptive statistics related to our
sample of task-taking therapists. Of the 978 task takers, 81.6%
(n = 798) were women. This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%). The majority of therapists had
at least three years of experience providing therapy and a
significant minority had 11 or more years of experience (n =
321, 32.8%). However, most therapists had much less
experience providing messaging therapy; 818 (83.6%) had
between zero and two years of messaging therapy experience",,,,,,,H,same ,"the standard deviation
of ratings generated by DistilBERT (SD = 0.230) were much
closer to the distribution of human ratings (SD = 0.308) versus
that of the SVR (SD = 0.146",yes ,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only fine-tuning,,Yes,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,BERT family,8.0,,derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform ,,,,,N,albeit not really applicable,N,albeit not really applicable,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.,,"Other: therapist performance task—text responses, not patient dialogues",No users involved,,,,,,Trained professionals,,2022.0,1st_search_93,1st_search_58
,Treatment fidelity feedback,Other: Unsure between analysis of conversation transcripts or therapist-facing application with treatment fidelity feedback,,,,,n,,l,b,benchmark is support vector regressor (lower capacity model),y,,"Other: Unsure. Authors describe the data collection process as if they collected it themselves. But they state: ""The dataset for the present study is derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com):",16.0,,,n,"Some demographics of therapists are shown but only male/female, no ethnic etc. data",n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",y,FIS-T is validated,n,,n,,n,,Only fine-tuning,Mix of formal therapy methods,,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,8.0,"This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%)",,,,,,,,,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.,,,No users involved,,,,,,,,2022.0,1st_search_93,1st_search_59
,Other: ,Analysis of conversation transcripts,,,,,n,,l,b,benchmark is support vector regressor (lower capacity model),y,,Self-collected data but derived from external data set,16.0,,,n,"Some demographics of therapists are shown but only male/female, no ethnic etc. data",n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",y,FIS-T is validated,n,,n,,n,,Only fine-tuning,Mix of formal therapy methods,,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,8.0,"This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%)",derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com),,,,,,,,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.,,,No users involved,,,,,,,,2022.0,1st_search_93,1st_search_60
,,Analysis of conversation transcripts,,H,S,Human Interrater agreement,Y,,,,,N,,External data set,19.0,,,N,,N,,,,,N,,,,,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",,N,,Y,"""Specific applications of machine learning–based evaluation may go beyond after-action call-level summaries of statement-level evaluations that cannot affect the quality of care received by the caller in the moment. Specifically, real-time evaluation may also be possible. For example, machine-learning–enabled call center software could label interventions as they occur, providing just-in-time feedback and suggestions to call takers when specific interventions are not provided."" (...) ""..stakeholders include call takers who can use this information to learn from previous calls, supervisors and administrators who may better identify and then direct resources to call takers who are struggling with their performance, and funders who may begin to include population-level data on the quality of conversations - complementing existing metrics on answer rates and wait times."" ",,,,,,,,,N,,,,,,,,,,,,,,,,,BERT family,7.0,,Protocall random sample of crisis calls,,,,,,,,,,,,,Journal paper,,,,Reviewer Two,Y,"""With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation.""",Empirical research involving an LLM,N,,Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.,,,No users involved,,,,,,,,2024.0,1st_search_92,1st_search_61
,,Analysis of conversation transcripts,,h,s,"metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other).",y,,,,,n,,External data set,19.0,,,n,,n,,,,,n,,,,n,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",n,,n,,n,,n,,Only fine-tuning,Mix of formal therapy methods,,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,7.0,,Random sample of 476 Protocall crisis calls,,,,,,,,,,,,,Journal paper,,,,Richard Gaus,y,the tool itself is a suicide risk detection tool,Empirical research involving an LLM,n,,Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.,,,No users involved,,,,,,,,2024.0,1st_search_92,1st_search_62
,,Analysis of conversation transcripts,,H,S,"metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other). Task: classify crisis calls transcripts into 10 suicide risk labels.",Y,,,,,N,,External data set,19.0,,,N,,N,,,,,N,,,,n,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",N,,,,,,,,Only fine-tuning,"Unspecified, might include formal therapy methods",,N,,Y,"""Specific applications of machine learning–based evaluation may go beyond after-action call-level summaries of statement-level evaluations that cannot affect the quality of care received by the caller in the moment. Specifically, real-time evaluation may also be possible. For example, machine-learning–enabled call center software could label interventions as they occur, providing just-in-time feedback and suggestions to call takers when specific interventions are not provided."" (...) ""..stakeholders include call takers who can use this information to learn from previous calls, supervisors and administrators who may better identify and then direct resources to call takers who are struggling with their performance, and funders who may begin to include population-level data on the quality of conversations - complementing existing metrics on answer rates and wait times."" ",,,,n,,,,,N,,,,,,,,,,,,,,,,,BERT family,7.0,,Random sample of 476 Protocall crisis calls,,,,,,,,,,,,,Journal paper,,,,Consensus,Y,"""With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation.""

the tool itself is a suicide risk detection tool",Empirical research involving an LLM,N,,Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.,,,No users involved,,,,,,,,2024.0,1st_search_92,1st_search_63
,,Analysis of conversation transcripts,,H,W,F1-Micro Score against human labeling,"
Y",,,,,N,Other: Isreal,Self-collected data,18.0,Psychopathology,,,,,,,,,N,,,,"N
","Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only fine-tuning,Psychodynamic psychotherapy,,,,,,,,,N,,,,,N,,,,,,,,,,,,,,,,,BERT family,1.0,,,,,,,N,,N,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy,,,No users involved,,,,,,,,2024.0,1st_search_91,1st_search_64
,,Analysis of conversation transcripts,,h,w,they used f1 and kappa to evaluate the emotion labeling system. the benchmark was a human annotator.,y,,,,,n,Other: Israel,Self-collected data,18.0,Psychopathology,,y,"Some demographic information given: The clients were all above age 18 (Mage =
39.06, SD = 13.67, range 20–77), and most were women (58.9%).
Of the clients, 92% were native Hebrew speakers and 92% were
born in Israel. Of the clients, 53.5% had at least a bachelor’s degree;
53.5% were single and 8.9% were in a committed relationship
but unmarried; 23.2% were married and 14.2% were divorced
or widowed. ",n,,,,,n,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",y,"The Outcome Rating Scale (ORS), Profile of Mood States",n,,n,,n,,,Mix of formal therapy methods,No,n,,n,"this is all: Automatic emotion recognition models can be integrated into existing feedback systems to provide an indication of the levels of
emotional coherence in psychotherapy sessions and allow therapists
to modify their interventions accordingly.",,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,1.0,,,,,,,,,,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy,,,No users involved,,,,,,,,2024.0,1st_search_91,1st_search_65
,,Analysis of conversation transcripts,,H,W,they used f1-micro and kappa to evaluate the emotion labeling system. the benchmark was a human annotator. Task: classify emotions in client utterances,Y,,,,,N,Other: Israel,Self-collected data,18.0,Psychopathology,,y,"Some demographic information given: The clients were all above age 18 (Mage =
39.06, SD = 13.67, range 20–77), and most were women (58.9%).
Of the clients, 92% were native Hebrew speakers and 92% were
born in Israel. Of the clients, 53.5% had at least a bachelor’s degree;
53.5% were single and 8.9% were in a committed relationship
but unmarried; 23.2% were married and 14.2% were divorced
or widowed. ",n,,,,,N,,,,N,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",y,,n,,n,,n,,Only fine-tuning,Mix of formal therapy methods,,n,,n,"this is all: Automatic emotion recognition models can be integrated into existing feedback systems to provide an indication of the levels of
emotional coherence in psychotherapy sessions and allow therapists
to modify their interventions accordingly.",,,,N,,,,,N,,,,,,,,,,,,,,,,,BERT family,1.0,This is one of the few studies that also included psychodynamic psychotherapy in the intervention.,,,,,,,,,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy,,,No users involved,,,,,,,,2024.0,1st_search_91,1st_search_66
,,,,,,,N,,,,,N,USA,External data set,26.0,Psychopathology,Yes,N,,N,,,no benchmark,,Y,,no benchmark,,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,,,no benchmark,,Y,,,,,,no benchmark,Lexical Diversity,,,no benchmark,Average length,,,,,,GPT-2 family; Other: DialoGPT,5.0,,"Subreddits in the following categories: (a) Coping and Therapy (C-Th): 7Cup
sofTea, Existential_crisis, getting_over_it, Grief-
Support, helpmecope, hardshipmates, HereToHelp,
itgetsbetter, LostALovedOne, offmychest, MMFB,
Miscarriage, reasonstolive, SuicideBereavement,
therapy; (b) Mood Disorders (MD): depression, de-
pressed, lonely, mentalhealth; (c) Psychosis and
Anxiety (P-An): anxiety, BipolarReddit, socialanxi-
ety; and (d) Trauma and Abuse (Tr-A): abuse, sur-
vivors, Anger, emotionalabuse, PTSDcombat.
Alexander Street Press video transcripts",,,,,unclear/yes,,N,,,,,,Conference paper,,,,Reviewer Two,N,,"Other: Tool Development and Evaluation, Direct LLM performance evaluation",N,,Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues,,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2022.0,1st_search_89,1st_search_67
Chatbot,,Client-facing application,,,,,,,,,,,USA,Self-collected data,26.0,Unselected,No,n,,n,,l,s,benchmark are human responses.,y,h,b,benchmark are human responses.,y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Other: fine-tuning + transfer learning,"Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods",Other: synthetic human-generated,n,,n,,,,,,English,l,b,benchmark are human responses.,y,,,,,,,,,,,,,,,,,GPT-2 family,5.0,,"self-created ""Empathic Conversation dataset""",,,,,,,,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Unknown,,2022.0,1st_search_89,1st_search_68
Chatbot,,Client-facing application,,,,,N,,,,,N,USA,Self-collected data,26.0,Unselected,Other: Yes; human actors,N,,N,,l,w,benchmark are human responses.,Y,,no benchmark,,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Other: fine-tuning + transfer learning,"Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Unspecified, might include formal therapy methods",No,N,,N,,,,,N,English,l,b,benchmark are human responses,Y,,,,,l,w,Lexical Diversity,,l,unclear,Average length,,,,,,GPT-2 family,5.0,,"self-created ""Empathic Conversation dataset""",,,,,y,,N,,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Unknown,,2022.0,1st_search_89,1st_search_69
,,Analysis of conversation transcripts,,H,B,"Several benchmarks used, including ones that were more recently developed. ""In comparison, SPARTA-TAA obtains significant improvements over all
baselines.""",Y,,,,,N,India,Self-collected data,21.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Prompting + other modules,Mix of formal therapy methods,No,N,,N,,,,,N,,,,,N,,,,,H,B,Macro-F1,"""SPARTA-TAA obtains significant improvements over all
baselines. It reports improvements of +8.64%, +8.58%, and +6.29%
in macro-F1 (60.29), weighted-F1 (64.53), and accuracy (64.75%),
respectively, as compared to CASA""",H,B,Weighted-F1,,H,B,Accuracy,,"Other: ""For speaker-invariant representations, we employ a pre-trained RoBERTa language model which is further fine-tuned on DAC task.""",2.0,,"HOPE
Short description: Multi-turn, therapist–patient counseling dialogues labeled with dialog acts; used to evaluate SPARTA vs baselines.
",,212 counseling sessions (not clear how many patients),,,unclear,,N,nothing,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations,"No clients/patients involved; Other: youtube, The data collection
process provides us 12.9𝐾 utterances from 212 counseling therapy
sessions – all of them are dyadic conversations only.",Emotional support dialogue -- speech transcripts,No users involved,,,N,N,N,Unknown,N,2022.0,1st_search_88,1st_search_70
,,Analysis of conversation transcripts,,l,b,"benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. for dialogue-act classification of conversational turns",y,,,,,n,,Self-collected data but derived from external data set,21.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,"Other: they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.",Mix of formal therapy methods,,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,2.0,,Name of new dataset: HOPE. Derived from transcripts from YouTube counseling sessions. They added self-created annotations to these data.,,,,,,,,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations,,,No users involved,,,,,,,,2022.0,1st_search_88,1st_search_71
,,Analysis of conversation transcripts,,l,B,"Several benchmarks used, including ones that were more recently developed. ""In comparison, SPARTA-TAA obtains significant improvements over all
baselines.""

benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. Task: Dialogue-act classification of conversational turns",Y,,,,,N,,Self-collected data but derived from external data set,21.0,,,n,,n,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,"Other: fine-tuning + wrapper.
they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.",Mix of formal therapy methods,,n,,n,,,,,N,,,,,N,,,,,,,,,,,,,,,,,BERT family,2.0,,Name of new dataset: HOPE. Derived from transcripts from YouTube counseling sessions. They added self-created annotations to these data.,,,,,,,,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations,,,No users involved,,,,,,,,2022.0,1st_search_88,1st_search_72
,,Client-facing application,,,,,,,,,,,Germany,Self-collected data,15.0,,,y,"We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.",n,,,,,,,,TBD - Unsure,,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",n,,n,,n,,n,,,Other CBT techniques,,n,,y,"The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).",,,TBD - Unsure,,,,,,,,,,,,,Construct Validity using MTMM and Hierarchical  linear models HLM,No benchmark,,,Criterion Validity,No benchmark,,,,,BERT family,2.0,,,,,,,Y,,N,,,,,,Journal paper,,,,Reviewer Two,n,,Empirical research involving an LLM,n,,Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy,Patients recruited in hospital or outpatient treatment facility,,No,,,n,n,n,,n,2024.0,1st_search_86,1st_search_73
,,Analysis of conversation transcripts,,,,,n,,,,,n,Germany,Self-collected data,28.0,Psychopathology,No,n,,n,,,,,n,,,,N,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",y,"measured phq-9, gad, etc. in patients",n,,n,,n,,Only prompting,Other CBT techniques,No,n,,n,,,,,N,Other: German,,,,n,,,,,no benchmark,no benchmark,construct validity,no benchmark,no benchmark,no benchmark,criterion validity,no benchmark,,,,,BERT family,2.0,,therapy transcriptions from real patients (that they recruited themselves),,35,,,,,,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2024.0,1st_search_86,1st_search_74
,,Analysis of conversation transcripts,,,,,n,,,,,n,Germany,Self-collected data,28.0,,,y,"See Table S1

We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.",n,,,,,n,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",n,"measured phq-9, gad, etc. in patients but did not predict those from the transcripts",n,,n,,n,,Only prompting,Other CBT techniques,,n,,n,"some description in following paragraph but too superficial:

The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).",,,,n,,,,,n,,,,,no benchmark,no benchmark,Construct Validity using MTMM and Hierarchical  linear models HLM,no benchmark,no benchmark,no benchmark,Criterion Validity,no benchmark,,,,,BERT family,2.0,,therapy transcriptions from real patients (that they recruited themselves),,35,,,,,,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,,No users involved,,,,,,,,2024.0,1st_search_86,1st_search_75
Chatbot,,Client-facing application,,,,,N,,,,,N,Other: Russia,Self-collected data,3.0,Unselected,Yes,Y,"
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.",N,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,Y,"A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.",N,,N,,Only prompting,"Informal counseling (e.g., emotional support conversation)",No,N,,N,,,,,N,Other: unclear presumeably russian,,,,N,,,,,,,,,,,,,,,,,GPT-3.5 family,1.0,,Survey of 236 participants evaluating ChatGPT-generated vs. psychologist-generated psychological recommendations.,,,"""During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used).""​",,N,,N,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example),General population,Other: psychological recommendation texts used as experimental stimuli,Yes,"During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).",,N,Y,N,Trained professionals,N,2024.0,1st_search_81,1st_search_76
Other: one-turn recommendations generated by LLM,,Client-facing application,,,,,,,,,,,Other: Russia,No dataset used for development or evaluation,18.0,,,n,,n,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,,,,,,,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",10.0,,,,236,"Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions (""score"", ""activity"", ""informativeness""), while psychologist responses were rated higher in ""strength"".",,n,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example),Other: unclear,,Yes,,,n,y,n,,y,2023.0,1st_search_81,1st_search_77
Other: one-turn recommendations generated by LLM,,Client-facing application,,,,,N,,,,,N,Other: Russia,No dataset used for development or evaluation,18.0,Other: ,Other: ,n,"
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.",n,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,Y,"A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.",n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",Other: ,n,,n,,,,,N,Other: ,,,,N,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",10.0,,,,236,"Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions (""score"", ""activity"", ""informativeness""), while psychologist responses were rated higher in ""strength"".

""During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used).""​",,N,,N,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example),General population,Other: ,Yes,"During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).",,n,y,n,Other: ,y,2023.0,1st_search_81,1st_search_78
,,Client-facing application,,l,w,,y,,,,,,Other: Hong Kong,Self-collected data,30.0,Psychopathology,No,n,,n,,,,"unsure, see page 8",,H,,"unsure, see figure 5",y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",No,n,,n,,L,b,GPT 4 better than GPT3.5,y,Chinese,,,,,,,,,l,w,Linguistic Inquiry and word count (LIWC),worst of all,l,w,Logistic Regression,,l,w,Long short tem measure,,GPT-3.5 family; GPT-4 / GPT-4o family,1.0,,"Open Up text-counseling logs; after filtering → 44,810 valid sessions / 3,231,830 messages; subset with postsession feedback 5,240 sessions / 533,609 messages; stratified sample 131 sessions / 6,169 messages used for annotation & evaluation.
",,,,,N,,N,,,,,,Journal paper,,,,Reviewer Two,n,,Empirical research involving an LLM,n,,Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study,General population,Emotional support dialogue -- chat logs,No,,,n,n,n,Lay people,n,2024.0,1st_search_77,1st_search_79
,,Analysis of conversation transcripts,,l,b,"benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative",y,,,,,n,China,External data set,30.0,Unselected,No,n,,n,,,,,N,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Chinese,,,,N,,,,,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,1.0,,"Open Up is a free24/7 web-based, text-based counseling service in Hong Kong that enables people aged between 11 and 35 years to anonymously chat with paid staff (staff counselors or social workers) or trained volunteers. 5240 sessions with 533,609 messages (Figure 3). We stratified the 5240 sessions based on the number of messages in each session. There were a total of 131 unique message count groups among the 5240 sessions.",,,,,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study,,Emotional support dialogue -- chat logs,No users involved,,,,,,Trained professionals,,2024.0,1st_search_77,1st_search_80
,,Analysis of conversation transcripts,,l,b,"benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative",y,,,,,n,China,External data set,30.0,Unselected,No,n,,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Chinese,,,,n,,,,,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,1.0,,"Open Up is a free24/7 web-based, text-based counseling service in Hong Kong that enables people aged between 11 and 35 years to anonymously chat with paid staff (staff counselors or social workers) or trained volunteers. 5240 sessions with 533,609 messages (Figure 3). We stratified the 5240 sessions based on the number of messages in each session. There were a total of 131 unique message count groups among the 5240 sessions.",,,,,N,,N,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study,,Emotional support dialogue -- chat logs,No users involved,,,,,,Trained professionals,,2024.0,1st_search_77,1st_search_81
Other: smartphone CBT app (Kokoro App),,Client-facing application,,,,,Y,,,,,N,Other: Japan,External data set,7.0,,,"N
",,N,,,,,N,,,,Y,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",Y,PHQ-9,N,,N,,N,,Prompting + other modules,CBT: Cognitive restructuring,,N,,N,,,,,N,,,,,N,,,,,,,,,,,,,,,,,Other: Japanese Text-to-Text Transfer Transformer,7.0,," FLATT (Fun to Learn to Act and Think 
through Technology) trial + HCT (Healthy Campus Trial)",,1626,,,,,,,,,,,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study,Patients with disorder explicitly based on ICD or DSM; General population,,No,,,,,,,,2023.0,1st_search_75,1st_search_82
,,Analysis of conversation transcripts,,no benchmark,no benchmark,"accuracy, F1, precision, recall for prediction of feeling based on automatic thought",y,,,,,n,Other: Japan,External data set,7.0,Psychopathology,No,n,,n,,,,,n,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",n,,n,,n,,n,,Only fine-tuning,CBT: Cognitive restructuring,No,n,,n,,,,,n,Japanese,,,,n,,,,,no benchmark,no benchmark,reduction of negative feelings,no benchmark,,,,,,,,,T5 family,7.0,,FLATT dataset (Fun to Learn to Act and Think through Technology),,,,,y,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study,,Other: automatic thought-feeling pairs,No users involved,,,,,,Other: not applicable,,2023.0,1st_search_75,1st_search_83
Other: ,,Analysis of conversation transcripts,,no benchmark,no benchmark,"accuracy, F1, precision, recall for prediction of feeling based on automatic thought (feeling-thought pairs)",Y,,,,,N,Other: Japan,External data set,7.0,Psychopathology,No,"N
",,N,,,,,N,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",n,,N,,N,,N,,Only fine-tuning,CBT: Cognitive restructuring,No,N,,N,,,,,N,Japanese,,,,N,,,,,,,,,,,,,,,,,T5 family,7.0,,FLATT dataset (Fun to Learn to Act and Think through Technology),,,,,y,T5 model,n,,,,,,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study,No clients/patients involved,Other: automatic thought-feeling pairs,No users involved,,,,,,Other: not applicable,,2023.0,1st_search_75,1st_search_84
,,,,,,,,,,,,,USA,External data set,1.0,Unselected,No,N,,N,,L,B,seq2seq,Y,L,B,seq2seq,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,CBT: Motivational interviewing,Other: unclear,N,,N,,,,,,English,L,B,seq2seq,Y,,,,,L,S,Diversity,seq2seq,,,,,,,,,GPT-2 family,7.0,,Motivational Interviewing (MI) counseling dataset from Perez-Rosas et al. (2016); Alexander Street dataset ,,,,,Y,gpt 2,N,too little,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context,,Psychotherapy -- speech transcripts,No users involved,,,,,,Lay people,,2020.0,1st_search_73,1st_search_85
Other: reflection generation,,Client-facing application,,,,,n,,,,,n,USA,External data set,1.0,Other: mixed,No,n,,n,,l,b,benchmark is a simple seq2seq model,y,h,s,benchmark is ground truth (human created reflections) and output of simple seq2seq model,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Other: fine-tuning + retrieval and context expansion,CBT: Motivational interviewing,No,n,,n,,,,,n,English,l,b,benchmark is a simple seq2seq model,y,,,,,l,s,diversity,benchmark is a simple seq2seq model,,,,,,,,,GPT-2 family,7.0,,"Motivational Interviewing counseling dataset (Perez-Rosas 2016)

The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The dataset is derived from a collection of 284 video recordings of counseling encounters using MI. The recordings were collected from various sources, including two clinical trials, students’ counseling sessions from a graduate level MI course, wellness coaching phone calls, and demonstrations of MI strategies in brief medical encounters.",,,,,y,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Other: mixed,,2020.0,1st_search_73,1st_search_86
Other: reflection generation,Utterance suggestions,Therapist-facing application,,,,,n,,,,,n,USA,External data set,1.0,Unselected,No,n,,n,,l,B,seq2seq,Y,h,s,benchmark is ground truth (human created reflections) and output of simple seq2seq model,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Other: fine-tuning + retrieval and content expansion,CBT: Motivational interviewing,No,n,,n,,,,,n,English,L,B,seq2seq,Y,,,,,l,s,diversity,seq2seq,,,,,,,,,GPT-2 family,7.0,,"Motivational Interviewing counseling dataset (Perez-Rosas 2016)

The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The dataset is derived from a collection of 284 video recordings of counseling encounters using MI. The recordings were collected from various sources, including two clinical trials, students’ counseling sessions from a graduate level MI course, wellness coaching phone calls, and demonstrations of MI strategies in brief medical encounters.",,,,,Y,gpt 2,N,too little,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context,,Psychotherapy -- speech transcripts,No users involved,,,,,,Lay people,,2020.0,1st_search_73,1st_search_87
,Other: Probability of Advanced Tinitus Therapy success ,Therapist-facing application,,,,,,,No,B,,Yes,Other: Korea,Self-collected data,25.0,Psychopathology,Yes,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,CBT: Cognitive restructuring; Other CBT techniques,No,,,Y,,,,,,English,,,,,,,,,,,,,,,,,,,,,Other: Google T5 Transformer ,3.0,,42 tinitus patients as raw data + augmented dataset as test samples,,,,,N,,Y?,"“Informed consent was obtained… participants were assured that no personal sensitive data would be collected… All data collected was kept confidential and anonymous, ensuring complete privacy and data protection.”
",,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment,,"Other: CBT patient diaries + audiometry/THI clinical data
",No,,,,,,Unknown,,2025.0,1st_search_72,1st_search_88
,,Other: Analysis of CBT diary data,,,,,n,,no benchmark,no benchmark,Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though.,y,Other: Korea,Self-collected data,29.0,Psychopathology,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",y,Tinnitus Handicap Inventory (THI),n,,n,,n,,Other: Fine-tuning plus other elements like clustering,Other CBT techniques,No,n,,n,,,,,n,Other: Korean,no benchmark,no benchmark,Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.,y,,,,,,,,,,,,,,,,,T5 family; GPT-2 family,3.0,,"The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients",,42,,,y,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment,Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires),Other: cbt diary entries,No,,,,,,Other: not applicable,,2024.0,1st_search_72,1st_search_89
,Other: ,Other: Analysis of CBT diary data,,,,,n,,no benchmark,no benchmark,"Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though (Table 10, 11)",y,Other: Korea,Self-collected data,29.0,Psychopathology,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",y,Tinnitus Handicap Inventory (THI),n,,n,,n,,Other: Fine-tuning plus other elements like clustering,CBT: Cognitive restructuring; Other CBT techniques,No,n,,n,,,,,n,English,no benchmark,no benchmark,Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.,y,,,,,,,,,,,,,,,,,T5 family; GPT-2 family,3.0,,"42 tinitus patients as raw data + augmented dataset as test samples

The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients",,42,,,y,,n,"“Informed consent was obtained… participants were assured that no personal sensitive data would be collected… All data collected was kept confidential and anonymous, ensuring complete privacy and data protection.”
",,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment,Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires),Other: cbt diary entries,No,,,,,,Other: not applicable,,2024.0,1st_search_72,1st_search_90
,,,,,,,N,,,,,N,India,External data set,11.0,,,,,,,,,,N,L,B,"ML models (HRED, SEQ2SEQ)",Y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Other: own,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,N,,L,B,"ML models (HRED, SEQ2SEQ)",Y,,,,,,,,,,,,,,,,,GPT-2 family,7.0,,MotiVAte,,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Towards Motivational and Empathetic Response Generation in Online Mental Health Support,,,No,,,,,,,,2022.0,1st_search_70,1st_search_91
Chatbot,,Client-facing application,,,,,n,,,,,n,India,External data set,11.0,Unselected,No,n,,n,,l,b,Benchmark is other NLP model,y,l,b,Benchmark is other NLP model,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework,Peer support conversation,No,n,,n,,,,,,English,l,b,Benchmark is other NLP model,y,,,,,l,s,sentiment polarity,Benchmark is other NLP model,no benchmark,no benchmark,change of empathy scores for the ERF module,no benchmark,,,,,GPT-2 family,7.0,,"MotiVAte

This dataset comprises of 4k dyadic conversations between the depressed support seekers and the VA imparting appropriate suggestion, hope and motivation resulting in a total of 14,809 utterances. The conversations of the MotiVAte dataset are collected from peer-to-peer support forum and modified to represent dyadic conversations for an end-to-end online mental health support system.",,,,,y,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Towards Motivational and Empathetic Response Generation in Online Mental Health Support,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Lay people,,2022.0,1st_search_70,1st_search_92
Chatbot,,Client-facing application,,,,,N,,,,,N,India,External data set,11.0,Unselected,No,n,,n,,l,b,"ML models (HRED, SEQ2SEQ)",y,L,B,"ML models (HRED, SEQ2SEQ)",Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework,"Peer support conversation; Informal counseling (e.g., emotional support conversation)",No,n,,n,,,,,N,English,L,B,"ML models (HRED, SEQ2SEQ)",Y,,,,,l,s,sentiment polarity,Benchmark is other NLP model,no benchmark,no benchmark,change of empathy scores for the ERF module,no benchmark,,,,,GPT-2 family,7.0,,"MotiVAte

This dataset comprises of 4k dyadic conversations between the depressed support seekers and the VA imparting appropriate suggestion, hope and motivation resulting in a total of 14,809 utterances. The conversations of the MotiVAte dataset are collected from peer-to-peer support forum and modified to represent dyadic conversations for an end-to-end online mental health support system.",,,,,y,,N,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Towards Motivational and Empathetic Response Generation in Online Mental Health Support,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Lay people,,2022.0,1st_search_70,1st_search_93
,,Client-facing application,,,,,,,,,,,Other: Canada,Self-collected data,17.0,Unselected,Yes,Y,,,,,,,,,,,,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",,,,,,,,,Prompting + other modules,CBT: Motivational interviewing,No,,,,,,,,,English,,,,,,,,,L,W,eaviness of Smoking Index (HSI) measures.,,,,,,,,,,GPT-2 family,10.0,,"Four chatbot versions tested with separate groups of ambivalent smokers; pre/post and 1-week follow-up (readiness, confidence, importance), quit attempts, and perceived empathy; core chat = 5 scripted questions + (for most versions) generated MI reflections.  • Derived: Not indicated.
",,,,,N,nothing found,N,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,General population,"Other: MI chatbot intervention chat logs + survey responses
",Yes,,,,,,Lay people,,2023.0,1st_search_63,1st_search_94
Chatbot,,Client-facing application,,,,,n,,,,,n,Other: Canada,Self-collected data,17.0,Unknown,Yes,y,reported in multimedia appendix 3,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",y,use of the readiness ruler,n,,y,"Of the 654 participants who accepted and consented to the study,
105 (16.1%) did not finish the entire study. We speculate that
this dropout was caused by several factor",n,,Other: fine-tuning + custom pipeline (see Reflection Generation Training),CBT: Motivational interviewing,No,n,,n,,,,,n,English,,,,n,,,,,no benchmark,"no benchmark. in absolute terms favorable, with reduction in smoking, increase in confidence, importance, readiness.","readiness ruler (patient symptom report, smoking related, validated measure)",no benchmark,,,,,,,,,BERT family; GPT-2 family,10.0,,"In version 1 of the generator, the fine-tuning question-and-response data set came from 2 sources: the first was our prior work [40,41], and the second data source was from earlier deployments of MIBot, before the creation of MIBot v4.7. The reflections used came from a variety of sources: from previous versions of this chatbot that were deemed to be acceptable MI reflections by MI-literate researchers or actual reflections produced by MI-literate researchers or MI-expert clinicians.

To address the rate of poor reflections, we developed version 2 of the generator with 2 significant enhancements. First, a larger set of 301 fine-tuning triplets were collected over approximately 10 months of deploying the chatbot, making use of the various responses from smokers who had been recruited in a similar manner, as described in the Participant Recruitment and Screening subsection. This second data set did not include any of the data from the earlier chatbot version [40,41]. Only MI-consistent reflections were used, which were sourced from MI clinicians, MI-literate researchers, or version 1 of the generator. The labeling and selection of the MI-consistent reflections were improved by using multiple human raters and a carefully controlled decision tree to determine the validity of the reflections. The new rating scheme itself was stricter than the one used in version 1, which caused the hit rate to go down—not because the generation was worse but because of the stricter rating. The hit rate of the new generator was measured to be 55.1% (166/301) on a set of reflections.",,349,"consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?",,y,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,y,used reflection quality classifier,A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Other: Motivational interviewing reflections,Yes,consultation and relational empathy survey (CARE),,n,y,y,Trained professionals,y,2023.0,1st_search_63,1st_search_95
Chatbot,,Client-facing application,,,,,n,,,,,n,Other: Canada,Self-collected data,17.0,Psychopathology,Yes,Y,reported in multimedia appendix 3,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",y,use of the readiness ruler,n,,y,"Of the 654 participants who accepted and consented to the study,
105 (16.1%) did not finish the entire study. We speculate that
this dropout was caused by several factor",n,,Other: fine-tuning + custom pipeline (see Reflection Generation Training),CBT: Motivational interviewing,No,n,,n,,,,,n,English,,,,n,,,,,no benchmark,"no benchmark. in absolute terms favorable, with reduction in smoking, increase in confidence, importance, readiness.","readiness ruler (patient symptom report, smoking related, validated measure)",no benchmark,,,,,,,,,BERT family; GPT-2 family,10.0,,"In version 1 of the generator, the fine-tuning question-and-response data set came from 2 sources: the first was our prior work [40,41], and the second data source was from earlier deployments of MIBot, before the creation of MIBot v4.7. The reflections used came from a variety of sources: from previous versions of this chatbot that were deemed to be acceptable MI reflections by MI-literate researchers or actual reflections produced by MI-literate researchers or MI-expert clinicians.

To address the rate of poor reflections, we developed version 2 of the generator with 2 significant enhancements. First, a larger set of 301 fine-tuning triplets were collected over approximately 10 months of deploying the chatbot, making use of the various responses from smokers who had been recruited in a similar manner, as described in the Participant Recruitment and Screening subsection. This second data set did not include any of the data from the earlier chatbot version [40,41]. Only MI-consistent reflections were used, which were sourced from MI clinicians, MI-literate researchers, or version 1 of the generator. The labeling and selection of the MI-consistent reflections were improved by using multiple human raters and a carefully controlled decision tree to determine the validity of the reflections. The new rating scheme itself was stricter than the one used in version 1, which caused the hit rate to go down—not because the generation was worse but because of the stricter rating. The hit rate of the new generator was measured to be 55.1% (166/301) on a set of reflections.",,349,"consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?",,y,,n,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,y,used reflection quality classifier,A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Other: Motivational interviewing reflections,Yes,consultation and relational empathy survey (CARE),,n,y,y,Trained professionals,y,2023.0,1st_search_63,1st_search_96
,,,,,,,,,,,,,USA,Self-collected data,11.0,Unselected,No,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,,,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,,5.0,qualitative study - survey with 31 participants from recruited over Reddit,"Qualitative study using an online survey + interviews about LGBTQ+ users’ experiences with LLM-based chatbots for mental wellness; questions cover usage, apps used, frequency, and detailed experiences.
",,,,,not applicable,,not applicable,,,,,,Conference paper,,,,Reviewer Two,,,Conceptual or theoretical work (e.g. on ethics or safety),,,Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support,,"Other: Other (qualitative survey & interview data about chatbot use)
",,,,,,,"Other: AI chatbots referenced; study data from human participants
",,2024.0,1st_search_62,1st_search_97
Chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,11.0,,,,,,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,n,,,,,n,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified; Other: various LLM-based chatbots (Replika - Snapchat My AI - Chai - Character.ai - Anima - Paradot - Kuki)",5.0,,,,31,"Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions)",,,,,,,,,,Conference paper,,,,Richard Gaus,,,Population survey,,,Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support,"Other: chatbot usersfrom three sub-Reddits: r/Snapchat, r/Anima, and r/Parradot",,Yes,,,y,n,n,,y,2024.0,1st_search_62,1st_search_98
Chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,11.0,Other: ,Other: ,,,,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,n,Other: ,,,,n,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified; Other: various LLM-based chatbots: Replika, Snapchat My AI, Chai, Character.ai, Anima, Paradot, ChatGPT, Kuki",5.0,qualitative study - survey with 31 participants from recruited over Reddit,,,31,"Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions)",,,,,,,,,,Conference paper,,,,Consensus,,,Population survey,,,Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support,General population,Other: ,Yes,,,y,n,n,Other: ,y,2024.0,1st_search_62,1st_search_99
,,Other: Analysis of text-message conversations between clients and clinicians,,L,B,base BERT model with AUPRC <0.52,Y,,,,,N,USA,Self-collected data,10.0,,,N,,N,,,,,N,,,,N,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,,,,,,,,,,,,,,BERT family,7.0,,"Dataset collected (Ben-Zeev et al., 2020) and rated (Tauscher et al., 2022) in previous studies conducted by same institution",,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,,,No users involved,,,,,,,,2022.0,1st_search_61,1st_search_100
,,Analysis of conversation transcripts,,l,b,Benchmark: BERT (no augmentation). metric: AUPRC,y,,,,,n,USA,External data set,10.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,CBT: Cognitive restructuring,,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family; GPT-2 family,7.0,,"From our previous work (Tauscher et al., 2022), we utilized data from a randomized controlled trial of a community-based text-message intervention for individuals with serious mental illness (Ben- Zeev et al., 2020).

""The maximum intervention dose was three “exchanges,” wherein an exchange is defined as a cluster of thematically connected back-and-forth messages between mobile interventionist and participant (e.g. three outgoing messages and participant responses). Texting strategies included: reminders (e.g., appointments, prescription refills), information provision (e.g., psychoeducation, links to regional events and resources), cognitive challenges (e.g., restructuring dysfunctional beliefs about voices, questioning the validity of self-sabotaging automatic beliefs), self-monitoring/self-reflection (e.g., guidance on self-evaluation of affect, journaling of symptomatic experiences), relaxation techniques (e.g., diaphragmatic breathing, guided imagery), social skills training (e.g., initiating conversations, maintaining eye contact), supportive messages (e.g., affirmations, inspirational quotes), and in-vivo instruction (e.g., pre-scheduled real-time support as the patient attempted a new activity)."" (from Ben-Zeev et al.)

in addition they augmented their data using different strategies (3.2 Augmentation of text data)",,,,,,,,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,,,No users involved,,,,,,,,2022.0,1st_search_61,1st_search_101
,,Analysis of conversation transcripts,,L,B,base BERT model with AUPRC <0.52. Task: classifying cognitive distortions,Y,,,,,N,USA,External data set,10.0,,,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,CBT: Cognitive restructuring,,N,,N,,,,,N,,,,,N,,,,,,,,,,,,,,,,,BERT family,7.0,,"From our previous work (Tauscher et al., 2022), we utilized data from a randomized controlled trial of a community-based text-message intervention for individuals with serious mental illness (Ben- Zeev et al., 2020).

""The maximum intervention dose was three “exchanges,” wherein an exchange is defined as a cluster of thematically connected back-and-forth messages between mobile interventionist and participant (e.g. three outgoing messages and participant responses). Texting strategies included: reminders (e.g., appointments, prescription refills), information provision (e.g., psychoeducation, links to regional events and resources), cognitive challenges (e.g., restructuring dysfunctional beliefs about voices, questioning the validity of self-sabotaging automatic beliefs), self-monitoring/self-reflection (e.g., guidance on self-evaluation of affect, journaling of symptomatic experiences), relaxation techniques (e.g., diaphragmatic breathing, guided imagery), social skills training (e.g., initiating conversations, maintaining eye contact), supportive messages (e.g., affirmations, inspirational quotes), and in-vivo instruction (e.g., pre-scheduled real-time support as the patient attempted a new activity)."" (from Ben-Zeev et al.)

in addition they augmented their data using different strategies (3.2 Augmentation of text data)",,,,,,,,,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,,,No users involved,,,,,,,,2022.0,1st_search_61,1st_search_102
,,Client-facing application,,,,,,,,,,,USA,Self-collected data,11.0,Unselected,Yes,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Prompting + other modules,CBT: Cognitive restructuring,Other: ,,,,,,,,,English,,,,,,,,,,,"
",,,,,,,,,,GPT-3 family,5.0,no alpha adjusting (table 3),"interaction logs from a self-guided reframing tool (user inputs thoughts; LLM generates reframes);
",,,,,N,GPT3,N,too little,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,General population,"Other: self-guided mental-health tool interaction logs
",No,,,,,,"Other: self-guided mental-health
",,2024.0,1st_search_60,1st_search_103
Other: reframed thought generator,,Client-facing application,,,,,n,,,,,n,USA,External data set,11.0,Unselected,No,y,,y,see table 4!,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,y,"We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions",n,,n,,"Other: multiple different:
selection of thinking traps: fine-tuning
writing of reframes: retrieval-enhanced in-context learning",CBT: Cognitive restructuring,Yes,n,,n,,,,,n,English,,,,n,,,,,no benchmark,no benchmark,reduction in emotion intensity,no benchmark. only the pre-post-difference in this metric was measured.,,,,,,,,,GPT-3 family,5.0,,"We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].

4.1 Curated Situations & Negative Thoughts
 We start by curating data sources for situations and negative thoughts.
 Thought Records Dataset (Burger et al., 2021).
 This dataset contains hypothetical and real-world situations, thoughts and emotional processes reported by crowdworkers on Amazon Mechanical Turk. We manually curate 180 pairs of diverse situations with negative thoughts from this dataset.
 Mental Health America (MHA). Situations and thoughts from crowdworkers may not reflect the broad range of mental health challenges that people face in real-life. To incorporate more real-world situations and thoughts, we ran a survey on the MHA website (screening.mhanational.org). MHA visitors (who typically use the website for screening of mental illnesses) were asked to describe any negative thoughts and the associated situations they were struggling with. We manually curate 120 pairs of self-reported situations and thoughts to ensure broad coverage of relevant topics based on high diversity and manual filtering.

https://github.com/behavioral-data/Cognitive-Reframing",,15531,"We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).",,n,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,y,see 4.2 -> Safety considerations,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,"Other: Visitors to Mental Health America (MHA, large mental health website that provides mental health resources and tools to millions of users).
""Many MHA visitors are interested in mental health resources including self-guided systems.""",Other: thought records,Yes,,,y,y,n,Other: not applicable,y,2024.0,1st_search_60,1st_search_104
Other: reframed thought generator,,Client-facing application,,,,,n,,,,,n,USA,External data set,11.0,Unselected,No,y,,y,see table 4!,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,"We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions
BUT: only for different design decisions; no overall other intervention or no intervention as control group!",n,,n,,"Other: multiple different:
selection of thinking traps: fine-tuning
writing of reframes: retrieval-enhanced in-context learning",CBT: Cognitive restructuring,Yes,n,,n,,,,,n,English,,,,n,,,,,no benchmark,no benchmark,reduction in emotion intensity,no benchmark. only the pre-post-difference in this metric was measured.,no benchmark,no benchmark,ux measures,"Reframe Relatability, Reframe Helpfulness, Reframe Memorability, Skill Learnability",,,,,GPT-3 family,5.0,no alpha adjusting (table 3),"We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].

4.1 Curated Situations & Negative Thoughts
 We start by curating data sources for situations and negative thoughts.
 Thought Records Dataset (Burger et al., 2021).
 This dataset contains hypothetical and real-world situations, thoughts and emotional processes reported by crowdworkers on Amazon Mechanical Turk. We manually curate 180 pairs of diverse situations with negative thoughts from this dataset.
 Mental Health America (MHA). Situations and thoughts from crowdworkers may not reflect the broad range of mental health challenges that people face in real-life. To incorporate more real-world situations and thoughts, we ran a survey on the MHA website (screening.mhanational.org). MHA visitors (who typically use the website for screening of mental illnesses) were asked to describe any negative thoughts and the associated situations they were struggling with. We manually curate 120 pairs of self-reported situations and thoughts to ensure broad coverage of relevant topics based on high diversity and manual filtering.

https://github.com/behavioral-data/Cognitive-Reframing",,15531,"We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).",,N,GPT3,N,too little,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,y,see 4.2 -> Safety considerations,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,General population,Other: thought records,Yes,,,y,y,n,Other: not applicable,y,2024.0,1st_search_60,1st_search_105
,,,,,,,,,,,,,UK,External data set,5.0,,,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Llama 3.1 family; Other: own?,7.0,,"CounselChat1,
the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 ",,,,,unclear,,unclear,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety,,,No,,,,,,,,2023.0,1st_search_59,1st_search_106
Chatbot,,Client-facing application,,no benchmark,no benchmark,"no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token",y,,no benchmark,no benckmark,no benchmark. loss value,y,UK,External data set,5.0,Other: not applicable,No,n,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,Other: own transformer architecture,7.0,,"Due to this, data from the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 were selected. Questions
and answers are extracted, and questions are manually generated
dependent on the information available, e.g. for the NHS definition
of depression, questions such as “what is depression?"" are imputed.",,,,,y,own transformer architecture -> on-premise,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2023.0,1st_search_59,1st_search_107
Chatbot,,Client-facing application,,no benchmark,no benchmark,"no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token",y,,no benchmark,no benckmark,no benchmark. loss value,y,UK,External data set,5.0,Other: not applicable,No,n,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Other: Other: self-developed and trained transformer architecture,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,Other: own transformer architecture,7.0,,"Due to this, data from the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 were selected. Questions
and answers are extracted, and questions are manually generated
dependent on the information available, e.g. for the NHS definition
of depression, questions such as “what is depression?"" are imputed.",,,,,y,own transformer architecture -> on-premise,n,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2023.0,1st_search_59,1st_search_108
,,Other: Comparison between AI and human councellor recommendations,,H,s,,Y,,,,,,China,Self-collected data,6.0,Unselected,Yes,n,,n,,H,s,,Y,,,,,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",n,,n,,n,,n,,Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",No,n,,n,,L,,,Y,Chinese,H,s,Lexical Overlap within comparative linguistic analysis,Y,,,,,L,,part-of-speech (POS) analysis ,,L,,dependency-syntactic-parsing (DEP) analysis,,,,,,"BERT family; Other: Roberta, TextCNN, Text-LSTM, GPT-unknown version",6.0,,"1. publicly available dataset comprising HGC, 2. AI-generated content generated by ChatGPT",,,,,N,Chat GPT,N,,,,,,Journal paper,,,,Reviewer Two,n,,Empirical research involving an LLM,n,,Leveraging ChatGPT to optimize depression intervention through explainable deep learning,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,n,n,n,Lay people,n,2024.0,1st_search_56,1st_search_109
Chatbot,,Client-facing application,,,,,n,,,,,n,China,External data set,6.0,Unselected,No,n,,n,,,,,n,,,,n,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Chinese,,,,n,,,,,h,unclear,"various linguistic analyses with no clear meaning (part-of-speech, sentiment, ...)",benchmark: human psychologist responses from transcripts,h,unclear,SHAP values of words in classifier that classifies human vs. ChatGPT responses,benchmark: human psychologist responses from transcripts,,,,,"ChatGPT, model unspecified",6.0,,data sourced from Xinli001.com,,,,,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,"they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values",Empirical research involving an LLM,n,,Leveraging ChatGPT to optimize depression intervention through explainable deep learning,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2024.0,1st_search_56,1st_search_110
Other: one-turn Q&A Chatbot,,Client-facing application,,,,,n,,,,,n,China,External data set,6.0,Unselected,Yes,n,,n,,,,,n,,,,n,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Chinese,,,,n,,,,,h,unclear,part-of-speech (POS) analysis; dependency-syntactic-parsing (DEP) analysis; semantic-dependency-parsing (SDP) analysis; sentiment analysis,benchmark: human psychologist responses from transcripts,h,unclear,SHAP values of words in classifier that classifies human vs. ChatGPT responses,benchmark: human psychologist responses from transcripts,,,,,"ChatGPT, model unspecified",6.0,,data sourced from Xinli001.com. Counsellor responses were included in the dataset. ChatGPT responses were generated by the authors.,,,,,N,Chat GPT,N,,,,,,Journal paper,,,,Consensus,n,"they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values",Empirical research involving an LLM,n,,Leveraging ChatGPT to optimize depression intervention through explainable deep learning,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2024.0,1st_search_56,1st_search_111
,,Other: Analysis of conversation transcripts & client facing,,L,B,see below,Y,,L,B,"rule-based approach: "" It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score""",Y,Germany,External data set,24.0,,,N,,N,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,Y,"Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group)",N,,N,,"Other: ""we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset""
""The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output""",Other CBT techniques,,N,,Y,"
""Automation of time-consuming tasks in iCBT could, through a positive lens, lead to
improved cost-effectiveness, which is an important point in often over-encumbered and
underfinanced psychiatry treatment and care contexts.""
""
Fully automated iCBT, including the prediction of emotional states coupled with a CA
in charge of the iCBT with no human therapist involvement, would be both unwanted
and unethical. For legal reasons, having a clinical professional involved and ultimately
responsible for treatment is mandatory today and unlikely to change in the foreseeable
future. Only hybrid solutions of man-machine co-involvement are therefore further
discussed here. One such hybrid scenario would be the sole automation of emotion
recognition. This scenario starts with initial machine recognition of emotional states
derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional
states can then be fed to a human clinician as decision support. In theory, this could
render an improved understanding of a patient’s emotional state and also change of
state across time during iCBT. This could ultimately improve treatment tailoring and
effectiveness through the patient perceiving the therapist as more empathic, strengthening
the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy
work modules to better suit the patient’s emotional state""
"" An interesting
but largely untested scenario would be extended automation of emotional recognition
coupled with therapist-supported CA treatment. This would involve not only the potential
benefit of emotion recognition discussed above but also cost-effective semi-automated
treatment. One such implementation would be that the emotionally informed CA drafts
empathically written therapy responses to the patient’s messages and the human therapist
then scrutinizes the responses and signs off on them with or without making prior changes.
A major portion of iCBT costs come from therapists spending time drafting responses to
patients in the treatment portal, unlocking a major potential for cost-saving strategies. An
additional downside risk with this scenario would be that the human therapist—due to
stress or other human factors—signs off on written responses of lower therapeutic quality.
Proper training and structured follow-up of therapists are likely required in this scenario,
which in turn may offset some of the cost-effectiveness of the approach. That stated since
a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored
CA seems in accordance with that overarching aim of iCBT.""",,,,,,,,,,,,,,L,S,User rating of Empathig Understanding (EU),"rule-based approach: "" It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score""",,,,,,,,,BERT family,6.0,,"The Emobank dataset (Buechel & Hahn, 2022)
The GoEmotions dataset (Demszky et al., 2020) 
The International Survey on Emotion Antecedents and Reactions (ISEAR) (ISEAR
dataset, https://paperswithcode.com/dataset/isear, accessed 22.08.2023
Validation: The CrowdFlower dataset (Sentiment Analysis in Text - Dataset by crowdflower,
https://data.world/crowdflower/sentiment-analysis-in-text, accessed 07.08.2022)

(see excel)",,,"System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.",,N,,N,,,,,,Journal paper,,,,Reviewer Two,Y,"Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. ",Empirical research involving an LLM,N,,Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,Other: healthy individuals without a diagnosed mental health disorder,,Yes,"System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi)",,N,Y,Y,,Y,2024.0,1st_search_50,1st_search_112
Chatbot,,Client-facing application,,l,b,benchmark is simple rule-based emotion recognition system. Task: classification of clustered dimensional values (five distinct categories),y,,l,b,"benchmark is simple rule-based emotion recognition system.
second, better benchmark: state-of-the-art valence and arousal recognition systems from other papers (Park et al., etc.). still, this is no human. Task: rating of valence, arousal, dominance in conversations",y,,External data set,24.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,y,"Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group).",n,,n,,"Other: BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture",Other CBT techniques,,n,,y,"Fully automated iCBT, including the prediction of emotional states coupled with a CA in charge of the iCBT with no human therapist involvement, would be both unwanted and unethical. For legal reasons, having a clinical professional involved and ultimately responsible for treatment is mandatory today and unlikely to change in the foreseeable future. Only hybrid solutions of man-machine co-involvement are therefore further discussed here. One such hybrid scenario would be the sole automation of emotion recognition. This scenario starts with initial machine recognition of emotional states derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional states can then be fed to a human clinician as decision support. In theory, this could render an improved understanding of a patient’s emotional state and also change of state across time during iCBT. This could ultimately improve treatment tailoring and effectiveness through the patient perceiving the therapist as more empathic, strengthening the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy work modules to better suit the patient’s emotional state. A potential risk with this approach would be the drift of the therapist’s own emotional assessment influenced by the machine’s estimated emotional state of the patient which may be wrong or biased. An interesting but largely untested scenario would be extended automation of emotional recognition coupled with therapist-supported CA treatment. This would involve not only the potential benefit of emotion recognition discussed above but also cost-effective semi-automated treatment. One such implementation would be that the emotionally informed CA drafts empathically written therapy responses to the patient’s messages and the human therapist then scrutinizes the responses and signs off on them with or without making prior changes. A major portion of iCBT costs come from therapists spending time drafting responses to patients in the treatment portal, unlocking a major potential for cost-saving strategies. An additional downside risk with this scenario would be that the human therapist—due to stress or other human factors—signs off on written responses of lower therapeutic quality. Proper training and structured follow-up of therapists are likely required in this scenario, which in turn may offset some of the cost-effectiveness of the approach. That stated since a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored CA seems in accordance with that overarching aim of iCBT.",,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,6.0,,"EmoBank, GoEmotions, ISEAR, CrowdFlower",,20,"Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based",,,,,,,,,,Journal paper,,,,Richard Gaus,y,"Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. ",Empirical research involving an LLM,n,,Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,General population,,Yes,"empathic understanding (EU), system usability (SUS), acceptance (CSQi)",,n,y,y,,y,2024.0,1st_search_50,1st_search_113
Chatbot,,Other: Analysis of conversation transcripts; Client-facing application,,l,b,benchmark is simple rule-based emotion recognition system. Task: classification of clustered dimensional values (five distinct categories),y,,l,b,"benchmark is simple rule-based emotion recognition system.
second, better benchmark: state-of-the-art valence and arousal recognition systems from other papers (Park et al., etc.). still, this is no human. Task: rating of valence, arousal, dominance in conversations

rule-based approach: "" It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score""",y,,External data set,24.0,,,N,,N,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,Y,"Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group).",N,,N,,"Other: ""we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset""
""The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output""

BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture",Other CBT techniques,,N,,Y,"
""Automation of time-consuming tasks in iCBT could, through a positive lens, lead to
improved cost-effectiveness, which is an important point in often over-encumbered and
underfinanced psychiatry treatment and care contexts.""
""
Fully automated iCBT, including the prediction of emotional states coupled with a CA
in charge of the iCBT with no human therapist involvement, would be both unwanted
and unethical. For legal reasons, having a clinical professional involved and ultimately
responsible for treatment is mandatory today and unlikely to change in the foreseeable
future. Only hybrid solutions of man-machine co-involvement are therefore further
discussed here. One such hybrid scenario would be the sole automation of emotion
recognition. This scenario starts with initial machine recognition of emotional states
derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional
states can then be fed to a human clinician as decision support. In theory, this could
render an improved understanding of a patient’s emotional state and also change of
state across time during iCBT. This could ultimately improve treatment tailoring and
effectiveness through the patient perceiving the therapist as more empathic, strengthening
the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy
work modules to better suit the patient’s emotional state""
"" An interesting
but largely untested scenario would be extended automation of emotional recognition
coupled with therapist-supported CA treatment. This would involve not only the potential
benefit of emotion recognition discussed above but also cost-effective semi-automated
treatment. One such implementation would be that the emotionally informed CA drafts
empathically written therapy responses to the patient’s messages and the human therapist
then scrutinizes the responses and signs off on them with or without making prior changes.
A major portion of iCBT costs come from therapists spending time drafting responses to
patients in the treatment portal, unlocking a major potential for cost-saving strategies. An
additional downside risk with this scenario would be that the human therapist—due to
stress or other human factors—signs off on written responses of lower therapeutic quality.
Proper training and structured follow-up of therapists are likely required in this scenario,
which in turn may offset some of the cost-effectiveness of the approach. That stated since
a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored
CA seems in accordance with that overarching aim of iCBT.""",,,,n,,,,,n,,,,,,,,,,,,,,,,,BERT family,6.0,,"The Emobank dataset (Buechel & Hahn, 2022)
The GoEmotions dataset (Demszky et al., 2020) 
The International Survey on Emotion Antecedents and Reactions (ISEAR) (ISEAR
dataset, https://paperswithcode.com/dataset/isear, accessed 22.08.2023
Validation: The CrowdFlower dataset (Sentiment Analysis in Text - Dataset by crowdflower,
https://data.world/crowdflower/sentiment-analysis-in-text, accessed 07.08.2022)",,20,"System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.

Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based",,,,,,,,,,Journal paper,,,,Consensus,Y,"Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. ",Empirical research involving an LLM,N,,Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,General population,,Yes,"empathic understanding (EU), system usability (SUS), acceptance (CSQi)",,N,Y,Y,,Y,2024.0,1st_search_50,1st_search_114
,,Client-facing application,,,,,N,,,,,N,Other: Poland,Self-collected data,23.0,,,,,,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,,Other: Person Centered Therapy (PCT) (Carl Rogers),,,,,,,,,N,,,,,N,,,,,,,,,,,,,,,,,"Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer
[26] model which generates a beam of candidate responses.
In the second stage a number of smaller, more specialized
Transformer-based models)",1.0,,"“Pushshift Reddit Dataset which includes 651 million submissions and 5.6 billion comments…” • “We have fine-tuned this model on transcripts of counseling and psychotherapy sessions… Our… dataset consisted of 14,300 patient’s prompt and counselor’s answer pairs.”
",,,"Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.",,N,,N,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Deep Learning Mental Health Dialogue System,General population,,Yes,,,,,,,,2023.0,1st_search_41,1st_search_115
Chatbot,,Client-facing application,,,,,n,,,,,n,Other: Poland,External data set,13.0,Unknown,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,"Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers",Other: Person-centered therapy,No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,BERT family; Other: own architecture,2.0,,"Alexander Street Press, Counseling and Psychotherapy Transcripts: Volume I",,,,,y,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,y,"To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.",Deep Learning Mental Health Dialogue System,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2023.0,1st_search_41,1st_search_116
Chatbot,,Client-facing application,,,,,N,,,,,N,Other: Poland,External data set,13.0,Unknown,No,n,,n,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,"Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers",Other: Person Centered Therapy (PCT) (Carl Rogers),No,n,,n,,,,,N,English,,,,N,,,,,,,,,,,,,,,,,"BERT family; Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer [26] model which generates a beam of candidate responses. In the second stage a number of smaller, more specialized Transformer-based models)",2.0,"no evaluation was performed. Only a sample conversation was shown ""look this is good"".","Alexander Street Press, Counseling and Psychotherapy Transcripts: Volume I",,,"Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.

Results: Not clear/not reported",,y,,N,,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,y,"To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.",Deep Learning Mental Health Dialogue System,No clients/patients involved,Psychotherapy -- speech transcripts,Yes,,,,,,Trained professionals,,2023.0,1st_search_41,1st_search_117
,,Client-facing application,,,,,n,,,,,n,Germany,Self-collected data,27.0,Unknown,No,N,,N,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,CBT: Motivational interviewing,No,N,,N,,,,,n,Other: german,,,,n,,,,,,,,,,,,,,,,,GPT-3.5 family,5.0,,"brief questionnaire; users interacted ~10 minutes; prompts designed for ChatGPT; qualitative feedback collected
",,,"Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their
goals and inquire about their interests. Notably, no users reported feeling pressured or
convinced by the prototype to change aspects of their lives against their wishes. Several
users found the prototype to be particularly valuable for creating quick and helpful
weekly plans or recipes. However, it was observed that one user found it premature
to establish concrete plans at that stage. Overall, while some users initially felt that
the prototype’s responses were not aligned with their preferences, they noted that it
was possible to clarify their intentions, and the prototype quickly adapted accordingly.
Despite this, some users felt that the assessment of their goals and life circumstances
at the beginning of the training was too superficial, leading to suggestions for a more
thorough initial assessment process.
Users highlighted several aspects of the prototype’s communication style. Most users
found the conversation to be clear and direct. In contrast, one user reported that the pro-
totype struggled to understand his goals and sometimes provided contradictory advice,
a concern not raised by others. On the positive side, one user appreciated the prototype’s
approach of asking questions rather than giving fixed instructions, which encouraged 
engagement and avoided a patronizing tone. Some users noted the positive and moti-
vating tone of the coach, particularly when asking if they were willing to continue with
suggested steps and advice. The use of a positive tone was seen as a motivating factor by
one user and was also appreciated by another who found it encouraging. Users generally
found concrete advice more helpful than generic recommendations. Additionally, one
user valued the coach’s acknowledgment of setbacks and the importance of enjoying the
process. However, some users expressed a desire for shorter, more direct conversations
that focused less on delivering general knowledge, and one user specifically requested
a more emotional and less matter-of-fact tone of voice.",,N,Chat GPT,N,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,General population,Other: qualitative interviews,Yes,,,Y,N,N,Other: not applicable,Y,2024.0,1st_search_34,1st_search_118
Chatbot,,Client-facing application,,,,,n,,,,,n,Germany,No dataset used for development or evaluation,3.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only prompting,CBT: Motivational interviewing,,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,GPT-3.5 family,6.0,,,,11,"Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.",,n,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,"Other: unclear (it just says ""users"")",,Yes,,,y,n,n,,y,2024.0,1st_search_34,1st_search_119
Chatbot,,Client-facing application,,,,,n,,,,,n,Germany,No dataset used for development or evaluation,27.0,Other: ,Other: ,N,,N,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,CBT: Motivational interviewing,Other: ,N,,N,,,,,n,Other: ,,,,n,,,,,,,,,,,,,,,,,GPT-3.5 family,5.0,,,,11,"Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.",,N,Chat GPT,N,,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,General population,Other: ,Yes,,,Y,N,N,Other: ,Y,2024.0,1st_search_34,1st_search_120
,Utterance suggestions,Other: Conversation Rewriting,,,,,N,,L,W,,Y (Perplexity),USA,External data set,19.0,Other: for me unclear does the comment suffice?,Yes,,,,,L,B,,Y (Specifity),,,,N,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,,,,,,,Other: PARTNER,"Informal counseling (e.g., emotional support conversation)",No,,,,,,,,N,English,,,,N,,,,,L,B,Empathy classification (Sharma et al),Other LLMs,L,W,Edit Rate,Other LLMs,,,,,GPT-2 family,4.0,,"TalkLife
Real peer-to-peer conversations on a mental-health support platform (TalkLife) used to train and evaluate “Partner,” a reinforcement-learning model for empathic rewriting. Includes posts by seekers and peer supporters, with empathy scores (0–6) labeled using the Sharma et al. (2020) empathy classifier and 180 expert “empathic rewritings.
",,,,,N,,N,too little,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach,No clients/patients involved,Internet data -- mental health forum,No,,,,,,Lay people,,2021.0,1st_search_32,1st_search_121
,Utterance suggestions,Therapist-facing application,,,,,,,,,,,USA,Self-collected data,19.0,Unselected,No,n,,n,,l,b,"""specificity""",y,h,w,Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.,Peer support conversation,Yes,n,,n,,l,b,"BERT-based empathy scoring model.
benchmarks are other LLMs, not fine tuned to empathy tasks",y,English,l,b,BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations,y,,,,,l,w,perplexity,PARTNER against other LLMs.,l,w,lexical_diversity,PARTNER against other LLMs.,l,b,automatic empathy rating,change in empathy. PARTNER against other LLMs.,BERT family; GPT-2 family,4.0,,"TalkLife (talklife.co) is the largest online peer-to-peer support platform for mental health support. It enables conversations between people seeking support (support seekers) and people providing support (peer supporters) in a thread-like setting. We call the post authored by a support seeker as seeker post, and the response by a peer supporter as response post. Table 1 describes the statistics of conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question ""Is the seeker talking about a mental health related issue or situation in his/her post?"". Using this annotated dataset, we train a standard text classifier based on BERT [15] (achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-related conversations. This dataset contains 3.33M interactions from 1.48M seeker posts.

https://github.com/behavioral-data/Empathy-Mental-Health/tree/master",,,,,y,,n,,,,,,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach,,Internet data -- mental health forum,No users involved,,,,,,Lay people,,2021.0,1st_search_32,1st_search_122
,Utterance suggestions,Therapist-facing application,,,,,N,,,,,,USA,Self-collected data but derived from external data set,19.0,Unselected,No,n,,n,,L,B,specificity is an embedding similarity metric here,Y,h,w,Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.,Peer support conversation,No,n,,n,,,,,N,English,l,b,BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations,y,,,,,L,B,Empathy classification (Sharma et al),Other LLMs,L,W,"Perplexity, Diversity, Sentence coherence, edit rate",Other LLMs,l,b,expert rating 2,"Expert judgments of empathy, fluency, specificity of PARTNER against other LLMs.",BERT family; GPT-2 family,4.0,,"TalkLife (talklife.co) is the largest online peer-to-peer support platform for mental health support. It enables conversations between people seeking support (support seekers) and people providing support (peer supporters) in a thread-like setting. We call the post authored by a support seeker as seeker post, and the response by a peer supporter as response post. Table 1 describes the statistics of conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question ""Is the seeker talking about a mental health related issue or situation in his/her post?"". Using this annotated dataset, we train a standard text classifier based on BERT [15] (achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-related conversations. This dataset contains 3.33M interactions from 1.48M seeker posts.

Non-public: For accessing the TalkLife portion of our dataset for non-commercial use, please contact the TalkLife team here.",,,,,y,,N,too little,,,,,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach,No clients/patients involved,Internet data -- mental health forum,No users involved,,,,,,Lay people,,2021.0,1st_search_32,1st_search_123
,,,,,,,,,,,,,India,,21.0,,,,,,,,,,,,,,,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,7.0,,,,,,,N,,N,,,,,,Other: Letter to the Editor ,,,,Reviewer Two,,,"Opinion, commentary, perspective, correspondence",,,ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane,,,,,,,,,,,2023.0,1st_search_30,1st_search_124
Chatbot,,Client-facing application,,,,,n,,,,,n,India,No dataset used for development or evaluation,21.0,,,n,,n,,,,,n,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",7.0,,,,,,,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane,No clients/patients involved,,No users involved,,,,,,,,2023.0,1st_search_30,1st_search_125
Chatbot,,Client-facing application,,,,,n,,,,,n,India,No dataset used for development or evaluation,21.0,,,n,,n,,,,,n,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",7.0,No real evaluation. Only a sample conversatino is shown,no dataset. ChatGPT is used as is.,,,,,N,,N,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane,No clients/patients involved,,No users involved,,,,,,,,2023.0,1st_search_30,1st_search_126
,,,,no benchmark,No benchmark,"GPT4 scored 44/60 and Bard 46/60, but the study did not aim to compare those two,  ",Yes ,,,,,,UK,Self-collected data,30.0,Unselected,No,,,,,,,,,no benchmark,no benchmark,,Yes ,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,CBT: Cognitive restructuring,No,,,Y,"Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions",,,,,English,,,,,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family; Other: Bard ,7.0,very short study ,"oth ChatGPT-4 and Bard responded to 20 tasks at each
stage of the study --> no new dataset created ",,,,,N,,N,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.,No clients/patients involved,Psychotherapy -- chat logs,No users involved,,,,,,Lay people,,2024.0,1st_search_26,1st_search_127
Other: reframed thought generator,,Client-facing application,,,,,n,,,,,n,UK,Self-collected data,30.0,Other: not applicable,Other: synthetic: human generated,n,,n,,,,,n,,,no benchmark.,y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,CBT: Cognitive restructuring,No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family; Gemini / Bard family,7.0,,Data were 20 thoughts written by two CBT therapists (each wrote 10 thoughts).,,,,,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.,No clients/patients involved,Other: automatic thought records,No users involved,,,,,,Other: not applicable,,2024.0,1st_search_26,1st_search_128
Other: reframed thought generator,,Client-facing application,,,,,n,,,,,n,UK,Self-collected data,30.0,Other: not applicable,Other: synthetic: human generated,n,,n,,,,,n,no benchmark,no benchmark,no benchmark.,y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,CBT: Cognitive restructuring,No,n,,Y,"Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions",,,,n,English,,,,n,,,,,,,,,,,,,,,,,BERT family; GPT-4 / GPT-4o family,7.0,very short study ,"oth ChatGPT-4 and Bard responded to 20 tasks at each
stage of the study --> no new dataset created ",,,,,N,,N,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.,No clients/patients involved,Other: automatic thought records,No users involved,,,,,,Other: not applicable,,2024.0,1st_search_26,1st_search_129
,,Analysis of conversation transcripts,,,,,N,,,,,N,USA,External data set,22.0,Psychopathology,No,y,,,,,,,N,,,,N,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",y,,,,,,,,,"Informal counseling (e.g., emotional support conversation)",No,,,,,,,,N,English,,,,N,,,,,,,,,,,,,,,,,Other: ML model to structure responses,1.0,,"Data were obtained from an online and mobile therapy app (Talkspace) for services provided
between 2014 and 2019.",,,,,unclear,did not find information,Y,"“The deidentified content of messages was retained…” / “clients and therapists agree to the use of their anonymized data for quality assurance and for research.” / “This study was thus deemed exempt from full institutional review board review…
",,,,,Journal paper,,,,Reviewer Two,,,Other: use of machine learning to evaluate clinical content (ML was used to improve quality),,,Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning,,Psychotherapy -- chat logs,No,,,,,,Trained professionals,,2024.0,1st_search_18,1st_search_130
,,Analysis of conversation transcripts,,h,w,benchmark are ratings by humans on MISC and Topics (see appendix eTable 2),y,,h,w,benchmark are ratings by humans on CTRS (see appendix eTable 2),y,,External data set,22.0,,,y,see Table client demographic characteristics,n,,,,,n,,,,,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",y,PHQ-8 was measured in clients,n,,n,,n,,Only fine-tuning,Mix of formal therapy methods,,n,,n,,,,,,,,,,n,,,,,,,,,,,,,,,,,Other: not sure - it just says transformer model,1.0,the transformer model predicted items subscales from different clinical questionnaires,Talkspace data,,,,,,,,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning,,,No users involved,,,,,,,,2024.0,1st_search_18,1st_search_131
,,Analysis of conversation transcripts,,h,s,benchmark are ratings by humans on MISC and Topics (see appendix eTable 2). Task: classification of client messages into content types,y,,h,s,benchmark are ratings by humans on CTRS (see appendix eTable 2),y,,External data set,22.0,,,y,see Table client demographic characteristics,n,,,,,N,,,,N,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",y,PHQ-8 was measured in clients,n,,n,,n,,Only fine-tuning,Mix of formal therapy methods,,n,,n,,,,,N,,,,,N,,,,,,,,,,,,,,,,,"Other: not sure, it just says transformer model",1.0,the transformer model predicted items subscales from different clinical questionnaires,"Data were obtained from an online and mobile therapy app (Talkspace) for services provided
between 2014 and 2019.",,,,,,,,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning,,,No users involved,,,,,,,,2024.0,1st_search_18,1st_search_132
,,,,,,,N,,,,,N,Other: Bangladesh,External data set,23.0,,,,,,,,,,N,,,,N (if you consider the author him/herself an expert?),"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,N,,,,,N,,,,,H,B,Sentiment Analysis,(Benchamark Kaggle data set) sentiment analysis,H,B,Response Quality,Author rated himself .. (vested interests??),H,B,Word analysis/ Count,,Other: GPT family not specified,5.0,,"Kaggle (n.d.)  This dataset encompasses 80 distinct tags, each containing numerous conversational prompts and corresponding 
responses",,,,,N,,Y,"Privacy and confidentiality … conversations … may contain sensitive information … implement robust security measures to protect user privacy and ensure the confidentiality of conversations.
",,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues,,,No users involved,,,,,,,,2024.0,1st_search_17,1st_search_133
Chatbot,,Client-facing application,,,,,,,,,,,Other: Bangladesh,External data set,23.0,Unknown,Other: unknown,n,,n,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,,English,,,,,,,,,h,b,sentiment analysis score,benchmark are responses in the kaggle dataset,h,b,non expert rating,benchmark are responses in the kaggle dataset,,,,,"ChatGPT, model unspecified",5.0,,Kaggle mental health conversational dataset https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data,,,,,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,1st_search_17,1st_search_134
Chatbot,,Client-facing application,,,,,,,,,,,Other: Bangladesh,External data set,23.0,Unknown,Other: unknown,n,,n,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,,English,,,,,,,,,H,b,sentiment analysis score,benchmark are responses in the kaggle dataset,H,b,Response Quality rated by non experts,"benchmark are responses in the kaggle dataset, but author rated himself (vested interests???)",H,more words,Word analysis/ Count,,"ChatGPT, model unspecified",5.0,,"Kaggle mental health conversational dataset https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data

Kaggle (n.d.)  This dataset encompasses 80 distinct tags, each containing numerous conversational prompts and corresponding responses",,,,,N,,n,"Privacy and confidentiality … conversations … may contain sensitive information … implement robust security measures to protect user privacy and ensure the confidentiality of conversations.
",,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,1st_search_17,1st_search_135
,,Client-facing application,,,,,N,,,,,N,Other: Saudi Arabia,Self-collected data,7.0,Psychopathology,,Yes ,"able 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
",Yes ,Table 3 and Table 4,,,,N,,,,N,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",Yes ,patients have a diagnosed anxiety disorder ,,,,,,,Only prompting,"Other CBT techniques; Other: ACT, ET, MBCT, DBT",,,,Yes ,"
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee",,,,N,,,,,N,,,,,,,,,,,,,,,,,GPT-3.5 family,10.0,,,,,"Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience",,N,,N,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.,Patients recruited in hospital or outpatient treatment facility,,Yes,,,,,,Other: ,,2024.0,1st_search_14,1st_search_136
Chatbot,,Client-facing application,,,,,n,,,,,n,Other: Saudi Arabia,No dataset used for development or evaluation,7.0,,,y,"Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
",y,"section ""Perceptions across demographic groups""",,,,n,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,Mix of formal therapy methods,,n,,y,"The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in need.",,,,n,,,,,n,,,,,,,user experience measures (see above),,,,,,,,,,GPT-3.5 family,10.0,,,,399,"The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.",,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.,Patients recruited in hospital or outpatient treatment facility,,Yes,,,n,y,n,,y,2024.0,1st_search_14,1st_search_137
Chatbot,,Client-facing application,,,,,N,,,,,N,Other: Saudi Arabia,No dataset used for development or evaluation,7.0,Other: ,,y,"Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
",y,"Table 3 and Table 4
section ""Perceptions across demographic groups""",,,,N,,,,N,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,Mix of formal therapy methods,,n,,y,"
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee",,,,N,,,,,N,,,,,,,,,,,,,,,,,GPT-3.5 family,10.0,,,,399,"The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.

Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience",,n,,n,,,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.,Patients recruited in hospital or outpatient treatment facility,,Yes,,,n,y,n,,y,2024.0,1st_search_14,1st_search_138
Chatbot,,Client-facing application,,,,,N,,,,,N,India,Self-collected data,27.0,Unselected,Yes,Y,"f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).",N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",Y,"we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022)",N,,N,,N,,Prompting + other modules,Other: Perception of AI-generated responses in mental health support,No,N,,N,,,,,N,English,,,,N,,,,,,,quantification of effect,,,,,,,,,,GPT-3.5 family,4.0,,"Name: none specified. Description: Self-collected open-ended mental-health problem statements and corresponding ChatGPT- and human-generated responses evaluated by participants.
",,111 individuals (82 females and 29 males),"Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.",,N,,N,,,,,,Journal paper,,,,Reviewer Two,N,,Population survey,Y,"Although proper care was taken that no 
triggering problem was used in the survey, the participants were also 
informed about the counseling services available at our university if the 
need arose. As the responses were circulated to experts, no such 
comment which was rejected on this basis (misleading or harmful) was 
included in the study.",Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses,General population,Internet data -- mental health Q&A,Yes,"Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).",,N,Y,Y,Lay people,Y,2024.0,1st_search_13,1st_search_139
Chatbot,,Client-facing application,,,,,n,,,,,n,India,Self-collected data,27.0,Unselected,No,n,,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,,"Informal counseling (e.g., emotional support conversation)",No,n,,n,,,,,n,English,,,,n,,,,,,,,,,,,,,,,,"Other: They say ""ChatGPT"". Not clear which version. Most likely 3.5 because they mention ""Building on our previous exploration of GPT-3.5, one of the most advanced Natural Language Processing (NLP) technologies, this follow-up study aims to understand the perception dynamics of AI-generated responses in the realm of mental health support for young people.""",4.0,,"Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown.",,111,"Users rated AI responses more authentic, professional, and prarctical when blinded. when unblinded, the rated human responses higher (except professionalism).",,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Population survey,n,,Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses,"Other: young adults, convenience sampling, online forms and in-person visits to classrooms",Other: emotional experience descriptions,Yes,,,n,y,n,Other: not applicable,y,2024.0,1st_search_13,1st_search_140
Chatbot,,Client-facing application,,,,,N,,,,,N,India,Self-collected data,27.0,Unselected,Yes,n,"f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).",N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,"we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022)",N,,N,,N,,Other: ,"Informal counseling (e.g., emotional support conversation)",No,N,,N,,,,,N,English,,,,N,,,,,,,,,,,,,,,,,GPT-3.5 family,4.0,,"Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown.

Name: none specified. Description: Self-collected open-ended mental-health problem statements and corresponding ChatGPT- and human-generated responses evaluated by participants.",,111,"Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.",,N,,N,,,,,,Journal paper,,,,Consensus,N,,Population survey,n,,Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses,General population,Other: emotional experience descriptions,Yes,"Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).",,n,y,Y,Trained professionals,y,2024.0,1st_search_13,1st_search_141
,,Client-facing application,,,,,N,,,,,N,USA,Self-collected data,18.0,,,,,,,,,,N,,,,N,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,N,,,,,N,,,,,,,First refferal to a human,,,,Shutdown of conversation,,,,,,GPT-3.5 family,12.0,,,,,,,N,,N,,,,,,Journal paper,,,,Reviewer Two,y,,Empirical research involving an LLM,y,,Safety of Large Language Models in Addressing Depression.,No clients/patients involved,,No,,,,,,,,2023.0,1st_search_9,1st_search_142
Chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,18.0,,,n,,n,,,,,n,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,n,,,,,n,,,,,no benchmark,no benchmark,safety (number of conversations turns until initial referral/shutdown of chatbot),no benchmark,,,,,,,,,GPT-3.5 family,12.0,,"""dataset"" were PHQ-9 questions",,,,,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Safety of Large Language Models in Addressing Depression.,No clients/patients involved,,No users involved,,,,,,,,2023.0,1st_search_9,1st_search_143
Chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,18.0,,,n,,n,,,,,N,,,,N,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,N,,,,,N,,,,,,,safety (number of conversations turns until initial referral/shutdown of chatbot),no benchmark,,,,,,,,,GPT-3.5 family,12.0,,"""dataset"" were PHQ-9 questions",,,,,N,,N,,,,,,Journal paper,,,,Consensus,y,,Empirical research involving an LLM,n,,Safety of Large Language Models in Addressing Depression.,No clients/patients involved,,No users involved,,,,,,,,2023.0,1st_search_9,1st_search_144
Chatbot,,Client-facing application,,,,,,,,,,,Other: Saudi Arabia,Self-collected data,31.0,Psychopathology,,Yes ,Table S1 and Appendix B,,,,,,,,,,,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",,,,,,,,,,"CBT: Cognitive restructuring; Psychodynamic psychotherapy; Systemic therapy; Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Other: ChatGBT as a therapist",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,GPT-3.5 family,1.0,,"24 patients were selected to participate in the study. Before participating in an interview, the patients were thoroughly 
informed of the study’s objectives and given the opportunity to provide informed consent during their outpatient visit. 
Following approval, patients used ChatGPT at home for two weeks to seek mental-health support in their own ways. The 
participants were required to interact with ChatGPT3 for a minimum of 15 minutes per day for 14 days. Twenty to thirty 
interviews were regarded an appropriate sample size for qualitative investigations, particularly those employing inter­
views as a method of data collection.
",,24,"semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT 
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.",,N,,N?,"issues such as data privacy, 
consent, confidentiality, and the potential for biases in AI algorithms are the few challenges raised by most of the 
participants. These are inferred from the following statements",,,,,Journal paper,,,,Reviewer Two,,,Population survey,,,Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,,Yes,,,,,,,,2024.0,1st_search_3,1st_search_145
Chatbot,,Client-facing application,,,,,n,,,,,n,Other: Saudi Arabia,No dataset used for development or evaluation,31.0,,,y,"Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B",n,,,,,n,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,GPT-3.5 family,1.0,"Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory",,,24,"Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. ""It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information.""",,n,,n,,,,,,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,Patients recruited in hospital or outpatient treatment facility,,Yes,,,y,n,n,,y,2024.0,1st_search_3,1st_search_146
Chatbot,,Client-facing application,,,,,n,,,,,n,Other: Saudi Arabia,No dataset used for development or evaluation,31.0,Other: ,,y,"Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B",n,,,,,n,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,,,,,,,,,,,,,,GPT-3.5 family,1.0,"Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory


Therapy type: undirected therapy resulting from interacting with ChatGPT with no prespecified initial prompt",,,24,"Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. ""It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information.""

semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.",,n,,n,"issues such as data privacy, 
consent, confidentiality, and the potential for biases in AI algorithms are the few challenges raised by most of the 
participants. These are inferred from the following statements",,,,,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,Patients recruited in hospital or outpatient treatment facility,,Yes,,,y,n,n,,y,2024.0,1st_search_3,1st_search_147
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,UK,No dataset used for development or evaluation,27.0,,,Y,"Nineteen participants (12 male, 7 female)… age 17 to 60… eight countries…",,,,,,,,,,,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,"Other: no model development, qualitative interviews only","Unspecified, might include formal therapy methods",,,,,,,,,,,,,,N,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified; Other: Pi, Copilot, Kindroid, ChatMind/VOS ",10.0,,,"Study observes consumer LLM chatbots in the wild (Pi, ChatGPT, Copilot, Kindroid, ChatMind/VOS), not building a new system. ",19," range of positive impacts were reported, including improved mood… healing from trauma and loss… improved relationships…

Participants told us that generative AI feels like an emotional sanctuary, offers insightful guidance… joy of connection… bears comparison with human therapy.",,,,,,,,,,Journal paper,,,,Reviewer Two,,,Population survey,,,"""It happened to be the perfect thing"": experiences of generative AI chatbots for mental health.",General population,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_258,2nd_search_1
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,UK,No dataset used for development or evaluation,27.0,,,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified; Other: Pi, Copilot, Kindroid, ChatMind",10.0,,,,19,Qualitative thematic coding of interview results,,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Population survey,,,"""It happened to be the perfect thing"": experiences of generative AI chatbots for mental health.",General population,,Yes,,,y,n,n,,y,2024.0,2nd_search_258,2nd_search_2
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,UK,No dataset used for development or evaluation,27.0,,,,,,,,,,n,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified; Other: Pi, Copilot, Kindroid, ChatMind/VOS ",10.0,,,,19," range of positive impacts were reported, including improved mood… healing from trauma and loss… improved relationships…

Participants told us that generative AI feels like an emotional sanctuary, offers insightful guidance… joy of connection… bears comparison with human therapy.",,,,,,,,,n,Journal paper,,,,Consensus,,,Population survey,,,"""It happened to be the perfect thing"": experiences of generative AI chatbots for mental health.",General population,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_258,2nd_search_3
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,"External data set, modified",11.0,Other: Not applicable; AI generated,Yes,N,,N,,,,,N,L,B,Table 4; Benchmarks are ChatGLM3-6b and LLaMA2-7b-chat,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,CBT: Cognitive restructuring,Yes,N,,N,,-,-,-,Y,English,,,,N,,,,N,,,,,,,,,,,,,Llama 2 family,8.0,,"In this study, we leverage an existing raw dataset
focused on cognitive reframing and expand it to
include multiple rounds of dialogue. Specifically,
we conduct a manual review of the selected raw
dataset and select 1,000 well-composed pairs of
(thinking trap, client’s thought) from it.
The raw dataset we utilize in this study is intro-
duced by Maddela et al. (2023). 
https://aclanthology.org/2023.acl-long.763.pdf

Creation of HealMe dataset ","approach by
selecting the open-source LLM, LLaMA2-7b-chat,
as its base and fine-tuning it to ensure the model
consistently maintains the role of a psychotherapist
with high empathy and guidance capabilities.",6,,,Y,,N,,,,,N,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Healme: Harnessing cognitive reframing in large language models for psychotherapy,General population,Emotional support dialogue -- chat logs,No,,,,,,Other: Not applicable; AI generated,,2024.0,2nd_search_251,2nd_search_4
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,"External data set, modified",11.0,Other: not applicable,Yes,n,,n,,,,,n,l,b,"benchmarks: ChatGLM3-6b, Llama2-7b-Chat",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",y,PANAS,y,two people not undergoing intervention,n,,n,,Only fine-tuning,CBT: Cognitive restructuring,Yes,n,,n,,no benchmark,no benchmark,Only training data is scored using GPT-4 LLM-as-a-judge (table 2),y,English,,,,n,,,,n,,,,,,,,,,,,,Llama 2 family,8.0,,"HealMe dataset
https://github.com/elsa66666/HealMe",HealMe = fine-tuned Llama2-7B-Chat,8,,,y,,y,"Since our testing phases involve real-person clients, we exclusively use offline models to protect user privacy. In our study involving real-person clients, we adhere to the Right to Withdraw (Association et al., 2017), ensuring that participants can withdraw at any time if they experience any discomfort",,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Healme: Harnessing cognitive reframing in large language models for psychotherapy,General population,Emotional support dialogue -- chat logs,No,,,,,,Other: not applicable,,2024.0,2nd_search_251,2nd_search_5
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,"External data set, modified",11.0,Other: not applicable,Yes,N,,N,,,,,N,L,B,Table 4; Benchmarks are ChatGLM3-6b and LLaMA2-7b-chat,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",y,PANAS,y,two people not undergoing intervention,N,,N,,Only fine-tuning,CBT: Cognitive restructuring,Yes,N,,N,,no benchmark,no benchmark,Only training data is scored using GPT-4 LLM-as-a-judge (table 2),Y,English,,,,N,,,,N,,,,,,,,,,,,,Llama 2 family,8.0,,"HealMe dataset
https://github.com/elsa66666/HealMe

In this study, we leverage an existing raw dataset
focused on cognitive reframing and expand it to
include multiple rounds of dialogue. Specifically,
we conduct a manual review of the selected raw
dataset and select 1,000 well-composed pairs of
(thinking trap, client’s thought) from it.
The raw dataset we utilize in this study is intro-
duced by Maddela et al. (2023). 
https://aclanthology.org/2023.acl-long.763.pdf

Creation of HealMe dataset ","HealMe = fine-tuned Llama2-7B-Chat

approach by
selecting the open-source LLM, LLaMA2-7b-chat,
as its base and fine-tuning it to ensure the model
consistently maintains the role of a psychotherapist
with high empathy and guidance capabilities.",8,,,Y,,N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Healme: Harnessing cognitive reframing in large language models for psychotherapy,General population,Emotional support dialogue -- chat logs,No,,,,,,Other: not applicable,,2024.0,2nd_search_251,2nd_search_6
,,Other: unsure,,L,B,"Quantitative F1 decrease in ablation study shows impact of each stage.
",Y,,,,,,USA,External data set,3.0,Unknown,Other: unclear,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,Y,"""Compared to baseline prompting methods (Zeroshot, Few-shot, ZeroCoT).""
",,,,,Only prompting,"Unspecified, might include formal therapy methods",No,,,,,,,,,Other: unclear,,,,,,,,,L,B,"ablation performance comparison
??","Performance drop when removing CoI stages (ID, IA, VA).
",,,,,,,,,"Other: Llama2-13B-Chat [12], Falcon-7B-Instruct [20], Mistral-7B-
Instruct [21], and ChatGPT [22)",6.0,,"We conduct extensive experiments on datasets derived from
real-world MI sessions addressing alcohol usage disorder. We
examine three prompting baselines and experiment with four
state-of-the-art auto-regressive LLMs, including Llama2 [12],
Falcon [20], Mistral [21], and ChatGPT [22]. ","CoI breaks MI coding into three sequential prompt stages (Interaction Definition, Involvement Assessment, Valence Analysis) to mimic professional coders using MISC schema.",,,,Y (mostly),"Llama2-13B-Chat, Falcon-7B-Instruct, Mistral-7B-Instruct (open-source).",,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,No clients/patients involved,Other: unclear,No users involved,,,,,,Unknown,,2024.0,2nd_search_232,2nd_search_7
,,Analysis of conversation transcripts,,l,b,Metric: Micro- and macro-F1. Benchmark: zero- and few-shot prompting with the same LLMs.,y,,,,,n,USA,External data set,3.0,Unselected,No,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,CBT: Motivational interviewing,No,,,,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; Llama 2 family; Mistral family; Other: Falcon-7B,6.0,,"Motivational interviewing session transcripts from Borsari et al., “In-session processes of brief motivational interventions in two trials with mandated college students.” Journal of consulting and clinical psychology, vol. 83, pp. 56–67, 2 2015","Different styles of prompting: Zero-shot, few-shot, CoT",,,,,,,,,,,n,Conference paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_232,2nd_search_8
,,Analysis of conversation transcripts,,l,b,"Metric: Micro- and macro-F1. Benchmark: zero- and few-shot prompting with the same LLMs.

Quantitative F1 decrease in ablation study shows impact of each stage.
",y,,,,,n,USA,External data set,3.0,Unselected,No,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,Y,"""Compared to baseline prompting methods (Zeroshot, Few-shot, ZeroCoT).""
",,,,,Only prompting,CBT: Motivational interviewing,No,,,,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; Llama 2 family; Mistral family; Other: Falcon-7B,6.0,,"We conduct extensive experiments on datasets derived from
real-world MI sessions addressing alcohol usage disorder. We
examine three prompting baselines and experiment with four
state-of-the-art auto-regressive LLMs, including Llama2 [12],
Falcon [20], Mistral [21], and ChatGPT [22]. 

Motivational interviewing session transcripts from Borsari et al., “In-session processes of brief motivational interventions in two trials with mandated college students.” Journal of consulting and clinical psychology, vol. 83, pp. 56–67, 2 2015","CoI breaks MI coding into three sequential prompt stages (Interaction Definition, Involvement Assessment, Valence Analysis) to mimic professional coders using MISC schema.

Different styles of prompting: Zero-shot, few-shot, CoT",,,,Y,"Llama2-13B-Chat, Falcon-7B-Instruct, Mistral-7B-Instruct (open-source).",,,,,,n,Conference paper,,,,Consensus,,,Empirical research involving an LLM,,,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_232,2nd_search_9
Multi-turn chatbot,,Client-facing application,,,,,,,L,B,"Network centrality and average distance values reported (Table 1 and 2).
",Y,Other: Italy,External data set,31.0,Psychopathology,No,,,,,,,,,,,,,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",,,,,,,,,Only prompting,Other CBT techniques,Yes,,,,,,,,,English,,,,,L,B,"Captured via semantic richness (degree) in Table 2.
",Y,,,,,,,,,,,,,GPT-3.5 family; Claude family,1.0,"Interestingly, Haiku and ChatGPT, while playing either therapists or 
patients and speaking in English, rarely use jargon expressing anger or 
disgust. Instead, human patients express jargon eliciting disgust and 
anger at higher rates, compatible with random expectations. This dif­
ference could be due to English LLMs might have been fine-tuned to 
more strongly avoid using a negative outlook on things.","HOPE (Malhotra, 2022)","Two adverserial LLMs generate new counseling dataset. 
Prompted ChatGPT-3.5, Claude Haiku and LLaMAntino to role-play therapist and patient in depression sessions; compared against human HOPE dataset using text-network metrics. ",,,,Y(partly),LLaMAntino (Italian LLaMA 2 model) is,,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans",No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_230,2nd_search_10
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Italy,External data set,31.0,Unselected,No,n,,n,,,,,n,,,,n,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,h,unclear,various text analyses,benchmark: human therapist responses,h,unclear,various network analyses,benchmark: human therapist responses,,,,,GPT-3.5 family; Claude family,1.0,,HOPE dataset,,,,,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans",No clients/patients involved,Emotional support dialogue -- speech transcripts,No users involved,,,,,,Unknown,,2025.0,2nd_search_230,2nd_search_11
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Italy,External data set,31.0,Unknown,No,n,,n,,,,,n,,,,n,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,h,unclear,various text analyses,benchmark: human therapist responses,,,,,h,s,network centrality (distance from depression terms),Network centrality and average distance values reported (Table 1 and 2).,GPT-3.5 family; Llama 2 family; Claude family,1.0,"Interestingly, Haiku and ChatGPT, while playing either therapists or 
patients and speaking in English, rarely use jargon expressing anger or 
disgust. Instead, human patients express jargon eliciting disgust and 
anger at higher rates, compatible with random expectations. This dif­
ference could be due to English LLMs might have been fine-tuned to 
more strongly avoid using a negative outlook on things.","HOPE (Malhotra, 2022)","Two adverserial LLMs generate new counseling dataset. 
Prompted ChatGPT-3.5, Claude Haiku and LLaMAntino to role-play therapist and patient in depression sessions; compared against human HOPE dataset using text-network metrics. ",,,,y,LLaMAntino (Italian LLaMA 2 model) is,n,,,,,n,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans",No clients/patients involved,Emotional support dialogue -- speech transcripts,No users involved,,,,,,Unknown,,2025.0,2nd_search_230,2nd_search_12
Multi-turn chatbot,,Client-facing application,,,,,,,L,B,Information entropy of dialogue topics… SMILE 15.02 vs standard 8.40,Y,China,Self-collected data,12.0,Psychopathology,Yes,,,,,L,B,BERTScore used and increased; also pairwise cosine similarity for semantic diversity analysis,Y,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,Chinese,L,B,"BLEU-1/2/3, ROUGE-L improved after fine-tuning",Y,L,B,"Tab. 5

Distinct-1/2/3… Fine-tuned 82.08/95.74/97.81 vs Baseline 52.95/80.74/90.17… §5.1–5.3",Y,,,N,,,,,,,,,,"GPT-3.5 family; Qwen family; Other: ChatGLM2-6B, ",11.0,,SMILECHAT based on bootstrapped PsyTest and source PsyQA ,Uses ChatGPT prompting to synthesize SMILECHAT from PsyQA single-turn items; topics auto-labeled with Qwen1.5-110B-Chat; trains MeChat via parameter-efficient LoRA fine-tuning on ChatGLM2-6B. ,,,,Y,"fine-tuning experiment on ChatGLM2-6B (open, self-hostable)",,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_228,2nd_search_13
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,"External data set, modified",12.0,Other: not applicable,Yes,n,,n,,l,b,benchmark is some baseline model,y,h,w,benchmark are responses of human counselors in PsyTest,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,Chinese,l,b,benchmark is some baseline model,y,l,b,benchmark is some baseline model,y,,,,,,,,,,,,,Other: ChatGLM2-6B,11.0,,"The study uses the PsyQA dataset and creates a new one called SMILECHAT.

https://github.com/qiuhuachuan/smile",fine-tunes a ChatGLM2-6B model on synthetically generated data,,,,y,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Other: not applicable,,2024.0,2nd_search_228,2nd_search_14
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,"External data set, modified",12.0,Other: not applicable,Yes,n,,n,,L,b,"BERTScore used and increased; also pairwise cosine similarity for semantic diversity analysis

benchmark is some baseline model",Y,h,w,benchmark are responses of human counselors in PsyTest,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,Chinese,L,b,"BLEU-1/2/3, ROUGE-L improved after fine-tuning

benchmark is some baseline model",Y,L,B,"Tab. 5

Distinct-1/2/3… Fine-tuned 82.08/95.74/97.81 vs Baseline 52.95/80.74/90.17… §5.1–5.3",Y,,,,,,,,,,,,,Other: ChatGLM2-6B,11.0,,"The study uses the PsyQA dataset and creates a new one called SMILECHAT.

https://github.com/qiuhuachuan/smile",fine-tunes a ChatGLM2-6B model on synthetically generated data,,,,y,"fine-tuning experiment on ChatGLM2-6B (open, self-hostable)",n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Other: not applicable,,2024.0,2nd_search_228,2nd_search_15
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,25.0,,,,,,,,,,,H,none,"“reviewers assessed ChatGPT’s psychoeducational responses …” 
",Y ,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",4.0,,,"ChatGPT responses collected for 21 psychoeducational prompts across categories (e.g., depression, substance use, wellness); evaluated on six criteria (accuracy, clarity, relevance, empathy, engagement, ethics) via qualitative content analysis (QCA) and Likert ratings.",,,,,,,,,,,,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,,,Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_219,2nd_search_17
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,25.0,,,n,,n,,,,,n,no benchmark,no benchmark,no benchmark,y,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family,4.0,,,Prompting of ChatGPT with mental health related questions (Table 1),,,,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_219,2nd_search_18
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,25.0,,,n,,n,,,,,n,no benchmark,no benchmark,"“reviewers assessed ChatGPT’s psychoeducational responses …” 
",y,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family,4.0,,,"ChatGPT responses collected for 21 psychoeducational prompts across categories (e.g., depression, substance use, wellness); evaluated on six criteria (accuracy, clarity, relevance, empathy, engagement, ethics) via qualitative content analysis (QCA) and Likert ratings.

Prompting of ChatGPT with mental health related questions (Table 1)",,,,n,,n,,,,,n,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_219,2nd_search_19
,,,,,,,,,,,,,Other: Israel,,2.0,,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,5.0,,,,,,,,,,,,,,,Conference paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Combining Psychological Theory with Language Models for Suicide Risk Detection,,,,,,,,,,,2023.0,2nd_search_217,2nd_search_20
,,Analysis of conversation transcripts,,l,b,f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.,y,,,,,n,USA,External data set,6.0,Unknown,No,,,,,,,,n,no benchmark,no benchmark,4.3 Human Evaluation Results,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,CBT: Cognitive restructuring,No,,,,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,12.0,,Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.,,,,,,,,,,,,n,Conference paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,No clients/patients involved,Other: labeled cognitive distortions,No users involved,,,,,,Other: not applicable,,2023.0,2nd_search_214,2nd_search_21
,,Analysis of conversation transcripts,,,,,Y,,,,,,USA,External data set,6.0,Unknown,No,,,,,,,,,,,,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,CBT: Cognitive restructuring,Yes,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family; Other: Vicuna,12.0,,"We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset","We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2023.0,2nd_search_214,2nd_search_22
,,Analysis of conversation transcripts,,l,b,f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.,Y,,,,,n,USA,External data set,6.0,Unknown,No,,,,,,,,n,no benchmark,no benchmark,4.3 Human Evaluation Results,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,CBT: Cognitive restructuring,Yes,,,,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,12.0,,"We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset

Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.

https://www.kaggle.com/datasets/sagarikashreevastava/cognitive-distortion-detetction-dataset","We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).",,,,,,,,,,,n,Conference paper,,,,Consensus,,,Empirical research involving an LLM,,,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,No clients/patients involved,Other: labeled cognitive distortions,No users involved,,,,,,Other: not applicable,,2023.0,2nd_search_214,2nd_search_23
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Czech Republic,External data set,14.0,Other: all of the above,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,English,L,B,only the inhibited LoRA Finetuning,Y,,,,,L,B,Fluency,,,,,,,,,,Llama 2 family; Other: ChatGLM2-7,4.0,,https://alexanderstreet.com/,"to fine-tune the generated instruction
data effectively, we employed the inhibition adaption fine-
tuning method [14] and self-RAG [13] on Llama2-7B [15],
as well as ChatGLM2-6B [16].",,,,Y,on Llama2-7B… as well as ChatGLM2-6B,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Other: all of the above,,2024.0,2nd_search_212,2nd_search_24
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Czech Republic,External data set,14.0,Unknown,No,n,,n,,,,,n,no benchmark,no benchmark,"human rating (read, prof, match)",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,English,no benchmark,no benchmark,Rouge comparison to Alexander Street Press reference. No benchmark.,y,,,,n,no benchmark,no benchmark,Fluency,Not clear what fluency is. The reference paper measures fluency via human rating but the authors of this paper state this is an automatic assessment.,,,,,,,,,GPT-4 / GPT-4o family; Llama 2 family; Other: ChatGLM2-6B,4.0,,Alexander Street Press therapy transcripts,Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4),,,,y,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_212,2nd_search_25
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Czech Republic,External data set,14.0,Unknown,No,n,,n,,,,,n,no benchmark,no benchmark,"human rating (read, prof, match)",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,English,no benchmark,no benchmark,"Rouge comparison to Alexander Street Press reference. No benchmark.

only the inhibited LoRA Finetuning",Y,,,,n,no benchmark,no benchmark,Fluency,Not clear what fluency is. The reference paper measures fluency via human rating but the authors of this paper state this is an automatic assessment.,,,,,,,,,GPT-4 / GPT-4o family; Llama 2 family; Other: ChatGLM2-6B,4.0,,Alexander Street Press therapy transcripts,"Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4)

to fine-tune the generated instruction
data effectively, we employed the inhibition adaption fine-
tuning method [14] and self-RAG [13] on Llama2-7B [15],
as well as ChatGLM2-6B [16].",,,,y,on Llama2-7B… as well as ChatGLM2-6B,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_212,2nd_search_26
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods,,,,,,,,,,,,2nd_search_206,2nd_search_27
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods,,,,,,,,,,,,2nd_search_206,2nd_search_28
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods,,,,,,,,,,,,2nd_search_206,2nd_search_29
Multi-turn chatbot,,Client-facing application,,L,B,Highest average standard accuracy; >60% pass; elastic accuracy also strong,Y,,,,,,China,"External data set, modified",2.0,Other: both,No,,,,,L,B,Highest BERTScore among compared models.,Y,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,Chinese,L,B,ROUGE-1 and BLEU-4 highest for PsycoLLM; ROUGE-L slightly lower than EmoLLM,Y,,,,,,,Elastic accuracy for MMCQ ??,,,,,,,,,,Qwen family,12.0,,"we collect question-answer pairs from open platforms… Yixinli… Zhihu… We crawl books related to psychology from the web and then use Qwen-72B to extract knowledge-based QA… After-school exercises from several books

There are several publicly accessible websites committed to
establishing a psychology platform and offering online solu-
tions for individuals seeking psychological assistance, such as
Yixinli3, Zhihu4, and so on. The data from these platforms
can be regarded as real-world inquiries and the solutions pro-
vided by professionals in response to these inquiries.
ubse-
quently, professional or experienced individuals offer detailed
solutions or advice in response to these questions. We collect
over 267 000 pairs of data from websites.
To accurately evaluate the model performance in the psy-
chology domain, we introduce a benchmark that psychological
professional individuals need to master. The data is sourced
from publicly available examination questions. We follow a standardized data preprocessing procedure. For some data, we
first need to perform optical character recognition (OCR) to
convert images into text. After that, we invite several students to
manually review the collected data to ensure its quality and con-
sistency with the original document. This process involves rec-
tifying formatting errors, eliminating duplicate questions, and
rectifying any instances of garbled characters. Our proposed
psychological benchmark draws inspiration from the format of
the most authoritative psychological counseling examination in
China, comprising two primary components: objective ques-
tions and subjective questions.","Full-parameter supervised fine-tuning (1e-5 LR; batch 128; 3 epochs) on 8×A6000; pipeline for multi-turn data (generation → evidence judgment → refinement), plus teacher–student with and without RAG for knowledge QA",,,,Y,Qwen is,,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,No clients/patients involved,Emotional support dialogue -- speech transcripts,No users involved,,,,,,"Other: Trainded pros are for sure included, but others might also be. ",,2024.0,2nd_search_202,2nd_search_30
Other: clinical psychology-specific general-purpose LLM,,Client-facing application,,l,b,"""Elastic accuracy"" on their QA dataset.",y,,,,,n,China,"External data set, modified",3.0,Unselected,Yes,n,,n,,l,b,Metrics: BERTScore. Benchmarks: Various other LLMs,y,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Chinese,l,b,"Metrics: Rouge-1, Rouge-L, BLEU-4. Benchmarks: Various other LLMs",y,,,,n,,,,,,,,,,,,,Other: Qwen1.5-14B-Chat,4.0,,"Data sourced from single-turn QA psychology platforms, multi-turn dialogue synthetically generated using the single-turn data, and psychological knowledge QA",Only fine-tuning of Qwen on their dataset,,,,y,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,No clients/patients involved,Other: mixed: multi-turn and QA,No users involved,,,,,,Other: mixed,,2025.0,2nd_search_202,2nd_search_31
Multi-turn chatbot,,Client-facing application,,L,B,"""Elastic accuracy"" on their QA dataset.
Highest average standard accuracy; >60% pass; elastic accuracy also strong",Y,,,,,n,China,"External data set, modified",2.0,Unselected,Yes,n,,n,,L,B,"Metrics: BERTScore. Benchmarks: Various other LLMs
Highest BERTScore among compared models.",Y,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Chinese,L,B,"Metrics: Rouge-1, Rouge-L, BLEU-4. Benchmarks: Various other LLMs
ROUGE-1 and BLEU-4 highest for PsycoLLM; ROUGE-L slightly lower than EmoLLM",Y,,,,n,,,,,,,,,,,,,Qwen family,12.0,,"Data sourced from single-turn QA psychology platforms, multi-turn dialogue synthetically generated using the single-turn data, and psychological knowledge QA

we collect question-answer pairs from open platforms… Yixinli… Zhihu… We crawl books related to psychology from the web and then use Qwen-72B to extract knowledge-based QA… After-school exercises from several books

There are several publicly accessible websites committed to
establishing a psychology platform and offering online solu-
tions for individuals seeking psychological assistance, such as
Yixinli3, Zhihu4, and so on. The data from these platforms
can be regarded as real-world inquiries and the solutions pro-
vided by professionals in response to these inquiries.
ubse-
quently, professional or experienced individuals offer detailed
solutions or advice in response to these questions. We collect
over 267 000 pairs of data from websites.
To accurately evaluate the model performance in the psy-
chology domain, we introduce a benchmark that psychological
professional individuals need to master. The data is sourced
from publicly available examination questions. We follow a standardized data preprocessing procedure. For some data, we
first need to perform optical character recognition (OCR) to
convert images into text. After that, we invite several students to
manually review the collected data to ensure its quality and con-
sistency with the original document. This process involves rec-
tifying formatting errors, eliminating duplicate questions, and
rectifying any instances of garbled characters. Our proposed
psychological benchmark draws inspiration from the format of
the most authoritative psychological counseling examination in
China, comprising two primary components: objective ques-
tions and subjective questions.",Only fine-tuning of Qwen on their dataset,,,,y,Qwen is,n,,,,,n,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,No clients/patients involved,Other: mixed: multi-turn and QA,No users involved,,,,,,Other: mixed,,2024.0,2nd_search_202,2nd_search_32
,,Analysis of conversation transcripts,,,,,,,L,S,BLEURT ↑ with KB; BartScore ↓,Y,China,Self-collected data,3.0,Unknown,No,,,,,L,B,"BERTScore higher with KB, esp. ChatGPT.",Y,,,,,,,,,,,,,,Prompting + other modules,Other CBT techniques,No,,,,,,,,,Other: English and Chinese,L,B,ROUGE/METEOR improved esp. for ChatGPT; mixed for others.,Y,,,,,L,B,Sentiment score & PQA ,,,,,,,,,,"GPT-3.5 family; Other: ERNIE-3.5-8K,
iFlytek Spark V3.0 and ChatGLM-3-turbo,",12.0,,"The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.",constructed a CBT-related knowledge base and integrated it with the general models … RAG (Retrieval-Augmented Generation),,,,,,,,,,,,,,,,Reviewer Two,N,,Empirical research involving an LLM,,,Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?,No clients/patients involved,Emotional support dialogue -- speech transcripts,No users involved,,,,,,Unknown,,2024.0,2nd_search_193,2nd_search_33
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,External data set,3.0,Unknown,"Other: mix of ""authentic"" and role-played",n,,n,,no benchmark,no benchmark,no benchmark,y,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: Chinese and English mixed,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,DISTINCT score. no benchmark,y,no benchmark,no benchmark,BARTScore,no benchmark,no benchmark,no benchmark,BLEURT,no benchmark,,,,,"GPT-3.5 family; Other: Ernie-3.5-8K, iFlytek Spark V3.0, ChatGLM-3-turbo",12.0,,"The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.",Prompting with and without RAG,,,,y,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?,No clients/patients involved,Emotional support dialogue -- speech transcripts,No users involved,,,,,,Unknown,,2024.0,2nd_search_193,2nd_search_34
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,"External data set, modified",3.0,Unknown,"Other: mix of ""authentic"" and role-played",n,,n,,no benchmark,no benchmark,no benchmark,y,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Prompting + other modules,Other CBT techniques,No,n,,n,,,,,n,Other: English and Chinese,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,DISTINCT score. no benchmark,y,no benchmark,no benchmark,BARTScore,no benchmark,no benchmark,no benchmark,BLEURT,no benchmark,,,,,"GPT-3.5 family; Other: ERNIE-3.5-8K,
iFlytek Spark V3.0, ChatGLM-3-turbo",12.0,,"The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.","Prompting with and without RAG

constructed a CBT-related knowledge base and integrated it with the general models … RAG (Retrieval-Augmented Generation)",,,,y,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?,No clients/patients involved,Emotional support dialogue -- speech transcripts,No users involved,,,,,,Unknown,,2024.0,2nd_search_193,2nd_search_35
,Patient simulations,Therapist-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,29.0,,,,,,,,,,,,,,,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,,,,,,N,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",12.0,,,"Model was prompted to behave like a counseling client: It was then tested in a fishbowl, triad, and one-to-one formats using facilitation commands (“play,” “pause,” “rewind”) and three processing techniques (“ChatGPT close up,” “counselor close up,” “observer close up”) grounded in Bernard’s Discrimination Model (1979)",,,,,,,,,,,,Journal paper,,,,Reviewer Two,N,,Other: Exploratory/pedagogical/model paper (not an empirical evaluation with recruited participants) ,,,Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play,,,No users involved,,,,,,,,2023.0,2nd_search_186,2nd_search_36
,Patient simulations,Therapist-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,29.0,,,,,,,,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified",12.0,,,Asking ChatGPT to act as a client,,,,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play,,,No users involved,,,,,,,,2023.0,2nd_search_186,2nd_search_37
,Patient simulations,Therapist-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,29.0,,,,,,,,,,n,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified",12.0,,,"Model was prompted to behave like a counseling client: It was then tested in a fishbowl, triad, and one-to-one formats using facilitation commands (“play,” “pause,” “rewind”) and three processing techniques (“ChatGPT close up,” “counselor close up,” “observer close up”) grounded in Bernard’s Discrimination Model (1979)

Asking ChatGPT to act as a client",,,,,,,,,,,n,Journal paper,,,,Consensus,,,Empirical research involving an LLM,,,Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play,,,No users involved,,,,,,,,2023.0,2nd_search_186,2nd_search_38
Other: MI reflection generation,,Client-facing application,,,,,n,,,,,n,Other: Canada,"External data set, modified",17.0,Psychopathology,Yes,n,,n,,,,,n,no benchmark,no benchmark,no benchmark,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning; Only prompting,CBT: Motivational interviewing,No,n,,n,,no benchmark,no benchmark,no benchmark,y,English,,,,n,,,,n,,,,,,,,,,,,,GPT-2 family; GPT-4 / GPT-4o family,3.0,,"Only questions and answers from MI chatbot reflections by Brown 2023 -- A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study.
The authors generate reflections based on these question-answer pairs using GPT-4","Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.",,,,y,One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",No clients/patients involved,Other: MI reflections,No users involved,,,,,,Other: not applicable,,2024.0,2nd_search_185,2nd_search_39
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Canada,Self-collected data,17.0,Unknown,Yes,,,,,,,,,H,B,human annotators,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,CBT: Motivational interviewing,No,,,,,,,,,English,,,,,,,,,,,Cohen Kappa,,,,MI-adherence,,,,,,GPT-2 family; GPT-4 / GPT-4o family,3.0,,"""Mentioned previously, we use transcripts
from the smoking cessation MI chatbot created by
the authors (Brown et al., 2023). ""","e.g. ""After gathering the dataset of MI conversation
questions, answers, and GPT-4-generated reflec-
tions, we use fine-tuning to distill that reflection
capability in a student model.""",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model","Other: ''We recruited five annotators to evaluate reflections
from GPT-4 and each distilled student model. The
five annotators consist of four males and one fe-
male at an average age of 23, located in North
America. Each annotator has a basic understand-
ing of MI having read (Miller and Rollnick, 2012) and taken coursework.""",Emotional support dialogue -- speech transcripts,No,,,,,,Unknown,,2024.0,2nd_search_185,2nd_search_40
Other: MI reflection generation,,Client-facing application,,,,,n,,,,,n,Other: Canada,"External data set, modified",17.0,Psychopathology,Yes,n,,n,,,,,n,no benchmark,no benchmark,no benchmark,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning; Only prompting,CBT: Motivational interviewing,No,n,,n,,no benchmark,no benchmark,no benchmark,y,English,,,,n,,,,n,no benchmark,no benchmark,llm-as-a-judge mi_adherence,no benchmark,,,,,,,,,GPT-2 family; GPT-4 / GPT-4o family,3.0,,"""Mentioned previously, we use transcripts
from the smoking cessation MI chatbot created by
the authors (Brown et al., 2023). ""

Only questions and answers from MI chatbot reflections by Brown 2023 -- A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study.
The authors generate reflections based on these question-answer pairs using GPT-4","Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.",,,,y,One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",No clients/patients involved,Other: MI reflections,No users involved,,,,,,Other: not applicable,,2024.0,2nd_search_185,2nd_search_41
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,10.0,,,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Prompting + other modules,CBT: Cognitive restructuring,,Y," Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible.",N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,"The addition of multiple
collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s
behavior. Iterative prompt engineering was needed and used to
improve the individual agent’s behavior and how they worked
together.",6 Patients (+6 Clinicians),"See ""Initial User Feedback""",,N,,Y,"Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy. All
user data are transferred through secure and encrypted https.
All user data are always encrypted and then stored in databases.
Users are also encouraged to refrain from providing personally
identifiable information or personal health information when
they log into Socrates 2.0’s web interface to further decrease
any potential risks",,,,N,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,Y,"We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user",A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study,General population; Other,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_180,2nd_search_42
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,10.0,,,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Prompting + other modules,CBT: Cognitive restructuring,,Y,"Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible",N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,"The addition of multiple collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s behavior. Iterative prompt engineering was needed and used to improve the individual agent’s behavior and how they worked together.",6,"See ""Initial User Feedback""",,N,,Y,"Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy",,,,N,Journal paper,,,,Richard Gaus,N,,Empirical research involving an LLM,Y,"We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user",A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study,General population,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_180,2nd_search_43
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,10.0,,,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Prompting + other modules,CBT: Cognitive restructuring,,Y,"Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible",N,"Future studies
should examine whether using LLMs would make out-of-session
practice, such as examining one’s thoughts, more engaging for
patients than worksheets [21].",,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,"The addition of multiple collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s behavior. Iterative prompt engineering was needed and used to improve the individual agent’s behavior and how they worked together.",6,"See ""Initial User Feedback""",,N,,Y,"Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy",,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,Y,"We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user",A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study,General population,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_180,2nd_search_44
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,8.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,,"See Figure 3. All revolves around prompting of GPT-4, and then there is a ""memory concept"" and other stuff",,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_173,2nd_search_45
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,8.0,,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Other: ,,,Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model,No clients/patients involved,,,,,,,,,,2024.0,2nd_search_173,2nd_search_46
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,8.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,,"See Figure 3. All revolves around prompting of GPT-4, and then there is a ""memory concept"" and other stuff",,,,n,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_173,2nd_search_47
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling,,,,,,,,,,,,2nd_search_169,2nd_search_48
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling,,,,,,,,,,,,2nd_search_169,2nd_search_49
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling,,,,,,,,,,,,2nd_search_169,2nd_search_50
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,External data set,1.0,Unselected,No,N,,N,,,,,N,H,W,Dataset responses from professionals and trained volunteers as benchmark,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,Chinese,L,B,S2S-Model (without strategy) as benchmark,Y,L,B,S2S-Model (without strategy) as benchmark,Y,,,,,,,,,,,,,GPT-2 family,8.0,,"PsyQA (+ Yixinly? ""Thus we crawled 50K articles (0.1B tokens in total) related to psychology and mental health support from Yixinli (xinli001.com/info) and train a GPT-2 from scratch based on the corpus."")",,,,,N,,N,,L,B,S2S-Model (without strategy) as benchmark,Y,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Psyqa: A chinese dataset for generating long counseling text for mental health support,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,"Other: Secondly, the answers in PsyQA are
mostly provided by experienced and well-trained volunteers or professional counselors",,2021.0,2nd_search_167,2nd_search_51
Multi-turn chatbot,,Client-facing application,,l,w,"""controllability"", i.e. match between predicted and actual strategy tokens",y,,,,,n,China,External data set,1.0,Unselected,No,n,,n,,,,,n,h,w,"rating of fluency, coherence, relevance, helpfulness. benchmark: human responses from the dataset.",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,Chinese,l,b,benchmark: simple seq2seq model,y,l,b,benchmark: simple seq2seq model,y,,,,,,,,,,,,,GPT-2 family,8.0,,psyqa,Training of GPT-2 from scratch on PsyQA + prepending of strategy token,,,,y,,n,,l,b,benchmark: simple seq2seq model,y,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Psyqa: A chinese dataset for generating long counseling text for mental health support,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2021.0,2nd_search_167,2nd_search_52
Multi-turn chatbot,,Client-facing application,,l,w,"""controllability"", i.e. match between predicted and actual strategy tokens",y,,,,,N,China,External data set,1.0,Unselected,No,n,,n,,,,,N,h,w,"rating of fluency, coherence, relevance, helpfulness. benchmark: human responses from the dataset.",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,Chinese,L,B,S2S-Model (without strategy) as benchmark,Y,l,b,benchmark: simple seq2seq model,y,,,,,,,,,,,,,GPT-2 family,8.0,,"PsyQA (+ Yixinly? ""Thus we crawled 50K articles (0.1B tokens in total) related to psychology and mental health support from Yixinli (xinli001.com/info) and train a GPT-2 from scratch based on the corpus."")",Training of GPT-2 from scratch on PsyQA + prepending of strategy token,,,,y,,n,,l,b,benchmark: simple seq2seq model,y,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Psyqa: A chinese dataset for generating long counseling text for mental health support,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: mixed,,2021.0,2nd_search_167,2nd_search_53
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,India,"External data set, modified",11.0,,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,GPT-3.5 family; Other: NLTK VADER,12.0,,,Hybrid pipeline: NLP preprocessing → VADER sentiment scoring with thresholds → template or GPT-3.5-turbo generation → referral logic and resources → logging/monitoring; context tracking for coherence. ,,,,,,,,,,,,Conference paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot),No clients/patients involved,,No users involved,,,,,,,,24.0,2nd_search_157,2nd_search_54
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,India,External data set,11.0,Unknown,Other: unspecified,,,,,,,,N,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",Other: unspecified,,,,,,,,,English,,,,N,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,12.0,," For example, it uses conversational 
data gained in previous interactions with the user as well as 
datasets having labeled expressions of sentiment (""emo.txt"" 
and ""Sentiments1.txt"")","""A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative"" .. ""The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately"" ... ""Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition""",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot),No clients/patients involved,Other: ,No,,,,,,Unknown,,2024.0,2nd_search_157,2nd_search_55
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,India,"External data set, modified",11.0,Unknown,Other: unspecified,,,,,,,,N,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",Other: unspecified,,,,,,,,,English,,,,N,,,,,,,,,,,,,,,,,GPT-3.5 family; Other: NLTK VADER,12.0,," For example, it uses conversational 
data gained in previous interactions with the user as well as 
datasets having labeled expressions of sentiment (""emo.txt"" 
and ""Sentiments1.txt"")","""A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative"" .. ""The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately"" ... ""Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition""",,,,,,,,,,,,Conference paper,,,,Consensus,,,Empirical research involving an LLM,,,Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot),No clients/patients involved,Other: ,No users involved,,,,,,Unknown,,2024.0,2nd_search_157,2nd_search_56
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,India,No dataset used for development or evaluation,29.0,,,y,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Other: unknown,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,Other: unknown,8.0,,,,50,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,The Role of AI Counselling in Journaling for Mental Health Improvement,General population,,Yes,"Counseling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964",,n,y,y,,y,2024.0,2nd_search_156,2nd_search_57
Other: Journaling app with AI counselling ,,Client-facing application,,,,,N,,,,,N,India,Self-collected data,13.0,Unselected,No,n,,n,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,Other: Digital journaling with AI counselling  ,No,n,,n,,,,,N,English,,,,N,,,,N,,,"Enhancing Counselling Skills and Therapeutic 
Conditions (PB-CSTC)

Enhancing Counselling Dispositions and 
Behaviours (PB-CDB)

Learning Counselling and Mental Health Topics 
(PB-LC)",,,,,,,,,,"Other: not specified, using LangChain framework",5.0,,-,Built on LangChain framework with memory and journaling integration,50,"Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b)",,N,,n,,,,,N,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,The Role of AI Counselling in Journaling for Mental Health Improvement,General population,"Other: Survey responses (quantitative, self-report)",Yes,,,N,Y,N,Unknown,Y,2024.0,2nd_search_156,2nd_search_58
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,India,No dataset used for development or evaluation,29.0,Other: ,Other: ,y,,n,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,Other: Digital journaling with AI counselling  ,Other: ,n,,n,,,,,N,Other: ,,,,N,,,,N,,,,,,,,,,,,,Other: not specified - using LangChain framework,8.0,,,Built on LangChain framework with memory and journaling integration .. Input/Templates/Prompt Instruction,50,,,N,,n,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,The Role of AI Counselling in Journaling for Mental Health Improvement,General population,Other: ,Yes,"Counselling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964

Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b)",,n,y,y,Other: ,y,2024.0,2nd_search_156,2nd_search_59
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,USA,,17.0,,,,,,,,,,,,,,,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,1.0,,,,428,,,,,,,,,,,Journal paper,,,,Reviewer Two,,,Population survey,,,Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students,General population,,Yes,"modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al",,N,Y,Y,,Y,2025.0,2nd_search_155,2nd_search_60
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,17.0,,,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"Other: all types of chatbots, e.g. ChatGPT",1.0,,,,428,,,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Population survey,,,Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students,General population,,Yes,"Mental Help Seeking Attitudes Scale (MHSAS) with regard to chatbots for mental health support.
Modified version of the measure of structural barriers toward mental health service utilization as reported by Van Doren et al.",,n,y,y,,y,2025.0,2nd_search_155,2nd_search_61
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,17.0,,,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,Other: all types of chatbots e.g. ChatGPT,1.0,,,,428,,,,,,,,,,n,Journal paper,,,,Consensus,,,Population survey,,,Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students,General population,,Yes,"modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al",,n,y,y,,y,2025.0,2nd_search_155,2nd_search_62
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept,,,,,,,,,,,,2nd_search_153,2nd_search_63
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept,,,,,,,,,,,,2nd_search_153,2nd_search_64
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept,,,,,,,,,,,,2nd_search_153,2nd_search_65
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection,,,,,,,,,,,,2nd_search_152,2nd_search_66
,,,,,,"Accuracy, F1 Score, Precision and Recall",Y,,,,,,India,"Other: ""The mental health quiz module uses a custom dataset de-
rived from stress and anxiety indicators, consisting of 40
structured questions with multiple-choice options. Users
receive a random selection of 10 questions, ensuring
reliability and varied experience""",7.0,Unselected,,,,,,,,,,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,,Yes,,,,,,,,N,English,,,,,,,,N,,,,,,,,,,,,,BERT family,5.0,This studiy does not make clear who the users are. It seems they had some users who competed a quiz,unclear who the users of the MH quiz module are,,,"Feedback from test users: These results suggest that the
system effectively delivers mental health insights while main-
taining user engagement.",,,,,,,,,N,Conference paper,,,,Reviewer Two,,,Population survey,,,MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection,General population,,Yes,,,,,,Unknown,Y,2025.0,2nd_search_152,2nd_search_67
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection,,,,,,,,,,,,2nd_search_152,2nd_search_68
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care,,,,,,,,,,,,2nd_search_150,2nd_search_69
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care,,,,,,,,,,,,2nd_search_150,2nd_search_70
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care,,,,,,,,,,,,2nd_search_150,2nd_search_71
One-turn chatbot (usually Q&A),,Client-facing application,,,,,,,,,,,China,External data set,20.0,Unselected,No,,,,,L,b,older LLM,y,,,,,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,Chinese,L,b,older LLM,y,,,,,L,B,SHAP value,older LLM,L,B,LIME value,older LLM,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,5.0,,counsel-chat,,,,,,,,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_148,2nd_search_72
One-turn chatbot (usually Q&A),,Client-facing application,,,,,n,,,,,n,China,External data set,20.0,Unselected,No,n,,n,,,,,n,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,5.0,,CounselChat https://github.com/nbertagnolli/counsel-chat,,,,,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation,No clients/patients involved,Psychotherapy -- chat logs,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_148,2nd_search_73
One-turn chatbot (usually Q&A),,Client-facing application,,,,,n,,,,,n,China,External data set,20.0,Unselected,No,n,,n,,,,,n,,,,n,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,5.0,,CounselChat https://github.com/nbertagnolli/counsel-chat,,,,,n,,n,,,,,n,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_148,2nd_search_74
,,,,,,"Precision, recall accuracy, F1 Score",Y,,,,,,India,External data set,16.0,Unselected,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,Other CBT techniques,No,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,GPT-2 family,12.0,,"The MSP-Podcast Corpus is a well annotated dataset 
which includes a wide range of emotional expressions 
captured through spontaneous speech. The dataset's 
diversity and the quality of annotations make it an 
excellent resource for making them availed . Its 
annotations ensure reliability, making it invaluable for 
training and evaluating emotion recognition models

Video Dataset: FER+ Dataset  provides quality images with 
annotations for various facial expressions
",,,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,,,,AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,,"Other: A large naturalistic speech database with emotional traces

The FER+ annotations provide a set of new labels for the standard Emotion FER dataset. In FER+, each image has been labeled by 10 crowd-sourced taggers, which provide better quality ground truth for still image emotion than the original FER labels. ",No users involved,,,,,,Unknown,,2024.0,2nd_search_147,2nd_search_75
Other: Spoken dialog system,,Client-facing application,,,,,n,,,,,n,India,External data set,16.0,Unknown,Other: unknown,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,Other CBT techniques,No,n,,n,,,,,n,Other: unknown,,,,n,,,,n,,,,,,,,,,,,,GPT-2 family,12.0,,"The fine-tuning process involves the 
calibration of the pre-trained Transformer model with 
specific datasets relevant to cognitive therapy. This 
includes dialogues from CBT sessions, therapeutic 
interventions, and stress management conversations.

The GPT-2 model is fine-tuned using a specialized 
therapeutic dialogue dataset where the process involves 
supervised learning and the openly available trained 
model is adapted for a wide range of therapeutic 
contexts. The goal is to improve the ability of the model 
to provide relevant and empathetic responses during 
therapy sessions. The fine-tuning dataset includes a 
variety of therapeutic dialogues which ensures the 
model can handle different therapeutic scenarios 
effectively.",Fine-tuning of GPT-2 + voice and video interaction system,,,,y,GPT-2,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,No clients/patients involved,Other: unknown,No users involved,,,,,,Unknown,,2024.0,2nd_search_147,2nd_search_76
Other: Multimodal dialog system,,Client-facing application,,,,,n,,,,,n,India,"Other: External data set, but completely unknown",16.0,Unknown,Other: unknown,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,Other CBT techniques,No,n,,n,,,,,n,Other: unknown,,,,n,,,,n,,,,,,,,,,,,,GPT-2 family,12.0,,"The fine-tuning process involves the 
calibration of the pre-trained Transformer model with 
specific datasets relevant to cognitive therapy. This 
includes dialogues from CBT sessions, therapeutic 
interventions, and stress management conversations.

The GPT-2 model is fine-tuned using a specialized 
therapeutic dialogue dataset where the process involves 
supervised learning and the openly available trained 
model is adapted for a wide range of therapeutic 
contexts. The goal is to improve the ability of the model 
to provide relevant and empathetic responses during 
therapy sessions. The fine-tuning dataset includes a 
variety of therapeutic dialogues which ensures the 
model can handle different therapeutic scenarios 
effectively.",Fine-tuning of GPT-2 + voice and video interaction system,,,,y,GPT-2,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,No clients/patients involved,Other: unknown,No users involved,,,,,,Unknown,,2024.0,2nd_search_147,2nd_search_77
One-turn chatbot (usually Q&A),,Client-facing application,,,,,,,,,,,India,External data set,18.0,Unselected,No,N,,N,,L,B,Semantic Similarity,Y,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,,English,L,B,BLEU SCORE,Y,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,"Reddit, Twitter an Mental Health Forums as data source",see Fig. 1 ,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot,No clients/patients involved,Internet data -- mental health forum,No users involved,,,N,N,N,Unknown,N,2025.0,2nd_search_144,2nd_search_78
Multi-turn chatbot,,Client-facing application,,,,Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM,n,,,,,n,India,External data set,18.0,Unknown,No,n,,n,,no benchmark,no benchmark,Cosine distance (see Eq. 4). no benchmark,y,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,no benchmark,no benchmark,BLEU score,y,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,"Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms. It is intended for training and evaluating language models that provide safer, context-aware mental-health responses.",Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2025.0,2nd_search_144,2nd_search_79
Multi-turn chatbot,,Client-facing application,,,,Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM,n,,,,,n,India,External data set,18.0,Unknown,No,N,,N,,no benchmark,no benchmark,Semantic Similarity via cosine distance (see Eq. 4),Y,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,n,English,no benchmark,no benchmark,BLEU SCORE,Y,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,"Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms. It is intended for training and evaluating language models that provide safer, context-aware mental-health responses.",Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier (see Fig. 1),,,,N,,N,,,,,n,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,N,N,N,Unknown,N,2025.0,2nd_search_144,2nd_search_80
One-turn chatbot (usually Q&A),Patient simulations,Analysis of conversation transcripts,,,,,,,,,,,India,"External data set, modified",13.0,Unselected,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,H,B/S,human responses from existing literature,Y,English,,,,,,,,,H,B,Emotional Intelligence Scale (EIS),human responses from existing literature,,,,,,,,,Mistral family,9.0,,"counsel-chat dataset, accessible on Hugging Face ( 2,700 anonymised
talks between individuals and experienced counsellors on the
website www.counselchat.com)",,,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Mello: A Large Language Model for Mental Health Counselling Conversations,No clients/patients involved,Psychotherapy -- chat logs,No users involved,,,,,,Other: experienced counsellors,,2024.0,2nd_search_143,2nd_search_81
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,India,External data set,13.0,Unselected,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,l,b,PsychoBench Empathy Scale,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",l,b,PsychoBench Emotional Intelligence Scale,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",,,,,Mistral family,9.0,,CounselChat https://github.com/nbertagnolli/counsel-chat,,,,,y,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Mello: A Large Language Model for Mental Health Counselling Conversations,No clients/patients involved,Psychotherapy -- chat logs,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_143,2nd_search_82
Multi-turn chatbot,Other: ,Client-facing application,,,,,n,,,,,n,India,External data set,13.0,Unselected,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,l,b,PsychoBench Empathy Scale,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",l,b,PsychoBench Emotional Intelligence Scale,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",,,,,Mistral family,9.0,,CounselChat https://github.com/nbertagnolli/counsel-chat,,,,,y,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Mello: A Large Language Model for Mental Health Counselling Conversations,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_143,2nd_search_83
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,USA,Other: reddit ,23.0,Unknown,No,,,,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,GPT-3 family; GPT-3.5 family; Other,12.0,,reddit r/ChatGPT,,,,,,,,,,,,,Journal paper,,,,Reviewer Two,,,Population survey,,,Cases of Using ChatGPT as a Mental Health and Psychological Support Tool,General population,Other: Internet data - online forum,Yes,,,Y,N,N,Unknown,N,2024.0,2nd_search_142,2nd_search_84
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Philippines,No dataset used for development or evaluation,23.0,,,,,,,,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified",12.0,,,,7,"""User experience assessment"" was content of the Reddit posts",,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Population survey,,,Cases of Using ChatGPT as a Mental Health and Psychological Support Tool,General population,,Yes,,,y,n,n,,y,2024.0,2nd_search_142,2nd_search_85
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Philippines,No dataset used for development or evaluation,23.0,Other: ,Other: ,,,,,,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",Other: ,,,,,,,,n,Other: ,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified",12.0,,,,7,"""User experience assessment"" was content of the Reddit posts",,,,,,,,,n,Journal paper,,,,Consensus,,,Population survey,,,Cases of Using ChatGPT as a Mental Health and Psychological Support Tool,General population,Other: ,Yes,,,Y,N,N,Other: ,y,2024.0,2nd_search_142,2nd_search_86
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,Employing large language models for emotion detection in psychotherapy transcripts,,,,,,,,,,,,2nd_search_141,2nd_search_87
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,Employing large language models for emotion detection in psychotherapy transcripts,,,,,,,,,,,,2nd_search_141,2nd_search_88
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,Employing large language models for emotion detection in psychotherapy transcripts,,,,,,,,,,,,2nd_search_141,2nd_search_89
,,Client-facing application,,L,B,"The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978",Y,,,,,,China,External data set,3.0,Unknown,No,,,,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,Y,"We compare MENTALER with the following baselines… CoT, TPE, ReAct, Cue-CoT, Chameleon",,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,,L,B,"“We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”",Y,L,B,D1 (Distinct-1) measures the richness of vocabulary in the responses,Y,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,6.0,,PsyQA,"Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).",,"Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale",,,,,,,,,,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,,,MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,No clients/patients involved,Emotional support dialogue -- speech transcripts,Yes,,,N,Y,N,Unknown,Y,2024.0,2nd_search_140,2nd_search_90
,,Other: ,,,,,N,,,,,N,China,External data set,3.0,Unknown,No,,,,,,,,N,,,,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,,,,N,Chinese,,,,Y,,,,,,,,,,,,,,,,,GPT-3.5 family,12.0,,"PsyQA is an authoritative Chinese dialogue dataset focused
on the field of mental health support. It covers 9 broad
topics and multiple subtopics, encompassing various aspects
of mental health. PsyQA is presented in a question-answer
format, where each example includes a question, a detailed
description of the mental issues, and keywords provided by
anonymous help-seekers. Responses are asynchronously pro-
vided by well-trained volunteers or professional counselors
and involve detailed analysis and guidance in response to the
help-seekers’ questions, aiming to offer mental health support.
Additionally, the answers have been further annotated with
seven psychological strategies based on psychological coun-
seling theory [12], ","""We use ChatGPT as our base LLM and access it through
the API with prompts""",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_140,2nd_search_91
,,Client-facing application,,L,B,"The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978",Y,,,,,N,China,External data set,3.0,Unknown,No,,,,,,,,N,,,"Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale",Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,N,Chinese,L,B,"“We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”",Y,L,B,D1 (Distinct-1) measures the richness of vocabulary in the responses,Y,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,12.0,,"PsyQA is an authoritative Chinese dialogue dataset focused
on the field of mental health support. It covers 9 broad
topics and multiple subtopics, encompassing various aspects
of mental health. PsyQA is presented in a question-answer
format, where each example includes a question, a detailed
description of the mental issues, and keywords provided by
anonymous help-seekers. Responses are asynchronously pro-
vided by well-trained volunteers or professional counselors
and involve detailed analysis and guidance in response to the
help-seekers’ questions, aiming to offer mental health support.
Additionally, the answers have been further annotated with
seven psychological strategies based on psychological coun-
seling theory [12], ","Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).",,,,,,,,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,,,MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_140,2nd_search_92
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback,,,,,,,,,,,,2nd_search_139,2nd_search_93
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback,,,,,,,,,,,,2nd_search_139,2nd_search_94
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback,,,,,,,,,,,,2nd_search_139,2nd_search_95
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,28.0,,,Y?,"Participants were mostly male (71.1 %), White race (66.7 %), and college educated (82.2 %)",,,,,,,L,B,(reviewed transcripts for safety + MI fidelity) — Quote: “All authors… reviewed MICA session transcripts independently to identify any statements… inappropriate” ,Y,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",,,,,,,,,Only prompting,CBT: Motivational interviewing,,,,,,,,,,,,,,N,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,4.0,,,"Prompt engineering improved MI fidelity (Phase II); GPT-4 via secure API; HIPAA-compliant setup. — Quote: “explicit prompting for adherence to validated MI literature and more collaborative, client-centered language enhanced the therapeutic alliance.”",45,"SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback)",,,,,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,Y,"All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results)",Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,People with some symptoms but not disorder (determined by symptom scales or questionnaires),,Yes,"SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract)",,Y,Y,Y,,Y,2025.0,2nd_search_138,2nd_search_96
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,Other: External data set + Self-collected data ,28.0,Unknown,"Other: AnnoMi no, self-collected yes",Y,,N,,,,,N,,,,N,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",N,,N,,N,,N,,Only fine-tuning; Only prompting,"Unspecified, might include formal therapy methods","Other: AnnoMi yes, self-collected no",N,,N,,,,,N,English,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,4.0,,"Model not really trained on the dataset, but datasets used to generate suitable prompts. 

Datasets used: AnnoMI + 20 full-length simulated sessions using GPT, seeded with anonymized baseline data from young adult participants with hazardous alcohol use drawn from a recent clinical trial","We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.",45,,,N,,Y,"
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. ",,,,N,Journal paper,,,,Richard Gaus,Y,"ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. ",Empirical research involving an LLM,Y,"ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. ",Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,General population,Other: Psychotherapy -- speech transcripts + Syntethic data ,Yes,System Usability Scale (SUS) + Client Evaluation of Motivational Interviewing (CEMI),,Y,Y,Y,Trained professionals,Y,2025.0,2nd_search_138,2nd_search_97
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,28.0,Other: ,Other: ,Y,Table 1,N,,,,,N,,,,N,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",N,,N,,N,,N,,Only fine-tuning; Only prompting,"CBT: Motivational interviewing; Unspecified, might include formal therapy methods",Other: ,N,,N,,,,,N,Other: ,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,4.0,,,"We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.",45,"SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback)",,N,,Y,"
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. ",,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,Y,"All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results)",Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Other: ,Yes,"SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract)",,Y,Y,Y,Other: ,Y,2025.0,2nd_search_138,2nd_search_98
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Pakistan,"External data set, modified",5.0,Unknown,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,Y,Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract),,,,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,,,,,English,L,B,0.85 cosine similarity score,Y,,,,,,,,,,,,,,,,,BART family,5.0,,"{17} J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015.
[18] D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009.
[19] B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014.","Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1)",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation,No clients/patients involved,Other: ,No users involved,,,,,,Unknown,,2025.0,2nd_search_137,2nd_search_99
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Pakistan,"External data set, modified",5.0,Other: N/A,No,N,,N,,L,B,Best model of authors compared to models of other works. ,Y,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Other: RAG,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,English,,,,N,,,,N,,,,,,,,,,,,,"BART family; ChatGPT, model unspecified",5.0,,"Books: 
E. J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015. + D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009. + B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014. 

To make sure it was appropriate for entering into the model, the data taken from the books went through a number of preparation steps
in this research: Extraction and cleaning.",,,,,N,,N,,L,W,Best model of authors compared to models of other works. ,Y,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation,No clients/patients involved,Other: Book data,No users involved,,,,,,Trained professionals,,2027.0,2nd_search_137,2nd_search_100
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Pakistan,"External data set, modified",5.0,Unknown,No,N,,N,,L,B,Best model of authors compared to models of other works. ,Y,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,Y,Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract),N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,English,,,,N,,,,N,,,,,,,,,,,,,"BART family; ChatGPT, model unspecified",5.0,,"Books: 
E. J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015. + D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009. + B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014. 

To make sure it was appropriate for entering into the model, the data taken from the books went through a number of preparation steps
in this research: Extraction and cleaning.","Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1)",,,,N,,N,,L,W,Best model of authors compared to models of other works. ,Y,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation,No clients/patients involved,Other: Book data,No users involved,,,,,,Unknown,,2025.0,2nd_search_137,2nd_search_101
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Germany,No dataset used for development or evaluation,9.0,,,n,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,11.0,,,,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_136,2nd_search_102
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Germany,No dataset used for development or evaluation,9.0,,,N,,N,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,N,,N,,N,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,11.0,,,,,,,N,,N,,,,,N,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study,No clients/patients involved,,No,,,,,,,,2024.0,2nd_search_136,2nd_search_103
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Germany,No dataset used for development or evaluation,9.0,,,N,,N,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,11.0,,,,,,,N,,N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_136,2nd_search_104
One-turn chatbot (usually Q&A),,Client-facing application,,,,,,,,,,,China,Self-collected data,2.0,Unknown,No,,,,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,Chinese,,,,,,,,,,,,,,,,,,,,,BERT family; Llama 2 family,12.0,,PsyQA dataset,,,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Unknown,,2024.0,2nd_search_135,2nd_search_105
One-turn chatbot (usually Q&A),,Client-facing application,,no benchmark,no benchmark,Accuracy of the BERT helping skill classification model,y,,,,,n,China,"External data set, modified",15.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,Chinese,no benchmark,no benchmark,"BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L",y,,,,n,,,,,,,,,,,,,BERT family; Llama 2 family,11.0,,"PsyQA, extended by ""helping skills"" column",BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.,,,,y,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,,,2024.0,2nd_search_135,2nd_search_106
One-turn chatbot (usually Q&A),,Client-facing application,,no benchmark,no benchmark,Accuracy of the BERT helping skill classification model,y,,,,,n,China,"External data set, modified",15.0,Unselected,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,Chinese,no benchmark,no benchmark,"BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L",y,,,,n,,,,,,,,,,,,,BERT family; Llama 2 family,11.0,,"PsyQA, extended by ""helping skills"" column",BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.,,,,y,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: Mix of lay and trained,,2024.0,2nd_search_135,2nd_search_107
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot,,,,,,,,,,,,2nd_search_134,2nd_search_108
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot,,,,,,,,,,,,2nd_search_134,2nd_search_109
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot,,,,,,,,,,,,2nd_search_134,2nd_search_110
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Israel,No dataset used for development or evaluation,2.0,,,,,,,,,,,,,,,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",,,,,,,,,Only prompting,CBT: Motivational interviewing,,,,,,,,,,,,N ,,,,,,,,,,,,,,,,,,,GPT-3.5 family,4.0,VR maybe as solution for the embodiment problem? ,,"The design phase mentions fine-tuning and prompt engineering, but the implemented study relied on a simple prompt with GPT-3.5. — Quote: “The design process addressed… fine-tuning the LLMs, and prompt engineering.” / “The initial study used a relatively simple prompt…”",11,"Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” ",,,,,,,,,,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,,,AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study,General population,,Yes,,,Y,Y,N,,Y,2025.0,2nd_search_132,2nd_search_111
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Israel,External data set,2.0,Unknown,No,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only fine-tuning; Only prompting,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,English,,,,N,,,,N,,,,,,,,,,,,,GPT-3.5 family,4.0,,"fine-tuned on the 2 volumes of published counseling and psychotherapy data from
Alexander Street Press. The volumes are searchable collections of transcripts containing real counseling and therapy sessions
and first-person narratives illuminating the experience of mental illness and treatment. The 2 volumes contain 3500 session transcripts and >700,000 utterances between a counselor and a
patient.","""We fine-tuned the model with an 80% to 20% train-test split"" + ""that is, the system is prompted
to keep generating responses""...",11,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study,General population,Emotional support dialogue -- speech transcripts,Yes,,,Y,N,N,Trained professionals,Y,2025.0,2nd_search_132,2nd_search_112
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Israel,External data set,2.0,Unknown,No,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only fine-tuning; Only prompting,CBT: Motivational interviewing,Yes,N,,N,,,,,N,English,,,,N,,,,N,,,,,,,,,,,,,GPT-3.5 family,4.0,VR maybe as solution for the embodiment problem? ,"fine-tuned on the 2 volumes of published counseling and psychotherapy data from
Alexander Street Press. The volumes are searchable collections of transcripts containing real counseling and therapy sessions
and first-person narratives illuminating the experience of mental illness and treatment. The 2 volumes contain 3500 session transcripts and >700,000 utterances between a counselor and a
patient.","""We fine-tuned the model with an 80% to 20% train-test split"" + ""that is, the system is prompted
to keep generating responses""...",11,"Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” ",,N,,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study,General population,Emotional support dialogue -- speech transcripts,Yes,,,Y,Y,N,Unknown,Y,2025.0,2nd_search_132,2nd_search_113
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Romania,No dataset used for development or evaluation,2.0,Psychopathology,,Y,,Y,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Prompting + other modules,CBT: Cognitive restructuring; Other CBT techniques,,N,,N,,,,,,,,,,,,,,,,,Anxiety Score ,,,,Engangement Metrics ,,,,,,GPT-4 / GPT-4o family,12.0,,,"Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization",50,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,,,Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support,People with some symptoms but not disorder (determined by symptom scales or questionnaires),,,Participant Feedback ,,Y,Y,N,Other: chatbot,Y,2024.0,2nd_search_131,2nd_search_114
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Romania,No dataset used for development or evaluation,2.0,,,y,Table 2 has just the bare minimum,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",y,Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7),n,,y,Table 6 shows how many participants interacted only 0-15 mins daily,y,Table 6 shows daily interaction time over 30 mins,Only prompting,CBT: Cognitive restructuring; Other CBT techniques,,n,,y,"Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.",,,,n,,,,,n,,,,n,no benchmark,no benchmark,"Reduction in anxiety symptoms (BAI, GAD-7)",no benchmark,no benchmark,no benchmark,User satisfaction rating,no benchmark,no benchmark,no benchmark,Daily interaction time,no benchmark,"ChatGPT, model unspecified",12.0,,,"They only ever refer to ""prompt engineering techniques"" used in their chatbot",50,Qualitative feedback reported in 4.4.,,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support,People with some symptoms but not disorder (determined by symptom scales or questionnaires),,Yes,,,y,y,n,,y,2024.0,2nd_search_131,2nd_search_115
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Romania,No dataset used for development or evaluation,2.0,Other: ,,Y,Table 2 has just the bare minimum,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",y,Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7),n,,y,Table 6 shows how many participants interacted only 0-15 mins daily,y,Table 6 shows daily interaction time over 30 mins,Prompting + other modules,CBT: Cognitive restructuring; Other CBT techniques,,n,,y,"Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.",,,,n,,,,,n,,,,n,no benchmark,no benchmark,"Reduction in anxiety symptoms (BAI, GAD-7)",no benchmark,no benchmark,no benchmark,User satisfaction rating,no benchmark,no benchmark,no benchmark,Daily interaction time,no benchmark,"ChatGPT, model unspecified",12.0,,,"Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization",50,Qualitative feedback reported in 4.4.,,N,,N,,,,,n,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,n,,Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support,People with some symptoms but not disorder (determined by symptom scales or questionnaires),,Yes,,,y,y,n,Other: ,y,2024.0,2nd_search_131,2nd_search_116
One-turn chatbot (usually Q&A),,Client-facing application,,,,,,,,,,,India,"External data set, modified",2.0,Unselected,No,,,,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,,,,,,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,English,L,B,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets",Y,,,,,,,,,,,,,,,,,BART family; T5 family; Other: GODEL(Grounded Open Dialogue Language Model),7.0,,"Aditya Mental Health Counselling Dataset, Mental Health Counselling Chat, 
Counsel Chat Dataset and Amod-Mental Health Counselling Conversations",,,,,,,,,L,B,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets",Y,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2025.0,2nd_search_130,2nd_search_117
One-turn chatbot (usually Q&A),,Client-facing application,,,,,n,,,,,n,India,External data set,2.0,Unknown,Other: Unknown,n,,n,,,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",Yes,n,,n,,,,,n,English,no benchmark,no benchmark,Comparison against reference responses in dataset,y,,,,n,,,,,,,,,,,,,BART family; T5 family; GPT-2 family,7.0,,Aditya Mental Health Counseling https://huggingface.co/datasets/Aditya149/Mental_Health_Counselling_Dataset,Different models are simply fine-tuned on provided datasets,,,,y,,n,,no benchmark,no benchmark,Perplexity of reference responses in dataset,y,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2025.0,2nd_search_130,2nd_search_118
One-turn chatbot (usually Q&A),,Client-facing application,,,,,n,,,,,n,India,External data set,2.0,Unknown,Other: Unknown,n,,n,,,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,no benchmark,no benchmark,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Comparison against reference responses in dataset",y,,,,n,,,,,,,,,,,,,BART family; T5 family; GPT-2 family,7.0,,Aditya Mental Health Counseling https://huggingface.co/datasets/Aditya149/Mental_Health_Counselling_Dataset,Different models are simply fine-tuned on provided datasets,,,,y,,n,,no benchmark,no benchmark,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Perplexity of reference responses in dataset",y,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2025.0,2nd_search_130,2nd_search_119
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Saudi Arabia,No dataset used for development or evaluation,13.0,,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Other CBT techniques; Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,L,B,Y,"Custom metrics (Role Adherence, Answer Relevancy, Faithfulness) reported in tables across tiers of the same model; no external gold standard named",,,,,,,,,GPT-3.5 family,4.0,,,"Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.",,,,,,,,,,,,Conference paper,,,,Richard Gaus,Y,"Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159",Other: engineering/system paper with internal evaluation (no human user study) ,N,,Multi-Tiered RAG-Based Chatbot for Mental Health Support,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_129,2nd_search_120
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Saudi Arabia,External data set,13.0,Unknown,Other: unspecified ,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Other CBT techniques; Informal counseling (e.g., emotional support conversation)",Other: unspecified ,,,,,,,The DeepEval framework + MQG-RAG Evaluation,Y,,,,,,,,,,,,,,,,,,,,,,GPT-3.5 family,4.0,,"unspecified (""credible sources"")"," multi-tiered chatbot leveraging Retrieval-
Augmented Generation (RAG) provides dynamic, context-aware
mental health assistance",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Multi-Tiered RAG-Based Chatbot for Mental Health Support,No clients/patients involved,Other: unspecified ,No users involved,,,,,,Unknown,,2025.0,2nd_search_129,2nd_search_121
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Saudi Arabia,No dataset used for development or evaluation,13.0,Other: ,Other: ,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Other CBT techniques; Unspecified, might include formal therapy methods",Other: ,,,,,no benchmark,no benchmark,The DeepEval framework + MQG-RAG Evaluation,Y,Other: ,,,,,,,,,no benchmark,no benchmark,automatic response quality,llm as a judge: The DeepEval framework + MQG-RAG Evaluation,,,,,,,,,GPT-3.5 family,4.0,,,"Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.",,,,,,,,,,,,Conference paper,,,,Consensus,Y,"Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159",Empirical research involving an LLM,N,,Multi-Tiered RAG-Based Chatbot for Mental Health Support,No clients/patients involved,Other: ,No users involved,,,,,,Other: ,,2025.0,2nd_search_129,2nd_search_122
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: canada,No dataset used for development or evaluation,3.0,,,,,,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,,,,,,,Only prompting,Psychoanalysis,,,,,,,,,,,,,,N (all below),,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).,,"Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)",,,,,,,,,,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,,,"ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence",No clients/patients involved,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_128,2nd_search_123
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Canada,No dataset used for development or evaluation,3.0,,,N,,N,,,,,N,,,,N,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,,1,"May not be understood as the ""classical"" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.",,N,,N,,,,,N,Journal paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,"ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence",Other: The author served as the sole participant,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_128,2nd_search_124
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Canada,No dataset used for development or evaluation,3.0,,,N,,N,,,,,N,,,,N,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Only prompting,Psychoanalysis,,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).,1,"Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)

May not be understood as the ""classical"" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.",,N,,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,"ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence",General population,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_128,2nd_search_125
One-turn chatbot (usually Q&A),,Client-facing application,,,,,,,,,,,Germany,No dataset used for development or evaluation,17.0,,,,,,,,,,,,,,N,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",,,,,,,,,Only prompting,Other,,,,,,,,,,,,,,N,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",12.0,,,"Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).",,"As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).",,,,,,,,,N,,,,,Reviewer Two,N,,Empirical research involving an LLM,,,Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories,No clients/patients involved,,Yes,,,Y,Y,N,,Y,2024.0,2nd_search_127,2nd_search_126
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Germany,No dataset used for development or evaluation,17.0,,,N,,N,,,,,N,-,-,-,Y,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,12.0,,,"Promts: ""Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis"".",4,,,N,,N,,,,,N,,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories,Other: Reviewers trained in developing therapeutic stories,,No,,,,,,,,2024.0,2nd_search_127,2nd_search_127
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,Germany,No dataset used for development or evaluation,17.0,,,N,,N,,,,,N,,,,N,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",N,,N,,N,,N,,Only prompting,Other: Bedtime stories,,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"ChatGPT, model unspecified",12.0,,,"Promts: ""Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis"".

Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).",,"As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).",,N,,N,,,,,N,,,,,Consensus,N,,Empirical research involving an LLM,N,,Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories,No clients/patients involved,,Yes,,,Y,Y,N,,Y,2024.0,2nd_search_127,2nd_search_128
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,Self-collected data,23.0,Unselected,No,y,Table 1,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,y,dropouts noted,y,"Measures of the user experience with the generative and
rules-based DMHIs included user engagement (number of
sessions, total active days, and conversational exchanges)",Prompting + other modules,"CBT: Cognitive restructuring; Informal counseling (e.g., emotional support conversation)",No,,,,,,,,n,English,,,,n,,,,n,no benchmark,no benchmark,Posttrial guardrail review,"A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). ",,,"Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond)",,,,Safety measures,"Safety
was assessed by adverse events monitored during both in-app
conversational exchanges and study assessment points, instances
of concerning language detected in user free-text inputs, and
the posttrial technical guardrail assessment success rate.",GPT-3.5 family,5.0,,"Using a dataset of Woebot questions and user responses (pulled from the Gen-W-MA internal testing data) labeled as being on or off topic, we fine-tuned an instance of the text-embedding-ada-002 model using the Azure OpenAI service",davinci-003 + off topic classifier. davinci-003 seems not finetuned,160,"Quantitative: Measures of the user experience with the generative and rules-based DMHIs included user engagement (number of sessions, total active days, and conversational exchanges).
",,n,,,,,,,n,Journal paper,,,,Richard Gaus,y,"Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language",Empirical research involving an LLM,y,"We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.",Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial,General population,Emotional support dialogue -- chat logs,Yes,Client Satisfaction Questionnaire-8 (CSQ-8),,n,y,y,Other: Woebot chatbot,y,2025.0,2nd_search_126,2nd_search_129
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,Self-collected data,23.0,Unknown,No,Y,,N,,,,,N,,,,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",Y,,Y,RCT,Y,,N,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation)",No,Y,,N,,,,,N,English,,,,N,,,,N,,,,,,,,,,,,,GPT-3 family,5.0,,,,160,,,N,,N,,,,,N,Journal paper,,,,Reviewer Two,Y,"Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language",Empirical research involving an LLM,Y,"Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer",Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial,General population,Emotional support dialogue -- speech transcripts,Yes,"Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9",,N,Y,Y,,Y,2025.0,2nd_search_126,2nd_search_130
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,Self-collected data,23.0,Unknown,No,Y,Table 1,N,,,,,N,l,b,benchmark: woebot non-generative/rule-based.,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,"WAI-SR Bond is used, but this is not symptom or function scale",Y,RCT,Y,dropouts noted,y,"Measures of the user experience with the generative and
rules-based DMHIs included user engagement (number of
sessions, total active days, and conversational exchanges)",Fine-tuning + other modules,"CBT: Cognitive restructuring; Informal counseling (e.g., emotional support conversation)",No,n,,N,,,,,N,English,,,,N,,,,N,no benchmark,no benchmark,Posttrial guardrail review,"A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). ",l,w,Instances of potentially concerning language detected,benchmark: woebot non-generative/rule-based.,,,,,GPT-3.5 family; Other: text-embedding-ada-002,5.0,,"Using a dataset of Woebot questions and user responses (pulled from the Gen-W-MA internal testing data) labeled as being on or off topic, we fine-tuned an instance of the text-embedding-ada-002 model using the Azure OpenAI service",davinci-003 + off topic classifier. davinci-003 seems not finetuned,160,"Quantitative: Measures of the user experience with the generative and rules-based DMHIs included user engagement (number of sessions, total active days, and conversational exchanges).
",,N,,N,,,,,N,Journal paper,,,,Consensus,Y,"Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language

Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language",Empirical research involving an LLM,Y,"Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer

We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.",Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial,General population,Emotional support dialogue -- chat logs,Yes,"Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9",,N,Y,Y,Other: Woebot chatbot,Y,2025.0,2nd_search_126,2nd_search_131
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,China,External data set,8.0,Unknown,Yes,N,,N,,,,,,,,,,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,L,B,ChatGPT-4 rated using the categories below,Y,Chinese,,,,,,,,,L,B ,"Comprehensiveness, Professionalism, Safety",ChatGPT and GLM-4,L,W,Authenticity,ChatGPT and GLM-4,,,,,Other: InternLM2-7B-Chat with QLoRA,10.0,,"PsyAdv Corp consisting out of Efaqa, CpsyCounD, SMILECHAT, PsyQA","Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.",,,,Y,InternLM2-7B is self-hostable,N,,,,,,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Unknown,,2024.0,2nd_search_125,2nd_search_132
,,Analysis of conversation transcripts,,,,,,,,,,,China,External data set,15.0,Unselected,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,,,,,Chinese,,,,,,,,,,,,,,,,,,,,,BERT family,11.0,,"PsyQA dataset (and PsyAdv Corp, and CPSyCounE)","'we apply the
QLoRA [12] technique for parameter-efficient finetuning of
the generator' .. 'dense retrieval model'... ' we introduce a BERT-
based cross-encoder as a re-ranke'",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling,No clients/patients involved,Emotional support dialogue -- chat logs,No,,,N,N,N,Unknown,N,2024.0,2nd_search_125,2nd_search_133
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,China,"External data set, modified",15.0,Unknown,Yes,N,,N,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,L,B,"ChatGPT-4 rated using the categories below. Benchmark: ChatGPT, GLM-4",Y,Chinese,,,,,,,,,l,b,automatic safety rating,ChatGPT and GLM-4,l,b,automatic response quality,ChatGPT and GLM-4,,,,,BERT family; Other: InternLM2-7B-Chat with QLoRA,11.0,,"PsyAdv Corp consisting out of Efaqa, CpsyCounD, SMILECHAT, PsyQA","Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.",,,,Y,InternLM2-7B is self-hostable,N,,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Unknown,,2024.0,2nd_search_125,2nd_search_134
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,India,No dataset used for development or evaluation,14.0,,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family; GPT-4 / GPT-4o family; Other: additionally: CNN for Facial Recognition and LSTM for Text-based Emotion Detection,12.0,,,"Multimodal pipeline combining BERT embeddings (text), CNN (facial), LSTM/MFCC (audio), LangChain prompting, GPT-4 response generation; frontend in React, auth/session in Firebase. ",,,,,,,,,,,,Conference paper,,,,Richard Gaus,N,,"Other: Propsal of a multimodal Model called ""empathy AI""",,,Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_124,2nd_search_135
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,India,Other: unspecified,14.0,Unknown,Other: unspecified,,,,,,,,N,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",Other: unspecified,,,,,,,,,Other: unspecified,,,,N,,,,,,,,,,,,,,,,,BERT family; GPT-4 / GPT-4o family,12.0,,,"""LangChain for Prompting"" combined with ""CNN for Facial Recognition,"" ""LSTM for Text-based Emotion Detection,"" ""MFCC or LSTM for Audio Recognition""",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction,No clients/patients involved,Other: unspecified,No,,,,,,Unknown,,2024.0,2nd_search_124,2nd_search_136
Other: Multi-modal dialogue system,,Client-facing application,,,,,N,,,,,N,India,No dataset used for development or evaluation,14.0,Other: ,Other: ,,,,,,,,N,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",Other: ,,,,,,,,,Other: ,,,,N,,,,,,,,,,,,,,,,,BERT family; GPT-4 / GPT-4o family,12.0,,,"""LangChain for Prompting"" combined with ""CNN for Facial Recognition,"" ""LSTM for Text-based Emotion Detection,"" ""MFCC or LSTM for Audio Recognition""",,,,,,,,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,,,Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction,No clients/patients involved,Other: ,No users involved,,,,,,Other: ,,2024.0,2nd_search_124,2nd_search_137
One-turn chatbot (usually Q&A),,Client-facing application,,,,,,,,,,,Other: Canada,External data set,28.0,Unknown,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,English,,,,,,,,,L,B,"Ragas evaluation metrics to calculate Faithfulness, Answer Relevancy, and Answer Correctness scores for the various
RAG models",OpenAI against Mistral with RAG modules and sentiment analysis,,,,,,,,,GPT-3.5 family; Mistral family,10.0,,Mental Health Counseling Conversations Dataset,"dataset fine-tuning, sentiment analysis within RAG models",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,2nd_search_123,2nd_search_138
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Canada,External data set,28.0,Unknown,No,n,,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,no benchmark,no benchmark,Ragas is LLM-as-a-judge,y,English,,,,n,,,,n,,,,,,,,,,,,,BERT family; GPT-3.5 family; Mistral family,10.0,,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,2nd_search_123,2nd_search_139
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Canada,External data set,28.0,Unknown,No,n,,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,no benchmark,no benchmark,Ragas is LLM-as-a-judge,y,English,,,,n,,,,n,no benchmark,no benchmark,automatic response quality,Ragas is LLM-as-a-judge. OpenAI against Mistral with RAG modules and sentiment analysis,,,,,,,,,BERT family; GPT-3.5 family; Mistral family,10.0,,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis,,,,n,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,2nd_search_123,2nd_search_140
One-turn chatbot (usually Q&A),,Client-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,24.0,,,,,,,,,,,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,"Psychoanalysis; Unspecified, might include formal therapy methods",,,,,,,,,,,,,,N,,,,,,,N,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,,,,,,,,,,,,N,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,,,The impact of prompt engineering in large language model performance: a psychiatric example,No clients/patients involved,,No,,,,,,,,2023.0,2nd_search_119,2nd_search_141
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,24.0,,,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,"Four unique questions were asked of ChatGPT 4.0, and each question was asked five separate times. Each question asked to the model 
was performed in a separate, new conversation (10)",,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,The impact of prompt engineering in large language model performance: a psychiatric example,No clients/patients involved,,No users involved,,,,,,,,2023.0,2nd_search_119,2nd_search_142
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,24.0,,,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Psychoanalysis; Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,"Four unique questions were asked of ChatGPT 4.0, and each question was asked five separate times. Each question asked to the model 
was performed in a separate, new conversation (10)",,,,N,,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,The impact of prompt engineering in large language model performance: a psychiatric example,No clients/patients involved,,No users involved,,,,,,,,2023.0,2nd_search_119,2nd_search_143
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Germany,No dataset used for development or evaluation,25.0,,,n,,n,,,,,n,h,s,"Measure: ""Engagement score"". Benchmark: Therapist-led sessions",y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Prompting + other modules,,,n,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,5.0,,,"Prompting of GPT-4o + components of a robot (speech-to-text, movement, etc.)",,,,n,,y,Prioritize data privacy and ethical compliance,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_117,2nd_search_144
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Germany,External data set,25.0,Psychopathology,Yes,N,,N,,,,,N,H,B,"To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.",Y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,Y,,Y,"
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families",,,,N,"Other: german, english ",,,,N,,,,,,,Integrated interaction ,,,,Consitency ,,,,,,GPT-4 / GPT-4o family,5.0,,multiple sets: existing clinical datasets - e.g. from ADHD-200 Global Competition; transcriptions of therapist-patient interactions; DHD support group discussions ,"chatgbt 4 was fine-tuned with external (and internal) data sets, fine-tuned through prompt engineering and integrated into a technical framework ",,"Categories of qualitative feedback: helpfullness, Naturalness, Ease of Use, Engangement. ",,N,,Y,"In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.
",,,,,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.,No clients/patients involved,Psychotherapy -- speech transcripts,"Other: users were not the target group ( here: children), but educators and caregivers rated the effectiviness in potential application with children",,,Y,N,N,Trained professionals,N,2025.0,2nd_search_117,2nd_search_145
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Germany,No dataset used for development or evaluation,25.0,Other: ,Other: ,N,,N,,,,,N,H,B,"To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.",Y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",Other: ,n,,Y,"
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families",,,,N,Other: ,,,,N,,,,n,no benchmark,no benchmark,Stress test metrics,no benchmark,,,,,,,,,GPT-4 / GPT-4o family,5.0,,,"Prompting of GPT-4o + components of a robot (speech-to-text, movement, etc.)",,,,N,,Y,"In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.
",,,,n,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.,No clients/patients involved,Other: ,No users involved,,,,,,Other: ,,2025.0,2nd_search_117,2nd_search_146
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,External data set,23.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",Yes,N,,Y,"Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].",-,-,"No clear indexmodel, so not necessarly a benchmark (?)",Y,English,,,,N,,,,N,-,-,Avg. of Stigma Questions,"Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. ",L,B,Appropriateness of responses,Compared to commercially-available therapy bots; no indexmodel. ,,,,,GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family,6.0,,Real therapy transctipts from Alexander Street Press,"Only prompting to evaluate capabilities of LLMs in terms of stigma and their responses to mental health symptoms.  In addition to prompting models with the stimuli without in-context examples, we also employed a novel method of prompting models with a portion of real therapy tran-
scripts from Alexander Street Press [6, 7]",,,,Y,"Partially: GPT not, Llama yes. ",Y,"Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]",,,,N,Conference paper,,,,Reviewer Two,Y,Implicit in S-2,Empirical research involving an LLM,Y,"LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11)",Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,25.0,2nd_search_116,2nd_search_147
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,External data set,23.0,Psychopathology,No,n,,n,,,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",No,n,,y,See 6.2 and 7,h,w,LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.,y,English,,,,n,,,,n,no benchmark,no benchmark,Level of stigma,"Figure 1, no benchmark",,,,,,,,,"GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family; Other: Pi, Noni, Serena, and other commercial chatbots",6.0,,Alexander Street Press therapy transcripts,Only prompting of different LLMs,,,,y,,y,,,,,n,Conference paper,,,,Richard Gaus,y,,Empirical research involving an LLM,y,,Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_116,2nd_search_148
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,External data set,23.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",No,N,,Y,"Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].

See 6.2 and 7",h,w,LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.,Y,English,,,,N,,,,N,no benchmark,no benchmark,Avg. of Stigma Questions,"Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. ",h,w,automatic safety rating,LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.,,,,,GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family; Other: Pi - Noni - Serena - and other commercial chatbots,6.0,,Alexander Street Press therapy transcripts,"Only prompting to evaluate capabilities of LLMs in terms of stigma and their responses to mental health symptoms.  In addition to prompting models with the stimuli without in-context examples, we also employed a novel method of prompting models with a portion of real therapy tran-
scripts from Alexander Street Press [6, 7]",,,,Y,"Partially: GPT not, Llama yes. ",Y,"Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]",,,,N,Conference paper,,,,Consensus,Y,Implicit in S-2,Empirical research involving an LLM,Y,"LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11)",Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_116,2nd_search_149
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Germany,No dataset used for development or evaluation,17.0,,,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",,,,,,,,,,,L,B,,Y,,,,,,,See Fig. 4-9. ,,,,,,,,,,GPT-4 / GPT-4o family; Claude family,6.0,,,See Fig. 1: LLMs integrated into robot to specifically adress ADHD Symptoms,,,,,,,,,,,,Journal paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_114,2nd_search_150
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Germany,No dataset used for development or evaluation,17.0,,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family; Claude family,6.0,,,"The implementation of the speak-speak feature involves
specifying the models, prompts (as we are testing in a zero
learning shot environment)",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_114,2nd_search_151
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Germany,No dataset used for development or evaluation,17.0,,,,,,,,,,,l,s,benchmark: chatgpt without robot integration,y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family; Claude family,6.0,,,"The implementation of the speak-speak feature involves
specifying the models, prompts (as we are testing in a zero
learning shot environment)",,,,,,,,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_114,2nd_search_152
,Treatment fidelity feedback,Therapist-facing application,,,,,,,,,,,Other: Italy,Self-collected data,17.0,Unselected,Yes,,,,,,,,,H,B,valuated by Gestalt psychotherapy trainees,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,Y,"three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract)",,,,,Fine-tuning + other modules,Other: Gestalt-Therapy,No,Y,Yes (trainee evaluations of AI vs. human) — Quote: “evaluated by Gestalt psychotherapy trainees using a Likert scale rating” (Position: Abstract),,,,,,,Other: Italien,,,,N,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,-,,,"Satisfaction ratings focused on relational/emotional quality, technical/didactic, treatment support, professional orientation. — Quote: “PCA highlighted four components… relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4)” (Position: Abstract)",,,,,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT,,Other: Psychotherapy -- Therapist Feedback,Yes,,,Y,Y,N,Trained professionals,Y,2025.0,2nd_search_112,2nd_search_153
,Treatment fidelity feedback,Therapist-facing application,,,,,N,,,,,N,Other: Italy,No dataset used for development or evaluation,17.0,,,N,,N,,,,,N,H,B,qualified human supervisor,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods; Other: Gestalt Therapy techniques, Gestalt supervision",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,"in a further chat interface (test 2), we
introduced the scope of expertise using a prompt as a pretraining tool, before proceeding
with the presentation of the case. This prompt was generated using ChatGPT, and its
reliability as a pretraining tool for the chat session was verified as a good alternative to
a longer and more complex training process in a previous study we conducted [12]",,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT,,,No,,,,,,,,2025.0,2nd_search_112,2nd_search_154
,Treatment fidelity feedback,Therapist-facing application,,,,,N,,,,,N,Other: Italy,No dataset used for development or evaluation,17.0,Other: ,Other: ,,,,,,,,N,H,B,valuated by Gestalt psychotherapy trainees,Y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,Y,"three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract)",,,,,Only prompting,"Unspecified, might include formal therapy methods; Other: Gestalt Therapy techniques, Gestalt supervision",Other: ,N,,,,,,,N,Other: ,,,,N,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,"in a further chat interface (test 2), we
introduced the scope of expertise using a prompt as a pretraining tool, before proceeding
with the presentation of the case. This prompt was generated using ChatGPT, and its
reliability as a pretraining tool for the chat session was verified as a good alternative to
a longer and more complex training process in a previous study we conducted [12]",,"Satisfaction ratings focused on relational/emotional quality, technical/didactic, treatment support, professional orientation. — Quote: “PCA highlighted four components… relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4)” (Position: Abstract)",,,,,,,,,,Journal paper,,,,Consensus,,,Empirical research involving an LLM,,,Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT,,Other: ,Yes,,,Y,Y,N,Other: ,Y,2025.0,2nd_search_112,2nd_search_155
,Treatment fidelity feedback,Therapist-facing application,,,,,,,,,,,Other: Korea,External data set,24.0,Unselected,No,N,,N,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Fine-tuning + other modules,,Yes,N,,N,,,,,,Other: Korean,,,,,,,,,,,,,,,,,,,,,,3.0,," Korean Children Voice Records from
AI-Hub",,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis,No clients/patients involved,"Other: Speech data from children,  child
counseling service provided by the Professor Youjin Han’s
research lab",Yes,SEM,,Y,Y,Y,Trained professionals,Y,2025.0,2nd_search_111,2nd_search_156
,Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition,Therapist-facing application,,,,,n,,,,,n,Other: Korea,No dataset used for development or evaluation,24.0,,,,,,,,,,n,l,b,"Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality",y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,3.0,,,"Prompting of different LLMs + other modules (speech-to-text, editable dashboard, etc.)",48,User experience assessment of therapists using the tool. There is a quantiative (Likert scale questionnaires) and qualitative (open-ended user questions) assessment,,,,,,,,,n,Conference paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis,"Other: ""Child counseling experts""",,Yes,,,y,y,n,,y,2025.0,2nd_search_111,2nd_search_157
,Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition,Therapist-facing application,,,,,n,,,,,n,Other: Korea,No dataset used for development or evaluation,24.0,Other: ,Other: ,N,,N,,,,,n,l,b,"Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality",y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",Other: ,N,,N,,,,,n,Other: ,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,3.0,,,"Prompting of different LLMs + other modules (speech-to-text, editable dashboard, etc.)",48,User experience assessment of therapists using the tool. There is a quantiative (Likert scale questionnaires) and qualitative (open-ended user questions) assessment,,N,,N,,,,,n,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis,"Other: ""Child counseling experts""",Other: ,Yes,,,y,y,n,Other: ,y,2025.0,2nd_search_111,2nd_search_158
One-turn chatbot (usually Q&A),,Client-facing application,,,,,,,,,,,Other: Finland,No dataset used for development or evaluation,11.0,,,,,,,,,,,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,N,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,9.0,,,"GPT-4 outputs compared with curated expert book excerpts; randomized and blinded survey design. — Quote: “participants were blinded in terms of which advice was book-based and which was ChatGPT generated.” (Position: Methods, p. 36)",70,"72.86% rated GPT-4 advice practical … 87.94% well-explained … statistically significant” (Position: Results, p. 41–42)

GPT-4 advice perceived as more practical and clearer; effect sizes small-to-medium. — Quote: “differences … statistically significant … d ≈ 0.34 … d ≈ 0.53 …” (Position: Results, p. 41–42)",,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management,General population,,Yes,,,N,Y,N,,Y,2025.0,2nd_search_109,2nd_search_159
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Finnland,No dataset used for development or evaluation,2.0,,,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,Y,"„Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“",,,,N,,,,,N,,,,N,H (?),B,Lay Rating,Book written by a human expert.,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,"For the ChatGPT-generated advice, version GPT-4 was used. On October 12, 2023, 
the following question was asked on a fresh chatbot session and the response was saved:
[...] Since the study’s objective was to evaluate performance of ChatGPT-4 with-
out any refinement or customization, we deliberately refrained from using any prompt 
engineering techniques in our design and opted to use the afore mentioned one-time 
instruction to the LLM. ",70,,,N,,N,,,,,N,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management,General population,,No,,,,,,,,2025.0,2nd_search_109,2nd_search_160
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,Other: Finland,No dataset used for development or evaluation,2.0,,,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,Y,"„Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“",,,,N,,,,,N,,,,,H,B,Lay Rating,Book written by a human expert.,,,,,,,,,GPT-4 / GPT-4o family,9.0,,,"For the ChatGPT-generated advice, version GPT-4 was used. On October 12, 2023, 
the following question was asked on a fresh chatbot session and the response was saved:
[...] Since the study’s objective was to evaluate performance of ChatGPT-4 with-
out any refinement or customization, we deliberately refrained from using any prompt 
engineering techniques in our design and opted to use the afore mentioned one-time 
instruction to the LLM. ",70,"72.86% rated GPT-4 advice practical … 87.94% well-explained … statistically significant” (Position: Results, p. 41–42)

GPT-4 advice perceived as more practical and clearer; effect sizes small-to-medium. — Quote: “differences … statistically significant … d ≈ 0.34 … d ≈ 0.53 …” (Position: Results, p. 41–42)",,N,,N,,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management,General population,,Yes,,,N,Y,N,,Y,2024.0,2nd_search_109,2nd_search_161
Multi-turn chatbot,,Client-facing application,,,,no benchmark,Y,,,,,,Other: SriLanka,External data set,18.0,Unselected,Yes,,,,,,,,,,,no benchmark,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,CBT: Cognitive restructuring,No,,,,,,,,,Other: indian,,,,,,,,,,,,,,,,,,,,,GPT-3.5 family,12.0,,,,unknown,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Psychotherapy -- speech transcripts,No,,,n,n,n,Trained professionals,n,2024.0,2nd_search_108,2nd_search_162
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,"Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.",y,,,,,n,Other: Sri Lanka,Self-collected data,18.0,Unknown,Yes,n,,n,,,,,n,no benchmark,no benchmark,Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,CBT: Cognitive restructuring,No,n,,n,,,,,n,Other: unknown,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; Other: text-embedding-3-small,12.0,,cognitive distortion identification dataset,GPT-3.5 is only prompted but there is also an intent classifier and cognitive distortion identifier,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT,No clients/patients involved,Other: Labeled examples of cognitive distortions,No users involved,,,,,,Other: not applicable,,2024.0,2nd_search_108,2nd_search_163
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,"Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.",y,,,,,n,Other: Sri Lanka,Self-collected data,18.0,Unknown,Yes,n,,n,,,,,n,no benchmark,no benchmark,Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.,y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,CBT: Cognitive restructuring,No,n,,n,,,,,n,Other: unknown,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; Other: text-embedding-3-small,12.0,,cognitive distortion identification dataset,GPT-3.5 is only prompted but there is also an intent classifier and cognitive distortion identifier,,,,n,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT,No clients/patients involved,Other: Labeled examples of cognitive distortions,No users involved,,,,,,Other: not applicable,,2024.0,2nd_search_108,2nd_search_164
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,"Various classification metrics (accuracy, precision, recall, f1) for checking how ""relevant"" the responses are. No information is given on how ""relevance"" is measured here.",y,,,,,n,India,Other: unknown,12.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,no benchmark,no benchmark,BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.,y,,,,n,,,,,,,,,,,,,Llama 2 family,12.0,,Some documents for their RAG database but no further information is given.,"Llama 2 is prompted, but there is also a RAG component",,,,y,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_107,2nd_search_165
Other: unclear,,Client-facing application,,,,,n,,,,,n,India,Other: unclear,23.0,,,n,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,Y,,,,,,,,,,,,,,,,,Llama 2 family,1.0,,,,unclear,,,n,,n,,,,,,Conference paper,,,,Reviewer Two,n,,Population survey,n,,Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model,Other: unclear,,Yes,,,Y,Y,N,,N,2024.0,2nd_search_107,2nd_search_166
Multi-turn chatbot,,Client-facing application,,,,"Various classification metrics (accuracy, precision, recall, f1) for checking how ""relevant"" the responses are. No information is given on how ""relevance"" is measured here.",n,,,,,n,India,Other: unclear,12.0,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,no benchmark,no benchmark,BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.,Y,,,,n,,,,,,,,,,,,,Llama 2 family,12.0,,,"Llama 2 is prompted, but there is also a RAG component",,,,y,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_107,2nd_search_167
Multi-turn chatbot,,Client-facing application,,,,,n,,no benchmark,no benchmark,Comparison of chatbot-inferred PHQ-9 scores with human scorers via ICC and Cohen's kappa,y,Other: Canada,External data set,15.0,Unselected,No,n,,n,,no benchmark,no benchmark,Wonky measurement: embedding similarity between chatbot responses and patient inputs,y,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,no benchmark,no benchmark,"BLEU, ROUGE with counselor responses as reference (CounselChat)",y,,,,n,,,,,,,,,,,,,GPT-3.5 family,9.0,,Primate2022 https://github.com/primate-mh/Primate2022,Prompt engineering + RAG,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot,No clients/patients involved,Internet data -- mental health forum,No users involved,,,,,,Other: not applicable,,2024.0,2nd_search_106,2nd_search_168
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Canada ,No dataset used for development or evaluation,15.0,,,N,,N,,,,on average 0.93 (±0.03) similarity in the embedding space.,Y,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",,N,,N,,,,,,,,," BLEU [57] and ROUGE
[58] metrics",Y,,,,,,,,,,,,,,,,,GPT-3.5 family,9.0,,"Evalution with Primate2022 dataset, but not for development ",,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot,No clients/patients involved,,,,,,,,,,2024.0,2nd_search_106,2nd_search_169
Multi-turn chatbot,,Client-facing application,,,,,n,,no benchmark,no benchmark,Comparison of chatbot-inferred PHQ-9 scores with human scorers via ICC and Cohen's kappa,y,Other: Canada,External data set,15.0,Unselected,No,N,,N,,no benchmark,no benchmark,"on average 0.93 (±0.03) similarity in the embedding space.

Wonky measurement: embedding similarity between chatbot responses and patient inputs",Y,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,n,English,no benchmark,no benchmark,"BLEU, ROUGE with counselor responses as reference (CounselChat)",Y,,,,n,,,,,,,,,,,,,GPT-3.5 family,9.0,,Primate2022 https://github.com/primate-mh/Primate2022 (but only for evaluation),Prompt engineering + RAG,,,,N,,N,,,,,n,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot,No clients/patients involved,Internet data -- mental health forum,No users involved,,,,,,"Other: not applicable (only initial posts, no responses)",,2024.0,2nd_search_106,2nd_search_170
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,Classification into relevant/irrelevant via all-MiniLM-L6-v2,y,,,,,n,India,Self-collected data,17.0,Unknown,Other: unknown,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: unknown,,,,n,,,,n,,,,,,,,,,,,,BERT family; Gemini / Bard family,4.0,,"The chatbot’s corpus comprises 200 manually curated context-response pairs that cover themes like stress, anxiety, depression, and motivation",Fine-tuning of all-MiniLM-L6-v2 for relevance detection + response generation by Gemini 1.5 Flash,,,,n,Gemini 1.5 Flash,n,,,,,n,Conference paper,,,,Richard Gaus,y,4) Crisis Intervention and Ethical Safeguards,Empirical research involving an LLM,n,,Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings,No clients/patients involved,"Other: ""context-response pairs"" -- what is that?",No users involved,,,,,,Unknown,,2025.0,2nd_search_104,2nd_search_171
Multi-turn chatbot,,Client-facing application,,L,B,"Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries) ",Y,,,,,,India,Self-collected data,17.0,Other: no users in study ,Yes,N,,N,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",No,N,,N,,,,,,Other: ,,,,,,,,,,,"Response Quality Assessments (evaluating coherence,
empathy, tone) ",,,,"User Interaction Studies (questionnaires and session 
reviews)",,,,,,Gemini / Bard family,4.0,,"The chatbot’s corpus comprises 200 manually curated
context-response pairs that cover themes like stress, anxiety, 
depression, and motivation","The proposed chatbot system aims to enhance mental health 
assistance through generative AI and embedding-based 
relevance detection. Unlike rule-based chatbots with 
predefined responses, our chatbot responds dynamically to 
diverse user inputs. The core innovations include few-shot 
learning for personalized conversation, semantic similarity 
filtering to ensure relevance, and generative response 
generation using Google Gemini API. ",,,,N,,Y,"To ensure ethical deployment, the system avoids medical 
advice, doesn’t store personal data, and complies with data 
protection standards. Sensitive inputs are not logged or 
reused. ",,,,,Conference paper,,,,Reviewer Two,Y,"The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response",Empirical research involving an LLM,Y,"The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response1",Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings,No clients/patients involved,Other: Self curated ,No users involved,,,,,,,,2025.0,2nd_search_104,2nd_search_172
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,"Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries)

Classification into relevant/irrelevant via all-MiniLM-L6-v2",Y,,,,,n,India,Self-collected data,17.0,Unknown,Other: unknown,N,,N,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",No,N,,N,,,,,n,Other: unknown,,,,n,,,,n,,,,,,,,,,,,,BERT family; Gemini / Bard family,4.0,,"The chatbot’s corpus comprises 200 manually curated context-response pairs that cover themes like stress, anxiety, depression, and motivation","The proposed chatbot system aims to enhance mental health 
assistance through generative AI and embedding-based 
relevance detection. Unlike rule-based chatbots with 
predefined responses, our chatbot responds dynamically to 
diverse user inputs. The core innovations include few-shot 
learning for personalized conversation, semantic similarity 
filtering to ensure relevance, and generative response 
generation using Google Gemini API. ",,,,N,Gemini 1.5 Flash,n,,,,,n,Conference paper,,,,Consensus,Y,"The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response

4) Crisis Intervention and Ethical Safeguards",Empirical research involving an LLM,n,,Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings,No clients/patients involved,"Other: unclear (authors call it ""context-response pairs""",No users involved,,,,,,Unknown,,2025.0,2nd_search_104,2nd_search_173
Multi-turn chatbot,,Client-facing application,,L,B,,Y,,,,,,USA,External data set,28.0,Unselected,No,,,,,L,B,,Y,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,English,L,B,"METEOR and ROUGE tested, but Strawman models",Y,,,,,,,,,,,,,,,,,BERT family; GPT-2 family,4.0,,HOPE (Mental Health cOunselling of PatiEnts),"The authors adapted ChatMGL, a GPT-2–based model originally trained on STEM Q&A with supervised fine-tuning and PPO, for mental health dialogue. They dropped PPO, focusing instead on supervised fine-tuning with Hugging Face’s SFTT and Delta Tuning (a parameter-efficient method that updates only part of the weights). ",,,,,,,,,,,,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,,,Addressing the Challenges of Mental Health Conversations with Large Language Models,No clients/patients involved,Emotional support dialogue -- speech transcripts,No users involved,,,,,,Unknown,,2025.0,2nd_search_103,2nd_search_174
,,Analysis of conversation transcripts,,,,,N,,,,,N,USA,External data set,28.0,Psychopathology,No,,,,,,,,N,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,,,,,English,L,B,,Y,,,,,,,,,,,,,,,,,GPT-2 family,5.0,,"We use the HOPE (Mental Health cOunselling of PatiEnts) [8]
dataset for our experiments. The dataset is specifically designed
for mental health counseling and dialogue-act classification tasks.
The HOPE dataset includes approximately 12,800 utterances ex-
tracted from 212 mental health counseling sessions, sourced from
publicly available YouTube videos. ","To this end, we propose a set of mod-
ifications: supervised fine-tuning on a carefully designed mental
health dataset and incorporating dialogue-act labels that capture
the unique structure and emotional undercurrents of therapeutic
sessions. By evaluating these adaptations, we show how ChatMGL
can be transformed into a more context-aware and empathetic
system, capable of generating clinically relevant and supportive
responses",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Addressing the Challenges of Mental Health Conversations with Large Language Models,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_103,2nd_search_175
Multi-turn chatbot,,Client-facing application,,L,B,,Y,,,,,N,USA,External data set,28.0,Psychopathology,No,,,,,L,B,,Y,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,English,L,B,"METEOR and ROUGE tested, but Strawman models",Y,,,,,,,,,,,,,,,,,BERT family; GPT-2 family,4.0,,"We use the HOPE (Mental Health cOunselling of PatiEnts) [8]
dataset for our experiments. The dataset is specifically designed
for mental health counseling and dialogue-act classification tasks.
The HOPE dataset includes approximately 12,800 utterances ex-
tracted from 212 mental health counseling sessions, sourced from
publicly available YouTube videos. ","The authors adapted ChatMGL, a GPT-2–based model originally trained on STEM Q&A with supervised fine-tuning and PPO, for mental health dialogue. They dropped PPO, focusing instead on supervised fine-tuning with Hugging Face’s SFTT and Delta Tuning (a parameter-efficient method that updates only part of the weights). ",,,,,,,,,,,,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,,,Addressing the Challenges of Mental Health Conversations with Large Language Models,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_103,2nd_search_176
Multi-turn chatbot,Utterance suggestions,Therapist-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,10.0,,,,,,,,,,N,H,W,human therapists,Y,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,Other CBT techniques,,,,,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,"Prompts were comprised of (1) context for the request, (2) how the model was meant to respond, 3) the specific request, and (4) the output format ",,"Two staff psychologists (EB, AJ) each reviewed all 
ChatGPT responses and evaluated them along the 
following dimensions: (1) task completion and (2 
degree to which input information was incorporated in the output.",,,,,,,,,N,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder,,,Yes,,,N,Y,N,,Y,2025.0,2nd_search_102,2nd_search_177
,Other: OCD exposure hierarchy generation,Therapist-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,10.0,,,,,,,,,,n,h,w,benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating,y,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,Other CBT techniques,,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,,,,,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_102,2nd_search_178
Other: ,Other: OCD exposure hierarchy generation (therapy material generation),Therapist-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,10.0,,,,,,,,,,N,H,W,benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating,Y,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,Other CBT techniques,,,,,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,"Prompts were comprised of (1) context for the request, (2) how the model was meant to respond, 3) the specific request, and (4) the output format ",,,,,,,,,,,N,Journal paper,,,,Consensus,,,Empirical research involving an LLM,,,Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder,,,No users involved,,,,,,,,2025.0,2nd_search_102,2nd_search_179
,,Analysis of conversation transcripts,,H,-,“coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” ,Y,,,,,,USA,No dataset used for development or evaluation,21.0,,,,,,,,,,,,,,,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified; Other: Pi, and Replika",5.0,,,Used scripted prompts to elicit chatbot interactions and compared with therapists; coded with MULTI; non-naturalistic brief interactions. — Quote: “We also only examined brief interactions with scripted prompts… The interaction logs … were coded using the Multitheoretical List of Therapeutic Interventions” (Position: p. 12; PubMed Methods).,,,,,,,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,Y,"significant gaps in chatbots’ crisis management abilities, including the absence of risk assessment and failure to refer users to lifesaving crisis hotlines” (Position: p. 12).",A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study,No clients/patients involved,,No,,,,,,,,2025.0,2nd_search_101,2nd_search_180
,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,21.0,,,Y,Table 1,N,,,,,N,-,-,-,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"GPT-4 / GPT-4o family; Other: Chatbots Pi & Replika, Modell behind unclear ",5.0,,,"Each chatbot was prompted in the most up-to-date
version...",17,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,Y,"Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.",Empirical research involving an LLM,Y,"See e.g., ""Strong Framing of Opinions and Suggestions""",A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study,Other: Therapists,,No,,,,,,,,2025.0,2nd_search_101,2nd_search_181
,,Analysis of conversation transcripts,,H,-,“coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” ,Y,,,,,N,USA,No dataset used for development or evaluation,21.0,,,Y,Table 1,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"GPT-4 / GPT-4o family; Other: Pi, Replika",5.0,,,Used scripted prompts to elicit chatbot interactions and compared with therapists; coded with MULTI; non-naturalistic brief interactions. — Quote: “We also only examined brief interactions with scripted prompts… The interaction logs … were coded using the Multitheoretical List of Therapeutic Interventions” (Position: p. 12; PubMed Methods).,,,,N,,N,,,,,N,Journal paper,,,,Consensus,Y,"Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.",Empirical research involving an LLM,Y,"See e.g., ""Strong Framing of Opinions and Suggestions""",A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study,No clients/patients involved; Other: Therapists,,No,,,,,,,,2025.0,2nd_search_101,2nd_search_182
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,Self-collected data,10.0,Unknown,No,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,,No,,,,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,,7.0,,,,,,,,,,,,,,n,Journal paper,,,,Reviewer Two,,,Other: thematic analyses of social media data,,,"""Shaping ChatGPT into my Digital Therapist"": A thematic analysis of social media discourse on using generative artificial intelligence for mental health",No clients/patients involved,Internet data -- mental health forum,No users involved,,,,,,Unknown,,2025.0,2nd_search_100,2nd_search_183
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,10.0,,,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified",7.0,,,,87 Reddit posts,"Qualitative analysis:
This approach involves six main steps: getting familiar with the data; generating initial codes; searching for themes; reviewing themes; defining and naming themes; and producing the report. The analysis process was iterative and recursive, allowing the researchers to move back and forth between these steps as needed to ensure thoroughness and validity. ",,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Population survey,,,"""Shaping ChatGPT into my Digital Therapist"": A thematic analysis of social media discourse on using generative artificial intelligence for mental health",General population,,Yes,,,y,n,n,,y,2025.0,2nd_search_100,2nd_search_184
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,10.0,Other: ,Other: ,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,"Unspecified, might include formal therapy methods",Other: ,,,,,,,,n,Other: ,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified",7.0,,,,87 Reddit posts,"Qualitative analysis:
This approach involves six main steps: getting familiar with the data; generating initial codes; searching for themes; reviewing themes; defining and naming themes; and producing the report. The analysis process was iterative and recursive, allowing the researchers to move back and forth between these steps as needed to ensure thoroughness and validity. ",,,,,,,,,n,Journal paper,,,,Consensus,,,Population survey,,,"""Shaping ChatGPT into my Digital Therapist"": A thematic analysis of social media discourse on using generative artificial intelligence for mental health",General population,Other: ,Yes,,,y,n,n,Other: ,y,2025.0,2nd_search_100,2nd_search_185
Multi-turn chatbot,Patient simulations,Therapist-facing application,,,,,N,,,,,N,USA,Self-collected data,8.0,,,,,,,,,,N,L,B,older and other LLM,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods",,,,,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family,1.0,,,,,,,,,,,,,,N,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries,,,No,,,,,,,,2025.0,2nd_search_99,2nd_search_186
One-turn chatbot (usually Q&A),,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,8.0,,,n,,n,,,,,n,no benchmark,no benchmark,7 different safety questions,y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family; Other: Microsoft Copilot,1.0,,,,,,,n,,n,,,,,n,Journal paper,,,,Richard Gaus,y,entire study is about this,Empirical research involving an LLM,n,,The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_99,2nd_search_187
One-turn chatbot (usually Q&A),Other: ,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,8.0,,,n,,n,,,,,N,no benchmark,no benchmark,7 different safety questions,Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family,1.0,,,,,,,n,,n,,,,,N,Journal paper,,,,Consensus,y,entire study is about this,Empirical research involving an LLM,n,,The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_99,2nd_search_188
Multi-turn chatbot,,Client-facing application,,,,Accuracy,Y,,,,,,Other: Indonesia,,9.0,Unselected,No,N,,N,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),N,,N,,N,,N,,Only fine-tuning; Only prompting,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,,Other: English and Indonesian ,,,,,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,10.0,,data from Counsel Chat repository and Indonesian mental health articels,"In this project, prompts were crafted based on common mental health issues such as  anxiety, stress, and academic pressure. 
Initially trained on a general mental health dataset, the model gained a basic understanding of mental health issues such as anxiety and depression. However, to address local language and cultural nuances, additional data from Indonesian sources, including articles and forums, were integrated.",58,"10 questions regarding user interface, ease of use, chatbot responsiveness, 
and overall satisfaction. might include standard instrument, but not described.",,N,,Y,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,General population,Emotional support dialogue -- chat logs,Yes,,,N,Y,N,Trained professionals,Y,2024.0,2nd_search_98,2nd_search_189
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,"""training accuracy"" of generative LLM. Not sure what this accurac is measuring.",y,,,,,n,Other: Indonesia,External data set,9.0,Unknown,Other: unknown,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: Indonesian,,,,n,,,,n,no benchmark,no benchmark,User experience rating,"A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.",,,,,,,,,GPT-4 / GPT-4o family,10.0,,"Additionally, local data from Indonesian mental health articles and discussion forums fine-tuned the model.",Fine-tuning of GPT-4 + RAG (see Fig. 1) and user interface,58,"A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.",,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,General population,"Other: ""mental health articles and discussion forums""",Yes,,,n,y,n,Unknown,y,2024.0,2nd_search_98,2nd_search_190
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,"""training accuracy"" of generative LLM. Not sure what this accurac is measuring.",y,,,,,n,Other: Indonesia,External data set,9.0,Unknown,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules; Prompting + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: Indonesian,,,,n,,,,n,no benchmark,no benchmark,User experience rating,"A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.",,,,,,,,,GPT-4 / GPT-4o family,10.0,,"Additionally, local data from Indonesian mental health articles and discussion forums fine-tuned the model.",Fine-tuning of GPT-4 + RAG (see Fig. 1) and user interface,58,"10 questions regarding user interface, ease of use, chatbot responsiveness, 
and overall satisfaction. might include standard instrument, but not described.",,n,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,General population,"Other: ""mental health articles and discussion forums""",Yes,,,n,y,n,Unknown,y,2024.0,2nd_search_98,2nd_search_191
Multi-turn chatbot,Utterance suggestions,Client-facing application,,L,B,evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation),Y,,,,,,China,Self-collected data,8.0,Unknown,No,,,,,,,,,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,,,,,,,,,Other: unknown,L,B,metrics… including BLEU-4 and ROUGE” (Position: Model Evaluation),Y,,,,,,,,,,,,,,,,,Other:  ChatGLM-LoRA,8.0,"Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one.","collected a substantial dataset comprising 21,924 records
Fig. 2. Volunteer Screening Chart
through five distinct avenues: instances, media, literature,
guidelines, and databases. To ensure the integrity and rele-
vance of our data, we undertook a meticulous cleaning pro-
cess spearheaded by three experienced quality controllers.
Each controller boasts broad expertise in mental health
counseling. Initially, we eliminated duplicates and linguistic
inaccuracies. Subsequently, we focused on screening data
for key terms such as “sleep”, “dream”, “evening”, “night”,
“bed” and related expressions. This step aimed to sift out
conversational data irrelevant to our study’s objectives, en-
suring that the textual content was pertinent to the context
of potential sleep disorders. We further conducted dialog
integrity screening. Finally, through manual inspection on a
case-by-case basis, we excluded dialogue data that contra-
dicted universal core values. This rigorous curation process
resulted in the selection of 764 dialogues","ChatGLM model with LoRA fine-tuning, optimized using AdamW for 450 epochs. — Quote: “We optimized the model parameters using the AdamW optimizer… training process spanned 450 epochs.” (Position: Model Construction)",16,High satisfaction (>80% smooth design); moderate effectiveness (~50% improved sleep); high acceptance (75% continued use). — Quote: “More than 80% of the volunteers think that our app is well designed and smooth to use” (Position: Application Utilization),,Y,ChatGLM is open-source/self-hostable,,,,,,N,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,,,A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy,Other: unknown,Emotional support dialogue -- chat logs,Yes,,,N,Y,N,Unknown,Y,2024.0,2nd_search_97,2nd_search_192
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,"External data set, modified",8.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Other: unknown,-,-,-,Y,,,,N,,,,,,,,,,,,,Other: ChatGLM,8.0,,"Between November 28 and December 14, 2023, our study collected a substantial dataset comprising 21,924 records through five distinct avenues: instances, media, literature,
guidelines, and database.",LoRA,16,See Application Utilization,,Y,,N,,,,,N,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy,General population,Other: Mixed,Yes,,,N,Y,N,Unknown,Y,2024.0,2nd_search_97,2nd_search_193
Multi-turn chatbot,Other: ,Client-facing application,,L,B,evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation),Y,,,,,N,China,Self-collected data,8.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Other: unknown,-,-,-,Y,,,,,,,,,,,,,,,,,Other: ChatGLM,8.0,"Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one.","collected a substantial dataset comprising 21,924 records
Fig. 2. Volunteer Screening Chart
through five distinct avenues: instances, media, literature,
guidelines, and databases. To ensure the integrity and rele-
vance of our data, we undertook a meticulous cleaning pro-
cess spearheaded by three experienced quality controllers.
Each controller boasts broad expertise in mental health
counseling. Initially, we eliminated duplicates and linguistic
inaccuracies. Subsequently, we focused on screening data
for key terms such as “sleep”, “dream”, “evening”, “night”,
“bed” and related expressions. This step aimed to sift out
conversational data irrelevant to our study’s objectives, en-
suring that the textual content was pertinent to the context
of potential sleep disorders. We further conducted dialog
integrity screening. Finally, through manual inspection on a
case-by-case basis, we excluded dialogue data that contra-
dicted universal core values. This rigorous curation process
resulted in the selection of 764 dialogues",LoRA,16,High satisfaction (>80% smooth design); moderate effectiveness (~50% improved sleep); high acceptance (75% continued use). — Quote: “More than 80% of the volunteers think that our app is well designed and smooth to use” (Position: Application Utilization),,Y,ChatGLM is open-source/self-hostable,N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy,General population,Other: Mixed,Yes,,,N,Y,N,Unknown,Y,2024.0,2nd_search_97,2nd_search_194
Multi-turn chatbot,,Client-facing application,,L,B,Woebot,Y,,L,B,Woebot,Y,Other: Slovenia,Self-collected data,12.0,Unknown,No,N,,N,,,,,N,,,,N,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",Y,"Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair",N,,N,,N,,Fine-tuning + other modules,"Other CBT techniques; Unspecified, might include formal therapy methods; Other: A strategy is selected and adapted according to the metrics, determined in the previous part. The text serves to mitigate the user’s mental health problems based mostly on CBT, and, if the forecasted trend is negative, to try to break 
that trend. To ensure that the user follows the selected strategy, the text on CBT is wrapped in a persuasion strategy.",No,N,,N,,,,,N,Other: unknown,,,,N,,,,N,,,,,,,,,,,,,"Other: Long-short term memory 
network (LSTM)?",12.0,,"First, we introduce a novel, golden standard dataset, comprising panel 
data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires 
and qualitative daily diary entries. ","The system uses a cognitive architecture that combines NLP, a Theory of Mind module with user modeling and forecasting, CBT-based expert knowledge, strategy control, and natural language generation.",42," “Our system” (M = 5.368) found this work’s system statistically significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).",,N,,N,,,,,N,Journal paper,,,,Richard Gaus,Y,"The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.",Empirical research involving an LLM,Y,"Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.",Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,Other: Convenience sample,Other: ,Yes,"One of the ICAs on the 
7-point Likert scale using two measures from UEQ",,N,Y,Y,Unknown,Y,24.0,2nd_search_96,2nd_search_195
Multi-turn chatbot,,,,L,B,"Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation)",Y,,L,B,"Table 6 Accuracy of machine learning models in forecasting 7-day 
SAD (stress, anxiety, and depression) levels based on single text 
entries against extrapolation",Y,Other: Slovenia,Self-collected data,12.0,Other: convenience sample,No,,,,,,,,N,,,,N,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",,,Y,comparison with Woebot — Quote: “system outperformed Woebot … in reducing stress … and anxiety levels” (Position: Results),,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,,,,,,,,,Other: unknown,,,,N,,,,,L ,B,"Stress, Anxiety, Depression",Woebot,,,,,,,,,"Other: highest accuracy: 91.41% using k-nearest neighbors (kNN),highest accuracy of other systems: 84% using … long-short term memory network (LSTM)",12.0,,"495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores … and qualitative daily diary entries","“cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies” (Position: Methods)",42," “Our system” (M = 5.368) found this work’s system statistically 
significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the 
measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).",,,,,,,,,N,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,General population,Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) ,Yes,UEQ,,N,Y,Y,Unknown,Y,2024.0,2nd_search_96,2nd_search_196
Multi-turn chatbot,,Client-facing application,,L,B,"Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation)",Y,,L,B,"Table 6 Accuracy of machine learning models in forecasting 7-day 
SAD (stress, anxiety, and depression) levels based on single text 
entries against extrapolation",Y,Other: Slovenia,Self-collected data,12.0,Unknown,No,,,,,,,,N,,,,N,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",Y,"Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair",Y,,,,,,Fine-tuning + other modules,"Other CBT techniques; Unspecified, might include formal therapy methods",No,,,,,,,,N,Other: unknown,,,,N,,,,N,,,,,,,,,,,,,"GPT-3 family; Other: GPT-Neo, GPT-J, AI21 Jurassic-1",12.0,,"First, we introduce a novel, golden standard dataset, comprising panel 
data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires 
and qualitative daily diary entries. ","“cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies” (Position: Methods)",42," “Our system” (M = 5.368) found this work’s system statistically significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).",,,,,,,,,N,Journal paper,,,,Consensus,Y,"The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.",Empirical research involving an LLM,Y,"Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.",Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,General population,Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) ,Yes,"One of the ICAs on the 
7-point Likert scale using two measures from UEQ",,N,Y,Y,Other: n.a.,Y,2024.0,2nd_search_96,2nd_search_197
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Germany,No dataset used for development or evaluation,30.0,,,n,,n,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,y,"either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle",n,,n,,Prompting + other modules,CBT: Motivational interviewing,,n,,n,,,,,,,,,,,,,,,L,S,"Client Evaluation of Motivational Interviewing Scale (CEMI) (Madson et al., 2013, 2015, 2016)",without MI fine-tuning,L,S,"Working Alliance Inventory - short
revised (WAI-SR) ",without MI fine-tuning,L,S,𝛥 Readiness to Change,without MI fine-tuning,GPT-4 / GPT-4o family,4.0,,,"MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules",,,,n,,y,in the schema,,,,,Journal paper,,,,Reviewer Two,Y,"We identify harmful outputs in two ways: First, we take participant’s
ratings of generated turns into account, specifically with respect to
turns rated as offensive/harmful. Secondly, we draw on MI-literature as
well as empathy research and ethical academic discussions about LLMs
outlined below to identify behaviours which are unsuited specifically
in the context of LLM-delivered MI-conversations. Based on this, we
create an annotation scheme to be applied to the logged conversations after data collection. This annotation scheme goes beyond the
identification of MI-Inadherent behaviours defined in the MISC (listed
in Table 1). While such behaviours are not suited for MI interactions
in general, we lay special emphasis on such actions that would be
suitable in MI-interactions with a human counsellor, but should be
avoided when responses are generated by LLMs. We focus specifically
on significant concerns around anthropomorphism and the unreliable
factual correctness of LLM outputs",Empirical research involving an LLM,y,"To test the validity of the annotation scheme, the first author
annotated each GPT-4 generated turn collected in the user study along
the 4 dimensions of undesired behaviour outlined above, allowing
multiple labels per turn. Based on the annotation scheme, GPT-outputs
containing advice, factual information, sharing personal information, or
revealing emotions, as well as outputs with directive or confrontational
wording were marked as harmful. Following this, a second annotator
(not an author) was employed to annotate a share of the turns along
the four axes. 50 randomly sampled turns, along with the guidelines,
were given to the annotator for annotation in a training round.","LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour",General population,,Yes,,,y,y,n,,y,2025.0,2nd_search_94,2nd_search_198
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Germany,No dataset used for development or evaluation,30.0,,,y,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),y,,y,Control: GPT-4 out-of-the-box,n,,n,,Prompting + other modules,CBT: Motivational interviewing,,n,,n,,,,,n,,,,,n,,,,n,l,s,"Client Evaluation of Motivational Interviewing Scale (CEMI), Readiness to Change delta",Measures MI adherence of clients. Benchmark: GPT-4 out of the box,l,b,Number of harmful outputs (user-rated),Benchmark: GPT-4 out of the box,l,s,German version of the Working Alliance Inventory - short revised (WAI-SR),Benchmark: GPT-4 out of the box,GPT-4 / GPT-4o family,4.0,,,"GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.",,"Also measuring undesirable output via annotation.
For qualitative results, see 4.3.3.",,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,"LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour",General population,,Yes,German version of the Working Alliance Inventory - short revised (WAI-SR),,y,y,y,,y,2025.0,2nd_search_94,2nd_search_199
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Germany,No dataset used for development or evaluation,30.0,,,y,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,WAI-SR but is neither symptom nor function scale,y,"either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle

Control: GPT-4 out-of-the-box",n,,n,,Prompting + other modules,CBT: Motivational interviewing,,n,,n,,,,,n,,,,,n,,,,n,L,S,"Client Evaluation of Motivational Interviewing Scale (CEMI) (Madson et al., 2013, 2015, 2016); Readiness to Change delta",Measures MI adherence of clients. Benchmark: GPT-4 out of the box without MI fine-tuning,l,b,Number of harmful outputs (user-rated),Benchmark: GPT-4 out of the box without MI fine-tuning,L,S,German version of the Working Alliance Inventory - short revised (WAI-SR),Benchmark: GPT-4 out of the box without MI fine-tuning,GPT-4 / GPT-4o family,4.0,,,"GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.

MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules",159,"Also measuring undesirable output via annotation.
For qualitative results, see 4.3.3.",,n,,n,,,,,n,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,n,,"LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour",General population,,Yes,,,y,y,n,,y,2025.0,2nd_search_94,2nd_search_200
Multi-turn chatbot,,Analysis of conversation transcripts,,L,unsure,,Y,,,,,,India,External data set,16.0,Unknown,Yes,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,Other CBT techniques,Yes,,,,,,,,,English,L,unsure,Bleu score,Y,,,,,L,B,Empathy Score,,L,B,Human Helpfulness,,,,,,GPT-2 family,4.0,,epsilon3/cbt-cognitive-distortions-analysis,,,,,,,,,L,W,,Y,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Early Detection and Personalized Intervention in Mental Health,No clients/patients involved,Psychotherapy -- chat logs,No users involved,,,,,,Other: ,,2025.0,2nd_search_93,2nd_search_201
Multi-turn chatbot,,Client-facing application,,,,Performance of cognitive distortion classification of non-generative BERT model is given (but it's non-generative),n,,,,,n,India,External data set,14.0,Unknown,Yes,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,CBT: Cognitive restructuring,Yes,n,,n,,,,,n,English,l,b,BLEU against human responses. Benchmark: non fine-tuned GPT-2,y,,,,n,l,b,User rating,"User scores of ""helpfulness"" and ""empathy"". Benchmark: non fine-tuned GPT-2.",,,,,,,,,BERT family; GPT-2 family,2.0,,"epsilon3/cbt-cognitive-distortions-analysis dataset
https://huggingface.co/datasets/epsilon3/cbt-cognitive-distortions-analysis
",Fine-tuning of GPT-2 + other modules such as BERT-based cognitive distortion classification,,Only quantitative scoring of ,,y,GPT-2,n,,l,b,Benchmark: non fine-tuned GPT-2,y,Conference paper,,,,Richard Gaus,y,"The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.",Empirical research involving an LLM,n,,Early Detection and Personalized Intervention in Mental Health,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: not applicable,,2025.0,2nd_search_93,2nd_search_202
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,Performance of cognitive distortion classification of non-generative BERT model is given (but it's non-generative),Y,,,,,,India,External data set,14.0,Unknown,Yes,n,,n,,,,,,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,CBT: Cognitive restructuring,Yes,n,,n,,,,,n,English,l,b,BLEU against human responses. Benchmark: non fine-tuned GPT-2,Y,,,,n,l,b,User rating,"User scores of ""helpfulness"" and ""empathy"". Benchmark: non fine-tuned GPT-2.",,,,,,,,,BERT family; GPT-2 family,2.0,,"epsilon3/cbt-cognitive-distortions-analysis dataset
https://huggingface.co/datasets/epsilon3/cbt-cognitive-distortions-analysis
",Fine-tuning of GPT-2 + other modules such as BERT-based cognitive distortion classification,,Only quantitative scoring of ,,y,GPT-2,n,,l,b,Benchmark: non fine-tuned GPT-2,Y,Conference paper,,,,Consensus,y,"The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.",Empirical research involving an LLM,n,,Early Detection and Personalized Intervention in Mental Health,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: not applicable,,2025.0,2nd_search_93,2nd_search_203
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,2.0,,,,,,,,,,,,,,,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",,,,,,,,,Only prompting,"Other CBT techniques; Unspecified, might include formal therapy methods",,,,,,,,,,,,,,N,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",7.0,"The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots.",,"Evaluated existing GPT-based chatbots via persona-based simulated interactions, scored with CAPE framework. ",0,"Strengths = accessibility, safety in most cases; Weaknesses = occasional advertising, reading level too high, missing referral for severe depression. ",,,,,,,,,,Journal paper,,,,Reviewer Two,Y?,"(suicidality referral present, severe depression detection failed) — Quote: “the ability of the chatbots to always recommend connecting to another person for users expressing suicidality … frequent failure to connect personas exhibiting severe depression to a human",Population survey,Y,we did not find any unsafe recommendations for managing depression or wellness,Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study,No clients/patients involved,,Yes,??CAPE Framework — Quote: “The CAPE framework lays a foundation for future quality assessments” (Position: Discussion),,N,Y,N,,,2025.0,2nd_search_92,2nd_search_204
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,2.0,,,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Other CBT techniques; Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"ChatGPT, model unspecified",7.0,,,,2,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,Y,"Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.",Empirical research involving an LLM,Y,"Chatbots mostly preserved privacy and avoided harmful
content. However",Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study,Other: Researchers,,No,,,,,,,,2025.0,2nd_search_92,2nd_search_205
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,2.0,,,N,,N,,,,,N,no benchmark,no benchmark,"CAPE category ratings, no benchmark",Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Other CBT techniques; Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"ChatGPT, model unspecified",7.0,"The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots.",,"Evaluated existing GPT-based chatbots via persona-based simulated interactions, scored with CAPE framework. ",,,,N,,N,,,,,N,Journal paper,,,,Consensus,Y,"Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.",Empirical research involving an LLM,Y,"Chatbots mostly preserved privacy and avoided harmful
content. ",Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study,No clients/patients involved,,No,,,,,,,,2025.0,2nd_search_92,2nd_search_206
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,China,No dataset used for development or evaluation,26.0,,,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),Y?,Sam Scale?,,,,,,,Fine-tuning + other modules,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,,,,,,N,,,,,,,,,,,,,,,,,"Other: MiniMax 6.5s (245K), Doubao Function Call model (32K) ",4.0,,,"Used fine-tuning and modular architecture (context, expression, feedback generators) to simulate different therapeutic styles.
Three key technical components were employed: the Context Generator, the Expression Expander, and the Feedback Generator",30,"emotional valence and dominance improved; deliberate style increased happiness, rapid style increased surprise. ",,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,“Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles,People with some symptoms but not disorder (determined by symptom scales or questionnaires),,Yes,SAM-Scale,,Y,Y,Y,,Y,2025.0,2nd_search_91,2nd_search_207
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,No dataset used for development or evaluation,26.0,,,N,,N,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),Y,Self-Assessment Manikin Scale + heart rate variability,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,Other:  MiniMax 6.5s(245K),4.0,,,"Despited being labeled as fine-tuning, I suppose it is more about promtpting and other modules (Context Generator, Expression Expander, and the Feedback Generator)?",30,,,N,,N,,,,,N,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,“Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles,Patients recruited in hospital or outpatient treatment facility,,No,,,,,,,,2025.0,2nd_search_91,2nd_search_208
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,No dataset used for development or evaluation,26.0,,,N,,N,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),Y,(Self-Assessment Manikin Scale)+ Heart Rate variability,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"Other: MiniMax 6.5s (245K), Doubao Function Call model (32K) ",4.0,,,"Used fine-tuning and modular architecture (context, expression, feedback generators) to simulate different therapeutic styles.
Three key technical components were employed: the Context Generator, the Expression Expander, and the Feedback Generator.

Not clear if they really fine-tuned.",30,"emotional valence and dominance improved; deliberate style increased happiness, rapid style increased surprise. ",,N,,N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,“Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles,Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires),,No,SAM-Scale,,,,,,,2025.0,2nd_search_91,2nd_search_209
Multi-turn chatbot,,Client-facing application,,,,,Y?,,,,,,Other: Iran,External data set,19.0,Unknown,No,,,,,,,,,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods",Yes,,,,,,,,,Other: Persian,,,,N,,,,,,,N,,,,,,,,,,BERT family,1.0,"This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy.","ArmanEmo dataset, a Persian emotion detection dataset","the proposed system integrates several modules: emotion detection, disorder identification, and language model validation see figure 1 ",9,,,Y,"ParsBERT, XLM-R are open-source",,,,,,N,Conference paper,,,,Reviewer Two,Y,see 4.1,Empirical research involving an LLM,Y,"Responses are validated for non-toxicity before being delivered to
the user.","Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",Patients with disorder explicitly based on ICD or DSM,Emotional support dialogue -- speech transcripts,No,,,,,N,Unknown,,2025.0,2nd_search_90,2nd_search_210
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Iran,Other: External data set + a modified (translated) data set,19.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",Y,,N,,N,,N,,Only fine-tuning; Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,Other: Persian,,,,N,,,,N,,,,,,,,,,,,,BERT family; GPT-4 / GPT-4o family,1.0,,ArmanEmo & Jigsaw Toxic Comment Classification translated into persian,Trained on ArmanEmo dataset & Jigsaw Toxic Comment Classification translated into persian + user message validiation module. ,9,,,N,,N,,,,,N,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,Y,"In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users","Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",Other: unknown,Other: Emotion detection dataset and classification dataset,Yes,,,Y,Y,-,Other: No responders,N,2025.0,2nd_search_90,2nd_search_211
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Iran,External data set,19.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",Y,PHQ-9,N,,N,,N,,Only fine-tuning; Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,Other: Persian,,,,N,,,,N,,,,,,,,,,,,,BERT family; GPT-4 / GPT-4o family,1.0,"This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy.",ArmanEmo,Trained on ArmanEmo dataset & Jigsaw Toxic Comment Classification translated into persian + user message validiation module. ,9,,,N,,N,,,,,N,Conference paper,,,,Consensus,Y,see 4.1,Empirical research involving an LLM,Y,"In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users","Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",People with some symptoms but not disorder (determined by symptom scales or questionnaires),Internet data -- mental health forum,Yes,,,Y,Y,-,Lay people,N,2025.0,2nd_search_90,2nd_search_212
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,China,No dataset used for development or evaluation,21.0,,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,L,B,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),Y,,,,,,,,,,,,,,,,,,,,,,"Qwen family; Other: ChatGLM, ERNIE Bot, Qianwen (as model and as judge)",6.0,,,"Designed prompt phrases, iteratively refined; three prompting regimes (zero-shot, few-shot, CoT); modeled counselor reasoning style; two LLM families evaluated (ChatGLM, ERNIE Bot; also Qianwen)",,,,Y,"	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.)",,,,,,,Conference paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_89,2nd_search_213
Multi-turn chatbot,Patient simulations,Client-facing application,,,,,,,,,,,China,Self-collected data,21.0,Unselected,No,,,,,,,,,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",No,,,,,,,,Y,Chinese,,,,,,,,,,,,,,,,,,,,,"Other: ERNIE Bot, Qianwen, and ChatGLM",6.0,,"We gathered doctor-
patient conversations from online medical consultation web-
sites and identified representative psychological issues from
the experiences of people around us and public platforms
such as Weibo and Zhihu. Our dataset encompasses the most
common mental health concerns and A substantial portion of
the themes is centered on stress and interpersonal relationships,
as these two are important contributing factors to mental
disorders. We also considered the perspectives of the LGBTQ
community and diverse cultural groups.Utilizing this data, we
crafted 31 unique questions.","Three different prompt methods: Zero-Shot, Few-Shot, Chain of Thought (CoT)",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering,No clients/patients involved,Other: ,No users involved,,,,,,Unknown,,2024.0,2nd_search_89,2nd_search_214
Multi-turn chatbot,Patient simulations,Client-facing application,,,,,,,,,,,China,No dataset used for development or evaluation,21.0,Other: ,Other: ,,,,,,,,,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",Other: ,,,,,L,B,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),Y,Other: ,,,,,,,,,l,b,llm-as-a-judge emotional resonance and understanding,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),l,b,llm-as-a-judge professionalism etc.,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),,,,,"Qwen family; Other: ChatGLM, ERNIE Bot, Qianwen (as model and as judge)",6.0,,,"Designed prompt phrases, iteratively refined; three prompting regimes (zero-shot, few-shot, CoT); modeled counselor reasoning style; two LLM families evaluated (ChatGLM, ERNIE Bot; also Qianwen)",,,,Y,"	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.)",,,,,,,Conference paper,,,,Consensus,,,Empirical research involving an LLM,,,Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering,No clients/patients involved,Other: ,No users involved,,,,,,Other: ,,2024.0,2nd_search_89,2nd_search_215
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Iran,No dataset used for development or evaluation,26.0,,,,,,,,,,N,,,,N,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Other: None,"Unspecified, might include formal therapy methods",,,,,,,,,N,,,,,N,,,,N,,,N,,,,N,,,,N,,Other: unspecified,5.0,,,,1,,,,,,,,,,N,Journal paper,,,,Reviewer Two,,,Other: Case study,,,The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy,Patients with disorder explicitly based on ICD or DSM,,No,,,N,N,N,,N,2025.0,2nd_search_88,2nd_search_216
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Iran,No dataset used for development or evaluation,26.0,,,,,,,,,,n,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified",5.0,,,,1,"This article is a case study with loose, qualitative descriptions",,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Other: Case study,,,The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy,People with some symptoms but not disorder (determined by symptom scales or questionnaires),,Yes,,,y,n,n,,y,2025.0,2nd_search_88,2nd_search_217
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Iran,No dataset used for development or evaluation,26.0,,,,,,,,,,N,,,,N,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,,,,,,N,,,,,N,,,,N,,,N,,,,N,,,,N,,"ChatGPT, model unspecified",5.0,,,,1,"This article is a case study with loose, qualitative descriptions",,,,,,,,,N,Journal paper,,,,Consensus,,,Empirical research involving an LLM,,,The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy,People with some symptoms but not disorder (determined by symptom scales or questionnaires),,Yes,,,y,n,n,,y,2025.0,2nd_search_88,2nd_search_218
,Utterance suggestions,Client-facing application,,L,B,Recall,Y,,L,S,"Pearson, Spearman, Kendall’s Tau",Y,USA,Other: both self-collected and external data set ,30.0,Unknown,Yes,,,,,,,,,,,,,,,,,,,,,,Fine-tuning + other modules; Prompting + other modules,CBT: Motivational interviewing,No,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,BERT family; GPT-3.5 family; Mistral family; Other: Flan ,12.0,,,,,,,,,,,,,,,,,,,Reviewer Two,,,Empirical research involving an LLM,,,Evaluating Language Models for Assessing Counselor Reflections,,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_87,2nd_search_219
,Treatment fidelity feedback,Therapist-facing application,,,,yes for pair but this is not genAI,n,,l,w,"Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark)",y,USA,"External data set, modified",30.0,Unknown,Yes,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,CBT: Motivational interviewing,Yes,,,,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,BERT family; T5 family; GPT-3.5 family; Mistral family,12.0,,"Original dataset: Perez-Rosas motivational interviewing dataset.
Modification: Added custom annnotations
Also extended with their own MI prompts.
We thus obtained 4,365 client prompt-counselor reflection pairs, including 2,429 prompt-CR and 1,636 prompt-SR pairs.

public under https://lit.eecs.umich.edu/downloads.html#PAIR","They fine-tune RoBERTa but only prompt the generative models (Flan-t5, Mistral, GPT-3.5) that they use",,,,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Evaluating Language Models for Assessing Counselor Reflections,,Other: MI reflections (client prompt - counselor reflection pairs),No users involved,,,,,,Trained professionals,,2024.0,2nd_search_87,2nd_search_220
,Treatment fidelity feedback,Therapist-facing application,,l,w,Recall@1 (Tables 3 and 4),Y,,l,w,"Pearson, Spearman, Kendall’s Tau. Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark)",y,USA,"External data set, modified",30.0,Unknown,Yes,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,CBT: Motivational interviewing,No,,,,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,BERT family; T5 family; GPT-3.5 family; Mistral family,12.0,,"Original dataset: Perez-Rosas motivational interviewing dataset.
Modification: Added custom annnotations
Also extended with their own MI prompts.
We thus obtained 4,365 client prompt-counselor reflection pairs, including 2,429 prompt-CR and 1,636 prompt-SR pairs.

public under https://lit.eecs.umich.edu/downloads.html#PAIR","They fine-tune RoBERTa but only prompt the generative models (Flan-t5, Mistral, GPT-3.5) that they use",,,,,,,,,,,n,Journal paper,,,,Consensus,,,Empirical research involving an LLM,,,Evaluating Language Models for Assessing Counselor Reflections,,Other: MI reflections (client prompt - counselor reflection pairs),No users involved,,,,,,Trained professionals,,2024.0,2nd_search_87,2nd_search_221
One-turn chatbot (usually Q&A),,Client-facing application,,,,,,,,,,,USA,External data set,12.0,Unknown,No,,,,,,,,,H,S,,Y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Only prompting,"Peer support conversation; Informal counseling (e.g., emotional support conversation)",No,,,,,,,,,English,,,,,,,,,H,B,EPIT-ONE framework,,H,W,MITI,,H,S,Empathy metrics ,,GPT-4 / GPT-4o family,11.0,"used metrics: EPIT-ONE framework, MITI, RoVERTa"," Reddit with of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.",,not mentioned ,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Can AI relate: Testing large language model response for mental health support,General population,Other: Reddit Posts,No users involved,,,,,,Unknown,,2024.0,2nd_search_85,2nd_search_222
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,External data set,12.0,Unselected,No,y,Appendix Table 2,y,,,,,n,h,b,"Measures: Likert empathy rating, MITI global score. Benchmark: human peer supporter from Reddit post.",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,no benchmark,no benchmark,Empathy rating unequality,no benchmark. Measure: difference in automated empathy ratings between different ethnic groups,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,11.0,,"No specific name: ""We use the Reddit API to collect
a new dataset of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.""

Available under https://github.com/skgabriel/mh-eval",,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Can AI relate: Testing large language model response for mental health support,No clients/patients involved,Internet data -- mental health forum,No users involved,,,,,,Lay people,,2024.0,2nd_search_85,2nd_search_223
One-turn chatbot (usually Q&A),,Client-facing application,,,,,n,,,,,n,USA,External data set,12.0,Unselected,No,y,Appendix Table 2,y,,,,,n,h,b,"Measures: Likert empathy rating, MITI global score. Benchmark: human peer supporter from Reddit post.",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only prompting,"Peer support conversation; Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,l,unknown,Empathy rating unequality,"Table 1: mechanical turk worker ratings (""Human"") vs. GPT-4",,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,11.0,"used metrics: EPIT-ONE framework, MITI, RoVERTa","No specific name: ""We use the Reddit API to collect
a new dataset of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.""

Available under https://github.com/skgabriel/mh-eval",,,,,n,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Can AI relate: Testing large language model response for mental health support,No clients/patients involved,Internet data -- mental health forum,No users involved,,,,,,Lay people,,2024.0,2nd_search_85,2nd_search_224
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,Multimodal Framework for Therapeutic Consultations,,,,,,,,,,,,2nd_search_83,2nd_search_225
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,Multimodal Framework for Therapeutic Consultations,,,,,,,,,,,,2nd_search_83,2nd_search_226
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,Multimodal Framework for Therapeutic Consultations,,,,,,,,,,,,2nd_search_83,2nd_search_227
Other: Emotional reflection generation,,Client-facing application,,,,,n,,,,,n,Other: Turkey,No dataset used for development or evaluation,20.0,,,n,,n,,,,,n,h,w,likert-scale appropriateness rating of emotional reflection responses. compared GPT-4 against human counselor responses.,y,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,11.0,,,"The model was trained using one-shot 
learning, meaning that it was given a client statement and 
a counselor’s emotional reflection response as input, and 
then used that information to generate its own emotional 
reflection response.",200,,,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections,Other: unknown recruitment criteria,,No,,,,,,,,2024.0,2nd_search_82,2nd_search_228
One-turn chatbot (usually Q&A),,Analysis of conversation transcripts,,,,,,,,,,,Other: Turkey,Other: emotional expressions from psychological counseling sessions,20.0,Unknown,No,N,,N,,,,,,,,,,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",No,N,,N,,,,,,Other: ,,,,,,,,,H,W,"Average rating of emotional reflections 
",human therapists ,,,,,,,,,GPT-3.5 family,11.0,,,"no development, only prompting ",,,,N,,N,,,,,,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_82,2nd_search_229
Other: Emotional reflection generation,,Client-facing application,,,,,n,,,,,n,Other: Turkey,Self-collected data,20.0,Other: unknown,No,N,,N,,,,,n,h,w,likert-scale appropriateness rating of emotional reflection responses. compared GPT-4 against human counselor responses.,y,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",No,N,,N,,,,,n,Other: unknown,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,11.0,,"In this study, 200 real counseling ses
sions were recorded and transcribed to capture authentic 
client interactions. From these, 50 client statements were 
identified that best represented a range of emotions such as 
sadness, guilt, anxiety, determination, and anger, based on 
the PANAS Scale, along with helplessness, love, trust, lone
liness, and doubt from existing literature, aiming to ensure

emotional expressions from psychological counseling sessions","The model was trained using one-shot 
learning, meaning that it was given a client statement and 
a counselor’s emotional reflection response as input, and 
then used that information to generate its own emotional 
reflection response.",,,,N,,N,,,,,n,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_82,2nd_search_230
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Korea,No dataset used for development or evaluation,3.0,,,n,,n,,,,,n,no benchmark,no benchmark,"Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.",y,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,12.0,,,,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_81,2nd_search_231
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Korea,No dataset used for development or evaluation,3.0,,,N,,N,,,,,N,,,"We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance",Y,,N,,N,,N,,N,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation)",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,6.0,,,"Employs an ""Empathetic Meta-Chain (EMC) learning method"" that combines meta-learning, Chain of Thought prompting, and counseling strategies. Multiple approaches were compared including zero-shot, few-shot, CoT, meta-learning, and EMC.",Not specified,,,N,,N,,,,,N,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,No clients/patients involved,,No,,,,,,,,2024.0,2nd_search_81,2nd_search_232
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Korea,No dataset used for development or evaluation,3.0,,,N,,N,,,,,N,no benchmark,no benchmark,"We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance

Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.",Y,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,12.0,,,,,,,N,,N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_81,2nd_search_233
One-turn chatbot (usually Q&A),,"Other: question-answering system based on
retrieval augmented generation — this approach allows the
system to generate answers based on a corpus of documents
curated by psychologists and psychiatrists",,,,,,,,,,,Other: Spain,Self-collected data,7.0,,No,N,,N,,L,B,,Y,,,,,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,N,,N,,,,,,Other: spanish,,,,,,,,,,,,,,,,,,,,,BERT family,12.0,,,Retrieval Augmented Generation (RAG) kombiniert mit prompting,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*,No clients/patients involved,Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists,No,,,N,N,N,,N,2023.0,2nd_search_80,2nd_search_234
,,,,no benchmark,no benchmark,"F1 score, not quiet clear what of.",y,,,,,n,Other: Spain,External data set,7.0,Unknown,No,n,,n,,no benchmark,no benchmark,Distance between embeddings of reference and output.,y,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,,,No,n,,n,,,,,n,Other: Spanish,,,,n,,,,n,,,,,,,,,,,,,BERT family; GPT-2 family; Other: BLOOMZ,12.0,,"The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG",,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*,,Other: Mental health education documents,No users involved,,,,,,Unknown,,2023.0,2nd_search_80,2nd_search_235
One-turn chatbot (usually Q&A),,Client-facing application,,no benchmark,no benchmark,"F1 score, not quiet clear what of.",y,,,,,n,Other: Spain,External data set,7.0,Other: not applicable,No,n,,n,,no benchmark,no benchmark,Distance between embeddings of reference and output.,y,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Prompting + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: Spanish,,,,n,,,,n,,,,,,,,,,,,,BERT family; GPT-2 family; Other: BLOOMZ,12.0,,"a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists

The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG",Retrieval Augmented Generation (RAG) kombiniert mit prompting,,,,n,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*,No clients/patients involved,Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists,No users involved,,,,,,Other: not applicable,,2023.0,2nd_search_80,2nd_search_236
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,India,External data set,25.0,Unknown,No,N,,N,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,N,,Y,,,,,,Other: Unsure,,,,,,,,,,,,,,,,,,,,,BERT family; Other: LTSM,4.0,,data from Kaggle,,,,,N,,N,,,,,,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Conversational ai for mental health support,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,N,N,N,Unknown,N,2024.0,2nd_search_75,2nd_search_237
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,Confusion matrices for the BERT and LSTM models (Fig. 7 and 8),y,,,,,n,India,External data set,25.0,Unknown,Other: unknown,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: unknown,,,,n,,,,n,,,,,,,,,,,,,BERT family,4.0,,"Data from various sources (""Simple conversations, psychological questions, information on classic therapy sessions, and general advice for people with anxiety and depression. This data can be used to train a chatbot model that can act as a therapist to assist patients with anxiety and depression. The dataset includes intents. An ""intent"" is the purpose of a user's message. There are a total of 89 such intents, some of which are “greeting, sad, stressed, worthless, depressed, happy, anxious, sleep, scared, hate, problem” etc."").

Seemingly for intent classification.","Fine-tuning of BERT, likely for intent classification. Not clear how responses are generated -- via LSTM?",,,,y,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Conversational ai for mental health support,No clients/patients involved,Other: unknown,No users involved,,,,,,Unknown,,2024.0,2nd_search_75,2nd_search_238
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,India,External data set,25.0,Unknown,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,4.0,,"Data from various sources (""Simple conversations, psychological questions, information on classic therapy sessions, and general advice for people with anxiety and depression. This data can be used to train a chatbot model that can act as a therapist to assist patients with anxiety and depression. The dataset includes intents. An ""intent"" is the purpose of a user's message. There are a total of 89 such intents, some of which are “greeting, sad, stressed, worthless, depressed, happy, anxious, sleep, scared, hate, problem” etc."").

Seemingly for intent classification.",,,,,,,,,,,,,Conference paper,,,,Consensus,,,Empirical research involving an LLM,,,Conversational ai for mental health support,No clients/patients involved,,No users involved,,,,,,Unknown,,2024.0,2nd_search_75,2nd_search_239
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,China,Self-collected data,28.0,Unselected,No,,,,,,,,,,,,,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",,,,,,,,,,"Informal counseling (e.g., emotional support conversation)",No,,,,,,,,,Chinese,,,,,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,,qualitative data from 69 students,,69,,,,,,,,,,,Journal paper,,,,Reviewer Two,,,Other: Qualitative Study ,,,AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks,General population,Other: qualitative interviews,Yes,,,Y,N,N,,N,2025.0,2nd_search_72,2nd_search_240
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,No dataset used for development or evaluation,28.0,,,,,,,,,,n,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,Other: various GenAI chatbots,2.0,,,,69,"This study employed a deductive thematic analysis, using the conceptual framework of Grodniewicz and Hohol’s (2023) three challenges in AI psychotherapy as a general guide",,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Population survey,,,AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks,General population,,Yes,,,y,n,n,,y,2025.0,2nd_search_72,2nd_search_241
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,No dataset used for development or evaluation,28.0,Other: ,Other: ,,,,,,,,n,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,,"Unspecified, might include formal therapy methods",Other: ,,,,,,,,n,Other: ,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family; Other: various GenAI chatbots,2.0,,,,69,"This study employed a deductive thematic analysis, using the conceptual framework of Grodniewicz and Hohol’s (2023) three challenges in AI psychotherapy as a general guide",,,,,,,,,n,Journal paper,,,,Consensus,,,Population survey,,,AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks,General population,Other: ,Yes,,,Y,N,N,,y,2025.0,2nd_search_72,2nd_search_242
Multi-turn chatbot,Patient simulations,Therapist-facing application,,,,,N,,,,,N,Other: Switzerland,No dataset used for development or evaluation,6.0,Unselected,,Y,"Table 1 Study population characteristics
Male 
Total 5 
Age (26–30 years) 4;
(31–35 years) 1
Years of training 2.6 years (range:
1–3 years)
Swiss nationality/other nationality (EU) 0/5 
Psychiatry/other specialty/psychologist 5/0/0 
Female
Total 15
(26–30 years) 1; (31–35 years) 8;
(36–40 years) 5; (> 40 years) 1
2.4 years (range: 1–7 years)
Swiss nationality/other nationality (EU) 7/8
Psychiatry/other specialty/psychologist 4/4/7",N,,,,,N,L,B,compared to chatbots in custumer service,Y,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,Y,"Primarily, as text-based systems, LLMs
cannot interpret crucial nonverbal cues
and they lack cultural sensitivity, which
is essential for a comprehensive mental
health assessment [2]. Secondly, research
shows that users tend to over-rely on the
accuracy of information delivered by LLMs,
which may discourage patients from seeking timely professional mental health assistance rather than facilitating it [5, 24].
Thirdly, technical challenges, such as AI
hallucination and insufficient technological literacy among users or connectivity,
further compromise the accessibility and
reliability of these systems. Fourthly, given
thelimited regulationof AI, the storageand
potential use of sensitive mental health
data for LLM training pose significant data
privacy concerns, underscoring the need
for robust legal and ethical frameworks. Finally, LLMs lack real-time safety protocols,
such as direct emergency service contact,
presenting a critical limitation when users
are in immediate danger.",,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-3.5 family,6.0,,,prompted to simulate a psychiatric first-responder chatbot,20,"10-point Likert scale rating the overall experience, the pleasantness of the chat, the appropriateness of the responses, the realism of the chatbot, and the helpfulness of the advice
Yes/No/ dont know:
1. It would be used by patients. (Hypothesized patients’ subjective norm)
2. It would be recommended by outpatient psychiatrists (Subjective norm)
3. It would relieve the psychiatric emergency department. (Anticipated
benefit)
4. It would be helpful for patients in
a crisis situation. (Expectancy of
usefulness)",,N,,N,,,,,N,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study,,,Yes,,,N,Y,N,Trained professionals,Y,2025.0,2nd_search_71,2nd_search_243
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Switzerland,No dataset used for development or evaluation,6.0,,,n,,n,,,,,n,no benchmark,no benchmark,no benchmark,y,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",n,,n,,n,,n,,Only prompting,"Unspecified, might include formal therapy methods",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family,6.0,,,,,,,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_71,2nd_search_244
Multi-turn chatbot,Other: ,Client-facing application,,,,,N,,,,,N,Other: Switzerland,No dataset used for development or evaluation,6.0,Other: ,,n,,N,,,,,N,no benchmark,no benchmark,no benchmark,y,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,n,,,,,n,,,,,N,,,,N,,,,,,,,,,,,,GPT-3.5 family,6.0,,,prompted to simulate a psychiatric first-responder chatbot,,,,N,,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study,No clients/patients involved,,No users involved,,,,,,Other: ,,2025.0,2nd_search_71,2nd_search_245
Multi-turn chatbot,,Client-facing application,,L,B,"between the rule-based sentiment analysis module and the
sentiment analysis capabilities of the GPT-powered model",y,,,,,n,USA,Self-collected data,4.0,Unknown,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,CBT: Cognitive restructuring,No,n,,n,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family,11.0,,,fine tuning + sentiment analysis module,,,,n,,y," Conducted thorough testing of the
chatbot’s responses (Fig. 6) to various user queries and
scenarios to assess its ability to uphold ethical standards
and protect user privacy. This testing involved simulating
user interactions with the chatbot and evaluating its
responses for any instances of inappropriate or unethical behavior, such as providing inaccurate information,
breaching confidentiality, or engaging in discriminatory
practices. Any identified issues were promptly addressed
and remediated to ensure that the chatbot consistently
adheres to ethical guidelines and respects user privacy.",,,,n,Conference paper,,,,Reviewer Two,y,"In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.",Empirical research involving an LLM,y," Conducted thorough testing of the
chatbot’s responses (Fig. 6) to various user queries and
scenarios to assess its ability to uphold ethical standards
and protect user privacy. This testing involved simulating
user interactions with the chatbot and evaluating its
responses for any instances of inappropriate or unethical behavior, such as providing inaccurate information,
breaching confidentiality, or engaging in discriminatory
practices. Any identified issues were promptly addressed
and remediated to ensure that the chatbot consistently
adheres to ethical guidelines and respects user privacy.",Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy,,Internet data -- mental health forum,Yes,,,n,y,n,Unknown,y,2024.0,2nd_search_69,2nd_search_246
Multi-turn chatbot,,Client-facing application,,no benchmark,no benchmark,Performance of the sentiment classification component.,y,,,,,n,USA,"External data set, modified",4.0,Other: n.a.,Other: Unknown,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,Other CBT techniques,No,n,,n,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family,11.0,,"Data Preparation: The sentiment analysis model is
trained on an annotated text classification dataset after
undergoing extensive cleaning, normalization, and tok-
enization processes to ensure the quality and integrity of
the data. The datasets used are compiled from a multitude
of sources, including a text classification repository of so-
cial media tweets, restaurant reviews, and airline reviews.
Data was also extracted from the Reddit API across vari-
ous forums such as those focused on CBT, mental health,
therapy, anxiety, depression, and CPTSD. Initially, the
dataset comprised 3,000 data points, with a text column
serving as the input and a target column providing labeled
sentiment. Recognizing the importance of data volume
in enhancing the accuracy and robustness of machine
learning models, we expand the dataset (TABLE I) to encompass 12,221 data points

for RAG",It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy,No clients/patients involved,Other: Mixed,No users involved,,,,,,Other: n.a.,,2024.0,2nd_search_69,2nd_search_247
Multi-turn chatbot,,Client-facing application,,L,B,"between the rule-based sentiment analysis module and the
sentiment analysis capabilities of the GPT-powered model",y,,,,,n,USA,"External data set, modified",4.0,Other: n.a.,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,CBT: Cognitive restructuring,No,n,,n,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-3.5 family,11.0,,"Data Preparation: The sentiment analysis model is
trained on an annotated text classification dataset after
undergoing extensive cleaning, normalization, and tok-
enization processes to ensure the quality and integrity of
the data. The datasets used are compiled from a multitude
of sources, including a text classification repository of so-
cial media tweets, restaurant reviews, and airline reviews.
Data was also extracted from the Reddit API across vari-
ous forums such as those focused on CBT, mental health,
therapy, anxiety, depression, and CPTSD. Initially, the
dataset comprised 3,000 data points, with a text column
serving as the input and a target column providing labeled
sentiment. Recognizing the importance of data volume
in enhancing the accuracy and robustness of machine
learning models, we expand the dataset (TABLE I) to encompass 12,221 data points

for RAG","It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot

fine tuning + sentiment analysis module",unknown,See V.B.,,n,,n,,,,,n,Conference paper,,,,Consensus,y,"In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.",Empirical research involving an LLM,n,,Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy,Other: unknown,"Other: Other: statements, labeled with sentiments (sentiment analysis data)",Yes,,,n,y,n,Other: n.a.,y,2024.0,2nd_search_69,2nd_search_248
Other: Development of new Framework,,Client-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,18.0,,,,,,,,,,,,,,N,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",,,,,,,,,Other: Assessment of OCD-Coach,Other CBT techniques,,,,Y,"Future research should explore how this tool can be integrated into the decision-making processes of mental health professionals, health care organizations, and technology developers.” (Position: p. 10)",,,,,,,,,N,,,,,,,N,,,,,,,,,,"ChatGPT, model unspecified",10.0,,,FAITA-mental health chatbot was designed to classify OCD-Coach's responses,,"Readability high (Flesch-Kincaid 10.8), messages long; insufficient cultural tailoring; transparency reactive not proactive; crisis responses generic (no local numbers) — Quote: “10.8 on the Flesch-Kincaid …”; “did not make an attempt to ask Sam about her cultural identity variables”; “grant … a score of 1 … provides details upon request, [but] does not offer full, immediate … disclosure”; “no specific numbers or links were provided … score of 1 … general recommendations but no local resources” (Position: p. 8–9)",,,,,,,,,N,Journal paper,,,,Reviewer Two,Y,"“After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9)",Empirical research involving an LLM,Y,“OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9),Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.,No clients/patients involved,,Yes,FAITA-Mental Health,,Y,Y,Y,,Y,2024.0,2nd_search_68,2nd_search_249
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,18.0,,,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Other: N/A,"Other CBT techniques; Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"ChatGPT, model unspecified",10.0,,Study describes FAITA Mental Health Framework and evaluates OCD Coach as an existing LLM-based Tool,,,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,Y,"With regard to crisis response, when Sam reported suicidal
ideation, OCD Coach directed her to emergency services or a crisis hotline “in [her] area” (Subdomain 2: Interactivity Quality)",Empirical research involving an LLM,Y,"Subdomain 2: Evidence-Based Content
",Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_68,2nd_search_250
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,18.0,,,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Other CBT techniques; Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"ChatGPT, model unspecified",10.0,,Study describes FAITA Mental Health Framework and evaluates OCD Coach as an existing LLM-based Tool,FAITA-mental health chatbot was designed to classify OCD-Coach's responses,,,,N,,N,,,,,N,Journal paper,,,,Consensus,Y,"“After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9)",Empirical research involving an LLM,Y,“OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9),Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_68,2nd_search_251
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction,,,,,,,,,,,,2nd_search_66,2nd_search_252
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction,,,,,,,,,,,,2nd_search_66,2nd_search_253
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction,,,,,,,,,,,,2nd_search_66,2nd_search_254
Multi-turn chatbot,,Client-facing application,,,,,,,,,,n,Other: japan,,25.0,,,,,,,,,,,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",,,,n,,,,,n,,,,,n,,,,n,,,n,,,,,,,,,,GPT-4 / GPT-4o family,2.0,,,"two age-appropriate AI chatbots, leveraging GPT-4, were developed to provide natural, empathetic conversations",5,"All participants noted improved treatment motivation … 80% disclosing personal concerns … 24/7 availability particularly benefited patients
Four out of five participants reported significant reductions in anxiety and stress levels post-intervention” (Results, p. 1)",,,,,,,,,n,Journal paper,,,,Reviewer Two,n,,Empirical research involving an LLM,,,"Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study",Patients recruited in hospital or outpatient treatment facility,,Yes,,,somewhat,somewhat,N,,y,2025.0,2nd_search_64,2nd_search_255
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Japan,No dataset used for development or evaluation,25.0,,,Y,Table 2,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,,,,5,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,Y,"The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.",Empirical research involving an LLM,Y,"Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.","Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study","Other: Pediatric and adolescent and
young adult (AYA) patients with cancer",,Yes,,,Y,Y,N,,Y,2025.0,2nd_search_64,2nd_search_256
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Japan,No dataset used for development or evaluation,25.0,,,Y,Table 2,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,,,"two age-appropriate AI chatbots, leveraging GPT-4, were developed to provide natural, empathetic conversations",5,"All participants noted improved treatment motivation … 80% disclosing personal concerns … 24/7 availability particularly benefited patients
Four out of five participants reported significant reductions in anxiety and stress levels post-intervention” (Results, p. 1)",,N,,N,,,,,N,Journal paper,,,,Consensus,Y,"The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.",Empirical research involving an LLM,Y,"Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.","Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study",Patients recruited in hospital or outpatient treatment facility,,Yes,,,Y,Y,N,,y,2025.0,2nd_search_64,2nd_search_257
,,,,,,,,,,,,,UK,,30.0,,,,,,,,,,,,,,,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,5.0,,,,,,,,,,,,,,,Journal paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Developing a single-session outcome measure using natural language processing on digital mental health transcripts,,,,,,,,,,,2024.0,2nd_search_63,2nd_search_258
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,UK,External data set,30.0,Unknown,No,N,,N,,,,,N,,,,N,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",N,,N,,N,,N,,Other: None of these ,"Unspecified, might include formal therapy methods",No,y,content validity with clinicians and SUs to improve the relevance and clarity of the items,N,,,,,N,English,,,,N,,,,N,-,-,Content validity index,-,,,,,,,,,BERT family,5.0,,"The data were provided and 
anonymised by Qwell",They used a pre-trained RoBERTa model via Hugging Face to analyze text patterns without mentioning any fine-tuning or prompt engineering. The model was applied directly to classify and interpret data.,,,,Y,RoBERTa should be on-premise-capable,N,,,,,N,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Developing a single-session outcome measure using natural language processing on digital mental health transcripts,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Other: Mixed,,2024.0,2nd_search_63,2nd_search_259
,,,,,,,,,,,,,UK,,30.0,,,,,,,,,,,,,,,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,5.0,,,,,,,,,,,,,,,Journal paper,,,,Consensus,,,Empirical research involving an LLM,,,Developing a single-session outcome measure using natural language processing on digital mental health transcripts,,,,,,,,,,,2024.0,2nd_search_63,2nd_search_260
One-turn chatbot (usually Q&A),,Client-facing application,,,,,n,,,,,n,China,"External data set, modified",24.0,Unselected,No,n,,n,,l,b,"PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.",y,l,b,"Fluency, helpfulness, relevance, empathy, professionalism, evaluated by psychology graduate students on a 5-Likert scale. Benchmark are other LLM-based methods.",y,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Fine-tuning + other modules,Mix of formal therapy methods,No,n,,n,,,,,n,Chinese,l,b,"Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.",y,l,s,Dist-2. Benchmark are other LLM-based methods.,y,,,,,,,,,,,,,"BERT family; ChatGPT, model unspecified",7.0,,"1,000 QA pairs from the PsyQA dataset.  two human annotators inspected and labeled the responses of professional supporters in the PsyQA dataset with the corresponding professional psychological counseling theory categories (Cognitive-Behavioral Therapy labeled as 1, Dialectical Behavior Therapy as 2, Person-Centered Therapy as 3, and Reality Therapy as 4). The original data had quantities of 189, 317, 196, and 298 for categories 1, 2, 3, and 4, respectively.","Prompting of general purpose LLMs (stage 1, probably ChatGPT) + fine-tuning of BERT (stage 2) + some type of RAG based on SimCSE (stage 3)",,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: Mix of trained and lay,,2024.0,2nd_search_62,2nd_search_261
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,China,External data set,24.0,Unselected,No,N,,N,,L,B,"Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",Y,H,B,"To evaluate the quality of generated supportive responses, we
also conduct a human evaluation. We recruit 12 graduate stu-
dents, major in psychology, to annotate the responses.",Y,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Fine-tuning + other modules; Prompting + other modules,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Chinese,L,B,"Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",Y,L,"B
","Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",Y,,,,,,,,,,,,,"BERT family; ChatGPT, model unspecified",7.0,,"PsyQA dataset (Sun et al., 2021). ",Prompting for emotional analysis + fine-tuning BERT for therapy classification + semantic retrieval with SimCSE for relevant case examples.,,,,N,,N,,,,,N,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: Well-trained volunteers or professional counselors = mixed,,2024.0,2nd_search_62,2nd_search_262
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,China,"External data set, modified",24.0,Unselected,No,N,,N,,L,B,"PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",Y,l,B,"Fluency, helpfulness, relevance, empathy, professionalism, evaluated by psychology graduate students on a 5-Likert scale. Benchmark are other LLM-based methods.",Y,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Fine-tuning + other modules; Prompting + other modules,Mix of formal therapy methods,No,N,,N,,,,,N,Chinese,L,B,"Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",Y,L,s,"Dist-2. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",Y,,,,,,,,,,,,,"BERT family; ChatGPT, model unspecified",7.0,,"1,000 QA pairs from the PsyQA dataset.  two human annotators inspected and labeled the responses of professional supporters in the PsyQA dataset with the corresponding professional psychological counseling theory categories (Cognitive-Behavioral Therapy labeled as 1, Dialectical Behavior Therapy as 2, Person-Centered Therapy as 3, and Reality Therapy as 4). The original data had quantities of 189, 317, 196, and 298 for categories 1, 2, 3, and 4, respectively.","Prompting of general purpose LLMs (stage 1, probably ChatGPT) + fine-tuning of BERT (stage 2) + some type of RAG based on SimCSE (stage 3)",,,,N,,N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: Mix of trained and lay,,2024.0,2nd_search_62,2nd_search_263
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: mexico,No dataset used for development or evaluation,11.0,,,,,,,,,,,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,,,,,,,,,,,,,n,,,,,,,n,,,,,,,,,,Other: unclear,9.0,,,,,,,,,,,,,,n,Conference paper,,,,Reviewer Two,n,,Empirical research involving an LLM,,,Conversational Agents for Dementia using Large Language Models,No clients/patients involved,,No,,,,,n,,,2023.0,2nd_search_60,2nd_search_264
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Mexico,No dataset used for development or evaluation,11.0,,,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"ChatGPT, model unspecified",9.0,,,Prompt engineering to shape ChatGPT’s dialogue,,,,N,,N,,,,,N,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,Conversational Agents for Dementia using Large Language Models,No clients/patients involved,,No users involved,,,,,,,,2023.0,2nd_search_60,2nd_search_265
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Mexico,No dataset used for development or evaluation,11.0,,,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,"ChatGPT, model unspecified",9.0,,,Prompt engineering to shape ChatGPT’s dialogue,,,,N,,N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Conversational Agents for Dementia using Large Language Models,No clients/patients involved,,No users involved,,,,,,,,2023.0,2nd_search_60,2nd_search_266
,,Analysis of conversation transcripts,,,,,,,,,,,USA,"Other: Digitial Standardized Patients- no collection of data, only creation via GPT 3.5",24.0,Psychopathology,Yes,,,,,,,,,,,,,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,Other CBT techniques,No,,,,,,,,,English,,,,,,,,,,,Average Tone,,,,,,,,,,GPT-3.5 family; GPT-4 / GPT-4o family,10.0,,"By using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
These characteristics encompassed a spectrum of ages (40 and
80 years old), genders (male, female, transgender male, and
transgender female), races (non-Hispanic black, non-Hispanic
white, and Hispanic), and annual income ($25,000, $50,000,
$500,000, and $1,000,000). We also generated a control group
without assigned identities. Each DSP represented a unique
mix of these demographic characteristics.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses","o systematically test
the conversational agent’s responses for potential bias, we
developed an experimental setup where the conversational
agent interacted with digital standardized patients (DSPs)
exhibiting symptoms of anxiety or depression. DSPs were
GPT-3.5-enabled chatbots prompted to emulate a patient
with depression and anxiety seeking professional help from
a therapist. This was achieved by prompting GPT-3.5 to:
“Pretend that you are [first_name], a human client speaking to
a therapist. You are NOT the therapist; you are the clienBy using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses",,,,,,,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support,,Psychotherapy -- chat logs,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_58,2nd_search_267
Other: Spoken dialog system,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,24.0,,,y,"yes, in simulated DSPs (digital standardized patient)",y,"yes, in simulated DSPs (digital standardized patient)",,,,n,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",n,,n,,n,,n,,Prompting + other modules,Other CBT techniques,,n,,n,,,,,n,,,,,n,,,,n,no benchmark,no benchmark,LIWC score domains and linguistic marker scores,no benchmark,,,,,,,,,GPT-4 / GPT-4o family,10.0,,,"Their intervention: The AI therapist tested in this study is an LLM-based pro- gram previously developed and tested at Cedars-Sinai Medical Center, with the goal of offering AI-enabled, self-
administered, mental health support within immersive virtual
reality environments.18 The user interacts with an AI conver-
sational agent designed for history-taking and therapeutic sup-
port for anxiety and depression. The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
The agent facilitates turn-based conversations, summarizing
and concluding each session, and was found to be an accepta-
ble and feasible support modality for patients with mild-to-
moderate anxiety or depression.18 Here, we focus on whether
there is evidence of any sociodemographic bias exhibited by
the conversational agent.",,,,n,,y,"The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).",,,,n,Journal paper,,,,Richard Gaus,y,"from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.",Empirical research involving an LLM,n,,Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support,No clients/patients involved,,No users involved,,,,,,,,2024.0,2nd_search_58,2nd_search_268
Other: Spoken dialog system,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,24.0,Other: ,Other: ,y,"yes, in simulated DSPs (digital standardized patient)",y,"yes, in simulated DSPs (digital standardized patient)",,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Prompting + other modules,Other CBT techniques,Other: ,n,,n,,,,,n,Other: ,,,,n,,,,n,no benchmark,no benchmark,LIWC score domains and linguistic marker scores,no benchmark,no benchmark,no benchmark,average tone (sentiment score),no benchmark,,,,,GPT-4 / GPT-4o family,10.0,,"By using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
These characteristics encompassed a spectrum of ages (40 and
80 years old), genders (male, female, transgender male, and
transgender female), races (non-Hispanic black, non-Hispanic
white, and Hispanic), and annual income ($25,000, $50,000,
$500,000, and $1,000,000). We also generated a control group
without assigned identities. Each DSP represented a unique
mix of these demographic characteristics.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses","Their intervention: The AI therapist tested in this study is an LLM-based pro- gram previously developed and tested at Cedars-Sinai Medical Center, with the goal of offering AI-enabled, self-
administered, mental health support within immersive virtual
reality environments.18 The user interacts with an AI conver-
sational agent designed for history-taking and therapeutic sup-
port for anxiety and depression. The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
The agent facilitates turn-based conversations, summarizing
and concluding each session, and was found to be an accepta-
ble and feasible support modality for patients with mild-to-
moderate anxiety or depression.18 Here, we focus on whether
there is evidence of any sociodemographic bias exhibited by
the conversational agent.",,,,n,,y,"The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).",,,,n,Journal paper,,,,Consensus,y,"from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.",Empirical research involving an LLM,y,"The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses",Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support,No clients/patients involved,Other: ,No users involved,,,,,,Other: ,,2024.0,2nd_search_58,2nd_search_269
,Other: self efficacy enhancement,Therapist-facing application,,,,,,,,,,n,Other: korea,Self-collected data,28.0,Unselected,No,,,,,,,,,,,,n,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation); Other: counselor-training simulation",No,,,n,,,,,n,Other: ,,,,n,,,,n,,,n,,,,,,,,n,,GPT-4 / GPT-4o family,3.0,,,ChatGPT played the role of a client and optionally provided AI-generated feedback to trainees,,Self-efficacy improved across sessions; anxiety reduced mainly between Session 1 and 2. — Quote: same as quantitative assessment line,,,,,,,,,n,Journal paper,,,,Reviewer Two,n,,Empirical research involving an LLM,,,AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction,,,Yes,,,Y,Y,N,Lay people,Y,2025.0,2nd_search_57,2nd_search_270
,Treatment fidelity feedback,Therapist-facing application,,,,,N,,,,,N,Other: Korea,No dataset used for development or evaluation,28.0,,,N,,N,,,,,N,,,,N,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",Y,CASES & STAI,Y,"The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.",N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,,,"Qualitative assessment only implicitly mentioned in the discussion section, where user experiences are reported.",,N,,N,,,,,N,Journal paper,,,,Richard Gaus,Y,"Although the risk was minimal in the present study because
the client was fictional, it may not be completely credible in real
situations involving real clients. Counselor educators would thus
have to be cautious about using AI to directly generate feedback
for the trainees.",Empirical research involving an LLM,N,,AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction,,,Yes,CSQ and PU,,Y,Y,Y,,Y,2025.0,2nd_search_57,2nd_search_271
,Treatment fidelity feedback,Therapist-facing application,,,,,N,,,,,N,Other: Korea,No dataset used for development or evaluation,28.0,Other: ,Other: ,N,,N,,,,,N,,,,N,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,(CASES &) STAI,,"The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.",N,,N,,Only prompting,"Unspecified, might include formal therapy methods; Other: counselor-training simulation",Other: ,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,ChatGPT played the role of a client and optionally provided AI-generated feedback to trainees,,Self-efficacy improved across sessions; anxiety reduced mainly between Session 1 and 2. — Quote: same as quantitative assessment line,,N,,N,,,,,N,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,N,,AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction,,,Yes,CSQ and PU,,Y,Y,Y,Other: ,Y,2025.0,2nd_search_57,2nd_search_272
Multi-turn chatbot,Other: No specific application,Therapist-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,21.0,,,,,,,,,,,,,,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",3.0,,,,21,"This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.",,,,,,,,,,Journal paper,,,,Richard Gaus,,,Population survey,,,"""I Don't Understand It, but Okay"": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models",Other: Mental health practitioners,,Yes,,,y,n,n,,y,2025.0,2nd_search_54,2nd_search_273
,Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs,Therapist-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,21.0,,,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",3.0,"Despite it being labeled an ""empirical study"", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. ",,,,"Both, user experience assessment and participant attitude assessment. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview. ",,,,,,,,,,Journal paper,,,,Reviewer Two,,,Population survey,,,"""I Don't Understand It, but Okay"": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models",,,Yes,,,Y,N,N,,Y,2025.0,2nd_search_54,2nd_search_274
Other: ,Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs. No one particular application type.,Therapist-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,21.0,,,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",3.0,"Despite it being labeled an ""empirical study"", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. ",,,21,"Both, user experience assessment and participant attitude assessment.

This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.",,,,,,,,,,Journal paper,,,,Consensus,,,Population survey,,,"""I Don't Understand It, but Okay"": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models",Other: Mental health practitioners,,Yes,,,Y,N,N,,Y,2025.0,2nd_search_54,2nd_search_275
Multi-turn chatbot,,Client-facing application,,l,b,Various classification metrics of BERT emotional distress detection. Benchmark: Bi-LSTM,y,,,,,n,UK,External data set,25.0,Unselected,No,n,,n,,l,s,BLEUScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.,y,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,l,s,"BLEU, ROUGE against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.",y,,,,n,,,,,,,,,,,,,BERT family; GPT-3.5 family,1.0,,EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues,"BERT and GPT-3.5 fine-tuned, interacting with each other",,,,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Lay people,,2025.0,2nd_search_51,2nd_search_276
Multi-turn chatbot,,Client-facing application,,L,B,Bi-LSTM for emotional distress detection,Y,,,,,N,UK,External data set,25.0,Unknown,No,N,,N,,L,B,Against chatGPT-3.5,Y,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules; Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,English,L,S,Against chatGPT-3.5,Y,,,,N,?,B,"Relevance, personalisation, PFA (advice and guidance)professional referral, and validation and empathy",In the text there is no statement on how or by whom this was evaluated. ,,,,,,,,,BERT family; GPT-3.5 family,1.0,,"Multiple external datasets used: 
a) From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger. 
b) From Bertagnolli [53], including high-quality therapist responses to real patient´s mental health questions.","The system integrates two main components: a BERT-based emotional distress detection module and a fine-tuned GPT-3.5 model for Psychological First Aid (PFA) response generation.
First, BERT analyzes the user’s input to detect their emotional state and distress level. These outputs are then passed, along with the original user input, into a custom prompt designed for GPT-3.5. GPT-3.5 was fine-tuned on therapy transcripts to improve its ability to produce empathetic, context-aware PFA responses.",,,,N,,,,,,,N,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,No clients/patients involved,"Other: a) Crowdsourced dialogues
b) Internet data -- mental health Q&A",No users involved,,,,,,"Other: a) Unknown 
b) Trained professionals (""verified therapists"")",,2025.0,2nd_search_51,2nd_search_277
Multi-turn chatbot,,Client-facing application,,L,B,Various classification metrics of BERT emotional distress detection. Benchmark: Bi-LSTM,Y,,,,,N,UK,External data set,25.0,Unselected,No,N,,N,,L,s,BERTScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.,Y,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules; Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,English,L,S,"BLEU, ROUGE against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.",Y,,,,N,,,,,,,,,,,,,BERT family; GPT-3.5 family,1.0,,"EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues

From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger.","The system integrates two main components: a BERT-based emotional distress detection module and a fine-tuned GPT-3.5 model for Psychological First Aid (PFA) response generation.
First, BERT analyzes the user’s input to detect their emotional state and distress level. These outputs are then passed, along with the original user input, into a custom prompt designed for GPT-3.5. GPT-3.5 was fine-tuned on therapy transcripts to improve its ability to produce empathetic, context-aware PFA responses.",,,,N,,n,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Lay people,,2025.0,2nd_search_51,2nd_search_278
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,10.0,,,,,,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,Other: various LLMs,12.0,,,,1000000 user reviews,"BERTopic analysis of app reviews, qualitative only",,,,,,,,,n,Conference paper,,,,Richard Gaus,,,Population survey,,,Digital Risk Considerations Across Generative AI-based Mental Health Apps,General population,,Yes,,,y,n,n,,y,2023.0,2nd_search_47,2nd_search_279
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,USA,No dataset used for development or evaluation,10.0,,,,,,,,,,,,,,,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,Other: unspecified,12.0,,,,,Qualitative analysis of user reviews focusing on risks,,,,,,,,,,Other: Workshop paper,,,,Reviewer Two,,,Other: Observational study (user-generated reviews) with unsupervised text mining,,,Digital Risk Considerations Across Generative AI-based Mental Health Apps,No clients/patients involved,,Yes,,,Y,N,N,,Y,2023.0,2nd_search_47,2nd_search_280
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,10.0,,,,,,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,Other: various LLMs,12.0,,,,1000000 user reviews,Qualitative analysis of user reviews focusing on risks using BERTopic,,,,,,,,,n,Conference paper,,,,Consensus,,,Population survey,,,Digital Risk Considerations Across Generative AI-based Mental Health Apps,General population,,Yes,,,Y,N,N,,Y,2023.0,2nd_search_47,2nd_search_281
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,USA,External data set,13.0,Unknown,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,English,,,,N,,,,,L,B,"Empathy
 Coherence 
Informativeness 
Fluency",,,,,,,,,,GPT-4 / GPT-4o family; Llama 2 family,3.0,,"
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. ",Retrieval + hierarchical fusion + attention + lexicon-enriched embeddings used to prompt LLMs.,,,,Y (some),Llama and DeepSeek are,,,,,,,Journal paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,"Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_46,2nd_search_282
,,Analysis of conversation transcripts,,,,,,,,,,,USA,External data set,13.0,Psychopathology,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,T5 family; GPT-4 / GPT-4o family; Llama 2 family; Other: DeepSeek-R1,3.0,,"The primary dataset used in this study consists of psychotherapy transcripts from
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. ","''We introduce Emotion-Aware Embedding Fusion, a novel framework integrating hierarchical fusion and attention mechanisms'' ... ",,,,,,,,,,,,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,"Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",No clients/patients involved,Psychotherapy -- speech transcripts,No,,,N,N,N,Trained professionals,N,2025.0,2nd_search_46,2nd_search_283
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,USA,External data set,13.0,Psychopathology,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,,,,,,,,,English,,,,N,,,,,L,B,automatic empathy rating,they study integration of empathy lexicon into different LLMs. benchmark are those LLMs without empathy lexicon integration.,l,w,"coherence, informativeness, fluency",they study integration of empathy lexicon into different LLMs. benchmark are those LLMs without empathy lexicon integration.,,,,,T5 family; GPT-4 / GPT-4o family; Llama 2 family; Other: DeepSeek-R1,3.0,,"The primary dataset used in this study consists of psychotherapy transcripts from
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. ",Retrieval + hierarchical fusion + attention + lexicon-enriched embeddings used to prompt LLMs.,,,,Y,Llama and DeepSeek are,,,,,,,Journal paper,,,,Consensus,,,Empirical research involving an LLM,,,"Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_46,2nd_search_284
Other: Unknown but I assume both multiturn and one turn for the forum answers.,,Analysis of conversation transcripts,,,,,,,,,,,India,External data set,16.0,Unknown,No,,,,,L,B,"BERT Score, Sentence Transformer score",Y,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Other: Unknown as the datasets are not fully described,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,BERT family,12.0,,"[COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. 

WebMD, Mayo Clinic and Heatlhline.com 
- One text exchange between a patient and their therapist is included in this dataset. The dataset was assembled from online FAQs, WebMD, Mayo Clinic, and HealthLine, among other well-known healthcare blogs","The proposed method involves enhancing the 
Falcon-7B model through the application of quantization low rank adaptation (Q-LoRA). ",,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Expert Patient Interaction Language Model (EPILM),General population,Other: Emotional support dialogue and Internet data,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_45,2nd_search_285
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,India,External data set,16.0,Unselected,No,n,,n,,l,b,benchmark is GPT-3.5. measure is BERTScore and sentence transformer score with dataset expert responses as reference,y,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,Other: Falcon-7B,12.0,,CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat,Q-LoRA fine-tuning,,,,y,falcon,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Expert Patient Interaction Language Model (EPILM),,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_45,2nd_search_286
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,India,External data set,16.0,Unselected,No,n,,n,,l,b,benchmark is GPT-3.5. measure is BERTScore and sentence transformer score with dataset expert responses as reference,Y,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,,,,,,,,,,,,,Other: Falcon-7B,12.0,,"CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat

[COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. ",Q-LoRA fine-tuning,,,,y,falcon,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Expert Patient Interaction Language Model (EPILM),No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2024.0,2nd_search_45,2nd_search_287
One-turn chatbot (usually Q&A),,Client-facing application,,,,,n,,no benchmark,no benchmark,Train loss,y,Other: Iraq,External data set,27.0,Unknown,No,n,,n,,,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,no benchmark,no benchmark,BLEU with reference,y,,,,n,,,,,,,,,,,,,GPT-2 family,11.0,,Alzheimer's Q&A dataset https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset,Fine-tuning of GPT-2 + memory mechanism (storing previous embeddings in an array),,,,y,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,2nd_search_43,2nd_search_288
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,Other: Iraq,External data set,27.0,Unknown,No,N,,N,,-,-,-,Y,,,,N,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Fine-tuning + other modules; Prompting + other modules,"Unspecified, might include formal therapy methods","Other: probably yes, but name not specified. ",N,,N ,,,,,N,English,-,-,-,Y,,,,N,-,-,Computational performance/ Training metrics,-,,,,,,,,,GPT-2 family,11.0,,"The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately! ","Developed by fine-tuning a GPT-2 model on a domain-specific dataset and integrating it into a larger architecture that included array-based context storage for multi-turn dialogue, Top-K/Top-P sampling for controlled text generation, and response ranking modules using BLEU scores and cosine similarity to select the most relevant and coherent reply.",,,,N,,N,,,,,N,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Other: Probably mixed,,2024.0,2nd_search_43,2nd_search_289
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,no benchmark,no benchmark,Train loss,y,Other: Iraq,External data set,27.0,Unknown,No,N,,N,,,,,n,,,,N,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",N,,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N ,,,,,N,English,no benchmark,no benchmark,BLEU with reference,Y,,,,N,,,,,,,,,,,,,GPT-2 family,11.0,,"The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately!

But user has other dataset that fits description: https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset","Developed by fine-tuning a GPT-2 model on a domain-specific dataset and integrating it into a larger architecture that included array-based context storage for multi-turn dialogue, Top-K/Top-P sampling for controlled text generation, and response ranking modules using BLEU scores and cosine similarity to select the most relevant and coherent reply.",,,,y,,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,2nd_search_43,2nd_search_290
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Israel,No dataset used for development or evaluation,4.0,,,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,,,,1,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,Y,"This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc",Empirical research involving an LLM,Y,"This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc","The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis",Other: unknown,,,,,Y,N,N,,Y,25.0,2nd_search_42,2nd_search_291
Other: image generator,,Client-facing application,,,,,,,,,,N,India,,4.0,,,,,,,,,,,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,"Psychodynamic psychotherapy; Unspecified, might include formal therapy methods",,somewhat," Quote: “therapist, patient, and AI agent … proof-of-concept in this article illustrates … therapeutic value” (Position: Clinical section, p. 11)",somewhat,"“SAFE-AI protocol … provides systematic guidelines for implementing AI-based externalization techniques … while ensuring cultural sensitivity, gender representation, and therapeutic authenticity.”",,,,,,,,,N,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,,,"The research team, consisting of an AI product manager and
prompt engineer (OP), several expert psychologists (YH, KG, ZE,
and DHS), and a music therapist (TA), jointly developed the two
AI tools from December 2023 to January 2024. Initially, a
primary prompt was created by YH. This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests
and added operational instructions to ensure safety and ethics.
Subsequently, group members with clinical backgrounds (YH,
KG, ZE, TA) tested the tool on themselves and shared their
experiences and suggestions for improvements. Throughout this
process, the team realized that due to the tools’ depth and their
ability to reflect on unconscious internal parts, the presence of a
mental health care professional would be essential. An expert in
social psychology, DHS, addressed cultural issues, which were
also incorporated into the prompt. Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications.",,"Results reported: Yes (case-level outcomes) — Quote: “the AI successfully managed to … become more experientially aligned with the patient” (Position: Clinical section, p. 11)

Empathic failures, uncanny feelings, corrective alignment described — Quote: “empathic failure occurred … however, the ability to overcome empathic failures … holds significant therapeutic value” (Position: Clinical section, p. 11)",,Y,"(recommend therapist-side implementation) — Quote: “Therefore, these tools should be used on the therapist’s computer … ensuring anonymity for the patient.” ",Y,"Quote: “Patients must be fully informed about how their data will be used and must provide explicit consent … robust data protection measures are paramount” (Position: Section 4.2, p. 12)",,,,N,Journal paper,,,,Reviewer Two,,,"Other: theoretical,clinical, and ethical analysis",Y,"Y (SAFE-AI protocol includes screening, monitoring, and evaluation) — Quote: “SAFE-AI protocol (Screening, Alignment, Facilitation, and Evaluation of AI-Enhanced Interventions) provides systematic guidelines … including consent, alliance monitoring, empathic failures” (Position: Section 4.7, p. 13)","The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis",No clients/patients involved,,Yes,,,Y,N,N,,Y,2025.0,2nd_search_42,2nd_search_292
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Israel,No dataset used for development or evaluation,4.0,,,N,,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting,"Psychodynamic psychotherapy; Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,,,"The research team, consisting of an AI product manager and
prompt engineer (OP), several expert psychologists (YH, KG, ZE,
and DHS), and a music therapist (TA), jointly developed the two
AI tools from December 2023 to January 2024. Initially, a
primary prompt was created by YH. This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests
and added operational instructions to ensure safety and ethics.
Subsequently, group members with clinical backgrounds (YH,
KG, ZE, TA) tested the tool on themselves and shared their
experiences and suggestions for improvements. Throughout this
process, the team realized that due to the tools’ depth and their
ability to reflect on unconscious internal parts, the presence of a
mental health care professional would be essential. An expert in
social psychology, DHS, addressed cultural issues, which were
also incorporated into the prompt. Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications.",1,"Results reported: Yes (case-level outcomes) — Quote: “the AI successfully managed to … become more experientially aligned with the patient” (Position: Clinical section, p. 11)

Empathic failures, uncanny feelings, corrective alignment described — Quote: “empathic failure occurred … however, the ability to overcome empathic failures … holds significant therapeutic value” (Position: Clinical section, p. 11)",,N,,N,,,,,N,Journal paper,,,,Consensus,,,Empirical research involving an LLM,Y,"This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc","The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis",Other: unknown,,Yes,,,Y,N,N,,Y,2025.0,2nd_search_42,2nd_search_293
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,India,Self-collected data,,,,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Other: ""Autism counselling""",,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,Other: Falcon-7B,,,"The research begins by curating a specialized User and
Counsellor Interaction CSV dataset by querying an autism
counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source
for reducing bias and adding a human touch. Alongside the
primary dataset, the additional data points ensure a compre-
hensive understanding of the domain.",Fine-tuned LLM + integration of other data modalities (vision),,,,y,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,"Multimodal Integration, Fine Tuning of Large Language Model for Autism Support",No clients/patients involved,Other: unclear,No users involved,,,,,,,,,2nd_search_40,2nd_search_294
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,India,Self-collected data,18.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,"Other: unknown, probably english or hindi",,,,N,,,,N,,,,,,,,,,,,,Other: Falcon7B,1.0,,,Fine-tuned with PEFT-LoRA,,,,Y,Falcon7B ,N,,,,,N,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,"Multimodal Integration, Fine Tuning of Large Language Model for Autism Support",No clients/patients involved,"Other: The research begins by curating a specialized User and Counsellor Interaction CSV dataset by querying an autism counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source for reducing bias and adding a human touch. ",No users involved,,,,,,Trained professionals,,2024.0,2nd_search_40,2nd_search_295
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,India,Self-collected data,18.0,Unknown,Other: unknown,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,"Other: unknown, probably english or hindi",,,,N,,,,N,,,,,,,,,,,,,Other: Falcon-7B,1.0,,"The research begins by curating a specialized User and
Counsellor Interaction CSV dataset by querying an autism
counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source
for reducing bias and adding a human touch. Alongside the
primary dataset, the additional data points ensure a compre-
hensive understanding of the domain.",Fine-tuned LLM + integration of other data modalities (vision),,,,Y,Falcon7B ,N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,"Multimodal Integration, Fine Tuning of Large Language Model for Autism Support",No clients/patients involved,Other: unclear,No users involved,,,,,,Unknown,,2024.0,2nd_search_40,2nd_search_296
Other: No specific subtype (AI use in general),Other: No specific subtype (AI use in general),Other: Client-facing AND therapist-facing,,,,,n,,,,,n,Other: Australia,No dataset used for development or evaluation,11.0,,,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"Other: AI in general, mostly ChatGPT",10.0,,,,107 community members AND 86 mental health providers,,,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Population survey,,,Use of AI in mental health care: Community and mental health professionals survey,General population,,Yes,AI attitude scale (AIAS-4): doi: 10.3389/fpsyg.2023.1191628,,y,y,y,,y,2024.0,2nd_search_39,2nd_search_297
,,Other: survey,,,,,n,,,,,n,Other: Australia,Self-collected data,,Unselected,No,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,Other: no intervention,No,Y,,Y,,,,,n,English,,,,n,,,,n,,,see UX,,,,,,,,,,Other: none,,,,,193 — Quote: “final sample consisted of 107 CMs and 86 MHPs”,"See section: Technology Comfort, AI Attitudes, and AI
Use Intention",,,,,,,,,n,Journal paper,,,,Reviewer Two,,,Population survey,,,Use of AI in mental health care: Community and mental health professionals survey,General population,,Yes,,,Y,Y,N,Other: both lay and pros,Y,,2nd_search_39,2nd_search_298
Other: No specific subtype (AI use in general),Other: No specific subtype (AI use in general),Other: Client-facing AND therapist-facing,,,,,n,,,,,n,Other: Australia,No dataset used for development or evaluation,11.0,Other: ,Other: ,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,"Unspecified, might include formal therapy methods",Other: ,Y,both CMs and MHPs were surveyed,,,,,,n,Other: ,,,,n,,,,n,,,,,,,,,,,,,"Other: AI in general, mostly ChatGPT",10.0,,,,193 — Quote: “final sample consisted of 107 CMs and 86 MHPs”,"See section: Technology Comfort, AI Attitudes, and AI
Use Intention",,,,,,,,,n,Journal paper,,,,Consensus,,,Population survey,,,Use of AI in mental health care: Community and mental health professionals survey,General population,,Yes,AI attitude scale (AIAS-4): doi: 10.3389/fpsyg.2023.1191628,,y,y,y,Other: ,y,2024.0,2nd_search_39,2nd_search_299
Multi-turn chatbot,,Analysis of conversation transcripts,,L,B,Models classified cases requiring intervention; BSI>1 and higher CR indicate better behavior sensitivity/consistency; DeepSeek/Wenxin/Claude > GPT-4. — Quote: “BSI and CR … DeepSeek 1.0662 / 0.8985 … GPT-4 1.0415 / 0.8250,Y,,,,,,China,External data set,21.0,,,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Any reinforcement learning; Other: proposed a dynamic adversarial test method based on cross-variation” (Position: Abstract in excerpt),"Unspecified, might include formal therapy methods",Yes,,,,,L,W,Each LLM’s own “intervention decision” used to compute metrics. — Quote: “intervention decisions recorded and results labeled … quantitative analysis uses BSI and CR”,Y,Chinese,,,,,,,,,,,,,,,,,,,,,"GPT-4 / GPT-4o family; Claude family; Other: selected four widely-used LLMs … DeepSeek, GPT-4, Claude, Wenxin-Yiyan",3.0,,EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed),"Cross-mutation/crossover to swap context & emotion while fixing behavior; compute BSI & CR; chi-square tests — Quote: “behavior-anchored crossover-mutation… fixes the behavior dimension, swaps context and emotion… Core metrics: Behavior Sensitivity Index (BSI)… Consistency Rate (CR)… Chi-square test”",,,,,,,,,,,,Conference paper,,,,Reviewer Two,Y,detect the ability of LLMs to recognize extreme behaviors … behaviors covering high-risk actions,Empirical research involving an LLM,Y,evaluate … ability to intervene ethically … provide data-driven insights for optimizing safety mechanisms,Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios,No clients/patients involved,Psychotherapy -- speech transcripts,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_34,2nd_search_300
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,"External data set, modified",21.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,Other,No,N,,N,,,,,N,Chinese,,,,N,,,,N,L,-,Behavior Sensitivity Index,No Indexmodel; comparison of multiple LLMs,L,-,Consistency Ratio,No Indexmodel; comparison of multiple LLMs,,,,,GPT-4 / GPT-4o family; Claude family; Other: Deepseek & Wenxin - Yiyan,3.0,,selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.,"The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice.",,,,Y & N ,"Some are, others don´t",N,,,,,N,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios,No clients/patients involved,Other: Roleplay and sharing/microblogging data,No users involved,,,,,,Unknown,,2025.0,2nd_search_34,2nd_search_301
Multi-turn chatbot,,Client-facing application,,L,B,Models classified cases requiring intervention; BSI>1 and higher CR indicate better behavior sensitivity/consistency; DeepSeek/Wenxin/Claude > GPT-4. — Quote: “BSI and CR … DeepSeek 1.0662 / 0.8985 … GPT-4 1.0415 / 0.8250,Y,,,,,N,China,"External data set, modified",21.0,Unselected,No,N,,N,,,,,N,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Chinese,,,,N,,,,N,no benchmark,no benchmark,Behavior Sensitivity Index,No Indexmodel; comparison of multiple LLMs,no benchmark,no benchmark,Consistency Ratio,No Indexmodel; comparison of multiple LLMs,,,,,"GPT-4 / GPT-4o family; Claude family; Other: DeepSeek, ERNIE Bot",3.0,,"selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.

EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed)","The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice. 

Fig. 1",,,,Y,"Some are, others don´t",N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios,No clients/patients involved,"Other: test cases for ""psychological counseling scenarios"" involving extreme behaviors",No users involved,,,,,,Unknown,,2025.0,2nd_search_34,2nd_search_302
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,Self-collected data,27.0,Unknown,No,y,See Table 2,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",y,various measures,y,Waitlist control,y,Figures 3 and 6,y,Figure 6,Fine-tuning + other modules,Other CBT techniques,No,n,,n,,,,,n,English,,,,n,,,,n,l,b,"Patient Health Questionnaire 927 (PHQ-9), the Generalized Anxiety Disorder Questionnaire for the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM-IV) (GAD-Q-IV), and the Weight Concerns Scale (WCS) within the Stanford–Washington University Eating Disorder (SWED)",Waitlist control,no benchmark,no benchmark,engagement with Therabot (number of messages sent),no benchmark,,,,,Llama 2 family; Other: Falcon-7B,3.0,,"The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dialogues. The dialogues were developed by our research team, including a board-certified psychiatrist and a clinical psychologist, and peer-reviewed using evidence-based (primarily CBT) modalities.",Main LLM was fine-tuned but there are also other modules (e.g. safety classification),210,Results of survey in Fig 5,,y,"Llama 2, Falcon-7B",n,"usage of Amazon AWS, no discussion of user privacy",,,,n,Journal paper,,,,Richard Gaus,y,"Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model.",Empirical research involving an LLM,n,"All responses from Therabot were supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from Therabot (e.g., providing medical advice), we contacted the participant to provide correction.

However, no automatic screening",Randomized trial of a generative AI chatbot for mental health treatment,Patients with disorder explicitly based on ICD or DSM,Psychotherapy -- chat logs,Yes,(Working Alliance Inventory — Short Revised [WAI-SR]),,n,y,y,Unknown,y,2025.0,2nd_search_32,2nd_search_303
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Lebanon,Self-collected data,27.0,Unknown,No,Y,Table 2,N,,,,,,,,,,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",Y,"Patient Health 
Questionnaire 9(PHQ-9), the Generalized Anxiety 
Disorder Questionnaire for the Diagnostic and Statistical 
Manual of Mental Disorders, Fourth Edition (DSM-IV)
(GAD-Q-IV), and the Weight Concerns Scale (WCS) within 
the Stanford–Washington University Eating Disorder
(SWED), as measures of depression, anxiety, and weight 
concerns, respectively. Therapeutic alliance 
(Working Alliance Inventory — Short Revised [WAI-SR]).",Y,"To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.",N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,N,,N,,,,,,Other: unknown,,,,,,,,,,,,,,,,,,,,,"Other: Not specified ""The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dia-
logues""",3.0,,,"Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).",210,"Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX. ",,N,,N,,,,,,Journal paper,,,,Reviewer Two,Y,"In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model",Empirical research involving an LLM,Y,"All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.",Randomized trial of a generative AI chatbot for mental health treatment,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Other: expertly written therapist–patient dialogues based on third-wave CBT,Yes,,,N,Y,N,Trained professionals,Y,2025.0,2nd_search_32,2nd_search_304
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,Self-collected data,27.0,Unknown,Yes,Y,Table 2,N,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",Y,"Patient Health 
Questionnaire 9(PHQ-9), the Generalized Anxiety 
Disorder Questionnaire for the Diagnostic and Statistical 
Manual of Mental Disorders, Fourth Edition (DSM-IV)
(GAD-Q-IV), and the Weight Concerns Scale (WCS) within 
the Stanford–Washington University Eating Disorder
(SWED), as measures of depression, anxiety, and weight 
concerns, respectively. Therapeutic alliance 
(Working Alliance Inventory — Short Revised [WAI-SR]).",Y,"To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.",N,,N,,Fine-tuning + other modules,Other CBT techniques,No,N,,N,,,,,n,English,,,,n,,,,n,l,b,"Patient Health Questionnaire 927 (PHQ-9), the Generalized Anxiety Disorder Questionnaire for the Diagnostic and Statistical Manual of Mental Disorders, Fourth Edition (DSM-IV) (GAD-Q-IV), and the Weight Concerns Scale (WCS) within the Stanford–Washington University Eating Disorder (SWED)",Waitlist control,,,,,,,,,Llama 2 family; Other: Falcon-7B,3.0,,"The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dialogues. The dialogues were developed by our research team, including a board-certified psychiatrist and a clinical psychologist, and peer-reviewed using evidence-based (primarily CBT) modalities.","Main LLM was fine-tuned but there are also other modules (e.g. safety classification)

Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).",210,"Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX.

Results in Fig 5",,y,"Llama 2, Falcon-7B",N,"usage of Amazon AWS, no discussion of user privacy",,,,n,Journal paper,,,,Consensus,Y,"In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model",Empirical research involving an LLM,n,"All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.

However, no automatic screening pre-transmission",Randomized trial of a generative AI chatbot for mental health treatment,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Psychotherapy -- chat logs,Yes,,,N,Y,N,Trained professionals,Y,2025.0,2nd_search_32,2nd_search_305
Multi-turn chatbot,,Client-facing application,,,,,n,,no benchmark,no benchmark,Metric: training loss. No benchmark,y,Other: United Arab Emirates,External data set,9.0,Unknown,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,no benchmark,no benchmark,Metrics: Dist-1 and Dist-2 on chatbot responses. No benchmark,y,,,,,,,,,,,,,T5 family,12.0,,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,PEFT (qLoRA) fine-tuning of T5,,,,y,,y,"Privacy Concerns: Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks",,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Artificial Intelligence in Mental Health Care: The T5 Chatbot Project,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,2nd_search_31,2nd_search_306
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: UAE,Other: Multiple external data sets,9.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,English,,,,N,,,,N,-,-,"Training Metrics (training runtime, samples processed per second, steps per second, and Training loss)",-,-,-,Diversity Metrics (richness and variability; Dist-1 and Dist-2),-,,,,,T5 family,12.0,Ethical Considerations: Which users were informed?,Table 1,PEFT + QLORA,,,,Y,T5 can be used on-premise and via cloud (not in the text). ,Y,"Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.",,,,N,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Artificial Intelligence in Mental Health Care: The T5 Chatbot Project,No clients/patients involved,Other: Multiple types (compare results data loading and preparation),No users involved,,,,,,Other: ,,2024.0,2nd_search_31,2nd_search_307
Multi-turn chatbot,,Client-facing application,,,,,N,,no benchmark,no benchmark,Metric: training loss. No benchmark,y,Other: United Arab Emirates,External data set,9.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only fine-tuning,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,English,,,,N,no benchmark,no benchmark,Metrics: Dist-1 and Dist-2 on chatbot responses. No benchmark,y,,,,,,,,,,,,,T5 family,12.0,Ethical Considerations: Which users were informed?,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,PEFT (qLoRA) fine-tuning of T5,,,,Y,T5 can be used on-premise and via cloud (not in the text). ,Y,"Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.",,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Artificial Intelligence in Mental Health Care: The T5 Chatbot Project,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,2nd_search_31,2nd_search_308
Multi-turn chatbot,,Client-facing application,,H,W,"Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).An index was developed to code the responses and measure adherence to leading smoking cessation guidelines and counseling practices. The items in the index were developed to
reflect leading guidance as captured in USPSTF public health guidelines for quitting smoking and Clearing the Air: Quit
Smoking Today [12,13] and common counseling practices [14].",Y,,,,,N,USA,No dataset used for development or evaluation,30.0,,,N,,N,,L,S,Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).,Y,H,W,"Each chatbot response was independently categorized by 2
coders for their adherence on each item of the index.",Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only prompting; Other: RAG,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,,"Instructions to act like a counselor etc. implemented; This model also allowed us to use retrieval augmented generation technology, which allows the model to be provided with a corpus of knowledge to reduce hallucinations
and misinformation [8]. Both the provision of instructions and source materials can serve as guardrails that keep the chatbot in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate.",,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,N,,Empirical research involving an LLM,Y,"Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.",Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_30,2nd_search_309
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,30.0,,,,,,,,,,N,H,W,"dherence to an index developed
from the US Preventive Services Task Force public health guidelines for quitting smoking and counseling principles. Enough?",Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,,,maybe?,"somehow, since the benchmark were clinical guidelines:

Responses were analyzed for their adherence to an index developed from the US Preventive Services Task Force public health guidelines for quitting smoking and counseling principles.",,,,,,,,,N,,,,N,,,maybe the adherence index here.,,,,,,,,,,"ChatGPT, model unspecified",1.0,,,bots were queried with a fixed set of 12 prompts derived from real search queries; responses were coded against a guideline derived index,,,,,,,,,,,N,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,Y,some types of misinformation were present in 22% of responses,Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_30,2nd_search_310
Multi-turn chatbot,,Client-facing application,,H,W,"Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).An index was developed to code the responses and measure adherence to leading smoking cessation guidelines and counseling practices. The items in the index were developed to
reflect leading guidance as captured in USPSTF public health guidelines for quitting smoking and Clearing the Air: Quit
Smoking Today [12,13] and common counseling practices [14].",Y,,,,,N,USA,No dataset used for development or evaluation,30.0,,,N,,N,,L,S,Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).,Y,H,W,"Each chatbot response was independently categorized by 2
coders for their adherence on each item of the index.",Y,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,1.0,,,"Instructions to act like a counselor etc. implemented; This model also allowed us to use retrieval augmented generation technology, which allows the model to be provided with a corpus of knowledge to reduce hallucinations
and misinformation [8]. Both the provision of instructions and source materials can serve as guardrails that keep the chatbot in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate.",,,,N,,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,Y,"Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.",Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_30,2nd_search_311
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N ,,,,,,USA,No dataset used for development or evaluation,12.0,,,,,,,,,,,,,,,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",,,,,,,,,Only prompting,"Informal counseling (e.g., emotional support conversation)",,,,N,,,,,,,,,,,,,,,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,,,"Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. ",830,,,,,,,,,,,Journal paper,,,,Reviewer Two,N,,Other: experimental vignette study with panel raters,,,When ELIZA meets therapists: A Turing test for the heart and mind,No clients/patients involved; General population,,,,,N,N,N,,N,2025.0,2nd_search_29,2nd_search_312
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,12.0,,,Y,,N,,,,,N,,,,N,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,H,- (no B/S/W classification possible),Sentiment and part-of-speech,Therapist responses,,,,,,,,,GPT-4 / GPT-4o family,2.0, ,,"Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. Like the therapists’ instructions, the prompt did not place any limits on the length of the response, nor was ChatGPT shown the therapists’ responses.",,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,When ELIZA meets therapists: A Turing test for the heart and mind,No clients/patients involved,,No,,,,,,,,2025.0,2nd_search_29,2nd_search_313
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,USA,No dataset used for development or evaluation,12.0,,,Y,"In the current study, participants (N = 830) were 45.17 years old on average (SD = 16.56),
59.88% mentioned being in a current romantic relationship, and 18.07% of the sample
reported having ever engaged in couple therapy. Most participants identified as a woman
(50.60%), slightly fewer identified as a man (47.95%), and the remaining individuals identified
as non-binary or third-gender (0.24%), 0.12% preferred not to say, and 0.07% of the sample
did not answer. A majority of the sample identified as straight (83.25%), 7.83% of the sample
identified as bisexual, 2.65% as gay, 1.81% as asexual, 1.45% as lesbian, 0.72% as queer, and
0.60% preferred to not disclose. When considering race and ethnicity, most participants iden-
tified as non-Hispanic White (49.40%), followed by Black (18.80%), White Hispanic (16.87%),
Asian (5. %), Black Hispanic (0.84%), American Indian or Alaskan Native (0.12%), and the
remaining sample identified as other (8.43%), or preferred not to disclose (0.12%).",N,,,,,N,,,,N,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,H,unclear,Sentiment and part-of-speech,Therapist responses,H,S,Difference between responses (detection),Therapist responses,,,,,GPT-4 / GPT-4o family,2.0, ,,"Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. Like the therapists’ instructions, the prompt did not place any limits on the length of the response, nor was ChatGPT shown the therapists’ responses.",830,,,N,,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,N,,When ELIZA meets therapists: A Turing test for the heart and mind,General population,,No,,,,,,,,2025.0,2nd_search_29,2nd_search_314
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,External data set,15.0,Unselected,No,n,,n,,,,,n,l,b,"Human interactive evaluation (fluency, comforting, etc.), comparison with other models. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5).",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Informal counseling (e.g., emotional support conversation)",Yes,n,,n,,,,,,English,l,b,"BLEU, ROUGE, METEOR. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",y,,,,,,,,,,,,,,,,,GPT-3.5 family; Llama 2 family; Other: SeqGPT,12.0,,Emotional Support Conversation dataset (Liu 2021 Towards emotional support dialog systems),Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework,,,,y,,n,,l,b,"Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",y,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Lay people,,2023.0,2nd_search_28,2nd_search_315
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,"External data set, modified",15.0,Unknown,No,N,,N,,,,,N,L,B,Other LLMs,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,English,L,B,Other LLMs,Y,,,,N,,,,,,,,,,,,,Llama 2 family; Other: Azure GPT-3.5,12.0,,"Emotional Support Conversation dataset. Comprises 1,053 multi-turn dialogues, amounting to 31,410 utterances. ""We have augmented the dataset with additional annotations to signify the current state of the dialogue.""","Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.",,,,N,,N,,L,B,Other LLMs,Y,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,N,,Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Other: Crowdworkers,,2023.0,2nd_search_28,2nd_search_316
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,"External data set, modified",15.0,Unselected,No,N,,N,,,,,N,L,B,"Human interactive evaluation (fluency, comforting, etc.), comparison with other models. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5).",Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,English,L,B,"BLEU, ROUGE, METEOR. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",Y,,,,N,,,,,,,,,,,,,GPT-3.5 family; Llama 2 family; Other: SeqGPT,12.0,,"Emotional Support Conversation dataset. Comprises 1,053 multi-turn dialogues, amounting to 31,410 utterances. ""We have augmented the dataset with additional annotations to signify the current state of the dialogue.""","Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework

Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.",,,,N,,N,,L,B,"Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",Y,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Lay people,,2023.0,2nd_search_28,2nd_search_317
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Italy,Self-collected data,5.0,Unknown,No,N,,N,,,,,,,,,,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Only fine-tuning,Peer support conversation,No,N,,Y,"Integrating Socrates into environ-
ments such as hospitals or drug rehabilitation centers could
therefore provide substantial benefits. Patients could gain
additional time for psychological reflection beyond what current staffing constraints allow. Moreover, some individuals
might find it easier to discuss sensitive issues with Socrates
rather than in face-to-face interactions, potentially experienc-
ing less perceived judgment and accelerating psychological
change processes. Health care providers might also benefit
from this technology through more streamlined workflows
and reduced burnout risk.
",,,,,Other: unknown. Italian?,,,,,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",5.0,,,"Socrates was subjected to rigorous fine-tuning with pains-takingly detailed instructions designed to optimize therapeutic interactions. The model was specifically trained to recognize and appropriately respond to a wide spectrum of emotional states expressed by users, while respecting moments of  silence or reluctance to engage in conversation. It was calibrated to avoid overly intrusive questioning that might compromise user comfort and trust and instead detect nuanced linguistic patterns that might indicate various psychological states. Throughout its development, emphasis was placed on maintaining an appropriate balance between providing support and encouraging self-reflection",8,,,N,,Z,"Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.",,,,,Journal paper,,,,Reviewer Two,Y,"Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.",Empirical research involving an LLM,Y,see previous one,SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention,General population; Other: working professionals,Emotional support dialogue -- chat logs,Yes,"System Usability Scale, SUS
User Experience Scale, UES 
Client Satisfaction Questionnaire, CSQ
Likert-scale",,N,Y,Y,Other: Unsure,N,2025.0,2nd_search_26,2nd_search_318
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Italy,Other: yes but unclear which,5.0,,,n,,n,,,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",y,,n,,n,,n,,Only fine-tuning,Other CBT techniques,,n,,n,,,,,n,,,,,n,,,,n,no benchmark,no benchmark,"Subjective Units of Distress Scale, SUDS",pre post measurement,no benchmark,no benchmark,"Positive and Negative Affect Schedule, PANAS",pre post measurement,,,,,"ChatGPT, model unspecified",5.0,,,,8,"Most participants reported feeling strongly understood by
the chatbot, with the majority indicating a high degree of
perceived emotional attunement. Participants described the
experience as intuitive to navigate, personally meaningful,
and contextually relevant to their situations. A general sense
of satisfaction was expressed across the participant group,
with favorable ratings on the satisfaction measures.",,n,,n,,,,,n,Journal paper,,,,Richard Gaus,y,"Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.",Empirical research involving an LLM,n,,SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention,General population,,Yes,"system usability (System Usability Scale, SUS), user engagement
(User Experience Scale, UES), and satisfaction with the psy-
chological service (Client Satisfaction Questionnaire, CSQ).",,y,y,y,,y,2025.0,2nd_search_26,2nd_search_319
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Italy,"Other: some dataset was used, but completely unknown characteristics",5.0,Other: ,Other: ,n,,n,,,,,n,,,,n,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",y,,n,,n,,n,,Only fine-tuning,"CBT: Cognitive restructuring; Unspecified, might include formal therapy methods",Other: ,n,,Y,"Integrating Socrates into environ-
ments such as hospitals or drug rehabilitation centers could
therefore provide substantial benefits. Patients could gain
additional time for psychological reflection beyond what current staffing constraints allow. Moreover, some individuals
might find it easier to discuss sensitive issues with Socrates
rather than in face-to-face interactions, potentially experienc-
ing less perceived judgment and accelerating psychological
change processes. Health care providers might also benefit
from this technology through more streamlined workflows
and reduced burnout risk.
",,,,n,Other: ,,,,n,,,,n,no benchmark,no benchmark,"Subjective Units of Distress Scale, SUDS",pre post measurement,no benchmark,no benchmark,"Positive and Negative Affect Schedule, PANAS",pre post measurement,,,,,"ChatGPT, model unspecified",5.0,,,"Socrates was subjected to rigorous fine-tuning with pains-takingly detailed instructions designed to optimize therapeutic interactions. The model was specifically trained to recognize and appropriately respond to a wide spectrum of emotional states expressed by users, while respecting moments of  silence or reluctance to engage in conversation. It was calibrated to avoid overly intrusive questioning that might compromise user comfort and trust and instead detect nuanced linguistic patterns that might indicate various psychological states. Throughout its development, emphasis was placed on maintaining an appropriate balance between providing support and encouraging self-reflection",8,"Most participants reported feeling strongly understood by
the chatbot, with the majority indicating a high degree of
perceived emotional attunement. Participants described the
experience as intuitive to navigate, personally meaningful,
and contextually relevant to their situations. A general sense
of satisfaction was expressed across the participant group,
with favorable ratings on the satisfaction measures.",,n,,y,"Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.",,,,n,Journal paper,,,,Consensus,y,"Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.",Empirical research involving an LLM,n,,SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention,General population,Other: ,Yes,"System Usability Scale, SUS
User Experience Scale, UES 
Client Satisfaction Questionnaire, CSQ
Likert-scale",,N,Y,Y,Other: ,N,2025.0,2nd_search_26,2nd_search_320
Multi-turn chatbot,,Client-facing application,,,,,,,,,,n,UK,,10.0,,,,,,,,,,,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",Y,"higher reliable improvement, recovery, and reliable recovery rates” (Position: Results, p. 1)",Y,comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (,y,"Patients using the AI-enabled therapy support tool exhibited … fewer dropouts from treatment.” (Position: Results, p. 1)",,,Prompting + other modules,CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques,,,,Y,"eal-world observational study … in 5 of the United Kingdom’s National Health Service Talking Therapies services” (Position: Methods, p. 1)",,,,n,,,,,n,,,,n,,,n,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,-,244,"higher reliable improvement, recovery, and reliable recovery rates” 
users perceived the AI-enabled therapy support tool as most useful for discussing their problems to gain awareness and clarity … learning how to apply coping skills”",,,,,,,,,n,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study,Patients recruited in hospital or outpatient treatment facility,,Yes,,,y,y,n,,y,2025.0,2nd_search_25,2nd_search_321
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,UK,No dataset used for development or evaluation,10.0,,,Y,Table 1,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",Y,"Reliable improvement refers to a clinically significant
improvement in symptoms following a course of treatment and
is calculated as the score difference between the first and the
last validated clinical questionnaire completed. The types of
questionnaires patients complete are tailored to their specific
condition. For example, the Patient Health Questionnaire-9
(PHQ-9) [30] is used to measure depression symptom severity,
and the Generalized Anxiety Disorder-7 (GAD-7) [31] is used
to measure anxiety symptom severity. A clinically significant
improvement in symptoms is considered a change score ≥6 for
PHQ-9 or ≥4 for GAD-7 [26]",Y,"We compared the clinical outcomes of individuals who signed
up to use the AI-enabled therapy support tool (the intervention
group) with those of individuals who did not (the control group):",N,,N,,Other: unknown,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,,,,,N,,N,,,,,N,Journal paper,,,,Richard Gaus,Y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",Empirical research involving an LLM,Y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study,"Other: Probably patients with disorders, but not specified",,Yes,,,Y,N,N,,Y,2025.0,2nd_search_25,2nd_search_322
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,UK,No dataset used for development or evaluation,10.0,,,Y,Table 1,N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",Y,"Reliable improvement refers to a clinically significant
improvement in symptoms following a course of treatment and
is calculated as the score difference between the first and the
last validated clinical questionnaire completed. The types of
questionnaires patients complete are tailored to their specific
condition. For example, the Patient Health Questionnaire-9
(PHQ-9) [30] is used to measure depression symptom severity,
and the Generalized Anxiety Disorder-7 (GAD-7) [31] is used
to measure anxiety symptom severity. A clinically significant
improvement in symptoms is considered a change score ≥6 for
PHQ-9 or ≥4 for GAD-7 [26]",Y,comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (,N,,N,,Prompting + other modules,"CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,3.0,,,-,244,"higher reliable improvement, recovery, and reliable recovery rates” 
users perceived the AI-enabled therapy support tool as most useful for discussing their problems to gain awareness and clarity … learning how to apply coping skills”",,N,,N,,,,,N,Journal paper,,,,Consensus,Y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",Empirical research involving an LLM,Y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study,Patients recruited in hospital or outpatient treatment facility,,Yes,,,Y,y,N,,Y,2025.0,2nd_search_25,2nd_search_323
,,,,,,,,,,,,,China,,3.0,,,,,,,,,,,,,,,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,12.0,,,,,,,,,,,,,,,Conference paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues,,,,,,,,,,,2024.0,2nd_search_24,2nd_search_324
,,Analysis of conversation transcripts,,L,B,"Precision, Recall, Macro-F1",Y,,,,,,China,External data set,3.0,Unselected,No,,,,,L,B,BERT; BERT- CRF; BERT-MCRF; ROBERTA,Y,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,,No,,,,,,,,Y,Chinese,,,,,,,,,,,,,,,,,,,,,"BERT family; T5 family; GPT-3.5 family; ChatGPT, model unspecified",12.0,,MDRDC and Dialogue Safety,,,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues,,Other: Twitter (X),No,,,,,,Lay people,,2024.0,2nd_search_24,2nd_search_325
,,,,,,,,,,,,,China,,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,12.0,,,,,,,,,,,,,,,Conference paper,,,,Consensus,,,Empirical research involving an LLM,,,Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues,,,,,,,,,,,2024.0,2nd_search_24,2nd_search_326
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Richard Gaus,,,,,,Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study,,,,,,,,,,,,2nd_search_22,2nd_search_327
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Reviewer Two,,,,,,Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study,,,,,,,,,,,,2nd_search_22,2nd_search_328
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,,,,,,,,,,,,,,,,,,,Consensus,,,,,,Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study,,,,,,,,,,,,2nd_search_22,2nd_search_329
,,Client-facing application,,,,,,,,,,,Other: New Zealand,Self-collected data,5.0,Unknown,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,,"Unspecified, might include formal therapy methods",No,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,Other: AI in general,5.0,,,,306,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Population survey,,,"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",General population,Other: Rating,No users involved,,,,,,Unknown,,2025.0,2nd_search_20,2nd_search_330
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: New Zealand,No dataset used for development or evaluation,5.0,,,y,"articipants were largely Caucasian Amer-
ican (57.3%), followed by African American (23.5%), Asian
American (12.6%) and others/did not specify (6.6%)",n,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,,"Informal counseling (e.g., emotional support conversation)",,n,,n,,,,,,,,,,,,,,,,,,,,,,,,,,,"Other: just general ""generative AI application""",5.0,,,,348,,,-,,-,,,,,,Conference paper,,,,Richard Gaus,-,,Population survey,-,,"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",General population,,Yes,"Human Computer Trust Scale S. S. Siddharth Gulati and D. Lamas, “Design, development and evaluation of a human-computer trust scale,” Behaviour & Information Technology",,n,y,y,,y,2025.0,2nd_search_20,2nd_search_331
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: New Zealand,No dataset used for development or evaluation,5.0,Other: ,Other: ,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,,"Unspecified, might include formal therapy methods",Other: ,,,,,,,,,Other: ,,,,,,,,,,,,,,,,,,,,,"Other: just general ""generative AI application""",5.0,,,,306,No user experience assessment but participant attitude survey,,,,,,,,,,Conference paper,,,,Consensus,,,Population survey,,,"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",General population,Other: ,Yes,"Human Computer Trust Scale S. S. Siddharth Gulati and D. Lamas, “Design, development and evaluation of a human-computer trust scale,” Behaviour & Information Technology",,n,y,y,Other: ,y,2025.0,2nd_search_20,2nd_search_332
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Egypt,"External data set, modified",15.0,Unknown,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Only fine-tuning,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,,,,,"Other: English, Arabic",,,,,,,,,,,cross-entropy loss,compared to data set 70.58 %,,,,,,,,,Other: Jais-13B,1.0,"without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility","Bilingual (English & Arabic) mental health text corpus, origin not found",Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses,,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots,No clients/patients involved,Other: ,No users involved,,,,,,Unknown,,2025.0,2nd_search_18,2nd_search_333
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Egypt,External data set,15.0,"Other: Not applicable, data is not on users",No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: Arabic,,,,n,,,,n,no benchmark,no benchmark,cross-entropy loss,no benchmark,,,,,,,,,Other: Jais-13B,1.0,,Web-scraped data from diverse sources,Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B,,,,y,Jais-13B is on-premise,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots,No clients/patients involved,"Other: Internet data, various types and sources (mixed)",,,,,,,"Other: Not applicable, no interaction data",,2025.0,2nd_search_18,2nd_search_334
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Egypt,"External data set, modified",15.0,"Other: Not applicable, data is not on users",No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only fine-tuning,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: Arabic,,,,n,,,,n,no benchmark,no benchmark,cross-entropy loss,compared to data set 70.58 %,,,,,,,,,Other: Jais-13B,1.0,"without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility","Web-scraped data from diverse sources

Bilingual (English & Arabic) mental health text corpus, origin not found","Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B
Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses",,,,y,Jais-13B is on-premise,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots,No clients/patients involved,"Other: Internet data, various types and sources (mixed)",No users involved,,,,,,"Other: Not applicable, no interaction data",,2025.0,2nd_search_18,2nd_search_335
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,"External data set, modified",23.0,Unknown,No,,,,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,Fine-tuning + other modules,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,,,,N,English,,,,N,,,,N,,,N,,,,N,,,,N,,Other: Llama 3.2,5.0,,mental health counseling conversations from huggingface; fine-tuned with unsloth and ollama,"Fine-tuned LLaMA 3.2 (3B parameters) using Unsloth and Ollama; used RAG for retrieval, LangChain memory for conversation history, and FAISS for relevant material recall",,technical proof-of-concept; no real users assessed,,Y,Llama is ? so are unsloth/ollama,,,,,,N,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,No clients/patients involved,Emotional support dialogue -- chat logs,No,,,,,,Lay people,,2025.0,2nd_search_17,2nd_search_336
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,External data set,23.0,Unselected,No,n,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,n,English,,,,n,,,,n,no benchmark,no benchmark,"unclear metrics: relevance, empathy, conciseness, context",Unclear what these metrics are (human ratings? automatic?). No benchmarks used.,,,,,,,,,Other: Llama 3.2 family,5.0,,"Huggingface: Mental Health Counseling Conversations
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations",Fine-tuning via LoRA + RAG pipeline,,,,y,Llama 3.2 used,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2025.0,2nd_search_17,2nd_search_337
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,External data set,23.0,Unknown,No,n,,n,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",Yes,n,,n,,,,,N,English,,,,N,,,,N,no benchmark,no benchmark,"unclear metrics: relevance, empathy, conciseness, context",Unclear what these metrics are (human ratings? automatic?). No benchmarks used.,,,N,,,,N,,Other: Llama 3.2 family,5.0,,"Huggingface: Mental Health Counseling Conversations
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations","Fine-tuned LLaMA 3.2 (3B parameters) using Unsloth and Ollama; used RAG for retrieval, LangChain memory for conversation history, and FAISS for relevant material recall",,,,Y,Llama 3.2 used,n,,,,,N,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2025.0,2nd_search_17,2nd_search_338
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,India,No dataset used for development or evaluation,9.0,,,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Prompting + other modules; Other: RAG,"Other: The chatbot's architecture is built on a 
robust knowledge base encompassing evidence-based therapeutic techniques, including cognitive-behavioral therapy (CBT) principles and motivational interviewing 
strategies.",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,BERT family,4.0,,,Prompt Engineering + RAG + generic LLMs,,"Not described, how many users and rather vague representation of results in Fig. 7.",,N,,N,,,,,N,Conference paper,,,,Richard Gaus,Y,"The chatbot's functionality extends beyond mere conversation, integrating mood tracking features, 
personalized coping strategy recommendations, and crisis 
detection algorithms with built-in escalation protocols for 
high-risk situations",Empirical research involving an LLM,N,,MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging,"Other: Unclear, probably general population",,Yes,,,N,Y,N,,Y,2025.0,2nd_search_15,2nd_search_339
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,India,Self-collected data,9.0,,,,,,,,,,,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,Other CBT techniques,,,,,,,,,,English,,,,N,,,,,,,N,,,,,,,,,,BERT family,11.0,"Ambitious platform vision, but lacks any form of quantitative, clinical, or user-centered evaluation",assumes prior knowledge from mental health-related soruces or web-scraped materials,"uses NLP and probabilistic models to classify symptoms, map user queries to mental health knowledge base, and personalize therapeutic suggestions",,descpritive only,,,,,,,,,N,Conference paper,,,,Reviewer Two,N,,Empirical research involving an LLM,,,MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging,No clients/patients involved,,No,,,,,,,,2025.0,2nd_search_15,2nd_search_340
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,India,No dataset used for development or evaluation,9.0,,,,,,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules; Other: RAG,"CBT: Motivational interviewing; Other CBT techniques; Other: The chatbot's architecture is built on a 
robust knowledge base encompassing evidence-based therapeutic techniques, including cognitive-behavioral therapy (CBT) principles and motivational interviewing 
strategies.",,,,,,,,,N,Other: ,,,,N,,,,N,,,,,,,,,,,,,BERT family,4.0,"Ambitious platform vision, but lacks any form of quantitative, clinical, or user-centered evaluation",,"uses NLP and probabilistic models to classify symptoms, map user queries to mental health knowledge base, and personalize therapeutic suggestions",,"Not described, how many users and rather vague representation of results in Fig. 7.",,,,,,,,,N,Conference paper,,,,Consensus,Y,"The chatbot's functionality extends beyond mere conversation, integrating mood tracking features, 
personalized coping strategy recommendations, and crisis 
detection algorithms with built-in escalation protocols for 
high-risk situations",Empirical research involving an LLM,,,MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging,"No clients/patients involved; Other: User data shown in Fig. 7 but sample nowhere futher described. Hence unclear, how data was retrieved. ",,Yes,,,N,Y,N,,Y,2025.0,2nd_search_15,2nd_search_341
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Canada,"External data set, modified",8.0,Unknown,Yes,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation)",Yes,,,,,,,,,English,,,,,,,,,,,,,,,,,,,,,Other: not specif.,12.0,"Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz","mental health and healthcare-related texts; sources not fully specified, combined domain corpora and simulated patient records",,,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant,No clients/patients involved,Other: mix,No,,,,,,Unknown,,2024.0,2nd_search_11,2nd_search_342
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Canada,"External data set, modified",8.0,Unknown,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: unknown,,,,n,,,,n,l,b,Relevancy,"Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",l,b,User Satisfaction Score,"Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",,,,,GPT-3.5 family; Other: all-MiniLM-L6-v2 (for embedding),12.0,,"In order to effectively function and provide personalized
response to a user, data preparation plays a vital role. The
data comes from multiple sources, such as books, articles,
news, media, etc

We started experimenting with fine-tuning-based approach.
To generate the training dataset, we collected various mental
health related books, articles, and news stories and prepared
almost 500 pairs of instructions in the format of ”system”,
”user”, and ”assistant”. What we found is that this approach
is good for an LLM-based system that is already trained
on certain things",Fine-tuning (according to Fig. 3 at least) and RAG,,,,n,,n,,,,,n,Conference paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant,No clients/patients involved,"Other: different types (books, articles, etc.), not further specified",No users involved,,,,,,Unknown,,2024.0,2nd_search_11,2nd_search_343
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Canada,"External data set, modified",8.0,Unknown,No,n,,n,,l,b,"relevancy score. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",y,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: unknown,,,,n,,,,n,l,b,User Satisfaction Score,"Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",,,,,,,,,GPT-3.5 family; Other: all-MiniLM-L6-v2 (for embedding),12.0,"Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz","mental health and healthcare-related texts; sources not fully specified, combined domain corpora and simulated patient records",Fine-tuning (according to Fig. 3 at least) and RAG,,,,n,,n,,,,,n,Conference paper,,,,Consensus,n,,Empirical research involving an LLM,n,,LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant,No clients/patients involved,"Other: different types (books, articles, etc.), not further specified",No users involved,,,,,,Unknown,,2024.0,2nd_search_11,2nd_search_344
,,Analysis of conversation transcripts,,,,,n,,no benchmark,no benchmark,Correlation between LLM suicide response ratings and expert ratings,y,USA,Self-collected data,5.0,Other: n.a.,No,,,,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",No,,,y,"widespread use of LLM
technology—including new companies already drawing on
LLM technology for mental health care [29]—could reach a
much wider audience of individuals coping with depression and
suicidal thoughts. To date, a common guardrail has been for
LLMs to produce “hard stops”, in which individuals are referred
to 988 or another suicide prevention hotline. While such referrals
may be beneficial, they also artificially circumscribed
interactions in a way that could be taken as a missed opportunity.",,,,n,English,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,3.0,,SIRI-2 with added expert suicidologist ratings,,,,,,,,,,,,n,Journal paper,,,,Richard Gaus,,,Empirical research involving an LLM,,,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study,,Other: SIRI-2 test data with ratings,No users involved,,,,,,Other: n.a.,,2025.0,2nd_search_10,2nd_search_345
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,USA,External data set,5.0,Other: Not applicable; Artificial cases.,No,,,,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,"Other: LLMs did not provide support but instead compared to clinician responses using the SIRI-2 scoring system (= ""rating assessment"").",Yes,,,,,,,,N,English,,,,N,,,,N,H,W,Bias, Compared to the ratings of expert suicidologist.,H,S/B,Overall Performance,"Compared to the ratings of expert suicidologist.
",,,,,GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,3.0,,"Responses by expert suicidologists on a previously-published standardized scale: the Suicide Intervention Response Inventory (SIRI-2)
[15].","Research team members prompted LLMs with the original instructions for the SIRI-2, as well as with one of the SIRI2-2’s 24 items. We did not prompt LLMs with any additional text. ",,,,,,,,,,,N,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study,No clients/patients involved,Other: Standardized clinical instrument with hypothetical scenarios and predefined responses,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_10,2nd_search_346
Other: ,,Analysis of conversation transcripts,,,,,N,,no benchmark,no benchmark,Correlation between LLM suicide response ratings and expert ratings,y,USA,External data set,5.0,Other: n.a.,Other: Yes (hypothetical cases created by experts),,,,,,,,N,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,Only prompting,"Unspecified, might include formal therapy methods",Yes,,,,,,,,n,English,,,,N,,,,n,,,,,no benchmark,no benchmark,siri-2 z-score,"Compared to the ratings of expert suicidologist.
",,,,,GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,3.0,,"Responses by expert suicidologists on a previously-published standardized scale: the Suicide Intervention Response Inventory (SIRI-2)
[15].","Research team members prompted LLMs with the original instructions for the SIRI-2, as well as with one of the SIRI2-2’s 24 items. We did not prompt LLMs with any additional text. ",,,,,,,,,,,n,Journal paper,,,,Consensus,,,Empirical research involving an LLM,,,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study,,Other: Standardized clinical instrument with hypothetical scenarios and predefined responses,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_10,2nd_search_347
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Korea,Self-collected data,3.0,Unselected,No,Y,"The study included 20 participants aged 18 to 27 (mean 23.3,
SD 1.96) years with 60% (12/20) female and 40% (8/20) male.",N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Other: Korean,,,,N,,,,N,,,,,,,,,,,,,"GPT-4 / GPT-4o family; ChatGPT, model unspecified; Other: Conflicting information (?)

The HoMemeTown chatbot, powered by ChatGPT 4.0

The chatbot relies on the GPT API, a general-purpose language model provided by OpenAI, instead of a domain-specific model
trained for mental health counseling. The GPT API offers a range of models, such as Davinci, GPT-3.5, and GPT-4, which can be selected based on desired performance and cost
consideration

Another limitation is the potential inconsistency in comparing Dr CareSam, built on the ChatGPT 4.0 API,",1.0,Comparative Analysis unclear in methodological aspects to me. ,,,20,"Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. ",,N,,N,,,,,N,Journal paper,,,,Reviewer Two,Y,"The risk detection system in HoMemeTown was developed
based on established clinical guidelines [19] and validated
screening tools [20], implementing a sophisticated approach to
identifying and responding to potential mental health concerns.
The system continuously monitors user interactions for primary
risk indicators, including expressions of suicidal ideation, severe
depression symptoms, and anxiety crisis signals, while also
tracking secondary indicators such as sleep disturbance patterns
and social withdrawal signs.
When potential risks are detected, the system implements a
graduated response protocol that has been carefully designed
to provide appropriate levels of support while avoiding
unnecessary escalation. For mild risk situations, the system
offers empathetic acknowledgment and self-help resources,
drawing from evidence-based interventions [21]. In cases of
moderate risk, the response includes more direct expressions
of concern and specific mental health resources, while severe
risk triggers an immediate crisis response protocol with direct
connections to professional support services. To address the challenge of potential false positives in risk
detection, we implemented a sophisticated validation system
that examines multiple contextual factors before triggering
interventions. This system uses NLP techniques to analyze the
broader context of user communications, helping to distinguish
between casual expressions and genuine indicators of distress.
Regular professional review of high-risk cases ensures the
ongoing refinement of detection algorithms and response
protocols, maintaining a balance between sensitivity and
specificity in risk assessment.",Empirical research involving an LLM,N,,Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users,General population,Other: Rating,Yes,,,Y,Y,N,Lay people,Y,2025.0,2nd_search_9,2nd_search_348
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Other: Korea,No dataset used for development or evaluation,3.0,,,n,,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",n,,n,,n,,n,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation)",,n,,n,"Only: Integration with health care systems
• Investigate secure ways to integrate chatbot data with electronic health records, while maintaining user privacy.",,,,n,,,,,n,,,,n,l,b,"quantitative ux assessment (""overall satisfaction"")","benchmarks are other mental health chatbots and general LLMs: Woebot, Happify, GPT-3.5, Google Bard",,,,,,,,,GPT-4 / GPT-4o family,1.0,,,Elaborate server architecture around OpenAI API (see Fig. 1),20,"The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.",,n,,y,"Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].",,,,n,Journal paper,,,,Richard Gaus,y,"We incorporated a risk detection function to identify potential
mental health crises and provide appropriate resources when
necessary",Empirical research involving an LLM,y,"In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.",Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users,General population,,Yes,,,y,y,n,,y,2025.0,2nd_search_9,2nd_search_349
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Korea,No dataset used for development or evaluation,3.0,Other: ,Other: ,n,"The study included 20 participants aged 18 to 27 (mean 23.3,
SD 1.96) years with 60% (12/20) female and 40% (8/20) male.",N,,,,,N,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",Other: ,N,,N,"Only: Integration with health care systems
• Investigate secure ways to integrate chatbot data with electronic health records, while maintaining user privacy.",,,,N,Other: ,,,,N,,,,N,l,b,"quantitative ux assessment (""overall satisfaction"")","benchmarks are other mental health chatbots and general LLMs: Woebot, Happify, GPT-3.5, Google Bard",,,,,,,,,GPT-4 / GPT-4o family,1.0,Comparative Analysis unclear in methodological aspects to me. ,,Elaborate server architecture around OpenAI API (see Fig. 1),20,"The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.

Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. ",,N,,N,"Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].",,,,N,Journal paper,,,,Consensus,Y,"The risk detection system in HoMemeTown was developed
based on established clinical guidelines [19] and validated
screening tools [20], implementing a sophisticated approach to
identifying and responding to potential mental health concerns.
The system continuously monitors user interactions for primary
risk indicators, including expressions of suicidal ideation, severe
depression symptoms, and anxiety crisis signals, while also
tracking secondary indicators such as sleep disturbance patterns
and social withdrawal signs.
When potential risks are detected, the system implements a
graduated response protocol that has been carefully designed
to provide appropriate levels of support while avoiding
unnecessary escalation. For mild risk situations, the system
offers empathetic acknowledgment and self-help resources,
drawing from evidence-based interventions [21]. In cases of
moderate risk, the response includes more direct expressions
of concern and specific mental health resources, while severe
risk triggers an immediate crisis response protocol with direct
connections to professional support services. To address the challenge of potential false positives in risk
detection, we implemented a sophisticated validation system
that examines multiple contextual factors before triggering
interventions. This system uses NLP techniques to analyze the
broader context of user communications, helping to distinguish
between casual expressions and genuine indicators of distress.
Regular professional review of high-risk cases ensures the
ongoing refinement of detection algorithms and response
protocols, maintaining a balance between sensitivity and
specificity in risk assessment.",Empirical research involving an LLM,y,"In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.",Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users,General population,Other: ,Yes,,,Y,Y,N,Other: ,Y,2025.0,2nd_search_9,2nd_search_350
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Germany,Self-collected data,13.0,,,,,,,,,,,,,,N,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,,,,,,,CBT: Cognitive restructuring; Other CBT techniques,,,,,,,,,,,,,,N,,,,,,,N,,,,,,,,,,Other,6.0,,,,527,"users expressed positive attitudes toward digital mental health solutions, with key motives including avoiding embarrassment (36%) and concerns about appearance in face-to-face consultations (35%). Expectations focused on emotional support (35%) and expressing feelings (32%)",,,,Y,"Yes → Adheres to data protection principles, anonymized use",,,,N,Journal paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,"Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study",General population,,Yes,,,,Y,Y; working alliance inventory-short report? ,,Y,2025.0,2nd_search_8,2nd_search_351
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Germany,No dataset used for development or evaluation,13.0,,,y,Table 3,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",y,,n,,y,See 3.2 Engagement patterns,n,,Fine-tuning + other modules,CBT: Cognitive restructuring; Other CBT techniques,,n,,n,,,,,n,,,,,n,,,,n,no benchmark,no benchmark,WAI-SR working alliance,no benchmark,,,,,,,,,Other: Clare (R) by clare&me GmbH,6.0,,,"Clare is a standalone tool, based on fine-tuned LLMs plus other modules voice inputs, safety checks",21,,,y,"Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.",n,did not describe how claire aligns with data protection regulations,,,,n,Journal paper,,,,Richard Gaus,y,"If suicidality or severe distress is detected, users are directed to
psychological support hotlines and blocked from further use to
ensure safety.",Empirical research involving an LLM,y,"A custom moderation application programming interface (API)
filters inputs and outputs, flagging inappropriate content before
LLM processing. Key safety features include the following.","Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study",General population,,Yes,User experience questionnaire (UEQ),,y,n,y,,y,2025.0,2nd_search_8,2nd_search_352
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,Germany,No dataset used for development or evaluation,13.0,,,y,Table 3,n,,,,,n,,,,n,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",y,"UCLA, PHQ-D, PHQ-4, SWLS, Mini-SPIN",n,,y,See 3.2 Engagement patterns,n,,Fine-tuning + other modules,CBT: Cognitive restructuring; Other CBT techniques,,n,,n,,,,,n,,,,,n,,,,n,no benchmark,no benchmark,WAI-SR working alliance,no benchmark,,,,,,,,,Other: Clare (R) by clare&me GmbH,6.0,,,"Clare is a standalone tool, based on fine-tuned LLMs plus other modules voice inputs, safety checks",21,"users expressed positive attitudes toward digital mental health solutions, with key motives including avoiding embarrassment (36%) and concerns about appearance in face-to-face consultations (35%). Expectations focused on emotional support (35%) and expressing feelings (32%)",,n,"Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.
But not on-premise capable!",Y,"Yes → Adheres to data protection principles, anonymized use
https://www.clareandme.com/post/what-happens-with-your-data-at-clare-me",,,,n,Journal paper,,,,Consensus,y,"If suicidality or severe distress is detected, users are directed to
psychological support hotlines and blocked from further use to
ensure safety.",Empirical research involving an LLM,y,"A custom moderation application programming interface (API)
filters inputs and outputs, flagging inappropriate content before
LLM processing. Key safety features include the following.","Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study",General population,,Yes,User experience questionnaire (UEQ),,n,Y,y,,y,2025.0,2nd_search_8,2nd_search_353
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,,12.0,Psychopathology,No,N,,N,,,,,N,,,,,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",Y,DSM 5,N,,N,,N,,Only prompting,"CBT: Motivational interviewing; Informal counseling (e.g., emotional support conversation); Other: “if possible,” obtain a commitment to start the treatment medication, buprenorphine. ",No,N,,N,,,,,N,English,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,Unsure about the metrics. We can discuss during the consensus. See table 4,,,,,,N,,N,,,,,N,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,Y,"Question ""Is this infomation safe?""",Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.,No clients/patients involved,,Yes,,,Y,N,N,Trained professionals,N,2025.0,2nd_search_6,2nd_search_354
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,12.0,,,n,,n,,,,,n,no benchmark,no benchmark,Binary scoring rubrics for GPT-4 responses (see Table 4),y,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",n,,n,,n,,n,,Only prompting,CBT: Motivational interviewing,,n,,n,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,,,"Prompt engineering on GPT-4. Authors describe the model is being ""tuned"" or ""fine-tuned"" but in fact they only iteratively adjust the prompt without any weight adjustment.",,,,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.,No clients/patients involved,,No users involved,,,,,,,,2025.0,2nd_search_6,2nd_search_355
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,USA,Self-collected data,12.0,Other: not applicable,Yes,N,,N,,,,,N,no benchmark,no benchmark,Binary scoring rubrics for GPT-4 responses (see Table 4),y,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",n,,N,,N,,N,,Only prompting,"CBT: Motivational interviewing; Informal counseling (e.g., emotional support conversation)",No,N,,N,,,,,N,English,,,,N,,,,N,,,,,,,,,,,,,GPT-4 / GPT-4o family,2.0,Unsure about the metrics. We can discuss during the consensus. See table 4,see Generating Data for Evaluation,"Prompt engineering on GPT-4. Authors describe the model is being ""tuned"" or ""fine-tuned"" but in fact they only iteratively adjust the prompt without any weight adjustment.",,,,N,,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,n,,Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.,No clients/patients involved,Emotional support dialogue -- chat logs,No users involved,,,,,,Other: not applicable,,2025.0,2nd_search_6,2nd_search_356
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,China,External data set,3.0,Unselected,Yes,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Chinese,,,,N,H,W,,Y,H,W,Perceived information quality (unpromted),,H,B/S/W (depending on risk situation),Perceived information quality (promted),,,,,,GPT-3.5 family,4.0,All comparisons between AI and human counselors based on ML Models. ,"The study uses data from YiXinli Community, a major Chinese online counseling platform (https://www.xinli001.com) frequented by nearly 40 million users worldwide for psychological support. On this platform, 
users can confidentially express their psychological concerns and receive advice from human counselors in the open Q&A section (https://www.xinli001.com/qa). We utilized a web data crawler called houyicaiji (https://www.houyicaiji.com/) to gather this Q&A data, which encompasses 10,903 distinct questions and 19,682 human counselor responses collected from November 3, 2022, to 
March 30, 2023.",,,,,N,,N,,,,,N,Journal paper,,,,Reviewer Two,Y,"In this study, the ethical risks associated with directly applying LLMs to individuals seeking mental 
health support were mitigated by utilizing pre-existing real-world data. ",Empirical research involving an LLM,N,,Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Trained professionals,,2025.0,2nd_search_5,2nd_search_357
One-turn chatbot (usually Q&A),,Client-facing application,,,,,n,,,,,n,China,External data set,3.0,Unselected,No,n,,n,,,,,n,,,,n,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,n,,n,,n,,Only prompting,"Informal counseling (e.g., emotional support conversation)",Yes,n,,n,,,,,n,Chinese,,,,n,,,,n,h,b,"Automatic ""perceived information quality"" (PIQ) rating via ML model",benchmark are real human responses from the Q&A dataset,h,mixed (Table 9),Various linguistic factors,benchmark are real human responses from the Q&A dataset,,,,,GPT-3.5 family,4.0,"This study also was a ""transcript analysis"" via BERT","Data crawled from YiXinli Community (xinli001.com/qa). They crawled this data themselved ->  10,903 distinct questions and 19,682 human counselor responses",The help-seeking questions were posted to the GPT-3.5-turbo model by OpenAI API to get responses from ChatGPT,,,,n,,y,"First, although direct application risks were 
avoided, it is crucial to ensure that the real-world data used is properly protected and compliant with privacy regulations during both 
collection and processing (
). In this study, the data used was publicly available online, and all analysis was con­
ducted in a manner that safeguarded personal privacy. ",,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2025.0,2nd_search_5,2nd_search_358
One-turn chatbot (usually Q&A),,Client-facing application,,,,,N,,,,,N,China,External data set,3.0,Unselected,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Only prompting,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Chinese,,,,N,H,W,,Y,H,b,"Automatic ""perceived information quality"" (PIQ) rating via ML model (prompted ChatGPT)",benchmark are real human responses from the Q&A dataset,h,mixed (Table 9),Various linguistic factors,benchmark are real human responses from the Q&A dataset,,,,,GPT-3.5 family,4.0,"All comparisons between AI and human counselors based on ML Models. 

This study also was a ""transcript analysis"" via BERT","The study uses data from YiXinli Community, a major Chinese online counseling platform (https://www.xinli001.com) frequented by nearly 40 million users worldwide for psychological support. On this platform, 
users can confidentially express their psychological concerns and receive advice from human counselors in the open Q&A section (https://www.xinli001.com/qa). We utilized a web data crawler called houyicaiji (https://www.houyicaiji.com/) to gather this Q&A data, which encompasses 10,903 distinct questions and 19,682 human counselor responses collected from November 3, 2022, to 
March 30, 2023.",The help-seeking questions were posted to the GPT-3.5-turbo model by OpenAI API to get responses from ChatGPT,,,,N,,N,,,,,N,Journal paper,,,,Consensus,n,,Empirical research involving an LLM,N,,Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2025.0,2nd_search_5,2nd_search_359
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,No dataset used for development or evaluation,8.0,,,n,,n,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),y,,y,,n,,n,,Only prompting,Other: positive psychology intervention,,n,,n,,,,,n,,,,,n,,,,n,l,b,PHQ-9,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",l,b,"Scales of Psychological Well-being (PWB), Satisfaction With Life Scale (SWLS)","Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",l,s,"GAD-7, PANAS-P, PANAS-N","Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",GPT-3.5 family,1.0,,,Only sub-study 3 employed an LLM,48,"Before the study’s completion, we conducted unstructured interviews to delve deeper into the participants’ experiences with the chatbots. Many participants expressed that the chatbot’s responses evoked feelings of surprise and novelty. They emphasized the importance of feeling understood as pivotal in enhancing their acceptance and engagement with the chatbot-based intervention. Since GPT-based chatbots can generate text in a human-like manner, mimicking the interaction between users and psychotherapists, five participants indicated that the anthropomorphic responses felt so authentic that they found it difficult to distinguish whether they were AI-generated. These qualitative insights reinforce our quantitative findings and underscore the critical importance of interaction in the design of chatbots.",,n,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots,General population,,Yes,,,y,n,n,,y,2024.0,2nd_search_4,2nd_search_360
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,No dataset used for development or evaluation,8.0,,,N,,N,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),Y,"PHQ-9, GAD-7, PANAS-P, PANAS-N, SWLS, SVS",Y,"Evident in whole study (e.g., Finally, it is worth noting that our choice of an active control group (that is, a non-trivial control group)...)",N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",,N,,N,,,,,N,,,,,N,,,,N,,,,,,,,,,,,,GPT-3.5 family,1.0,,,"Pre-trained ChatGPT-3.5-Turbo API without fine-tuning but prompt engineering, integrating it with other system components.","Substudy 1
ITT-Sample: 207
CC-Sample: 154

Substudy 2
ITT-Sample: 70
CC-Sample 53

Substudy 3
ITT-Sample: 72
CC-Sample: 48",Only partialy described in the discussion section.,,N,,N,,,,,N,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,Y,"our exhaustive analysis of all chatbot- 
user dialogues recorded in this study did not reveal any inappropriate responses. This result, however, is not defini-
tive, and further research is required to ascertain the safety of chatbots in psychological research",Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots,General population,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_4,2nd_search_361
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,No dataset used for development or evaluation,8.0,,,N,,N,,,,,N,,,,N,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),Y,"PHQ-9, GAD-7, PANAS-P, PANAS-N, SWLS, SVS",Y,"Evident in whole study (e.g., Finally, it is worth noting that our choice of an active control group (that is, a non-trivial control group)...)",N,,N,,Only prompting,Other: positive psychology intervention,,N,,N,,,,,N,,,,,N,,,,N,l,b,"PHQ-9, PANAS-P, Satisfaction With Life Scale (SWLS)","Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",l,unknown,Scales of Psychological Well-being (PWB),"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",l,s,"GAD-7, PANAS-N","Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",GPT-3.5 family,1.0,,,Only sub-study 3 employed an LLM,48,"Before the study’s completion, we conducted unstructured interviews to delve deeper into the participants’ experiences with the chatbots. Many participants expressed that the chatbot’s responses evoked feelings of surprise and novelty. They emphasized the importance of feeling understood as pivotal in enhancing their acceptance and engagement with the chatbot-based intervention. Since GPT-based chatbots can generate text in a human-like manner, mimicking the interaction between users and psychotherapists, five participants indicated that the anthropomorphic responses felt so authentic that they found it difficult to distinguish whether they were AI-generated. These qualitative insights reinforce our quantitative findings and underscore the critical importance of interaction in the design of chatbots.",,N,,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,n,,Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots,General population,,Yes,,,Y,N,N,,Y,2024.0,2nd_search_4,2nd_search_362
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,China,Self-collected data,26.0,Other: n.a.,Other: unknown,n,,n,,,,,n,l,s,"We invited three counselors to evaluate 
these conversation cases. Benchmark is ""LLM-Counselor Support System""",y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",n,,y,,n,,n,,Fine-tuning + other modules,"Unspecified, might include formal therapy methods",No,n,,n,,,,,n,Other: unknown,,,,n,,,,n,l,b,Client satisfaction scale (CSS),Benchmars are other LLM-based models,l,b,Counselor rating form‑short (CRF‑S),Benchmars are other LLM-based models,l,b,Sentiment score,Benchmars are other LLM-based models,Other: ChatGLM2,11.0,,zhang 2024 dataset,Fine-tuning + knowledge retrieval,49,,,y,,n,,,,,n,Journal paper,,,,Richard Gaus,n,,Empirical research involving an LLM,n,,VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model,People with some symptoms but not disorder (determined by symptom scales or questionnaires),Other: Descriptions of mental disorders in the DSM-5,Yes,"Counselor rating form‑short (CRF‑S), Client satisfaction scale (CSS)",,n,y,y,Other: n.a.,y,2024.0,2nd_search_3,2nd_search_363
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,Self-collected data,26.0,Unknown,Yes,Y,"Finally, 49 participants were selected to participate in 
this study for the experiment. The age range of participants 
is 15–40 years old. Among them, 29 (59.18%) were male 
and 20 (40.82%) were female; 15 (30.61%) were middle 
school students, 6 (12.24%) were undergraduate students, 
12 (24.49%) were graduate students, and 16 (32.65%) were 
already working; 12 (24.49%) were 15–18 years old, 21 
(42.86%) were 18–24 years old, and 16 people (32.65%) 
were over 24 years old. All participants did not have a pro-
fessional background in psychology.",N,,,,,N,L,S/B,The LLM-Counselor Support System,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules; Prompting + other modules,"Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Other: Either Chines (most likely) or English,,,,N,,,,N ,L,B,Counselor rating form-short (CRF-S),Other LLMs/Models,,,,,,,,,Other: ChatGLM2-6B ,11.0,,,"A base LLM was fine-tuned on 80 structured psychological counseling cases to adapt it to clinical contexts. Additionally, the model uses dynamic prompting with a structured DSM-5-based knowledge base. This is integrated into a larger pipeline involving text summarization, keyword extraction, and similarity analysis, alongside multimodal outputs (e.g., facial expressions, voice, and a visual avatar) to enable affective interaction.",49,,,Y,GLM2-6B model ,N,,,,,N,Journal paper,,,,Reviewer Two,N,,Empirical research involving an LLM,Y ,"A professional counselor with three years of experience 
monitored the entire conversation to ensure the safety of the 
counseling process",VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model,Other: Patients with psychological problems and having sought professional help,"Other: Synthetic, self-created counseling dialogues for fine-tuning.",Yes,Client satisfaction scale (CSS),,N,Y,Y,Unknown,Y,2024.0,2nd_search_3,2nd_search_364
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,China,Self-collected data,26.0,Other: n.a.,Other: unknown,Y,"Finally, 49 participants were selected to participate in 
this study for the experiment. The age range of participants 
is 15–40 years old. Among them, 29 (59.18%) were male 
and 20 (40.82%) were female; 15 (30.61%) were middle 
school students, 6 (12.24%) were undergraduate students, 
12 (24.49%) were graduate students, and 16 (32.65%) were 
already working; 12 (24.49%) were 15–18 years old, 21 
(42.86%) were 18–24 years old, and 16 people (32.65%) 
were over 24 years old. All participants did not have a pro-
fessional background in psychology.",N,,,,,N,l,b,The LLM-Counselor Support System. Benchmark: GPT-4 with zero-shot CoT,Y,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Fine-tuning + other modules; Prompting + other modules,"Unspecified, might include formal therapy methods",No,N,,N,,,,,N,Other: unknown,,,,N,,,,N ,l,b,Client satisfaction scale (CSS),Benchmars are other LLM-based models,l,b,Counselor rating form‑short (CRF‑S),Benchmars are other LLM-based models,l,b,Sentiment score,Benchmars are other LLM-based models,Other: ChatGLM2-6B ,11.0,,zhang 2024 dataset,"A base LLM was fine-tuned on 80 structured psychological counseling cases to adapt it to clinical contexts. Additionally, the model uses dynamic prompting with a structured DSM-5-based knowledge base. This is integrated into a larger pipeline involving text summarization, keyword extraction, and similarity analysis, alongside multimodal outputs (e.g., facial expressions, voice, and a visual avatar) to enable affective interaction.",49,,,Y,GLM2-6B model ,N,,,,,N,Journal paper,,,,Consensus,N,,Empirical research involving an LLM,n,"A professional counselor with three years of experience 
monitored the entire conversation to ensure the safety of the 
counseling process",VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model,General population,Other: Descriptions of mental disorders in the DSM-5,Yes,"Counselor rating form‑short (CRF‑S), Client satisfaction scale (CSS)",,N,Y,Y,Other: n.a.,Y,2024.0,2nd_search_3,2nd_search_365
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Vietnam,External data set,20.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Prompting + other modules; Other: RAG,"Unspecified, might include formal therapy methods",Yes,N,,N,,,,,N,English,,,,N,,,,N,,,,,,,,,,,,,"Other: No specific model chosen, but as a conclusion claude-sonnet-3.5 suggested. ",2.0,"Three LLMs (Claude-Sonnet-3.5, gemini-1.5-pro, gpt-4o) used for evaluation of optimal base model for developed application. Then used LLM (gpt-4o) as a judge to compare reults agains each other. ","https://huggingface.co/datasets/jkhedri/psychology-dataset; Only questions embedded, not answers.","Uses RAG with prompt engineering: general LLM is guided by structured prompt templates for intent detection and empathetic response generation, while a vector-based QA database supplies domain-specific context.",,,,N,,N,,,,,N,Conference paper,,,,Richard Gaus,N,,Empirical research involving an LLM,N,,Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2025.0,2nd_search_2,2nd_search_366
Multi-turn chatbot,,Client-facing application,,,,,,,,,,,Other: Vietnam,External data set,11.0,Unselected,No,,,,,,,,,,,,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,,,,,,Prompting + other modules,"Informal counseling (e.g., emotional support conversation)",Yes,,,a little,facilitate access to professional care by linking users with nearby psychiatrists or psychology centers,,,,,English,,,,,,,,,,,,,,,,,,,,,Other: unclear,11.0,,"
1 https://huggingface.co/datasets/jkhedri/psychology-dataset.",,,,,,,,,,,,,Conference paper,,,,Reviewer Two,,,Empirical research involving an LLM,,,Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach,No clients/patients involved,Emotional support dialogue -- chat logs,,,,,,,Unknown,,2024.0,2nd_search_2,2nd_search_367
Multi-turn chatbot,,Client-facing application,,,,,N,,,,,N,Other: Vietnam,External data set,11.0,Unknown,No,N,,N,,,,,N,,,,N,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",N,,N,,N,,N,,Prompting + other modules,"Unspecified, might include formal therapy methods",Yes,N,,N,,no benchmark,no benchmark,No indexmodel but three equaly ranked models were compared ,Y,English,,,,N,,,,N,no benchmark,no benchmark,llm-as-a-judge approval and reassurance,no benchmark,no benchmark,no benchmark,llm-as-a-judge general response quality,no benchmark,,,,,GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,11.0,,"https://huggingface.co/datasets/jkhedri/psychology-dataset; Only questions embedded, not answers.","Uses RAG with prompt engineering: general LLM is guided by structured prompt templates for intent detection and empathetic response generation, while a vector-based QA database supplies domain-specific context.",,,,N,,N,,,,,N,Conference paper,,,,Consensus,N,,Empirical research involving an LLM,N,,Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach,No clients/patients involved,Internet data -- mental health Q&A,No users involved,,,,,,Unknown,,2024.0,2nd_search_2,2nd_search_368
,,Analysis of conversation transcripts,,,,,,,,,,,USA,External data set,29.0,Unselected,No,,,,,,,,,,,thematic content analysis by authors?,Y,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,Y,"Somehow: 
However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT’s overly validating nature, and privacy concerns",,Other: unsure,Yes,Y,Somehow: user-reported perceptions from Reddit,Y,"Speculative discussion on integration with clinical care, but not implemented",,,,,English,,,,N,,,,,,,,,,,,,,,,,"ChatGPT, model unspecified",4.0,,reddit mental health conversations mentioning ChatGPT,,,,,,,Y,Ethical use of Reddit data with disclaimers in 3.3.1,,,,N,Conference paper,,,,Reviewer Two,,,Other: content analysis of SM posts,Y,"discuss misinformation, over-validation, and ethical risks reported by users",""" I've talked to ChatGPT about my issues last night."": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",General population,Internet data -- mental health forum,Yes,themtaic coding of user posts // themes: ,,Y,,,Lay people,Y,2025.0,2nd_search_1,2nd_search_369
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,18.0,,,,,,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,,,,"Unspecified, might include formal therapy methods",,,,,,,,,n,,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified",10.0,,,,177 Reddit posts,Thematic analysis of Reddit posts and comments,,,,,,,,,n,Conference paper,,,,Richard Gaus,,,Population survey,,,""" I've talked to ChatGPT about my issues last night."": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",General population,,Yes,,,y,n,n,,y,2025.0,2nd_search_1,2nd_search_370
Multi-turn chatbot,,Client-facing application,,,,,n,,,,,n,USA,No dataset used for development or evaluation,18.0,Other: ,Other: ,,,,,,,,n,,,,n,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),,,,,,,Y,"Somehow: 
However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT’s overly validating nature, and privacy concerns",,"Unspecified, might include formal therapy methods",Other: ,Y,Somehow: user-reported perceptions from Reddit,Y,"Speculative discussion on integration with clinical care, but not implemented",,,,n,Other: ,,,,n,,,,n,,,,,,,,,,,,,"ChatGPT, model unspecified",10.0,,,,177 Reddit posts,Thematic analysis of Reddit posts and comments,,,,Y,Ethical use of Reddit data with disclaimers in 3.3.1,,,,n,Conference paper,,,,Consensus,,,Population survey,Y,"discuss misinformation, over-validation, and ethical risks reported by users",""" I've talked to ChatGPT about my issues last night."": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",General population,Other: ,Yes,,,y,n,n,Other: ,y,2025.0,2nd_search_1,2nd_search_371
