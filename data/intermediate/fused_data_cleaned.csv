covidence_id,study_id,title,reviewer_name,outlet_type,outlet_field,day,month,year,author_country,study_type,application_type,application_subtype_client_facing,client_type,client_count,application_subtype_therapist_facing,dataset_source,dataset_notes,dataset_type,dataset_language,dataset_contains_synthetic_data,dataset_is_public,dataset_user_psychopathology_status,dataset_responder_type,llm_development_approach,llm_development_approach_notes,intervention_type,models_employed,ux_assessment_is_present,ux_uses_standard_instrument,ux_uses_qualitative_assessment,ux_uses_quantitative_assessment,ux_results_reported,ux_assessment_instrument,ux_assessment_notes,lexical_overlap_used,lexical_overlap_vs_benchmark,lexical_overlap_benchmark_quality,lexical_overlap_notes,embedding_similarity_used,embedding_similarity_vs_benchmark,embedding_similarity_benchmark_quality,embedding_similarity_notes,classification_used,classification_vs_benchmark,classification_benchmark_quality,classification_notes,continuous_metrics_used,continuous_metrics_vs_benchmark,continuous_metrics_benchmark_quality,continuous_metrics_notes,expert_rating_used,expert_rating_vs_benchmark,expert_rating_benchmark_quality,expert_rating_notes,llm_judge_used,llm_judge_vs_benchmark,llm_judge_benchmark_quality,llm_judge_notes,perplexity_used,perplexity_vs_benchmark,perplexity_benchmark_quality,perplexity_notes,lexical_diversity_used,lexical_diversity_vs_benchmark,lexical_diversity_benchmark_quality,lexical_diversity_notes,metric1_name,metric1_vs_benchmark,metric1_benchmark_quality,metric1_notes,metric2_name,metric2_vs_benchmark,metric2_benchmark_quality,metric2_notes,metric3_name,metric3_vs_benchmark,metric3_benchmark_quality,metric3_notes,s1_risk_detection_considered,s1_risk_detection_notes,s2_content_safety_considered,s2_content_safety_notes,p1_on_premise_model_considered,p1_on_premise_model_notes,p2_privacy_awareness_considered,p2_privacy_awareness_notes,e1_demographics_reporting_considered,e1_demographics_reporting_notes,e2_outcomes_by_demographics_considered,e2_outcomes_by_demographics_notes,g1_early_discontinuation_considered,g1_early_discontinuation_notes,g2_overuse_considered,g2_overuse_notes,f1_validated_outcomes_considered,f1_validated_outcomes_notes,f2_control_condition_considered,f2_control_condition_notes,i1_multilevel_feasibility_considered,i1_multilevel_feasibility_notes,i2_healthcare_integration_considered,i2_healthcare_integration_notes,general_notes
1st_search_224,1st_search_1,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.,Reviewer Two,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",11.0,7.0,2022.0,Other: Japan,Empirical research involving an LLM,Analysis of conversation transcripts,Chatbot,No clients/patients involved,,,External data set,". All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021",Psychotherapy -- chat logs,Japanese,No,No,Unknown,Trained professionals,Other: logistic regression classifier,,Other: classification model,Other: logistic regression mode,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,not applicable,no LLM used,y,"This dataset was anonymized before being provided to the authors. All efforts for this study were made with the approval of Osaka Prefecture.
",n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_224,1st_search_2,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,7.0,2022.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,Data from messenger application LINE (line.me) from Osaka Prefectural Government,,,,,,,"Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier",,"Unspecified, might include formal therapy methods",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"no benchmark. used metrics: F1, precision, recall, AUC. Task: classify mental health and other issues (mental health, suicide thoughts, family, physical health, etc.) in counseling sessions",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_224,1st_search_3,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.,Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",11.0,7.0,2022.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,". All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021",,,,,,,"Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier",,"Unspecified, might include formal therapy methods",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,w,l,benchmark: TF-IDF-based classification via logistic regression model,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_214,1st_search_4,Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",9.0,2.0,2024.0,Other: Portugal,Population survey,Client-facing application,,Patients with disorder explicitly based on ICD or DSM,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,,"Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods",GPT-3.5 family,Yes,n,n,n,n,,secondary outcome was patient satisfaction on a Likert scale ,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"hoqol-bref
score",b,,, patient satisfaction likert scale ,,,,n,,,,y,"n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist",n,,n,,n,,n,,n,,n,,n,,y,"We included patients aged between 18 to 65 years who
were currently undergoing psychiatric inpatient care and
had received a mental health disorder diagnosis according
to the DSM-5. Each patient
’
s assistant physician evaluated
the presence of criteria for a DSM-5 diagnosis.",y,"They were then randomly allocated to either the con-
trol or intervention group using the 
“
coin flip
” method",n,,y,"n conclusion, our pilot study suggests that AI chatbots,
such as ChatGPT
, 
can positively impact the quality of life
of psychiatric inpatients while being well-received. Despite
the limitations inherent in a pilot study
, 
such as a small
sample size and the use of convenience sampling, our find-
ings provide valuable insights into the potential role of AI
in psychiatric care","intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes"
1st_search_214,1st_search_5,Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",9.0,2.0,2024.0,Other: Portugal,Empirical research involving an LLM,Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,12,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,n,n,y,y,,"As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,whoqol-bref score change,b,h,intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points),n,,,,n,,,,y,attending psychiatrist was checking responses,y,attending psychiatrist was checking responses,n,,y,"Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.",n,,n,,n,,y,only a set number of sessions was administered,y,World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF),y,control group that received standard care,n,,n,,
1st_search_214,1st_search_6,Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",9.0,2.0,2024.0,Other: Portugal,Empirical research involving an LLM,Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,12,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,n,n,y,y,,"As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,whoqol-bref score change,b,h,intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points),n,,,,n,,,,y,"n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist

attending psychiatrist was checking responses",n,,n,,y,"Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.",n,,n,,n,,y,only a set number of sessions was administered,y,World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF),y,control group that received standard care,n,,n,,"intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes"
1st_search_212,1st_search_7,Feasibility of combining spatial computing and AI for mental health support in anxiety and depression,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26.0,1.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),,,Self-collected data,VR/AI therapy sessions with GPT-4 avatar in biophilic scenes; sessions recorded and transcribed; qualitative thematic analysis after single ~30-min session per participant. • Derived: Not indicated.,Psychotherapy -- speech transcripts,English,No,,Unknown,Trained professionals,Prompting + other modules,,Other CBT techniques,GPT-4 / GPT-4o family,Yes,n,y,n,y,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_212,1st_search_8,Feasibility of combining spatial computing and AI for mental health support in anxiety and depression,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26.0,1.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Other: speech-based conversation system,People with some symptoms but not disorder (determined by symptom scales or questionnaires),14,,Self-collected data,"Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”",Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,Prompting + other modules,,"CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,n,y,n,y,,"Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.",n,,n,,y,Table-1,n,,n,,n,,n,,n,,n,,n,,
1st_search_212,1st_search_9,Feasibility of combining spatial computing and AI for mental health support in anxiety and depression,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26.0,1.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Other: Spoken dialogue system,People with some symptoms but not disorder (determined by symptom scales or questionnaires),14,,Self-collected data,"Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”",Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,Prompting + other modules,,"CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,n,y,n,y,,"Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.",n,,n,,y,Table-1,n,,n,,n,,n,,n,,n,,n,,
1st_search_204,1st_search_10,An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4.0,6.0,2024.0,UK,Empirical research involving an LLM,Client-facing application,Chatbot,"People with some symptoms but not disorder (determined by symptom scales or questionnaires); Other: Human evaluations relied on two groups: volunteers who believe they are suffering mental 
health issue, mental healthcare professionals and researchers.",10,,Self-collected data but derived from external data set,"created an own based on therapy transcript documents from different websites

As we could not find a suitable data set, we created one … We used real-world therapy transcript documents from websites and converted the HTML conversation texts…
",Psychotherapy -- speech transcripts,English,No,Other: did not find info. ,Psychopathology,Trained professionals,Only fine-tuning,,"Unspecified, might include formal therapy methods",GPT-3 family; GPT-3.5 family,Yes,n,y,y,y,,"1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5)",y,bleu score (b),l,"ChatGPTbased conversations 
(Approach 1), fine-tuned DialoGPT transformer 
conversations (Approach 2), and fine-tuned DialoGPT 
transformer conversations combined with the GPT3 prompts 
API (Approach 3)",n,,,,n,,,,y,perplexity evaluations (b),l,"ChatGPTbased conversations 
(Approach 1), fine-tuned DialoGPT transformer 
conversations (Approach 2), and fine-tuned DialoGPT 
transformer conversations combined with the GPT3 prompts 
API (Approach 3)",y,no benchmark,,s. user experience,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,no,"only:  such as personal details, to protect privacy and reduce computation complexities
",n,,n,,n,,n,,n,,n,,y,"Human evaluations relied on two groups: 
mental healthcare professionals and researchers who believe they are suffering mental 
health issue. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness and satisfaction of the 
chatbot, its conversation quality, and potential areas of 
improvement. ",y,"We want to emphasize, though, that our chatbot, while 
a step forward, is not a replacement for human therapists. 
Instead, we envision it as an auxiliary resource that can 
provide support in scenarios where human resources are 
stretched thin, or as an additional tool to complement 
traditional therapeutic processes.

 Integration with clinical systems: future research 
could look into integrating the chatbot with chatbot to provide more personalized and context-
aware support. It could also facilitate better 
coordination with healthcare professionals, alerting 
them when the chatbot identifies potential serious 
concerns.
existing clinical systems. This would allow the ",
1st_search_204,1st_search_11,An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4.0,6.0,2024.0,UK,Empirical research involving an LLM,Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),10,,External data set,"We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: ""This site is fiction and all the characters are fictional.""",Psychotherapy -- chat logs,English,Other: synthetic; human-generated (fictional),No,Other: not applicable,Other: not applicable,"Other: Fine-tuning of DialogGPT. This fine-tuned model is integrated into a custom chatbot pipeline, together with ChatGPT 3.5",,"Unspecified, might include formal therapy methods",GPT-2 family; GPT-3.5 family,Yes,n,y,y,y,,"To gather crucial user insights and evaluate the chatbots 
performance from a user’s perspective, we conducted 
surveys among a select group of mental health users and 
carers. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness of the 
chatbot, its conversation quality, and potential areas of 
improvement. Below, we detail the survey’s findings:
- Willingness for continued chatbot usage: when 
questioned about their willingness to engage with 
the chatbot again after the initial interaction, an 
overwhelming majority (90%) expressed a positive 
intent to reuse the service
- Rating of conversation quality (human-likeness): 
participants rated the chatbot’s conversational 
quality in terms of its human-like language on a 
scale from 1 to 5. The chatbot received an average 
score of 4.3, indicating a high degree of satisfaction 
with the chatbot’s language quality.
- Rating of conversation quality (supportiveness): 
when assessing the chatbot’s supportive nature in 
the conversation, participants gave an average score 
of 4.2 of 5, reflecting their positive experience in 
terms of perceived support.
- Overall rating of the chatbot application: when 
asked to provide an overall rating of the chatbot 
application, participants gave an average score 
of 4.6 out of 5. Notably, no participant rated the 
application lower than 4, indicating a high level of 
user satisfaction.
-  Positive feedback: participants were invited to 
share any positive feedback about their experience. 
We got 8 responses for this question and the most 
important points are the LLM-based chatbot can 
always provide useful suggestions and they feel very 
safe to talk to someone who are always available 
and talkable about their issue and sadness.
- Areas for improvement: we also encouraged 
users to suggest areas where the chatbot could 
be improved. The survey participants found the 
chatbot to be generally helpful, but suggested 
improvements such as exposing the training data 
to more diverse circumstances, enhancing the 
emotional support aspect, avoiding risky responses 
to sensitive inquiries, reducing repetition of 
examples, and focusing on more teaching sessions 
to make the interactions feel less robotic and more 
like conversing with a human friend.
- User interface (UI) suggestions: participants were 
also asked to provide suggestions for improving 
the chatbot’s UI functionalities to enhance its 
usefulness and usability. The feedback is very useful 
for us to implement further improved version. The 
suggestions include voice and image combined 
responses, able to track chat history, virtual reality 
(VR) or mixed reality innovation and realistic 
human tongues enhancement.",y,b,l,benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts),n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"no benchmark. however, experts rated the chatbot highly in absolute terms.",n,,,,n,,,,n,,,,perplexity,b,l,benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts),n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,rated by both patients and professionals,y,"e.g. "" The suggestion to integrate structured therapy 
techniques and conduct research on the efficacy 
of chatbots in mental health support indicates the 
potential for collaboration between the chatbot 
application and professionals in the field. Future 
policies could encourage partnerships between 
developers and professionals in psychology and 
counseling to enhance the chatbots effectiveness 
and provide a well-rounded approach to mental 
healthcare (23).""",
1st_search_204,1st_search_12,An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4.0,6.0,2024.0,UK,Empirical research involving an LLM,Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),10,,External data set,"We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: ""This site is fiction and all the characters are fictional.""",Other: not applicable,English,Other: synthetic; human-generated (fictional),No,Other: not applicable,Other: not applicable,"Other: Fine-tuning of DialogGPT. This fine-tuned model is integrated into a custom chatbot pipeline, together with ChatGPT 3.5",,"Unspecified, might include formal therapy methods",GPT-2 family; GPT-3.5 family,Yes,n,y,y,y,,"1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5)",y,b,l,"benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)

ChatGPTbased conversations
(Approach 1), fine-tuned DialoGPT transformer
conversations (Approach 2), and fine-tuned DialoGPT
transformer conversations combined with the GPT3 prompts
API (Approach 3)",n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"no benchmark. however, experts rated the chatbot highly in absolute terms.",n,,,,n,,,,n,,,,perplexity,b,l,benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts),n,,,,n,,,,n,,n,,n,,n,"only:  such as personal details, to protect privacy and reduce computation complexities
",n,,n,,n,,n,,n,,n,,y,"Human evaluations relied on two groups: 
mental healthcare professionals and researchers who believe they are suffering mental 
health issue. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness and satisfaction of the 
chatbot, its conversation quality, and potential areas of 
improvement. ",y,"We want to emphasize, though, that our chatbot, while 
a step forward, is not a replacement for human therapists. 
Instead, we envision it as an auxiliary resource that can 
provide support in scenarios where human resources are 
stretched thin, or as an additional tool to complement 
traditional therapeutic processes.

 Integration with clinical systems: future research 
could look into integrating the chatbot with chatbot to provide more personalized and context-
aware support. It could also facilitate better 
coordination with healthcare professionals, alerting 
them when the chatbot identifies potential serious 
concerns.
existing clinical systems. This would allow the ",
1st_search_203,1st_search_13,Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26.0,6.0,2024.0,UK,,,,,,,External data set,"CounselChat2,
provided through the HuggingFace Hub platform3. ",,,,Yes,,,,,"Unspecified, might include formal therapy methods",Mistral family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,see table 3,w,h,,linguistic features as mentioned under 3.3,w,h,,n,,,,n,,n,,y,mistral,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_203,1st_search_14,Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26.0,6.0,2024.0,UK,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only prompting,,Mix of formal therapy methods,Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,lexical diversity and richness,w,h,human psychologist responses in the CounselChat transcripts,readability scores,unclear,unclear,it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.,various other traditional nlp metrics,,,,n,,n,,y,mistral 7b,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_203,1st_search_15,Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26.0,6.0,2024.0,UK,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only prompting,,"Unspecified, might include formal therapy methods",Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,lexical diversity and richness,w,h,human psychologist responses in the CounselChat transcripts,linguistic features as mentioned under 3.3,unclear,h,it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.,n,,,,n,,n,,y,mistral 7b,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_200,1st_search_16,Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs),Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",22.0,12.0,2023.0,Other: Australia,Empirical research involving an LLM,Client-facing application,,General population,,,Other: Mix of external and self collected via data crawling,"PsyQA and Crawl of various chinese social media platforms Tianya, Zhihu, Yixinli

PsyQA (external); web-crawled psychology corpus (unnamed). Description: Crawled Chinese psychology-forum data (Yixinli, Tianya) to pretrain PanGu-350M; then fine-tuned on 56,000 PsyQA Q&A pairs.
",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Unknown,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-2 family; GPT-3 family; Other: PanGu (similar to GPT-3)and WenZhong (GPT-2),Yes,n,n,n,n,,"Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.",y,b,l,ROUGE-L - PanGu better than WenZhong,n,,,,n,,,,n,,,,y,w,h,"Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model",n,,,,n,,,,n,,,,perplexity,b,l,PanGu better than WenZhong,distinct 1,b,l,PanGu better than WenZhong,"distinct 2
",b,l,PanGu better than WenZhong,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_200,1st_search_17,Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs),Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",22.0,12.0,2023.0,Other: Australia,Empirical research involving an LLM,Client-facing application,Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.,No clients/patients involved,,,External data set,"training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.",Other: mixed,Chinese,No,No,Unknown,Unknown,Other: both original training of the transformer model and subsequent fine-tuning,,"Unspecified, might include formal therapy methods",GPT-2 family; Other: PanGu,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,ROUGE-L. no benchmark,n,,,,n,,,,n,,,,y,w,h,"psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set",n,,,,n,,,,n,,,,perplexity,no benchmark,no benchmark,no benchmark,"distinct-1, -2",no benchmark,no benchmark,no benchmark,n,,,,n,,n,,y,,y,"Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls",n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_200,1st_search_18,Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs),Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",22.0,12.0,2023.0,Other: Australia,Empirical research involving an LLM,Client-facing application,Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.,No clients/patients involved,,,External data set,"training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.",Other: mixed,Chinese,No,No,Unselected,Unknown,Other: both original training of the transformer model and subsequent fine-tuning,,"Unspecified, might include formal therapy methods","GPT-2 family; Other: PanGu, WenZhong (based on GPT-2)",No users involved,n,n,n,n,,"Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.",y,no benchmark,no benchmark,ROUGE-L - PanGu better than WenZhong,n,,,,n,,,,n,,,,y,w,h,"psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set

Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model",n,,,,n,,,,n,,,,perplexity,no benchmark,no benchmark,no benchmark,distinct 1,no benchmark,no benchmark,no benchmark,"distinct 2
",no benchmark,no benchmark,no benchmark,n,,n,,y,,y,"Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls",n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_198,1st_search_19,"Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,3.0,2024.0,India,Empirical research involving an LLM,Client-facing application,,No clients/patients involved,,,External data set,"publically available online data resources;
Kaggle mental-health conversation/Q&A sets; SAMHSA SOAR sample Medical Summary Reports
",Other: unclear,English,Other: unclear,Yes,,,Prompting + other modules,,"Informal counseling (e.g., emotional support conversation)",Llama 2 family,No users involved,n,n,n,n,,,n,,,,n,,,,yes ,better,low,Benchmark are different models,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,Llama,y,"User privacy and data security are paramount. The system operates under strict ethical guidelines and secure data storage protocols.”
",n,,n,,n,,n,,n,,n,,n,,n,"Authors mention that model is designed as a ""solution that could be used on common 
hardware by users without having the knowledge and 
technical proficiency regarding large language models""",
1st_search_198,1st_search_20,"Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14.0,3.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website,Other: mixed,Other: unknown,Other: unknown,No,Other: unknown,Other: unknown,Prompting + other modules,,Other: unclear,Llama 2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"human rating, not sure by whom",no benchmark,no benchmark,different models were rated by human non-experts. no comparison with any benchmark.,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_198,1st_search_21,"Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14.0,3.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website,Other: mixed,Other: unknown,Other: unknown,No,Other: unknown,Other: unknown,Prompting + other modules,,"Unspecified, might include formal therapy methods",Llama 2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"human rating, not sure by whom",no benchmark,no benchmark,different models were rated by human non-experts. no comparison with any benchmark.,n,,,,n,,,,n,,n,,y,Llama,n,"User privacy and data security are paramount. The system operates under strict ethical guidelines and secure data storage protocols.”
",n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_194,1st_search_22,Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference,Reviewer Two,,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2.0,11.0,2023.0,Other: Kyrgyz Republic,Empirical research involving an LLM,Client-facing application,,Other: unclear,,,Self-collected data,,,,,,,,Prompting + other modules,,Other CBT techniques,GPT-3 family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_194,1st_search_23,Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2.0,11.0,2023.0,Other: Kyrgyzstan,Empirical research involving an LLM,Client-facing application,Chatbot,"Other: 68 university students recruited using convenience sampling (social media campaign, institutional newsletter)",68,,Self-collected data,"The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.",Other: descriptions of cognitive distortions,Other: unknown,"Other: synthetic, human created",No,Unselected,Other: not applicable,Only fine-tuning,,CBT: Cognitive restructuring; Other CBT techniques,GPT-3 family,No,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.,n,,,,n,,,,n,,,,n,,,,n,,,,aaq + cds,s,h,aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.,n,,,,n,,,,n,,n,,n,,n,,n,,n,,y,fig 6 participants flow,y,number of messages sent is reported,y,"unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures",y,"After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. ",n,,n,,
1st_search_194,1st_search_24,Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2.0,11.0,2023.0,Other: Kyrgyzstan,Empirical research involving an LLM,Client-facing application,Chatbot,General population,68,,Self-collected data,"The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.",Other: descriptions of cognitive distortions,Other: unknown,"Other: synthetic, human created",No,Other: not applicable,Other: not applicable,Only fine-tuning,,CBT: Cognitive restructuring; Other CBT techniques,GPT-3 family,No,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.,n,,,,n,,,,n,,,,n,,,,n,,,,aaq + cds,s,h,aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.,n,,,,n,,,,n,,n,,n,,n,,n,,n,,y,fig 6 participants flow,y,number of messages sent is reported,y,"unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures",y,"After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. ",n,,n,,
1st_search_147,1st_search_25,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,3.0,2024.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,Treatment fidelity feedback,Self-collected data,"Counseling/chat conversations annotated for utterance‐level features; session-level features and summaries also produced (some via prompts). Annotation: Human annotators used a codebook of counseling strategy features
",Emotional support dialogue -- chat logs,English,No,No,Unselected,Trained professionals,,,"Unspecified, might include formal therapy methods",BERT family,No,n,n,n,n,,,n,,,,n,,,,y,b,l,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_147,1st_search_26,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,3.0,2024.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.,Emotional support dialogue -- chat logs,English,No,No,Unknown,Trained professionals,Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only,,"Unspecified, might include formal therapy methods",BERT family; GPT-3 family; GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,"Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.",n,,n,,n,,n,,n,,n,,n,,
1st_search_147,1st_search_27,Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,3.0,2024.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,"from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.

Counseling/chat conversations annotated for utterance‐level features; session-level features and summaries also produced (some via prompts). Annotation: Human annotators used a codebook of counseling strategy features",Emotional support dialogue -- chat logs,English,No,No,Unknown,Trained professionals,Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only,,"Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,w,l,"Table 4: Benchmark is fine-tuned DistilBERT model. ChatGPT is compared against this and performs worse.
Table 2: Prompted GPT-3 model (text-davinci-003) is compared against DistilBERT.

1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,"Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.",n,,n,,n,,n,,n,,n,,n,,
1st_search_134,1st_search_28,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",20.0,5.0,2024.0,Other: Australia,Empirical research involving an LLM,,,,,,Self-collected data but derived from external data set,"PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
CBT QA dataset. Derived from: PsyQA → CBT QA (ChatGPT-generated via CBT prompt). Short description: Chinese mental-health Q&A where responses are generated with a structured CBT prompt and used to fine-tune CBT-LLM.

",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,"Other: LLM responses, not human responders
",Prompting + other modules,,Other CBT techniques,GPT-3.5 family,,n,n,n,n,,,y,b,"l

",similar chinese language LLMs,y,b,"l
",similar chinese language LLMs,n,,,,n,,,,y,b,l,Relevance; CBT Structure; Helpfulness,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_134,1st_search_29,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",20.0,5.0,2024.0,Other: Australia,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,Self-collected data but derived from external data set,"External: PsyQA
Derived: CBT QA",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Other: not applicable,Only fine-tuning,,Other CBT techniques,Other: Baichuan-7B,No users involved,n,n,n,n,,,y,b,l,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",y,b,l,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",y,no benchmark,no benchmark,"accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection.",n,,,,y,b,l,Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,they say they release the data but where is it?,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_134,1st_search_30,CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",20.0,5.0,2024.0,Other: Australia,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,Self-collected data but derived from external data set,"PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
CBT QA dataset. Derived from: PsyQA → CBT QA (ChatGPT-generated via CBT prompt). Short description: Chinese mental-health Q&A where responses are generated with a structured CBT prompt and used to fine-tune CBT-LLM.

",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Other: not applicable,Only fine-tuning,,Other CBT techniques,"GPT-3.5 family; Qwen family; Other: Baichuan-7B, Llama 1, Alpaca 1",No users involved,n,n,n,n,,,y,b,l,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",y,b,l,"Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.",y,no benchmark,no benchmark,"accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection. Task: Cognitive distortion detection in client questions.",n,,,,y,b,l,"Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.

Measures: Relevance, CBT structure, helpfulness",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,they say they release the data but where is it?,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_128,1st_search_31,Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support,Reviewer Two,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),28.0,7.0,2023.0,USA,Conceptual or theoretical work (e.g. on ethics or safety),,,,,,Self-collected data,,,,,,,,,,,,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_128,1st_search_32,Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",9.0,11.0,2024.0,USA,Population survey,Client-facing application,Chatbot,Other: reddit users (r/Replika),462,,No dataset used for development or evaluation,,,,,,,,,,"Informal counseling (e.g., emotional support conversation)",GPT-3 family,Yes,n,y,n,y,,"Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_128,1st_search_33,Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support,Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",9.0,11.0,2024.0,USA,Population survey,Client-facing application,Chatbot,General population,462,,No dataset used for development or evaluation,,,,,,,,,,"Informal counseling (e.g., emotional support conversation)",GPT-3 family,Yes,n,y,n,y,,"Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_124,1st_search_34,"Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings",Reviewer Two,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),8.0,11.0,2024.0,China,Empirical research involving an LLM,Analysis of conversation transcripts,Chatbot,Other: The questions in PsyQA data set cover a variety of user groups.,,,External data set,"PsyQA
he data set contains … about 22,000 pieces of psychological counseling data… Topics included… growth, emotion, love… About 8% of the answers come from national second-class psychological counselors; 35% … from volunteers…” / “The data source for the knowledge graph is PsyQA …
",Internet data -- mental health Q&A,Chinese,No,Other: unclear,Unselected,Other: mixed,Prompting + other modules,,"Informal counseling (e.g., emotional support conversation)",GPT-2 family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,proof-of-concept prototype rather than a full-fledged clinical tool
1st_search_124,1st_search_35,"Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings",Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",28.0,10.0,2022.0,China,Empirical research involving an LLM,Client-facing application,Other: One-turn chatbot,No clients/patients involved,,,External data set,PsyQA,Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Trained professionals,"Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.",,"Unspecified, might include formal therapy methods",BERT family; GPT-2 family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,"even though this is a crisis call intervention system, there is not detection of acute risk",n,GPT-2 outputs are not checked,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"No evaluation of the conversation system. Study only displayed some conversation sample ""look this is good""."
1st_search_124,1st_search_36,"Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings",Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",28.0,10.0,2022.0,China,Empirical research involving an LLM,Client-facing application,"Other: ""one turn chatbot"", i.e., user inputs ""feelings and confusion"" and system makes analysis and outputs intervention text once. There is no turn-based interaction with the tool.",No clients/patients involved,,,External data set,PsyQA,Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Trained professionals,"Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.",,"Unspecified, might include formal therapy methods",BERT family; GPT-2 family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,"even though this is a crisis call intervention system, there is not detection of acute risk",n,GPT-2 outputs are not checked,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"proof-of-concept prototype rather than a full-fledged clinical tool

No evaluation of the conversation system. Study only displayed some conversation sample ""look this is good""."
1st_search_120,1st_search_37,"Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5.0,11.0,2023.0,China,Empirical research involving an LLM,Client-facing application,,No clients/patients involved,,,Self-collected data but derived from external data set,"MESConv dataset generated from YT videos

Large-scale multi-modal emotional-support dialogues with utterance-level emotion annotations and strategy labels; used for Emotion/Strategy/Response tasks.
",Emotional support dialogue -- speech transcripts,Other: did not find only guessed,Yes,No,Unselected,Unknown,,,"Informal counseling (e.g., emotional support conversation)",Other: none,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"emotion
task",b,l, recognizing the utterance-level emotion of the help-seeke,strategy task,b,l,predicting support strategies,response task,b,l,"generating supportive
response",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_120,1st_search_38,"Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5.0,11.0,2023.0,China,Empirical research involving an LLM,Client-facing application,"Other: conversation system with video, audio, and text input",No clients/patients involved,,,External data set,"MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube)",Other: Emotional support dialogue -- multimodal data,Other: unknown,No,No,Unselected,Unknown,"Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response",,"Unspecified, might include formal therapy methods",Other: BlenderBot,No users involved,n,n,n,n,,,y,b,l,benchmark: other conversation systems,n,,,,y,b,l,benchmark: other conversation systems,n,,,,n,,,,n,,,,n,,,,n,,,,perplexity,b,l,benchmark: other conversation systems,n,,,,n,,,,n,,n,,y,BlenderBot,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_120,1st_search_39,"Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5.0,11.0,2023.0,China,Empirical research involving an LLM,Client-facing application,"Other: conversation system with video, audio, and text input",No clients/patients involved,,,Self-collected data but derived from external data set,"MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube)

Large-scale multi-modal emotional-support dialogues with utterance-level emotion annotations and strategy labels; used for Emotion/Strategy/Response tasks.",Other: Emotional support dialogue -- multimodal data,Other: unknown,No,No,Unselected,Unknown,"Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response",,"Unspecified, might include formal therapy methods",Other: BlenderBot,No users involved,n,n,n,n,,,y,b,l,benchmark: other conversation systems,n,,,,y,b,l,benchmark: other conversation systems. Task: emotion classification in current utterance of help-seeker,n,,,,n,,,,n,,,,n,,,,n,,,,perplexity,b,l,benchmark: other conversation systems,n,,,,n,,,,n,,n,,y,BlenderBot,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_119,1st_search_40,Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",30.0,4.0,2023.0,India,Empirical research involving an LLM,,,,,,External data set,"HOPE;
Switchboard Dialogue-act Corpus; (also mentions a “Dialog-act corpus”). Description: Counseling conversations with dialogue-act labels (HOPE) and a general telephone dialogue-act corpus (Switchboard) used for evaluation/generalizability. Derived: Not indicated.
",Psychotherapy -- chat logs,English,Yes,Other: unclear,Unknown,Other: not applicable,"Other: not sure, ""READER is built on transformer to jointly predict a potential dialogue-act for the
next utterance and to generate an appropriate
response""",,"Informal counseling (e.g., emotional support conversation)",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,"DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",y,b,l,"DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",n,,,,n,,,,y,no benchmark,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_119,1st_search_41,Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",30.0,4.0,2023.0,India,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"HOPE
212 multi-turn English psychotherapy sessions (≈ 12.9 K utterances) between therapist and patient, transcribed from YouTube videos",Psychotherapy -- speech transcripts,English,No,No,Unknown,Unknown,"Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization",,"Unspecified, might include formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,"metrics: ROUGE, METEOR. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...)",y,b,l,"metrics: BERTScore. benchmarks: other language models (DialoGPT, GPT-2, DialogVED, ...)",n,,,,n,,,,y,b,l,"metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_119,1st_search_42,Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",30.0,4.0,2023.0,India,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"HOPE
212 multi-turn English psychotherapy sessions (≈ 12.9 K utterances) between therapist and patient, transcribed from YouTube videos

HOPE;
Switchboard Dialogue-act Corpus; (also mentions a “Dialog-act corpus”). Description: Counseling conversations with dialogue-act labels (HOPE) and a general telephone dialogue-act corpus (Switchboard) used for evaluation/generalizability. Derived: Not indicated.",Psychotherapy -- speech transcripts,English,No,No,Unknown,Unknown,"Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization",,"Unspecified, might include formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,"metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",y,b,l,"metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR",n,,,,n,,,,y,b,l,"metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_116,1st_search_43,"A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,10.0,2023.0,China,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data,"Multi-turn real counseling chat sessions collected on a Chinese online text-based free counseling platform, plus public QA; filtered to 7,935 sessions; labeled with an 8-category safety taxonomy; stratified split 90/10.
",Emotional support dialogue -- chat logs,Chinese,No,No,Unselected,Unknown,Only fine-tuning,,"Informal counseling (e.g., emotional support conversation)",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,ChatGPT (GPT 3.5),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.",n,Chat GPT 3.5 used + BERT ,n,Too little,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_116,1st_search_44,"A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,10.0,2023.0,China,Other: Development of new taxonomy for utterances of LLM psychotherapists. Fine-tuning of BERT. Direct evaluation of ChatGPT 3.5.,Analysis of conversation transcripts,,,,,Self-collected data,"We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

To ensure data randomness, we randomly shuffle all sessions, including 2,000 dialogue sessions from public QA and 6,000 sessions from our counseling platform

https://github.com/qiuhuachuan/DialogueSafety",Emotional support dialogue -- chat logs,Chinese,No,Yes,Unknown,Unknown,,,"Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"accuracy, precision, recall, F1 for classifying utterances",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,study is about this exactly,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_116,1st_search_45,"A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,10.0,2023.0,China,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data,"We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

Multi-turn real counseling chat sessions collected on a Chinese online text-based free counseling platform",Emotional support dialogue -- chat logs,Chinese,No,No,Unknown,Unknown,Only fine-tuning,,"Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,w,l,"accuracy, precision, recall, F1 for classifying utterances. Task: classification of mental health dialogue turns into safe/various types of unsafe responses. benchmark: fine-tuned BERT-base and RoBERTa-large",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.",n,Chat GPT 3.5 used + BERT ,n,Too little,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_115,1st_search_46,"Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,7.0,2023.0,Other: Poland,Empirical research involving an LLM,,,,,,Other: Translation of English datasets to Polish ,"DailyDialog and EmpatheticDialogues as basis; translated and merged: CORTEX; (enriched by Polish Common Crawl);

A Polish emotion-labeled text corpus was semi-supervisedly expanded with unlabeled web data to enlarge the training set
",Other: emotion-labeled text corpus,Other: Polish,No,Yes,Other: not applicable,Other: not applicable,Only fine-tuning,,Other CBT techniques,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_115,1st_search_47,"Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,7.0,2023.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data but derived from external data set,"Name of new dataset: CORTEX. Derived from DailyDialog, EmpatheticDialogues",,,,,,,Only fine-tuning,,"Informal counseling (e.g., emotional support conversation)",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_115,1st_search_48,"Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,7.0,2023.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data but derived from external data set,DailyDialog and EmpatheticDialogues as basis; translated and merged: CORTEX; (enriched by Polish Common Crawl) ,,,,,,,Only fine-tuning,,"Informal counseling (e.g., emotional support conversation)",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_101,1st_search_49,Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",19.0,3.0,2024.0,Germany,Other: experimental research,Client-facing application,,No clients/patients involved,,,Self-collected data,,,,,,,,,,"Unspecified, might include formal therapy methods","Other: ""custom GPT"", version not further specified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"Based on the analysis of the results, we will consider that implementing a custom
ChatGPT in a robot to support ADHD therapies presents considerable potential. Its advan-
tages include personalization, where ChatGPT can tailor interactions to each patient’s unique
needs and responses, thus potentially enhancing the therapeutic experience. Consistency is
another benefit, as a ChatGPT-equipped robot can offer stable support, which is crucial in
ADHD therapies where routine and predictability play vital roles. Additionally, ChatGPT’s
capability to understand and generate natural language can significantly increase the en-
gagement and interactivity of therapy sessions for children with ADHD, thus making them
more dynamic and effective. However, we also found a range of complex challenges and
considerations. Key among these is the need for emotional intelligence.
Significantly, ChatGPT and its use by a robotic assistant should complement, not
replace, human therapists (as also mentioned by the studies [14,21]), as the human element
is critical, especially for children with ADHD. ","""Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality."""
1st_search_101,1st_search_50,Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement,Richard Gaus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",19.0,3.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,Self-collected data but derived from external data set,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,y," The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.",n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_101,1st_search_51,Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement,Consensus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",19.0,3.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,Self-collected data,Some PDFs for configuring a custom GPT (type of PDFs not further specified),,,,,,,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,"Not sufficient: The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.",n,,n,,n,,n,,n,,n,,n,,n,,"""Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality."""
1st_search_98,1st_search_52,Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26.0,9.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,,No clients/patients involved,,,External data set,MIBot v5.1 dataset,Emotional support dialogue -- chat logs,English,No,Other: unclear,Unknown,Unknown,Only prompting,,CBT: Motivational interviewing,GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_98,1st_search_53,Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26.0,9.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"Dataset was used for evaluation only.

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539])",Emotional support dialogue -- chat logs,English,Yes,No,Unselected,Other: chatbot response,Only prompting,,CBT: Motivational interviewing,GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,rating scale to determine quality of backward looking reflections (Textbox 5). ,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_98,1st_search_54,Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26.0,9.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"Dataset was used for evaluation only.

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539])",Emotional support dialogue -- chat logs,English,Yes,No,Psychopathology,Other: not applicable,Only prompting,,CBT: Motivational interviewing,GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,human rating (unclear if expert or not),no benchmark,no benchmark,rating scale to determine quality of backward looking reflections (Textbox 5).,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_95,1st_search_55,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23.0,7.0,2024.0,India,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,External data set,"To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Only prompting,,"Informal counseling (e.g., emotional support conversation)",,No,n,n,n,n,,,y,,l,other models,y,,l,other models,n,,,,n,,,,y,,l,No relative comparisons where conducted; the experts rated in absolute terms,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_95,1st_search_56,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23.0,7.0,2024.0,India,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data but derived from external data set,"external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS",Psychotherapy -- speech transcripts,English,No,No,Unknown,Unknown,Only prompting,,"Unspecified, might include formal therapy methods","BART family; T5 family; GPT-2 family; Llama 2 family; Mistral family; Other: Phi-2, GPT-J, GPT-Neo",No users involved,n,n,n,n,,,y,no benchmark,no benchmark,"no benchmark. metric: ROUGE-1, -2, -L",y,no benchmark,no benchmark,BERTScore,n,,,,n,,,,y,no benchmark,no benchmark,"no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,acceptability data collected only from one stakeholder level (clinicians),n,,
1st_search_95,1st_search_57,Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23.0,7.0,2024.0,India,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,Self-collected data but derived from external data set,"external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS

To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube",Psychotherapy -- speech transcripts,English,No,No,Unknown,Unknown,Only prompting,,"Unspecified, might include formal therapy methods","BART family; T5 family; GPT-2 family; Llama 2 family; Mistral family; Other: Phi-2, GPT-J, GPT-Neo",No users involved,n,n,n,n,,,y,no benchmark,no benchmark,"no benchmark. metric: ROUGE-1, -2, -L",y,no benchmark,no benchmark,BERTScore,n,,,,n,,,,y,no benchmark,no benchmark,"no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,acceptability data collected only from one stakeholder level (clinicians),n,,
1st_search_93,1st_search_58,Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",16.0,8.0,2022.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data,derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform ,"Other: therapist performance task—text responses, not patient dialogues",English,No,Yes,Other: Providers/ not patients,Trained professionals,Only fine-tuning,,,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,yes ,same ,h,"the standard deviation
of ratings generated by DistilBERT (SD = 0.230) were much
closer to the distribution of human ratings (SD = 0.308) versus
that of the SVR (SD = 0.146",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,albeit not really applicable,n,albeit not really applicable,y,"Table 2 summarizes key descriptive statistics related to our
sample of task-taking therapists. Of the 978 task takers, 81.6%
(n = 798) were women. This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%). The majority of therapists had
at least three years of experience providing therapy and a
significant minority had 11 or more years of experience (n =
321, 32.8%). However, most therapists had much less
experience providing messaging therapy; 818 (83.6%) had
between zero and two years of messaging therapy experience",n,,n,,n,,n,,n,,n,,n,,
1st_search_93,1st_search_59,Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",16.0,8.0,2022.0,,Empirical research involving an LLM,Other: Unsure between analysis of conversation transcripts or therapist-facing application with treatment fidelity feedback,,,,Treatment fidelity feedback,"Other: Unsure. Authors describe the data collection process as if they collected it themselves. But they state: ""The dataset for the present study is derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com):",,,,,,,,Only fine-tuning,,Mix of formal therapy methods,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,y,b,l,benchmark is support vector regressor (lower capacity model),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,"Some demographics of therapists are shown but only male/female, no ethnic etc. data",n,,n,,n,,y,FIS-T is validated,n,,n,,n,,"This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%)"
1st_search_93,1st_search_60,Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",16.0,8.0,2022.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data but derived from external data set,derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com),,,,,,,Only fine-tuning,,Mix of formal therapy methods,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,y,b,l,benchmark is support vector regressor (lower capacity model),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,"Some demographics of therapists are shown but only male/female, no ethnic etc. data",n,,n,,n,,y,FIS-T is validated,n,,n,,n,,"This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%)"
1st_search_92,1st_search_61,Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",19.0,7.0,2024.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,Protocall random sample of crisis calls,,,,,,,Only fine-tuning,,"Unspecified, might include formal therapy methods",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,s,h,Human Interrater agreement,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"""With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation.""",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"""Specific applications of machine learning–based evaluation may go beyond after-action call-level summaries of statement-level evaluations that cannot affect the quality of care received by the caller in the moment. Specifically, real-time evaluation may also be possible. For example, machine-learning–enabled call center software could label interventions as they occur, providing just-in-time feedback and suggestions to call takers when specific interventions are not provided."" (...) ""..stakeholders include call takers who can use this information to learn from previous calls, supervisors and administrators who may better identify and then direct resources to call takers who are struggling with their performance, and funders who may begin to include population-level data on the quality of conversations - complementing existing metrics on answer rates and wait times."" ",
1st_search_92,1st_search_62,Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",19.0,7.0,2024.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,Random sample of 476 Protocall crisis calls,,,,,,,Only fine-tuning,,Mix of formal therapy methods,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,s,h,"metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other).",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,the tool itself is a suicide risk detection tool,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_92,1st_search_63,Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",19.0,7.0,2024.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,Random sample of 476 Protocall crisis calls,,,,,,,Only fine-tuning,,"Unspecified, might include formal therapy methods",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,s,h,"metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other). Task: classify crisis calls transcripts into 10 suicide risk labels.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"""With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation.""

the tool itself is a suicide risk detection tool",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"""Specific applications of machine learning–based evaluation may go beyond after-action call-level summaries of statement-level evaluations that cannot affect the quality of care received by the caller in the moment. Specifically, real-time evaluation may also be possible. For example, machine-learning–enabled call center software could label interventions as they occur, providing just-in-time feedback and suggestions to call takers when specific interventions are not provided."" (...) ""..stakeholders include call takers who can use this information to learn from previous calls, supervisors and administrators who may better identify and then direct resources to call takers who are struggling with their performance, and funders who may begin to include population-level data on the quality of conversations - complementing existing metrics on answer rates and wait times."" ",
1st_search_91,1st_search_64,Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",18.0,1.0,2024.0,Other: Isreal,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data,,,,,,Psychopathology,,Only fine-tuning,,Psychodynamic psychotherapy,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,"
y",w,h,F1-Micro Score against human labeling,n,,,,"n
",,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_91,1st_search_65,Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",18.0,1.0,2024.0,Other: Israel,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data,,,,,No,Psychopathology,,,,Mix of formal therapy methods,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,w,h,they used f1 and kappa to evaluate the emotion labeling system. the benchmark was a human annotator.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,"Some demographic information given: The clients were all above age 18 (Mage =
39.06, SD = 13.67, range 20–77), and most were women (58.9%).
Of the clients, 92% were native Hebrew speakers and 92% were
born in Israel. Of the clients, 53.5% had at least a bachelor’s degree;
53.5% were single and 8.9% were in a committed relationship
but unmarried; 23.2% were married and 14.2% were divorced
or widowed. ",n,,n,,n,,y,"The Outcome Rating Scale (ORS), Profile of Mood States",n,,n,,n,"this is all: Automatic emotion recognition models can be integrated into existing feedback systems to provide an indication of the levels of
emotional coherence in psychotherapy sessions and allow therapists
to modify their interventions accordingly.",
1st_search_91,1st_search_66,Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",18.0,1.0,2024.0,Other: Israel,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data,,,,,,Psychopathology,,Only fine-tuning,,Mix of formal therapy methods,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,w,h,they used f1-micro and kappa to evaluate the emotion labeling system. the benchmark was a human annotator. Task: classify emotions in client utterances,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,"Some demographic information given: The clients were all above age 18 (Mage =
39.06, SD = 13.67, range 20–77), and most were women (58.9%).
Of the clients, 92% were native Hebrew speakers and 92% were
born in Israel. Of the clients, 53.5% had at least a bachelor’s degree;
53.5% were single and 8.9% were in a committed relationship
but unmarried; 23.2% were married and 14.2% were divorced
or widowed. ",n,,n,,n,,y,,n,,n,,n,"this is all: Automatic emotion recognition models can be integrated into existing feedback systems to provide an indication of the levels of
emotional coherence in psychotherapy sessions and allow therapists
to modify their interventions accordingly.",This is one of the few studies that also included psychodynamic psychotherapy in the intervention.
1st_search_89,1st_search_67,Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",26.0,5.0,2022.0,USA,"Other: Tool Development and Evaluation, Direct LLM performance evaluation",,,,,,External data set,"Subreddits in the following categories: (a) Coping and Therapy (C-Th): 7Cup
sofTea, Existential_crisis, getting_over_it, Grief-
Support, helpmecope, hardshipmates, HereToHelp,
itgetsbetter, LostALovedOne, offmychest, MMFB,
Miscarriage, reasonstolive, SuicideBereavement,
therapy; (b) Mood Disorders (MD): depression, de-
pressed, lonely, mentalhealth; (c) Psychosis and
Anxiety (P-An): anxiety, BipolarReddit, socialanxi-
ety; and (d) Trauma and Abuse (Tr-A): abuse, sur-
vivors, Anger, emotionalabuse, PTSDcombat.
Alexander Street Press video transcripts",Internet data -- mental health Q&A,,Yes,No,Psychopathology,Trained professionals,Only fine-tuning,,"Unspecified, might include formal therapy methods",GPT-2 family; Other: DialoGPT,No users involved,n,n,n,n,,,y,no benchmark,,,y,no benchmark,,,n,,,,n,,,,y,no benchmark,,,n,,,,n,,,,n,,,,lexical diversity,no benchmark,,,average length,no benchmark,,,n,,,,n,,n,,unclear/yes,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_89,1st_search_68,Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26.0,5.0,2022.0,USA,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,Self-collected data,"self-created ""Empathic Conversation dataset""",Emotional support dialogue -- chat logs,English,No,Other: synthetic human-generated,Unselected,Unknown,Other: fine-tuning + transfer learning,,"Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,benchmark are human responses.,y,s,l,benchmark are human responses.,n,,,,n,,,,y,b,h,benchmark are human responses.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_89,1st_search_69,Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues,Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",26.0,5.0,2022.0,USA,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,Self-collected data,"self-created ""Empathic Conversation dataset""",Emotional support dialogue -- chat logs,English,Other: Yes; human actors,No,Unselected,Unknown,Other: fine-tuning + transfer learning,,"Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Unspecified, might include formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,benchmark are human responses,y,w,l,benchmark are human responses.,n,,,,n,,,,y,no benchmark,,,n,,,,n,,,,n,,,,lexical diversity,w,l,,average length,unclear,l,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_88,1st_search_70,Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21.0,2.0,2022.0,India,Empirical research involving an LLM,Analysis of conversation transcripts,,"No clients/patients involved; Other: youtube, The data collection
process provides us 12.9𝐾 utterances from 212 counseling therapy
sessions – all of them are dyadic conversations only.",212 counseling sessions (not clear how many patients),,Self-collected data,"HOPE
Short description: Multi-turn, therapist–patient counseling dialogues labeled with dialog acts; used to evaluate SPARTA vs baselines.
",Emotional support dialogue -- speech transcripts,,No,No,Unknown,Unknown,Prompting + other modules,,Mix of formal therapy methods,"Other: ""For speaker-invariant representations, we employ a pre-trained RoBERTa language model which is further fine-tuned on DAC task.""",No users involved,n,n,n,n,,,n,,,,n,,,,y,b,h,"Several benchmarks used, including ones that were more recently developed. ""In comparison, SPARTA-TAA obtains significant improvements over all
baselines.""",n,,,,n,,,,n,,,,n,,,,n,,,,macro-f1,b,h,"""SPARTA-TAA obtains significant improvements over all
baselines. It reports improvements of +8.64%, +8.58%, and +6.29%
in macro-F1 (60.29), weighted-F1 (64.53), and accuracy (64.75%),
respectively, as compared to CASA""",weighted-f1,b,h,,accuracy,b,h,,n,,n,,unclear,,n,nothing,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_88,1st_search_71,Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21.0,2.0,2022.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data but derived from external data set,Name of new dataset: HOPE. Derived from transcripts from YouTube counseling sessions. They added self-created annotations to these data.,,,,,,,"Other: they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.",,Mix of formal therapy methods,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,"benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. for dialogue-act classification of conversational turns",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_88,1st_search_72,Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21.0,2.0,2022.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data but derived from external data set,Name of new dataset: HOPE. Derived from transcripts from YouTube counseling sessions. They added self-created annotations to these data.,,,,,,,"Other: fine-tuning + wrapper.
they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.",,Mix of formal therapy methods,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,"Several benchmarks used, including ones that were more recently developed. ""In comparison, SPARTA-TAA obtains significant improvements over all
baselines.""

benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. Task: Dialogue-act classification of conversational turns",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_86,1st_search_73,Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",15.0,2.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,,Patients recruited in hospital or outpatient treatment facility,,,Self-collected data,,,,,,,,,,Other CBT techniques,BERT family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,TBD - Unsure,n,,,TBD - Unsure,n,,,,n,,,,construct validity using mtmm and hierarchical  linear models hlm,,,No benchmark,criterion validity,,,No benchmark,n,,,,n,,n,,y,,n,,y,"We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.",n,,n,,n,,n,,n,,n,,y,"The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).",
1st_search_86,1st_search_74,Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28.0,2.0,2024.0,Germany,Empirical research involving an LLM,Analysis of conversation transcripts,,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,35,,Self-collected data,therapy transcriptions from real patients (that they recruited themselves),Psychotherapy -- speech transcripts,Other: German,No,No,Psychopathology,Trained professionals,Only prompting,,Other CBT techniques,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,construct validity,no benchmark,no benchmark,no benchmark,criterion validity,no benchmark,no benchmark,no benchmark,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,y,"measured phq-9, gad, etc. in patients",n,,n,,n,,
1st_search_86,1st_search_75,Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28.0,2.0,2024.0,Germany,Empirical research involving an LLM,Analysis of conversation transcripts,,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,35,,Self-collected data,therapy transcriptions from real patients (that they recruited themselves),,,,,,,Only prompting,,Other CBT techniques,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,construct validity using mtmm and hierarchical  linear models hlm,no benchmark,no benchmark,no benchmark,criterion validity,no benchmark,no benchmark,no benchmark,n,,,,n,,n,,n,,n,,y,"See Table S1

We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.",n,,n,,n,,n,"measured phq-9, gad, etc. in patients but did not predict those from the transcripts",n,,n,,n,"some description in following paragraph but too superficial:

The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).",
1st_search_81,1st_search_76,Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example),Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),3.0,1.0,2024.0,Other: Russia,Empirical research involving an LLM,Client-facing application,Chatbot,General population,,,Self-collected data,Survey of 236 participants evaluating ChatGPT-generated vs. psychologist-generated psychological recommendations.,Other: psychological recommendation texts used as experimental stimuli,Other: unclear presumeably russian,Yes,No,Unselected,Trained professionals,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,n,n,y,n,"During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).","""During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used).""​",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,"
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.",n,,n,,n,,n,,y,"A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.",n,,n,,
1st_search_81,1st_search_77,Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example),Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,10.0,2023.0,Other: Russia,Empirical research involving an LLM,Client-facing application,Other: one-turn recommendations generated by LLM,Other: unclear,236,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",Yes,n,n,y,y,,"Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions (""score"", ""activity"", ""informativeness""), while psychologist responses were rated higher in ""strength"".",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_81,1st_search_78,Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example),Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,10.0,2023.0,Other: Russia,Empirical research involving an LLM,Client-facing application,Other: one-turn recommendations generated by LLM,General population,236,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",Yes,n,n,y,y,"During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).","Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions (""score"", ""activity"", ""informativeness""), while psychologist responses were rated higher in ""strength"".

""During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used).""​",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,"
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.",n,,n,,n,,n,,y,"A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.",n,,n,,
1st_search_77,1st_search_79,Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,1.0,2024.0,Other: Hong Kong,Empirical research involving an LLM,Client-facing application,,General population,,,Self-collected data,"Open Up text-counseling logs; after filtering → 44,810 valid sessions / 3,231,830 messages; subset with postsession feedback 5,240 sessions / 533,609 messages; stratified sample 131 sessions / 6,169 messages used for annotation & evaluation.
",Emotional support dialogue -- chat logs,Chinese,No,No,Psychopathology,Lay people,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family; GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,"unsure, see page 8",y,w,l,,n,,,,y,,h,"unsure, see figure 5",y,b,l,GPT 4 better than GPT3.5,n,,,,n,,,,linguistic inquiry and word count (liwc),w,l,worst of all,logistic regression,w,l,,long short tem measure,w,l,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_77,1st_search_80,Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,1.0,2024.0,China,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,"Open Up is a free24/7 web-based, text-based counseling service in Hong Kong that enables people aged between 11 and 35 years to anonymously chat with paid staff (staff counselors or social workers) or trained volunteers. 5240 sessions with 533,609 messages (Figure 3). We stratified the 5240 sessions based on the number of messages in each session. There were a total of 131 unique message count groups among the 5240 sessions.",Emotional support dialogue -- chat logs,Chinese,No,No,Unselected,Trained professionals,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,"benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_77,1st_search_81,Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,1.0,2024.0,China,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,"Open Up is a free24/7 web-based, text-based counseling service in Hong Kong that enables people aged between 11 and 35 years to anonymously chat with paid staff (staff counselors or social workers) or trained volunteers. 5240 sessions with 533,609 messages (Figure 3). We stratified the 5240 sessions based on the number of messages in each session. There were a total of 131 unique message count groups among the 5240 sessions.",Emotional support dialogue -- chat logs,Chinese,No,No,Unselected,Trained professionals,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,"benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_75,1st_search_82,Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",7.0,7.0,2023.0,Other: Japan,Empirical research involving an LLM,Client-facing application,Other: smartphone CBT app (Kokoro App),Patients with disorder explicitly based on ICD or DSM; General population,1626,,External data set," FLATT (Fun to Learn to Act and Think 
through Technology) trial + HCT (Healthy Campus Trial)",,,,,,,Prompting + other modules,,CBT: Cognitive restructuring,Other: Japanese Text-to-Text Transfer Transformer,No,n,n,n,n,,,n,,,,n,,,,y,,,,n,,,,y,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,"n
",,n,,n,,n,,y,PHQ-9,n,,n,,n,,
1st_search_75,1st_search_83,Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",7.0,7.0,2023.0,Other: Japan,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,FLATT dataset (Fun to Learn to Act and Think through Technology),Other: automatic thought-feeling pairs,Japanese,No,No,Psychopathology,Other: not applicable,Only fine-tuning,,CBT: Cognitive restructuring,T5 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"accuracy, F1, precision, recall for prediction of feeling based on automatic thought",n,,,,n,,,,n,,,,n,,,,n,,,,reduction of negative feelings,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_75,1st_search_84,Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",7.0,7.0,2023.0,Other: Japan,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,External data set,FLATT dataset (Fun to Learn to Act and Think through Technology),Other: automatic thought-feeling pairs,Japanese,No,No,Psychopathology,Other: not applicable,Only fine-tuning,,CBT: Cognitive restructuring,T5 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"accuracy, F1, precision, recall for prediction of feeling based on automatic thought (feeling-thought pairs)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,T5 model,n,,"n
",,n,,n,,n,,n,,n,,n,,n,,
1st_search_73,1st_search_85,Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1.0,7.0,2020.0,USA,Empirical research involving an LLM,,,,,,External data set,Motivational Interviewing (MI) counseling dataset from Perez-Rosas et al. (2016); Alexander Street dataset ,Psychotherapy -- speech transcripts,English,No,Other: unclear,Unselected,Lay people,Only fine-tuning,,CBT: Motivational interviewing,GPT-2 family,No users involved,n,n,n,n,,,y,b,l,seq2seq,y,b,l,seq2seq,n,,,,n,,,,y,b,l,seq2seq,n,,,,n,,,,n,,,,diversity,s,l,seq2seq,n,,,,n,,,,n,,n,,y,gpt 2,n,too little,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_73,1st_search_86,Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1.0,7.0,2020.0,USA,Empirical research involving an LLM,Client-facing application,Other: reflection generation,No clients/patients involved,,,External data set,"Motivational Interviewing counseling dataset (Perez-Rosas 2016)

The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The dataset is derived from a collection of 284 video recordings of counseling encounters using MI. The recordings were collected from various sources, including two clinical trials, students’ counseling sessions from a graduate level MI course, wellness coaching phone calls, and demonstrations of MI strategies in brief medical encounters.",Psychotherapy -- speech transcripts,English,No,No,Other: mixed,Other: mixed,Other: fine-tuning + retrieval and context expansion,,CBT: Motivational interviewing,GPT-2 family,No users involved,n,n,n,n,,,y,b,l,benchmark is a simple seq2seq model,y,b,l,benchmark is a simple seq2seq model,n,,,,n,,,,y,s,h,benchmark is ground truth (human created reflections) and output of simple seq2seq model,n,,,,n,,,,n,,,,diversity,s,l,benchmark is a simple seq2seq model,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_73,1st_search_87,Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1.0,7.0,2020.0,USA,Empirical research involving an LLM,Therapist-facing application,Other: reflection generation,,,Utterance suggestions,External data set,"Motivational Interviewing counseling dataset (Perez-Rosas 2016)

The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The dataset is derived from a collection of 284 video recordings of counseling encounters using MI. The recordings were collected from various sources, including two clinical trials, students’ counseling sessions from a graduate level MI course, wellness coaching phone calls, and demonstrations of MI strategies in brief medical encounters.",Psychotherapy -- speech transcripts,English,No,No,Unselected,Lay people,Other: fine-tuning + retrieval and content expansion,,CBT: Motivational interviewing,GPT-2 family,No users involved,n,n,n,n,,,y,b,l,seq2seq,y,b,l,seq2seq,n,,,,n,,,,y,s,h,benchmark is ground truth (human created reflections) and output of simple seq2seq model,n,,,,n,,,,n,,,,diversity,s,l,seq2seq,n,,,,n,,,,n,,n,,y,gpt 2,n,too little,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_72,1st_search_88,Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25.0,3.0,2025.0,Other: Korea,Empirical research involving an LLM,Therapist-facing application,,,,Other: Probability of Advanced Tinitus Therapy success ,Self-collected data,42 tinitus patients as raw data + augmented dataset as test samples,"Other: CBT patient diaries + audiometry/THI clinical data
",English,Yes,No,Psychopathology,Unknown,Only fine-tuning,,CBT: Cognitive restructuring; Other CBT techniques,Other: Google T5 Transformer ,No,n,n,n,n,,,n,,,,n,,,,n,,,,yes,b,no,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,y?,"“Informed consent was obtained… participants were assured that no personal sensitive data would be collected… All data collected was kept confidential and anonymous, ensuring complete privacy and data protection.”
",n,,n,,n,,n,,n,,n,,n,,y,,
1st_search_72,1st_search_89,Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",29.0,3.0,2024.0,Other: Korea,Empirical research involving an LLM,Other: Analysis of CBT diary data,,Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires),42,,Self-collected data,"The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients",Other: cbt diary entries,Other: Korean,No,No,Psychopathology,Other: not applicable,Other: Fine-tuning plus other elements like clustering,,Other CBT techniques,T5 family; GPT-2 family,No,n,n,n,n,,,y,no benchmark,no benchmark,Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.,n,,,,n,,,,y,no benchmark,no benchmark,Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,y,Tinnitus Handicap Inventory (THI),n,,n,,n,,
1st_search_72,1st_search_90,Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",29.0,3.0,2024.0,Other: Korea,Empirical research involving an LLM,Other: Analysis of CBT diary data,,Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires),42,,Self-collected data,"42 tinitus patients as raw data + augmented dataset as test samples

The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients",Other: cbt diary entries,English,No,No,Psychopathology,Other: not applicable,Other: Fine-tuning plus other elements like clustering,,CBT: Cognitive restructuring; Other CBT techniques,T5 family; GPT-2 family,No,n,n,n,n,,,y,no benchmark,no benchmark,Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.,n,,,,n,,,,y,no benchmark,no benchmark,"Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though (Table 10, 11)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,"“Informed consent was obtained… participants were assured that no personal sensitive data would be collected… All data collected was kept confidential and anonymous, ensuring complete privacy and data protection.”
",n,,n,,n,,n,,y,Tinnitus Handicap Inventory (THI),n,,n,,n,,
1st_search_70,1st_search_91,Towards Motivational and Empathetic Response Generation in Online Mental Health Support,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11.0,7.0,2022.0,India,Empirical research involving an LLM,,,,,,External data set,MotiVAte,,,,,,,Other: own,,"Informal counseling (e.g., emotional support conversation)",GPT-2 family,No,n,n,n,n,,,y,b,l,"ML models (HRED, SEQ2SEQ)",n,,,,n,,,,n,,,,y,b,l,"ML models (HRED, SEQ2SEQ)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_70,1st_search_92,Towards Motivational and Empathetic Response Generation in Online Mental Health Support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,7.0,2022.0,India,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"MotiVAte

This dataset comprises of 4k dyadic conversations between the depressed support seekers and the VA imparting appropriate suggestion, hope and motivation resulting in a total of 14,809 utterances. The conversations of the MotiVAte dataset are collected from peer-to-peer support forum and modified to represent dyadic conversations for an end-to-end online mental health support system.",Emotional support dialogue -- chat logs,English,No,No,Unselected,Lay people,Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework,,Peer support conversation,GPT-2 family,No users involved,n,n,n,n,,,y,b,l,Benchmark is other NLP model,y,b,l,Benchmark is other NLP model,n,,,,n,,,,y,b,l,Benchmark is other NLP model,n,,,,n,,,,n,,,,sentiment polarity,s,l,Benchmark is other NLP model,change of empathy scores for the erf module,no benchmark,no benchmark,no benchmark,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_70,1st_search_93,Towards Motivational and Empathetic Response Generation in Online Mental Health Support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,7.0,2022.0,India,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"MotiVAte

This dataset comprises of 4k dyadic conversations between the depressed support seekers and the VA imparting appropriate suggestion, hope and motivation resulting in a total of 14,809 utterances. The conversations of the MotiVAte dataset are collected from peer-to-peer support forum and modified to represent dyadic conversations for an end-to-end online mental health support system.",Emotional support dialogue -- chat logs,English,No,No,Unselected,Lay people,Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework,,"Peer support conversation; Informal counseling (e.g., emotional support conversation)",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,"ML models (HRED, SEQ2SEQ)",y,b,l,"ML models (HRED, SEQ2SEQ)",n,,,,n,,,,y,b,l,"ML models (HRED, SEQ2SEQ)",n,,,,n,,,,n,,,,sentiment polarity,s,l,Benchmark is other NLP model,change of empathy scores for the erf module,no benchmark,no benchmark,no benchmark,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_63,1st_search_94,A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",17.0,10.0,2023.0,Other: Canada,Empirical research involving an LLM,Client-facing application,,General population,,,Self-collected data,"Four chatbot versions tested with separate groups of ambivalent smokers; pre/post and 1-week follow-up (readiness, confidence, importance), quit attempts, and perceived empathy; core chat = 5 scripted questions + (for most versions) generated MI reflections.  • Derived: Not indicated.
","Other: MI chatbot intervention chat logs + survey responses
",English,Yes,No,Unselected,Lay people,Prompting + other modules,,CBT: Motivational interviewing,GPT-2 family,Yes,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,eaviness of smoking index (hsi) measures.,w,l,,n,,,,n,,,,n,,n,,n,nothing found,n,,y,,n,,n,,n,,n,,n,,n,,n,,
1st_search_63,1st_search_95,A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17.0,10.0,2023.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),349,,Self-collected data,"In version 1 of the generator, the fine-tuning question-and-response data set came from 2 sources: the first was our prior work [40,41], and the second data source was from earlier deployments of MIBot, before the creation of MIBot v4.7. The reflections used came from a variety of sources: from previous versions of this chatbot that were deemed to be acceptable MI reflections by MI-literate researchers or actual reflections produced by MI-literate researchers or MI-expert clinicians.

To address the rate of poor reflections, we developed version 2 of the generator with 2 significant enhancements. First, a larger set of 301 fine-tuning triplets were collected over approximately 10 months of deploying the chatbot, making use of the various responses from smokers who had been recruited in a similar manner, as described in the Participant Recruitment and Screening subsection. This second data set did not include any of the data from the earlier chatbot version [40,41]. Only MI-consistent reflections were used, which were sourced from MI clinicians, MI-literate researchers, or version 1 of the generator. The labeling and selection of the MI-consistent reflections were improved by using multiple human raters and a carefully controlled decision tree to determine the validity of the reflections. The new rating scheme itself was stricter than the one used in version 1, which caused the hit rate to go down—not because the generation was worse but because of the stricter rating. The hit rate of the new generator was measured to be 55.1% (166/301) on a set of reflections.",Other: Motivational interviewing reflections,English,Yes,No,Unknown,Trained professionals,Other: fine-tuning + custom pipeline (see Reflection Generation Training),,CBT: Motivational interviewing,BERT family; GPT-2 family,Yes,y,n,y,y,consultation and relational empathy survey (CARE),"consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"readiness ruler (patient symptom report, smoking related, validated measure)","no benchmark. in absolute terms favorable, with reduction in smoking, increase in confidence, importance, readiness.",no benchmark,no benchmark,n,,,,n,,,,n,,y,used reflection quality classifier,y,,n,,y,reported in multimedia appendix 3,n,,y,"Of the 654 participants who accepted and consented to the study,
105 (16.1%) did not finish the entire study. We speculate that
this dropout was caused by several factor",n,,y,use of the readiness ruler,n,,n,,n,,
1st_search_63,1st_search_96,A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17.0,10.0,2023.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),349,,Self-collected data,"In version 1 of the generator, the fine-tuning question-and-response data set came from 2 sources: the first was our prior work [40,41], and the second data source was from earlier deployments of MIBot, before the creation of MIBot v4.7. The reflections used came from a variety of sources: from previous versions of this chatbot that were deemed to be acceptable MI reflections by MI-literate researchers or actual reflections produced by MI-literate researchers or MI-expert clinicians.

To address the rate of poor reflections, we developed version 2 of the generator with 2 significant enhancements. First, a larger set of 301 fine-tuning triplets were collected over approximately 10 months of deploying the chatbot, making use of the various responses from smokers who had been recruited in a similar manner, as described in the Participant Recruitment and Screening subsection. This second data set did not include any of the data from the earlier chatbot version [40,41]. Only MI-consistent reflections were used, which were sourced from MI clinicians, MI-literate researchers, or version 1 of the generator. The labeling and selection of the MI-consistent reflections were improved by using multiple human raters and a carefully controlled decision tree to determine the validity of the reflections. The new rating scheme itself was stricter than the one used in version 1, which caused the hit rate to go down—not because the generation was worse but because of the stricter rating. The hit rate of the new generator was measured to be 55.1% (166/301) on a set of reflections.",Other: Motivational interviewing reflections,English,Yes,No,Psychopathology,Trained professionals,Other: fine-tuning + custom pipeline (see Reflection Generation Training),,CBT: Motivational interviewing,BERT family; GPT-2 family,Yes,y,n,y,y,consultation and relational empathy survey (CARE),"consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"readiness ruler (patient symptom report, smoking related, validated measure)","no benchmark. in absolute terms favorable, with reduction in smoking, increase in confidence, importance, readiness.",no benchmark,no benchmark,n,,,,n,,,,n,,y,used reflection quality classifier,y,,n,,y,reported in multimedia appendix 3,n,,y,"Of the 654 participants who accepted and consented to the study,
105 (16.1%) did not finish the entire study. We speculate that
this dropout was caused by several factor",n,,y,use of the readiness ruler,n,,n,,n,,
1st_search_62,1st_search_97,Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11.0,5.0,2024.0,USA,Conceptual or theoretical work (e.g. on ethics or safety),,,,,,Self-collected data,"Qualitative study using an online survey + interviews about LGBTQ+ users’ experiences with LLM-based chatbots for mental wellness; questions cover usage, apps used, frequency, and detailed experiences.
","Other: Other (qualitative survey & interview data about chatbot use)
",English,No,,Unselected,"Other: AI chatbots referenced; study data from human participants
",,,,,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,not applicable,,not applicable,,n,,n,,n,,n,,n,,n,,n,,n,,qualitative study - survey with 31 participants from recruited over Reddit
1st_search_62,1st_search_98,Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11.0,5.0,2024.0,USA,Population survey,Client-facing application,Chatbot,"Other: chatbot usersfrom three sub-Reddits: r/Snapchat, r/Anima, and r/Parradot",31,,No dataset used for development or evaluation,,,,,,,,,,"Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified; Other: various LLM-based chatbots (Replika - Snapchat My AI - Chai - Character.ai - Anima - Paradot - Kuki)",Yes,n,y,n,y,,"Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_62,1st_search_99,Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11.0,5.0,2024.0,USA,Population survey,Client-facing application,Chatbot,General population,31,,No dataset used for development or evaluation,,,,,,,,,,"Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified; Other: various LLM-based chatbots: Replika, Snapchat My AI, Chai, Character.ai, Anima, Paradot, ChatGPT, Kuki",Yes,n,y,n,y,,"Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,qualitative study - survey with 31 participants from recruited over Reddit
1st_search_61,1st_search_100,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",10.0,7.0,2022.0,USA,Empirical research involving an LLM,Other: Analysis of text-message conversations between clients and clinicians,,,,,Self-collected data,"Dataset collected (Ben-Zeev et al., 2020) and rated (Tauscher et al., 2022) in previous studies conducted by same institution",,,,,,,Only fine-tuning,,"Unspecified, might include formal therapy methods",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,base BERT model with AUPRC <0.52,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_61,1st_search_101,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",10.0,7.0,2022.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,"From our previous work (Tauscher et al., 2022), we utilized data from a randomized controlled trial of a community-based text-message intervention for individuals with serious mental illness (Ben- Zeev et al., 2020).

""The maximum intervention dose was three “exchanges,” wherein an exchange is defined as a cluster of thematically connected back-and-forth messages between mobile interventionist and participant (e.g. three outgoing messages and participant responses). Texting strategies included: reminders (e.g., appointments, prescription refills), information provision (e.g., psychoeducation, links to regional events and resources), cognitive challenges (e.g., restructuring dysfunctional beliefs about voices, questioning the validity of self-sabotaging automatic beliefs), self-monitoring/self-reflection (e.g., guidance on self-evaluation of affect, journaling of symptomatic experiences), relaxation techniques (e.g., diaphragmatic breathing, guided imagery), social skills training (e.g., initiating conversations, maintaining eye contact), supportive messages (e.g., affirmations, inspirational quotes), and in-vivo instruction (e.g., pre-scheduled real-time support as the patient attempted a new activity)."" (from Ben-Zeev et al.)

in addition they augmented their data using different strategies (3.2 Augmentation of text data)",,,,,,,Only fine-tuning,,CBT: Cognitive restructuring,BERT family; GPT-2 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,Benchmark: BERT (no augmentation). metric: AUPRC,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_61,1st_search_102,Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",10.0,7.0,2022.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,"From our previous work (Tauscher et al., 2022), we utilized data from a randomized controlled trial of a community-based text-message intervention for individuals with serious mental illness (Ben- Zeev et al., 2020).

""The maximum intervention dose was three “exchanges,” wherein an exchange is defined as a cluster of thematically connected back-and-forth messages between mobile interventionist and participant (e.g. three outgoing messages and participant responses). Texting strategies included: reminders (e.g., appointments, prescription refills), information provision (e.g., psychoeducation, links to regional events and resources), cognitive challenges (e.g., restructuring dysfunctional beliefs about voices, questioning the validity of self-sabotaging automatic beliefs), self-monitoring/self-reflection (e.g., guidance on self-evaluation of affect, journaling of symptomatic experiences), relaxation techniques (e.g., diaphragmatic breathing, guided imagery), social skills training (e.g., initiating conversations, maintaining eye contact), supportive messages (e.g., affirmations, inspirational quotes), and in-vivo instruction (e.g., pre-scheduled real-time support as the patient attempted a new activity)."" (from Ben-Zeev et al.)

in addition they augmented their data using different strategies (3.2 Augmentation of text data)",,,,,,,Only fine-tuning,,CBT: Cognitive restructuring,BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,base BERT model with AUPRC <0.52. Task: classifying cognitive distortions,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_60,1st_search_103,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11.0,5.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,,General population,,,Self-collected data,"interaction logs from a self-guided reframing tool (user inputs thoughts; LLM generates reframes);
","Other: self-guided mental-health tool interaction logs
",English,Yes,,Unselected,"Other: self-guided mental-health
",Prompting + other modules,,CBT: Cognitive restructuring,GPT-3 family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,GPT3,n,too little,n,,n,,n,,n,,n,,n,,n,,n,,no alpha adjusting (table 3)
1st_search_60,1st_search_104,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11.0,5.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Other: reframed thought generator,"Other: Visitors to Mental Health America (MHA, large mental health website that provides mental health resources and tools to millions of users).
""Many MHA visitors are interested in mental health resources including self-guided systems.""",15531,,External data set,"We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].

4.1 Curated Situations & Negative Thoughts
 We start by curating data sources for situations and negative thoughts.
 Thought Records Dataset (Burger et al., 2021).
 This dataset contains hypothetical and real-world situations, thoughts and emotional processes reported by crowdworkers on Amazon Mechanical Turk. We manually curate 180 pairs of diverse situations with negative thoughts from this dataset.
 Mental Health America (MHA). Situations and thoughts from crowdworkers may not reflect the broad range of mental health challenges that people face in real-life. To incorporate more real-world situations and thoughts, we ran a survey on the MHA website (screening.mhanational.org). MHA visitors (who typically use the website for screening of mental illnesses) were asked to describe any negative thoughts and the associated situations they were struggling with. We manually curate 120 pairs of self-reported situations and thoughts to ensure broad coverage of relevant topics based on high diversity and manual filtering.

https://github.com/behavioral-data/Cognitive-Reframing",Other: thought records,English,No,Yes,Unselected,Other: not applicable,"Other: multiple different:
selection of thinking traps: fine-tuning
writing of reframes: retrieval-enhanced in-context learning",,CBT: Cognitive restructuring,GPT-3 family,Yes,n,y,y,y,,"We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,reduction in emotion intensity,no benchmark,no benchmark,no benchmark. only the pre-post-difference in this metric was measured.,n,,,,n,,,,n,,y,see 4.2 -> Safety considerations,n,,n,,y,,y,see table 4!,n,,n,,n,,y,"We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions",n,,n,,
1st_search_60,1st_search_105,Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11.0,5.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Other: reframed thought generator,General population,15531,,External data set,"We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].

4.1 Curated Situations & Negative Thoughts
 We start by curating data sources for situations and negative thoughts.
 Thought Records Dataset (Burger et al., 2021).
 This dataset contains hypothetical and real-world situations, thoughts and emotional processes reported by crowdworkers on Amazon Mechanical Turk. We manually curate 180 pairs of diverse situations with negative thoughts from this dataset.
 Mental Health America (MHA). Situations and thoughts from crowdworkers may not reflect the broad range of mental health challenges that people face in real-life. To incorporate more real-world situations and thoughts, we ran a survey on the MHA website (screening.mhanational.org). MHA visitors (who typically use the website for screening of mental illnesses) were asked to describe any negative thoughts and the associated situations they were struggling with. We manually curate 120 pairs of self-reported situations and thoughts to ensure broad coverage of relevant topics based on high diversity and manual filtering.

https://github.com/behavioral-data/Cognitive-Reframing",Other: thought records,English,No,Yes,Unselected,Other: not applicable,"Other: multiple different:
selection of thinking traps: fine-tuning
writing of reframes: retrieval-enhanced in-context learning",,CBT: Cognitive restructuring,GPT-3 family,Yes,n,y,y,y,,"We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,reduction in emotion intensity,no benchmark,no benchmark,no benchmark. only the pre-post-difference in this metric was measured.,ux measures,no benchmark,no benchmark,"Reframe Relatability, Reframe Helpfulness, Reframe Memorability, Skill Learnability",n,,,,n,,y,see 4.2 -> Safety considerations,n,GPT3,n,too little,y,,y,see table 4!,n,,n,,n,,n,"We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions
BUT: only for different design decisions; no overall other intervention or no intervention as control group!",n,,n,,no alpha adjusting (table 3)
1st_search_59,1st_search_106,Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),5.0,7.0,2023.0,UK,Empirical research involving an LLM,,,,,,External data set,"CounselChat1,
the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 ",,,,,,,Prompting + other modules,,"Unspecified, might include formal therapy methods",Llama 3.1 family; Other: own?,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,unclear,,unclear,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_59,1st_search_107,Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),5.0,7.0,2023.0,UK,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"Due to this, data from the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 were selected. Questions
and answers are extracted, and questions are manually generated
dependent on the information available, e.g. for the NHS definition
of depression, questions such as “what is depression?"" are imputed.",Internet data -- mental health Q&A,English,No,No,Other: not applicable,Unknown,Only fine-tuning,,"Unspecified, might include formal therapy methods",Other: own transformer architecture,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token",y,no benckmark,no benchmark,no benchmark. loss value,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,own transformer architecture -> on-premise,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_59,1st_search_108,Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),5.0,7.0,2023.0,UK,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"Due to this, data from the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 were selected. Questions
and answers are extracted, and questions are manually generated
dependent on the information available, e.g. for the NHS definition
of depression, questions such as “what is depression?"" are imputed.",Internet data -- mental health Q&A,English,No,No,Other: not applicable,Trained professionals,Other: Other: self-developed and trained transformer architecture,,"Unspecified, might include formal therapy methods",Other: own transformer architecture,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token",y,no benckmark,no benchmark,no benchmark. loss value,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,own transformer architecture -> on-premise,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_56,1st_search_109,Leveraging ChatGPT to optimize depression intervention through explainable deep learning,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6.0,6.0,2024.0,China,Empirical research involving an LLM,Other: Comparison between AI and human councellor recommendations,,No clients/patients involved,,,Self-collected data,"1. publicly available dataset comprising HGC, 2. AI-generated content generated by ChatGPT",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Lay people,Only fine-tuning,,"Informal counseling (e.g., emotional support conversation)","BERT family; Other: Roberta, TextCNN, Text-LSTM, GPT-unknown version",No users involved,n,n,n,n,,,y,s,h,Lexical Overlap within comparative linguistic analysis,y,s,h,,y,s,h,,n,,,,n,,,,y,,l,,n,,,,n,,,,part-of-speech (pos) analysis ,,l,,dependency-syntactic-parsing (dep) analysis,,l,,n,,,,n,,n,,n,Chat GPT,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_56,1st_search_110,Leveraging ChatGPT to optimize depression intervention through explainable deep learning,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6.0,6.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,data sourced from Xinli001.com,Internet data -- mental health Q&A,Chinese,No,No,Unselected,Trained professionals,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"various linguistic analyses with no clear meaning (part-of-speech, sentiment, ...)",unclear,h,benchmark: human psychologist responses from transcripts,shap values of words in classifier that classifies human vs. chatgpt responses,unclear,h,benchmark: human psychologist responses from transcripts,n,,,,n,"they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_56,1st_search_111,Leveraging ChatGPT to optimize depression intervention through explainable deep learning,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6.0,6.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Other: one-turn Q&A Chatbot,No clients/patients involved,,,External data set,data sourced from Xinli001.com. Counsellor responses were included in the dataset. ChatGPT responses were generated by the authors.,Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Trained professionals,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,part-of-speech (pos) analysis; dependency-syntactic-parsing (dep) analysis; semantic-dependency-parsing (sdp) analysis; sentiment analysis,unclear,h,benchmark: human psychologist responses from transcripts,shap values of words in classifier that classifies human vs. chatgpt responses,unclear,h,benchmark: human psychologist responses from transcripts,n,,,,n,"they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values",n,,n,Chat GPT,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_50,1st_search_112,Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",24.0,6.0,2024.0,Germany,Empirical research involving an LLM,Other: Analysis of conversation transcripts & client facing,,Other: healthy individuals without a diagnosed mental health disorder,,,External data set,"The Emobank dataset (Buechel & Hahn, 2022)
The GoEmotions dataset (Demszky et al., 2020) 
The International Survey on Emotion Antecedents and Reactions (ISEAR) (ISEAR
dataset, https://paperswithcode.com/dataset/isear, accessed 22.08.2023
Validation: The CrowdFlower dataset (Sentiment Analysis in Text - Dataset by crowdflower,
https://data.world/crowdflower/sentiment-analysis-in-text, accessed 07.08.2022)

(see excel)",,,,,,,"Other: ""we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset""
""The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output""",,Other CBT techniques,BERT family,Yes,y,n,y,y,"System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi)","System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.",n,,,,n,,,,y,b,l,see below,y,b,l,"rule-based approach: "" It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score""",n,,,,n,,,,n,,,,n,,,,user rating of empathig understanding (eu),s,l,"rule-based approach: "" It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score""",n,,,,n,,,,y,"Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. ",n,,n,,n,,n,,n,,n,,n,,n,,y,"Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group)",n,,y,"
""Automation of time-consuming tasks in iCBT could, through a positive lens, lead to
improved cost-effectiveness, which is an important point in often over-encumbered and
underfinanced psychiatry treatment and care contexts.""
""
Fully automated iCBT, including the prediction of emotional states coupled with a CA
in charge of the iCBT with no human therapist involvement, would be both unwanted
and unethical. For legal reasons, having a clinical professional involved and ultimately
responsible for treatment is mandatory today and unlikely to change in the foreseeable
future. Only hybrid solutions of man-machine co-involvement are therefore further
discussed here. One such hybrid scenario would be the sole automation of emotion
recognition. This scenario starts with initial machine recognition of emotional states
derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional
states can then be fed to a human clinician as decision support. In theory, this could
render an improved understanding of a patient’s emotional state and also change of
state across time during iCBT. This could ultimately improve treatment tailoring and
effectiveness through the patient perceiving the therapist as more empathic, strengthening
the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy
work modules to better suit the patient’s emotional state""
"" An interesting
but largely untested scenario would be extended automation of emotional recognition
coupled with therapist-supported CA treatment. This would involve not only the potential
benefit of emotion recognition discussed above but also cost-effective semi-automated
treatment. One such implementation would be that the emotionally informed CA drafts
empathically written therapy responses to the patient’s messages and the human therapist
then scrutinizes the responses and signs off on them with or without making prior changes.
A major portion of iCBT costs come from therapists spending time drafting responses to
patients in the treatment portal, unlocking a major potential for cost-saving strategies. An
additional downside risk with this scenario would be that the human therapist—due to
stress or other human factors—signs off on written responses of lower therapeutic quality.
Proper training and structured follow-up of therapists are likely required in this scenario,
which in turn may offset some of the cost-effectiveness of the approach. That stated since
a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored
CA seems in accordance with that overarching aim of iCBT.""",
1st_search_50,1st_search_113,Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",24.0,6.0,2024.0,,Empirical research involving an LLM,Client-facing application,Chatbot,General population,20,,External data set,"EmoBank, GoEmotions, ISEAR, CrowdFlower",,,,,,,"Other: BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture",,Other CBT techniques,BERT family,Yes,y,n,y,y,"empathic understanding (EU), system usability (SUS), acceptance (CSQi)","Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based",n,,,,n,,,,y,b,l,benchmark is simple rule-based emotion recognition system. Task: classification of clustered dimensional values (five distinct categories),y,b,l,"benchmark is simple rule-based emotion recognition system.
second, better benchmark: state-of-the-art valence and arousal recognition systems from other papers (Park et al., etc.). still, this is no human. Task: rating of valence, arousal, dominance in conversations",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. ",n,,n,,n,,n,,n,,n,,n,,n,,y,"Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group).",n,,y,"Fully automated iCBT, including the prediction of emotional states coupled with a CA in charge of the iCBT with no human therapist involvement, would be both unwanted and unethical. For legal reasons, having a clinical professional involved and ultimately responsible for treatment is mandatory today and unlikely to change in the foreseeable future. Only hybrid solutions of man-machine co-involvement are therefore further discussed here. One such hybrid scenario would be the sole automation of emotion recognition. This scenario starts with initial machine recognition of emotional states derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional states can then be fed to a human clinician as decision support. In theory, this could render an improved understanding of a patient’s emotional state and also change of state across time during iCBT. This could ultimately improve treatment tailoring and effectiveness through the patient perceiving the therapist as more empathic, strengthening the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy work modules to better suit the patient’s emotional state. A potential risk with this approach would be the drift of the therapist’s own emotional assessment influenced by the machine’s estimated emotional state of the patient which may be wrong or biased. An interesting but largely untested scenario would be extended automation of emotional recognition coupled with therapist-supported CA treatment. This would involve not only the potential benefit of emotion recognition discussed above but also cost-effective semi-automated treatment. One such implementation would be that the emotionally informed CA drafts empathically written therapy responses to the patient’s messages and the human therapist then scrutinizes the responses and signs off on them with or without making prior changes. A major portion of iCBT costs come from therapists spending time drafting responses to patients in the treatment portal, unlocking a major potential for cost-saving strategies. An additional downside risk with this scenario would be that the human therapist—due to stress or other human factors—signs off on written responses of lower therapeutic quality. Proper training and structured follow-up of therapists are likely required in this scenario, which in turn may offset some of the cost-effectiveness of the approach. That stated since a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored CA seems in accordance with that overarching aim of iCBT.",
1st_search_50,1st_search_114,Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",24.0,6.0,2024.0,,Empirical research involving an LLM,Other: Analysis of conversation transcripts; Client-facing application,Chatbot,General population,20,,External data set,"The Emobank dataset (Buechel & Hahn, 2022)
The GoEmotions dataset (Demszky et al., 2020) 
The International Survey on Emotion Antecedents and Reactions (ISEAR) (ISEAR
dataset, https://paperswithcode.com/dataset/isear, accessed 22.08.2023
Validation: The CrowdFlower dataset (Sentiment Analysis in Text - Dataset by crowdflower,
https://data.world/crowdflower/sentiment-analysis-in-text, accessed 07.08.2022)",,,,,,,"Other: ""we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset""
""The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output""

BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture",,Other CBT techniques,BERT family,Yes,y,n,y,y,"empathic understanding (EU), system usability (SUS), acceptance (CSQi)","System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.

Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based",n,,,,n,,,,y,b,l,benchmark is simple rule-based emotion recognition system. Task: classification of clustered dimensional values (five distinct categories),y,b,l,"benchmark is simple rule-based emotion recognition system.
second, better benchmark: state-of-the-art valence and arousal recognition systems from other papers (Park et al., etc.). still, this is no human. Task: rating of valence, arousal, dominance in conversations

rule-based approach: "" It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score""",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. ",n,,n,,n,,n,,n,,n,,n,,n,,y,"Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group).",n,,y,"
""Automation of time-consuming tasks in iCBT could, through a positive lens, lead to
improved cost-effectiveness, which is an important point in often over-encumbered and
underfinanced psychiatry treatment and care contexts.""
""
Fully automated iCBT, including the prediction of emotional states coupled with a CA
in charge of the iCBT with no human therapist involvement, would be both unwanted
and unethical. For legal reasons, having a clinical professional involved and ultimately
responsible for treatment is mandatory today and unlikely to change in the foreseeable
future. Only hybrid solutions of man-machine co-involvement are therefore further
discussed here. One such hybrid scenario would be the sole automation of emotion
recognition. This scenario starts with initial machine recognition of emotional states
derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional
states can then be fed to a human clinician as decision support. In theory, this could
render an improved understanding of a patient’s emotional state and also change of
state across time during iCBT. This could ultimately improve treatment tailoring and
effectiveness through the patient perceiving the therapist as more empathic, strengthening
the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy
work modules to better suit the patient’s emotional state""
"" An interesting
but largely untested scenario would be extended automation of emotional recognition
coupled with therapist-supported CA treatment. This would involve not only the potential
benefit of emotion recognition discussed above but also cost-effective semi-automated
treatment. One such implementation would be that the emotionally informed CA drafts
empathically written therapy responses to the patient’s messages and the human therapist
then scrutinizes the responses and signs off on them with or without making prior changes.
A major portion of iCBT costs come from therapists spending time drafting responses to
patients in the treatment portal, unlocking a major potential for cost-saving strategies. An
additional downside risk with this scenario would be that the human therapist—due to
stress or other human factors—signs off on written responses of lower therapeutic quality.
Proper training and structured follow-up of therapists are likely required in this scenario,
which in turn may offset some of the cost-effectiveness of the approach. That stated since
a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored
CA seems in accordance with that overarching aim of iCBT.""",
1st_search_41,1st_search_115,Deep Learning Mental Health Dialogue System,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",23.0,1.0,2023.0,Other: Poland,Empirical research involving an LLM,Client-facing application,,General population,,,Self-collected data,"“Pushshift Reddit Dataset which includes 651 million submissions and 5.6 billion comments…” • “We have fine-tuned this model on transcripts of counseling and psychotherapy sessions… Our… dataset consisted of 14,300 patient’s prompt and counselor’s answer pairs.”
",,,,,,,,,Other: Person Centered Therapy (PCT) (Carl Rogers),"Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer
[26] model which generates a beam of candidate responses.
In the second stage a number of smaller, more specialized
Transformer-based models)",Yes,n,n,n,n,,"Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_41,1st_search_116,Deep Learning Mental Health Dialogue System,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,2.0,2023.0,Other: Poland,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"Alexander Street Press, Counseling and Psychotherapy Transcripts: Volume I",Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,"Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers",,Other: Person-centered therapy,BERT family; Other: own architecture,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.",y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_41,1st_search_117,Deep Learning Mental Health Dialogue System,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,2.0,2023.0,Other: Poland,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"Alexander Street Press, Counseling and Psychotherapy Transcripts: Volume I",Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,"Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers",,Other: Person Centered Therapy (PCT) (Carl Rogers),"BERT family; Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer [26] model which generates a beam of candidate responses. In the second stage a number of smaller, more specialized Transformer-based models)",Yes,n,n,n,n,,"Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.

Results: Not clear/not reported",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.",y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"no evaluation was performed. Only a sample conversation was shown ""look this is good""."
1st_search_34,1st_search_118,Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",27.0,5.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,,General population,,,Self-collected data,"brief questionnaire; users interacted ~10 minutes; prompts designed for ChatGPT; qualitative feedback collected
",Other: qualitative interviews,Other: german,No,No,Unknown,Other: not applicable,Only prompting,,CBT: Motivational interviewing,GPT-3.5 family,Yes,n,y,n,y,,"Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their
goals and inquire about their interests. Notably, no users reported feeling pressured or
convinced by the prototype to change aspects of their lives against their wishes. Several
users found the prototype to be particularly valuable for creating quick and helpful
weekly plans or recipes. However, it was observed that one user found it premature
to establish concrete plans at that stage. Overall, while some users initially felt that
the prototype’s responses were not aligned with their preferences, they noted that it
was possible to clarify their intentions, and the prototype quickly adapted accordingly.
Despite this, some users felt that the assessment of their goals and life circumstances
at the beginning of the training was too superficial, leading to suggestions for a more
thorough initial assessment process.
Users highlighted several aspects of the prototype’s communication style. Most users
found the conversation to be clear and direct. In contrast, one user reported that the pro-
totype struggled to understand his goals and sometimes provided contradictory advice,
a concern not raised by others. On the positive side, one user appreciated the prototype’s
approach of asking questions rather than giving fixed instructions, which encouraged 
engagement and avoided a patronizing tone. Some users noted the positive and moti-
vating tone of the coach, particularly when asking if they were willing to continue with
suggested steps and advice. The use of a positive tone was seen as a motivating factor by
one user and was also appreciated by another who found it encouraging. Users generally
found concrete advice more helpful than generic recommendations. Additionally, one
user valued the coach’s acknowledgment of setbacks and the importance of enjoying the
process. However, some users expressed a desire for shorter, more direct conversations
that focused less on delivering general knowledge, and one user specifically requested
a more emotional and less matter-of-fact tone of voice.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,Chat GPT,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_34,1st_search_119,Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,6.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Chatbot,"Other: unclear (it just says ""users"")",11,,No dataset used for development or evaluation,,,,,,,,Only prompting,,CBT: Motivational interviewing,GPT-3.5 family,Yes,n,y,n,y,,"Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_34,1st_search_120,Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",27.0,5.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Chatbot,General population,11,,No dataset used for development or evaluation,,,,,,,,Only prompting,,CBT: Motivational interviewing,GPT-3.5 family,Yes,n,y,n,y,,"Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,Chat GPT,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_32,1st_search_121,Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",19.0,4.0,2021.0,USA,Empirical research involving an LLM,Other: Conversation Rewriting,,No clients/patients involved,,Utterance suggestions,External data set,"TalkLife
Real peer-to-peer conversations on a mental-health support platform (TalkLife) used to train and evaluate “Partner,” a reinforcement-learning model for empathic rewriting. Includes posts by seekers and peer supporters, with empathy scores (0–6) labeled using the Sharma et al. (2020) empathy classifier and 180 expert “empathic rewritings.
",Internet data -- mental health forum,English,Yes,No,Other: for me unclear does the comment suffice?,Lay people,Other: PARTNER,,"Informal counseling (e.g., emotional support conversation)",GPT-2 family,No,n,n,n,n,,,n,,,,y (specifity),b,l,,n,,,,y (perplexity),w,l,,n,,,,n,,,,n,,,,n,,,,empathy classification (sharma et al),b,l,Other LLMs,edit rate,w,l,Other LLMs,n,,,,n,,n,,n,,n,too little,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_32,1st_search_122,Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19.0,4.0,2021.0,USA,Empirical research involving an LLM,Therapist-facing application,,,,Utterance suggestions,Self-collected data,"TalkLife (talklife.co) is the largest online peer-to-peer support platform for mental health support. It enables conversations between people seeking support (support seekers) and people providing support (peer supporters) in a thread-like setting. We call the post authored by a support seeker as seeker post, and the response by a peer supporter as response post. Table 1 describes the statistics of conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question ""Is the seeker talking about a mental health related issue or situation in his/her post?"". Using this annotated dataset, we train a standard text classifier based on BERT [15] (achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-related conversations. This dataset contains 3.33M interactions from 1.48M seeker posts.

https://github.com/behavioral-data/Empathy-Mental-Health/tree/master",Internet data -- mental health forum,English,No,Yes,Unselected,Lay people,Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.,,Peer support conversation,BERT family; GPT-2 family,No users involved,n,n,n,n,,,y,b,l,BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations,y,b,l,"""specificity""",n,,,,n,,,,y,w,h,Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.,y,b,l,"BERT-based empathy scoring model.
benchmarks are other LLMs, not fine tuned to empathy tasks",n,,,,n,,,,perplexity,w,l,PARTNER against other LLMs.,lexical_diversity,w,l,PARTNER against other LLMs.,automatic empathy rating,b,l,change in empathy. PARTNER against other LLMs.,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_32,1st_search_123,Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19.0,4.0,2021.0,USA,Empirical research involving an LLM,Therapist-facing application,,No clients/patients involved,,Utterance suggestions,Self-collected data but derived from external data set,"TalkLife (talklife.co) is the largest online peer-to-peer support platform for mental health support. It enables conversations between people seeking support (support seekers) and people providing support (peer supporters) in a thread-like setting. We call the post authored by a support seeker as seeker post, and the response by a peer supporter as response post. Table 1 describes the statistics of conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question ""Is the seeker talking about a mental health related issue or situation in his/her post?"". Using this annotated dataset, we train a standard text classifier based on BERT [15] (achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-related conversations. This dataset contains 3.33M interactions from 1.48M seeker posts.

Non-public: For accessing the TalkLife portion of our dataset for non-commercial use, please contact the TalkLife team here.",Internet data -- mental health forum,English,No,No,Unselected,Lay people,Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.,,Peer support conversation,BERT family; GPT-2 family,No users involved,n,n,n,n,,,y,b,l,BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations,y,b,l,specificity is an embedding similarity metric here,n,,,,n,,,,y,w,h,Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.,n,,,,n,,,,n,,,,empathy classification (sharma et al),b,l,Other LLMs,"perplexity, diversity, sentence coherence, edit rate",w,l,Other LLMs,expert rating 2,b,l,"Expert judgments of empathy, fluency, specificity of PARTNER against other LLMs.",n,,n,,y,,n,too little,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_30,1st_search_124,ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane,Reviewer Two,Other: Letter to the Editor ,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",21.0,7.0,2023.0,India,"Opinion, commentary, perspective, correspondence",,,,,,,,,,,,,,,,,,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_30,1st_search_125,ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane,Richard Gaus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",21.0,7.0,2023.0,India,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_30,1st_search_126,ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane,Consensus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",21.0,7.0,2023.0,India,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,No dataset used for development or evaluation,no dataset. ChatGPT is used as is.,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,No real evaluation. Only a sample conversatino is shown
1st_search_26,1st_search_127,Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,7.0,2024.0,UK,Empirical research involving an LLM,,,No clients/patients involved,,,Self-collected data,"oth ChatGPT-4 and Bard responded to 20 tasks at each
stage of the study --> no new dataset created ",Psychotherapy -- chat logs,English,No,No,Unselected,Lay people,Only prompting,,CBT: Cognitive restructuring,GPT-4 / GPT-4o family; Other: Bard ,No users involved,n,n,n,n,,,n,,,,n,,,,yes ,no benchmark,no benchmark,"GPT4 scored 44/60 and Bard 46/60, but the study did not aim to compare those two,  ",n,,,,yes ,no benchmark,no benchmark,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions",very short study 
1st_search_26,1st_search_128,Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,7.0,2024.0,UK,Empirical research involving an LLM,Client-facing application,Other: reframed thought generator,No clients/patients involved,,,Self-collected data,Data were 20 thoughts written by two CBT therapists (each wrote 10 thoughts).,Other: automatic thought records,English,Other: synthetic: human generated,No,Other: not applicable,Other: not applicable,Only prompting,,CBT: Cognitive restructuring,GPT-4 / GPT-4o family; Gemini / Bard family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,,,no benchmark.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_26,1st_search_129,Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,7.0,2024.0,UK,Empirical research involving an LLM,Client-facing application,Other: reframed thought generator,No clients/patients involved,,,Self-collected data,"oth ChatGPT-4 and Bard responded to 20 tasks at each
stage of the study --> no new dataset created ",Other: automatic thought records,English,Other: synthetic: human generated,No,Other: not applicable,Other: not applicable,Only prompting,,CBT: Cognitive restructuring,BERT family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions",very short study 
1st_search_18,1st_search_130,Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",22.0,1.0,2024.0,USA,Other: use of machine learning to evaluate clinical content (ML was used to improve quality),Analysis of conversation transcripts,,,,,External data set,"Data were obtained from an online and mobile therapy app (Talkspace) for services provided
between 2014 and 2019.",Psychotherapy -- chat logs,English,No,No,Psychopathology,Trained professionals,,,"Informal counseling (e.g., emotional support conversation)",Other: ML model to structure responses,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,unclear,did not find information,y,"“The deidentified content of messages was retained…” / “clients and therapists agree to the use of their anonymized data for quality assurance and for research.” / “This study was thus deemed exempt from full institutional review board review…
",y,,n,,n,,n,,y,,n,,n,,n,,
1st_search_18,1st_search_131,Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning,Richard Gaus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",22.0,1.0,2024.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,Talkspace data,,,,,,,Only fine-tuning,,Mix of formal therapy methods,Other: not sure - it just says transformer model,No users involved,n,n,n,n,,,n,,,,n,,,,y,w,h,benchmark are ratings by humans on MISC and Topics (see appendix eTable 2),y,w,h,benchmark are ratings by humans on CTRS (see appendix eTable 2),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,see Table client demographic characteristics,n,,n,,n,,y,PHQ-8 was measured in clients,n,,n,,n,,the transformer model predicted items subscales from different clinical questionnaires
1st_search_18,1st_search_132,Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning,Consensus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",22.0,1.0,2024.0,,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,"Data were obtained from an online and mobile therapy app (Talkspace) for services provided
between 2014 and 2019.",,,,,,,Only fine-tuning,,Mix of formal therapy methods,"Other: not sure, it just says transformer model",No users involved,n,n,n,n,,,n,,,,n,,,,y,s,h,benchmark are ratings by humans on MISC and Topics (see appendix eTable 2). Task: classification of client messages into content types,y,s,h,benchmark are ratings by humans on CTRS (see appendix eTable 2),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,see Table client demographic characteristics,n,,n,,n,,y,PHQ-8 was measured in clients,n,,n,,n,,the transformer model predicted items subscales from different clinical questionnaires
1st_search_17,1st_search_133,Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",23.0,5.0,2024.0,Other: Bangladesh,Empirical research involving an LLM,,,,,,External data set,"Kaggle (n.d.)  This dataset encompasses 80 distinct tags, each containing numerous conversational prompts and corresponding 
responses",,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",Other: GPT family not specified,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n (if you consider the author him/herself an expert?),,,,n,,,,n,,,,n,,,,sentiment analysis,b,h,(Benchamark Kaggle data set) sentiment analysis,response quality,b,h,Author rated himself .. (vested interests??),word analysis/ count,b,h,,n,,n,,n,,y,"Privacy and confidentiality … conversations … may contain sensitive information … implement robust security measures to protect user privacy and ensure the confidentiality of conversations.
",n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_17,1st_search_134,Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23.0,5.0,2024.0,Other: Bangladesh,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,Kaggle mental health conversational dataset https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data,Internet data -- mental health Q&A,English,Other: unknown,Yes,Unknown,Unknown,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,sentiment analysis score,b,h,benchmark are responses in the kaggle dataset,non expert rating,b,h,benchmark are responses in the kaggle dataset,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_17,1st_search_135,Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23.0,5.0,2024.0,Other: Bangladesh,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,External data set,"Kaggle mental health conversational dataset https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data

Kaggle (n.d.)  This dataset encompasses 80 distinct tags, each containing numerous conversational prompts and corresponding responses",Internet data -- mental health Q&A,English,Other: unknown,Yes,Unknown,Unknown,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,sentiment analysis score,b,h,benchmark are responses in the kaggle dataset,response quality rated by non experts,b,h,"benchmark are responses in the kaggle dataset, but author rated himself (vested interests???)",word analysis/ count,more words,h,,n,,n,,n,,n,"Privacy and confidentiality … conversations … may contain sensitive information … implement robust security measures to protect user privacy and ensure the confidentiality of conversations.
",n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_14,1st_search_136,ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",7.0,10.0,2024.0,Other: Saudi Arabia,Empirical research involving an LLM,Client-facing application,,Patients recruited in hospital or outpatient treatment facility,,,Self-collected data,,,,,,Psychopathology,,Only prompting,,"Other CBT techniques; Other: ACT, ET, MBCT, DBT",GPT-3.5 family,Yes,n,n,n,n,,"Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,yes ,"able 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
",yes ,Table 3 and Table 4,n,,n,,yes ,patients have a diagnosed anxiety disorder ,n,,n,,yes ,"
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee",
1st_search_14,1st_search_137,ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.,Richard Gaus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",7.0,10.0,2024.0,Other: Saudi Arabia,Empirical research involving an LLM,Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility,399,,No dataset used for development or evaluation,,,,,,,,Only prompting,,Mix of formal therapy methods,GPT-3.5 family,Yes,n,n,y,y,,"The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,user experience measures (see above),,,,n,,,,n,,,,n,,n,,n,,n,,y,"Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
",y,"section ""Perceptions across demographic groups""",n,,n,,n,,n,,n,,y,"The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in need.",
1st_search_14,1st_search_138,ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.,Consensus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",7.0,10.0,2024.0,Other: Saudi Arabia,Empirical research involving an LLM,Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility,399,,No dataset used for development or evaluation,,,,,,,,Only prompting,,Mix of formal therapy methods,GPT-3.5 family,Yes,n,n,y,y,,"The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.

Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,"Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
",y,"Table 3 and Table 4
section ""Perceptions across demographic groups""",n,,n,,n,,n,,n,,y,"
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee",
1st_search_13,1st_search_139,Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27.0,4.0,2024.0,India,Population survey,Client-facing application,Chatbot,General population,111 individuals (82 females and 29 males),,Self-collected data,"Name: none specified. Description: Self-collected open-ended mental-health problem statements and corresponding ChatGPT- and human-generated responses evaluated by participants.
",Internet data -- mental health Q&A,English,Yes,No,Unselected,Lay people,Prompting + other modules,,Other: Perception of AI-generated responses in mental health support,GPT-3.5 family,Yes,y,n,y,y,"Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).","Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,quantification of effect,,,,n,,,,n,,,,n,,y,"Although proper care was taken that no 
triggering problem was used in the survey, the participants were also 
informed about the counseling services available at our university if the 
need arose. As the responses were circulated to experts, no such 
comment which was rejected on this basis (misleading or harmful) was 
included in the study.",n,,n,,y,"f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).",n,,n,,n,,y,"we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022)",n,,n,,n,,
1st_search_13,1st_search_140,Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27.0,4.0,2024.0,India,Population survey,Client-facing application,Chatbot,"Other: young adults, convenience sampling, online forms and in-person visits to classrooms",111,,Self-collected data,"Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown.",Other: emotional experience descriptions,English,No,No,Unselected,Other: not applicable,,,"Informal counseling (e.g., emotional support conversation)","Other: They say ""ChatGPT"". Not clear which version. Most likely 3.5 because they mention ""Building on our previous exploration of GPT-3.5, one of the most advanced Natural Language Processing (NLP) technologies, this follow-up study aims to understand the perception dynamics of AI-generated responses in the realm of mental health support for young people.""",Yes,n,n,y,y,,"Users rated AI responses more authentic, professional, and prarctical when blinded. when unblinded, the rated human responses higher (except professionalism).",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_13,1st_search_141,Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27.0,4.0,2024.0,India,Population survey,Client-facing application,Chatbot,General population,111,,Self-collected data,"Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown.

Name: none specified. Description: Self-collected open-ended mental-health problem statements and corresponding ChatGPT- and human-generated responses evaluated by participants.",Other: emotional experience descriptions,English,Yes,No,Unselected,Trained professionals,,,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,y,n,y,y,"Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).","Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,"f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).",n,,n,,n,,n,"we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022)",n,,n,,n,,
1st_search_9,1st_search_142,Safety of Large Language Models in Addressing Depression.,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",18.0,12.0,2023.0,USA,Empirical research involving an LLM,Client-facing application,,No clients/patients involved,,,Self-collected data,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,first refferal to a human,,,,shutdown of conversation,,,,n,,,,y,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_9,1st_search_143,Safety of Large Language Models in Addressing Depression.,Richard Gaus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",18.0,12.0,2023.0,USA,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,No dataset used for development or evaluation,"""dataset"" were PHQ-9 questions",,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,safety (number of conversations turns until initial referral/shutdown of chatbot),no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_9,1st_search_144,Safety of Large Language Models in Addressing Depression.,Consensus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",18.0,12.0,2023.0,USA,Empirical research involving an LLM,Client-facing application,Chatbot,No clients/patients involved,,,No dataset used for development or evaluation,"""dataset"" were PHQ-9 questions",,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,safety (number of conversations turns until initial referral/shutdown of chatbot),,,no benchmark,n,,,,n,,,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
1st_search_3,1st_search_145,Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",31.0,1.0,2024.0,Other: Saudi Arabia,Population survey,Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility; Patients with disorder explicitly based on ICD or DSM,24,,Self-collected data,"24 patients were selected to participate in the study. Before participating in an interview, the patients were thoroughly 
informed of the study’s objectives and given the opportunity to provide informed consent during their outpatient visit. 
Following approval, patients used ChatGPT at home for two weeks to seek mental-health support in their own ways. The 
participants were required to interact with ChatGPT3 for a minimum of 15 minutes per day for 14 days. Twenty to thirty 
interviews were regarded an appropriate sample size for qualitative investigations, particularly those employing inter­
views as a method of data collection.
",,,,,Psychopathology,,,,"CBT: Cognitive restructuring; Psychodynamic psychotherapy; Systemic therapy; Informal counseling (e.g., emotional support conversation); Mix of formal therapy methods; Other: ChatGBT as a therapist",GPT-3.5 family,Yes,n,n,n,n,,"semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT 
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n?,"issues such as data privacy, 
consent, confidentiality, and the potential for biases in AI algorithms are the few challenges raised by most of the 
participants. These are inferred from the following statements",yes ,Table S1 and Appendix B,n,,n,,n,,n,,n,,n,,n,,
1st_search_3,1st_search_146,Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,Richard Gaus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",31.0,1.0,2024.0,Other: Saudi Arabia,Empirical research involving an LLM,Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility,24,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,n,y,n,y,,"Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. ""It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information.""",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,"Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B",n,,n,,n,,n,,n,,n,,n,,"Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory"
1st_search_3,1st_search_147,Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study,Consensus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",31.0,1.0,2024.0,Other: Saudi Arabia,Empirical research involving an LLM,Client-facing application,Chatbot,Patients recruited in hospital or outpatient treatment facility,24,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family,Yes,n,y,n,y,,"Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. ""It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information.""

semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,"issues such as data privacy, 
consent, confidentiality, and the potential for biases in AI algorithms are the few challenges raised by most of the 
participants. These are inferred from the following statements",y,"Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B",n,,n,,n,,n,,n,,n,,n,,"Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory


Therapy type: undirected therapy resulting from interacting with ChatGPT with no prespecified initial prompt"
2nd_search_258,2nd_search_1,"""It happened to be the perfect thing"": experiences of generative AI chatbots for mental health.",Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",27.0,10.0,2024.0,UK,Population survey,Client-facing application,Multi-turn chatbot,General population,19,,No dataset used for development or evaluation,,,,,,,,"Other: no model development, qualitative interviews only","Study observes consumer LLM chatbots in the wild (Pi, ChatGPT, Copilot, Kindroid, ChatMind/VOS), not building a new system. ","Unspecified, might include formal therapy methods","ChatGPT, model unspecified; Other: Pi, Copilot, Kindroid, ChatMind/VOS ",Yes,n,y,n,y,," range of positive impacts were reported, including improved mood… healing from trauma and loss… improved relationships…

Participants told us that generative AI feels like an emotional sanctuary, offers insightful guidance… joy of connection… bears comparison with human therapy.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,"Nineteen participants (12 male, 7 female)… age 17 to 60… eight countries…",n,,n,,n,,n,,n,,n,,n,,
2nd_search_258,2nd_search_2,"""It happened to be the perfect thing"": experiences of generative AI chatbots for mental health.",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27.0,10.0,2024.0,UK,Population survey,Client-facing application,Multi-turn chatbot,General population,19,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified; Other: Pi, Copilot, Kindroid, ChatMind",Yes,n,y,n,y,,Qualitative thematic coding of interview results,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_258,2nd_search_3,"""It happened to be the perfect thing"": experiences of generative AI chatbots for mental health.",Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",27.0,10.0,2024.0,UK,Population survey,Client-facing application,Multi-turn chatbot,General population,19,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified; Other: Pi, Copilot, Kindroid, ChatMind/VOS ",Yes,n,y,n,y,," range of positive impacts were reported, including improved mood… healing from trauma and loss… improved relationships…

Participants told us that generative AI feels like an emotional sanctuary, offers insightful guidance… joy of connection… bears comparison with human therapy.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_251,2nd_search_4,Healme: Harnessing cognitive reframing in large language models for psychotherapy,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,8.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,6,,"External data set, modified","In this study, we leverage an existing raw dataset
focused on cognitive reframing and expand it to
include multiple rounds of dialogue. Specifically,
we conduct a manual review of the selected raw
dataset and select 1,000 well-composed pairs of
(thinking trap, client’s thought) from it.
The raw dataset we utilize in this study is intro-
duced by Maddela et al. (2023). 
https://aclanthology.org/2023.acl-long.763.pdf

Creation of HealMe dataset ",Emotional support dialogue -- chat logs,English,Yes,Yes,Other: Not applicable; AI generated,Other: Not applicable; AI generated,Only fine-tuning,"approach by
selecting the open-source LLM, LLaMA2-7b-chat,
as its base and fine-tuning it to ensure the model
consistently maintains the role of a psychotherapist
with high empathy and guidance capabilities.",CBT: Cognitive restructuring,Llama 2 family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,b,l,Table 4; Benchmarks are ChatGLM3-6b and LLaMA2-7b-chat,y,-,-,-,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_251,2nd_search_5,Healme: Harnessing cognitive reframing in large language models for psychotherapy,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,8.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,8,,"External data set, modified","HealMe dataset
https://github.com/elsa66666/HealMe",Emotional support dialogue -- chat logs,English,Yes,Yes,Other: not applicable,Other: not applicable,Only fine-tuning,HealMe = fine-tuned Llama2-7B-Chat,CBT: Cognitive restructuring,Llama 2 family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,b,l,"benchmarks: ChatGLM3-6b, Llama2-7b-Chat",y,no benchmark,no benchmark,Only training data is scored using GPT-4 LLM-as-a-judge (table 2),n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,y,"Since our testing phases involve real-person clients, we exclusively use offline models to protect user privacy. In our study involving real-person clients, we adhere to the Right to Withdraw (Association et al., 2017), ensuring that participants can withdraw at any time if they experience any discomfort",n,,n,,n,,n,,y,PANAS,y,two people not undergoing intervention,n,,n,,
2nd_search_251,2nd_search_6,Healme: Harnessing cognitive reframing in large language models for psychotherapy,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,8.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,8,,"External data set, modified","HealMe dataset
https://github.com/elsa66666/HealMe

In this study, we leverage an existing raw dataset
focused on cognitive reframing and expand it to
include multiple rounds of dialogue. Specifically,
we conduct a manual review of the selected raw
dataset and select 1,000 well-composed pairs of
(thinking trap, client’s thought) from it.
The raw dataset we utilize in this study is intro-
duced by Maddela et al. (2023). 
https://aclanthology.org/2023.acl-long.763.pdf

Creation of HealMe dataset ",Emotional support dialogue -- chat logs,English,Yes,Yes,Other: not applicable,Other: not applicable,Only fine-tuning,"HealMe = fine-tuned Llama2-7B-Chat

approach by
selecting the open-source LLM, LLaMA2-7b-chat,
as its base and fine-tuning it to ensure the model
consistently maintains the role of a psychotherapist
with high empathy and guidance capabilities.",CBT: Cognitive restructuring,Llama 2 family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,b,l,Table 4; Benchmarks are ChatGLM3-6b and LLaMA2-7b-chat,y,no benchmark,no benchmark,Only training data is scored using GPT-4 LLM-as-a-judge (table 2),n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,y,PANAS,y,two people not undergoing intervention,n,,n,,
2nd_search_232,2nd_search_7,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,6.0,2024.0,USA,Empirical research involving an LLM,Other: unsure,,No clients/patients involved,,,External data set,"We conduct extensive experiments on datasets derived from
real-world MI sessions addressing alcohol usage disorder. We
examine three prompting baselines and experiment with four
state-of-the-art auto-regressive LLMs, including Llama2 [12],
Falcon [20], Mistral [21], and ChatGPT [22]. ",Other: unclear,Other: unclear,Other: unclear,No,Unknown,Unknown,Only prompting,"CoI breaks MI coding into three sequential prompt stages (Interaction Definition, Involvement Assessment, Valence Analysis) to mimic professional coders using MISC schema.","Unspecified, might include formal therapy methods","Other: Llama2-13B-Chat [12], Falcon-7B-Instruct [20], Mistral-7B-
Instruct [21], and ChatGPT [22)",No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,"Quantitative F1 decrease in ablation study shows impact of each stage.
",n,,,,n,,,,n,,,,n,,,,n,,,,"ablation performance comparison
??",b,l,"Performance drop when removing CoI stages (ID, IA, VA).
",n,,,,n,,,,n,,n,,y (mostly),"Llama2-13B-Chat, Falcon-7B-Instruct, Mistral-7B-Instruct (open-source).",n,,n,,n,,n,,n,,n,,y,"""Compared to baseline prompting methods (Zeroshot, Few-shot, ZeroCoT).""
",n,,n,,
2nd_search_232,2nd_search_8,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3.0,6.0,2024.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,"Motivational interviewing session transcripts from Borsari et al., “In-session processes of brief motivational interventions in two trials with mandated college students.” Journal of consulting and clinical psychology, vol. 83, pp. 56–67, 2 2015",Psychotherapy -- speech transcripts,English,No,No,Unselected,Trained professionals,Only prompting,"Different styles of prompting: Zero-shot, few-shot, CoT",CBT: Motivational interviewing,GPT-3.5 family; Llama 2 family; Mistral family; Other: Falcon-7B,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,Metric: Micro- and macro-F1. Benchmark: zero- and few-shot prompting with the same LLMs.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_232,2nd_search_9,Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts,Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3.0,6.0,2024.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,External data set,"We conduct extensive experiments on datasets derived from
real-world MI sessions addressing alcohol usage disorder. We
examine three prompting baselines and experiment with four
state-of-the-art auto-regressive LLMs, including Llama2 [12],
Falcon [20], Mistral [21], and ChatGPT [22]. 

Motivational interviewing session transcripts from Borsari et al., “In-session processes of brief motivational interventions in two trials with mandated college students.” Journal of consulting and clinical psychology, vol. 83, pp. 56–67, 2 2015",Psychotherapy -- speech transcripts,English,No,No,Unselected,Trained professionals,Only prompting,"CoI breaks MI coding into three sequential prompt stages (Interaction Definition, Involvement Assessment, Valence Analysis) to mimic professional coders using MISC schema.

Different styles of prompting: Zero-shot, few-shot, CoT",CBT: Motivational interviewing,GPT-3.5 family; Llama 2 family; Mistral family; Other: Falcon-7B,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,"Metric: Micro- and macro-F1. Benchmark: zero- and few-shot prompting with the same LLMs.

Quantitative F1 decrease in ablation study shows impact of each stage.
",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,"Llama2-13B-Chat, Falcon-7B-Instruct, Mistral-7B-Instruct (open-source).",n,,n,,n,,n,,n,,n,,y,"""Compared to baseline prompting methods (Zeroshot, Few-shot, ZeroCoT).""
",n,,n,,
2nd_search_230,2nd_search_10,"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans",Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",31.0,1.0,2025.0,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"HOPE (Malhotra, 2022)",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Only prompting,"Two adverserial LLMs generate new counseling dataset. 
Prompted ChatGPT-3.5, Claude Haiku and LLaMAntino to role-play therapist and patient in depression sessions; compared against human HOPE dataset using text-network metrics. ",Other CBT techniques,GPT-3.5 family; Claude family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,y,b,l,"Network centrality and average distance values reported (Table 1 and 2).
",n,,,,n,,,,n,,,,y,b,l,"Captured via semantic richness (degree) in Table 2.
",n,,,,n,,,,n,,,,n,,n,,y(partly),LLaMAntino (Italian LLaMA 2 model) is,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Interestingly, Haiku and ChatGPT, while playing either therapists or 
patients and speaking in English, rarely use jargon expressing anger or 
disgust. Instead, human patients express jargon eliciting disgust and 
anger at higher rates, compatible with random expectations. This dif­
ference could be due to English LLMs might have been fine-tuned to 
more strongly avoid using a negative outlook on things."
2nd_search_230,2nd_search_11,"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans",Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",31.0,1.0,2025.0,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,HOPE dataset,Emotional support dialogue -- speech transcripts,English,No,Yes,Unselected,Unknown,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family; Claude family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,various text analyses,unclear,h,benchmark: human therapist responses,various network analyses,unclear,h,benchmark: human therapist responses,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_230,2nd_search_12,"Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans",Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",31.0,1.0,2025.0,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"HOPE (Malhotra, 2022)",Emotional support dialogue -- speech transcripts,English,No,Yes,Unknown,Unknown,Only prompting,"Two adverserial LLMs generate new counseling dataset. 
Prompted ChatGPT-3.5, Claude Haiku and LLaMAntino to role-play therapist and patient in depression sessions; compared against human HOPE dataset using text-network metrics. ","Unspecified, might include formal therapy methods",GPT-3.5 family; Llama 2 family; Claude family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,various text analyses,unclear,h,benchmark: human therapist responses,n,,,,network centrality (distance from depression terms),s,h,Network centrality and average distance values reported (Table 1 and 2).,n,,n,,y,LLaMAntino (Italian LLaMA 2 model) is,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Interestingly, Haiku and ChatGPT, while playing either therapists or 
patients and speaking in English, rarely use jargon expressing anger or 
disgust. Instead, human patients express jargon eliciting disgust and 
anger at higher rates, compatible with random expectations. This dif­
ference could be due to English LLMs might have been fine-tuned to 
more strongly avoid using a negative outlook on things."
2nd_search_228,2nd_search_13,Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,11.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,SMILECHAT based on bootstrapped PsyTest and source PsyQA ,Emotional support dialogue -- chat logs,Chinese,Yes,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,Uses ChatGPT prompting to synthesize SMILECHAT from PsyQA single-turn items; topics auto-labeled with Qwen1.5-110B-Chat; trains MeChat via parameter-efficient LoRA fine-tuning on ChatGLM2-6B. ,"Unspecified, might include formal therapy methods","GPT-3.5 family; Qwen family; Other: ChatGLM2-6B, ",No users involved,n,n,n,n,,,y,b,l,"BLEU-1/2/3, ROUGE-L improved after fine-tuning",y,b,l,BERTScore used and increased; also pairwise cosine similarity for semantic diversity analysis,n,,,,y,b,l,Information entropy of dialogue topics… SMILE 15.02 vs standard 8.40,n,,,,n,,,,n,,,,y,b,l,"Tab. 5

Distinct-1/2/3… Fine-tuned 82.08/95.74/97.81 vs Baseline 52.95/80.74/90.17… §5.1–5.3",n,,,,n,,,,n,,,,n,,n,,y,"fine-tuning experiment on ChatGLM2-6B (open, self-hostable)",n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_228,2nd_search_14,Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,11.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","The study uses the PsyQA dataset and creates a new one called SMILECHAT.

https://github.com/qiuhuachuan/smile",Emotional support dialogue -- chat logs,Chinese,Yes,Yes,Other: not applicable,Other: not applicable,Only fine-tuning,fine-tunes a ChatGLM2-6B model on synthetically generated data,"Unspecified, might include formal therapy methods",Other: ChatGLM2-6B,No users involved,n,n,n,n,,,y,b,l,benchmark is some baseline model,y,b,l,benchmark is some baseline model,n,,,,n,,,,y,w,h,benchmark are responses of human counselors in PsyTest,n,,,,n,,,,y,b,l,benchmark is some baseline model,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_228,2nd_search_15,Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,11.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","The study uses the PsyQA dataset and creates a new one called SMILECHAT.

https://github.com/qiuhuachuan/smile",Emotional support dialogue -- chat logs,Chinese,Yes,Yes,Other: not applicable,Other: not applicable,Only fine-tuning,fine-tunes a ChatGLM2-6B model on synthetically generated data,"Unspecified, might include formal therapy methods",Other: ChatGLM2-6B,No users involved,n,n,n,n,,,y,b,l,"BLEU-1/2/3, ROUGE-L improved after fine-tuning

benchmark is some baseline model",y,b,l,"BERTScore used and increased; also pairwise cosine similarity for semantic diversity analysis

benchmark is some baseline model",n,,,,n,,,,y,w,h,benchmark are responses of human counselors in PsyTest,n,,,,n,,,,y,b,l,"Tab. 5

Distinct-1/2/3… Fine-tuned 82.08/95.74/97.81 vs Baseline 52.95/80.74/90.17… §5.1–5.3",n,,,,n,,,,n,,,,n,,n,,y,"fine-tuning experiment on ChatGLM2-6B (open, self-hostable)",n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_219,2nd_search_17,Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",25.0,4.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"ChatGPT responses collected for 21 psychoeducational prompts across categories (e.g., depression, substance use, wellness); evaluated on six criteria (accuracy, clarity, relevance, empathy, engagement, ethics) via qualitative content analysis (QCA) and Likert ratings.","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y ,none,h,"“reviewers assessed ChatGPT’s psychoeducational responses …” 
",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_219,2nd_search_18,Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",25.0,4.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,Prompting of ChatGPT with mental health related questions (Table 1),"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_219,2nd_search_19,Assessing the use of ChatGPT as a psychoeducational tool for mental health practice,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",25.0,4.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"ChatGPT responses collected for 21 psychoeducational prompts across categories (e.g., depression, substance use, wellness); evaluated on six criteria (accuracy, clarity, relevance, empathy, engagement, ethics) via qualitative content analysis (QCA) and Likert ratings.

Prompting of ChatGPT with mental health related questions (Table 1)","Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"“reviewers assessed ChatGPT’s psychoeducational responses …” 
",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_214,2nd_search_21,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",6.0,12.0,2023.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,External data set,Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.,Other: labeled cognitive distortions,English,No,No,Unknown,Other: not applicable,Only prompting,,CBT: Cognitive restructuring,GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.,n,,,,y,no benchmark,no benchmark,4.3 Human Evaluation Results,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_214,2nd_search_22,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",6.0,12.0,2023.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,External data set,"We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Only prompting,"We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).",CBT: Cognitive restructuring,GPT-3.5 family; GPT-4 / GPT-4o family; Other: Vicuna,No users involved,n,n,n,n,,,n,,,,n,,,,y,,,,n,,,,y,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_214,2nd_search_23,Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",6.0,12.0,2023.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,External data set,"We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset

Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.

https://www.kaggle.com/datasets/sagarikashreevastava/cognitive-distortion-detetction-dataset",Other: labeled cognitive distortions,English,No,Yes,Unknown,Other: not applicable,Only prompting,"We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).",CBT: Cognitive restructuring,GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.,n,,,,y,no benchmark,no benchmark,4.3 Human Evaluation Results,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_212,2nd_search_24,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14.0,4.0,2024.0,Other: Czech Republic,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,https://alexanderstreet.com/,Psychotherapy -- speech transcripts,English,No,Yes,Other: all of the above,Other: all of the above,Fine-tuning + other modules,"to fine-tune the generated instruction
data effectively, we employed the inhibition adaption fine-
tuning method [14] and self-RAG [13] on Llama2-7B [15],
as well as ChatGLM2-6B [16].","Unspecified, might include formal therapy methods",Llama 2 family; Other: ChatGLM2-7,No users involved,n,n,n,n,,,y,b,l,only the inhibited LoRA Finetuning,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,fluency,b,l,,n,,,,n,,,,n,,n,,y,on Llama2-7B… as well as ChatGLM2-6B,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_212,2nd_search_25,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14.0,4.0,2024.0,Other: Czech Republic,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Alexander Street Press therapy transcripts,Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,Fine-tuning + other modules,Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4),"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family; Other: ChatGLM2-6B,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,Rouge comparison to Alexander Street Press reference. No benchmark.,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"human rating (read, prof, match)",n,,,,n,,,,n,,,,fluency,no benchmark,no benchmark,Not clear what fluency is. The reference paper measures fluency via human rating but the authors of this paper state this is an automatic assessment.,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_212,2nd_search_26,Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14.0,4.0,2024.0,Other: Czech Republic,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Alexander Street Press therapy transcripts,Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,Fine-tuning + other modules,"Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4)

to fine-tune the generated instruction
data effectively, we employed the inhibition adaption fine-
tuning method [14] and self-RAG [13] on Llama2-7B [15],
as well as ChatGLM2-6B [16].","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family; Other: ChatGLM2-6B,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,"Rouge comparison to Alexander Street Press reference. No benchmark.

only the inhibited LoRA Finetuning",n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"human rating (read, prof, match)",n,,,,n,,,,n,,,,fluency,no benchmark,no benchmark,Not clear what fluency is. The reference paper measures fluency via human rating but the authors of this paper state this is an automatic assessment.,n,,,,n,,,,n,,n,,y,on Llama2-7B… as well as ChatGLM2-6B,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_206,2nd_search_27,Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_206,2nd_search_28,Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_206,2nd_search_29,Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_202,2nd_search_30,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2.0,12.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","we collect question-answer pairs from open platforms… Yixinli… Zhihu… We crawl books related to psychology from the web and then use Qwen-72B to extract knowledge-based QA… After-school exercises from several books

There are several publicly accessible websites committed to
establishing a psychology platform and offering online solu-
tions for individuals seeking psychological assistance, such as
Yixinli3, Zhihu4, and so on. The data from these platforms
can be regarded as real-world inquiries and the solutions pro-
vided by professionals in response to these inquiries.
ubse-
quently, professional or experienced individuals offer detailed
solutions or advice in response to these questions. We collect
over 267 000 pairs of data from websites.
To accurately evaluate the model performance in the psy-
chology domain, we introduce a benchmark that psychological
professional individuals need to master. The data is sourced
from publicly available examination questions. We follow a standardized data preprocessing procedure. For some data, we
first need to perform optical character recognition (OCR) to
convert images into text. After that, we invite several students to
manually review the collected data to ensure its quality and con-
sistency with the original document. This process involves rec-
tifying formatting errors, eliminating duplicate questions, and
rectifying any instances of garbled characters. Our proposed
psychological benchmark draws inspiration from the format of
the most authoritative psychological counseling examination in
China, comprising two primary components: objective ques-
tions and subjective questions.",Emotional support dialogue -- speech transcripts,Chinese,No,Yes,Other: both,"Other: Trainded pros are for sure included, but others might also be. ",Fine-tuning + other modules,"Full-parameter supervised fine-tuning (1e-5 LR; batch 128; 3 epochs) on 8×A6000; pipeline for multi-turn data (generation → evidence judgment → refinement), plus teacher–student with and without RAG for knowledge QA","Unspecified, might include formal therapy methods",Qwen family,No users involved,n,n,n,n,,,y,b,l,ROUGE-1 and BLEU-4 highest for PsycoLLM; ROUGE-L slightly lower than EmoLLM,y,b,l,Highest BERTScore among compared models.,y,b,l,Highest average standard accuracy; >60% pass; elastic accuracy also strong,n,,,,n,,,,n,,,,n,,,,n,,,,elastic accuracy for mmcq ??,,,,n,,,,n,,,,n,,n,,y,Qwen is,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_202,2nd_search_31,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,Richard Gaus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",3.0,4.0,2025.0,China,Empirical research involving an LLM,Client-facing application,Other: clinical psychology-specific general-purpose LLM,No clients/patients involved,,,"External data set, modified","Data sourced from single-turn QA psychology platforms, multi-turn dialogue synthetically generated using the single-turn data, and psychological knowledge QA",Other: mixed: multi-turn and QA,Chinese,Yes,No,Unselected,Other: mixed,Only fine-tuning,Only fine-tuning of Qwen on their dataset,"Unspecified, might include formal therapy methods",Other: Qwen1.5-14B-Chat,No users involved,n,n,n,n,,,y,b,l,"Metrics: Rouge-1, Rouge-L, BLEU-4. Benchmarks: Various other LLMs",y,b,l,Metrics: BERTScore. Benchmarks: Various other LLMs,y,b,l,"""Elastic accuracy"" on their QA dataset.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_202,2nd_search_32,PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2.0,12.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","Data sourced from single-turn QA psychology platforms, multi-turn dialogue synthetically generated using the single-turn data, and psychological knowledge QA

we collect question-answer pairs from open platforms… Yixinli… Zhihu… We crawl books related to psychology from the web and then use Qwen-72B to extract knowledge-based QA… After-school exercises from several books

There are several publicly accessible websites committed to
establishing a psychology platform and offering online solu-
tions for individuals seeking psychological assistance, such as
Yixinli3, Zhihu4, and so on. The data from these platforms
can be regarded as real-world inquiries and the solutions pro-
vided by professionals in response to these inquiries.
ubse-
quently, professional or experienced individuals offer detailed
solutions or advice in response to these questions. We collect
over 267 000 pairs of data from websites.
To accurately evaluate the model performance in the psy-
chology domain, we introduce a benchmark that psychological
professional individuals need to master. The data is sourced
from publicly available examination questions. We follow a standardized data preprocessing procedure. For some data, we
first need to perform optical character recognition (OCR) to
convert images into text. After that, we invite several students to
manually review the collected data to ensure its quality and con-
sistency with the original document. This process involves rec-
tifying formatting errors, eliminating duplicate questions, and
rectifying any instances of garbled characters. Our proposed
psychological benchmark draws inspiration from the format of
the most authoritative psychological counseling examination in
China, comprising two primary components: objective ques-
tions and subjective questions.",Other: mixed: multi-turn and QA,Chinese,Yes,No,Unselected,Other: mixed,Only fine-tuning,Only fine-tuning of Qwen on their dataset,"Unspecified, might include formal therapy methods",Qwen family,No users involved,n,n,n,n,,,y,b,l,"Metrics: Rouge-1, Rouge-L, BLEU-4. Benchmarks: Various other LLMs
ROUGE-1 and BLEU-4 highest for PsycoLLM; ROUGE-L slightly lower than EmoLLM",y,b,l,"Metrics: BERTScore. Benchmarks: Various other LLMs
Highest BERTScore among compared models.",y,b,l,"""Elastic accuracy"" on their QA dataset.
Highest average standard accuracy; >60% pass; elastic accuracy also strong",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,Qwen is,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_193,2nd_search_33,Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?,Reviewer Two,,,3.0,12.0,2024.0,China,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,Self-collected data,"The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.",Emotional support dialogue -- speech transcripts,Other: English and Chinese,No,No,Unknown,Unknown,Prompting + other modules,constructed a CBT-related knowledge base and integrated it with the general models … RAG (Retrieval-Augmented Generation),Other CBT techniques,"GPT-3.5 family; Other: ERNIE-3.5-8K,
iFlytek Spark V3.0 and ChatGLM-3-turbo,",No users involved,n,n,n,n,,,y,b,l,ROUGE/METEOR improved esp. for ChatGPT; mixed for others.,y,b,l,"BERTScore higher with KB, esp. ChatGPT.",n,,,,y,s,l,BLEURT ↑ with KB; BartScore ↓,n,,,,n,,,,n,,,,n,,,,sentiment score & pqa ,b,l,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_193,2nd_search_34,Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?,Richard Gaus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3.0,12.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.",Emotional support dialogue -- speech transcripts,Other: Chinese and English mixed,"Other: mix of ""authentic"" and role-played",No,Unknown,Unknown,Prompting + other modules,Prompting with and without RAG,"Unspecified, might include formal therapy methods","GPT-3.5 family; Other: Ernie-3.5-8K, iFlytek Spark V3.0, ChatGLM-3-turbo",No users involved,n,n,n,n,,,y,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,DISTINCT score. no benchmark,bartscore,no benchmark,no benchmark,no benchmark,bleurt,no benchmark,no benchmark,no benchmark,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_193,2nd_search_35,Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?,Consensus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3.0,12.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.",Emotional support dialogue -- speech transcripts,Other: English and Chinese,"Other: mix of ""authentic"" and role-played",No,Unknown,Unknown,Prompting + other modules,"Prompting with and without RAG

constructed a CBT-related knowledge base and integrated it with the general models … RAG (Retrieval-Augmented Generation)",Other CBT techniques,"GPT-3.5 family; Other: ERNIE-3.5-8K,
iFlytek Spark V3.0, ChatGLM-3-turbo",No users involved,n,n,n,n,,,y,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,DISTINCT score. no benchmark,bartscore,no benchmark,no benchmark,no benchmark,bleurt,no benchmark,no benchmark,no benchmark,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_186,2nd_search_36,Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",29.0,12.0,2023.0,USA,Other: Exploratory/pedagogical/model paper (not an empirical evaluation with recruited participants) ,Therapist-facing application,,,,Patient simulations,No dataset used for development or evaluation,,,,,,,,Only prompting,"Model was prompted to behave like a counseling client: It was then tested in a fishbowl, triad, and one-to-one formats using facilitation commands (“play,” “pause,” “rewind”) and three processing techniques (“ChatGPT close up,” “counselor close up,” “observer close up”) grounded in Bernard’s Discrimination Model (1979)","Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_186,2nd_search_37,Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",29.0,12.0,2023.0,USA,Empirical research involving an LLM,Therapist-facing application,,,,Patient simulations,No dataset used for development or evaluation,,,,,,,,Only prompting,Asking ChatGPT to act as a client,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_186,2nd_search_38,Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",29.0,12.0,2023.0,USA,Empirical research involving an LLM,Therapist-facing application,,,,Patient simulations,No dataset used for development or evaluation,,,,,,,,Only prompting,"Model was prompted to behave like a counseling client: It was then tested in a fishbowl, triad, and one-to-one formats using facilitation commands (“play,” “pause,” “rewind”) and three processing techniques (“ChatGPT close up,” “counselor close up,” “observer close up”) grounded in Bernard’s Discrimination Model (1979)

Asking ChatGPT to act as a client","Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_185,2nd_search_39,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,3.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Other: MI reflection generation,No clients/patients involved,,,"External data set, modified","Only questions and answers from MI chatbot reflections by Brown 2023 -- A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study.
The authors generate reflections based on these question-answer pairs using GPT-4",Other: MI reflections,English,Yes,No,Psychopathology,Other: not applicable,Only fine-tuning; Only prompting,"Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.",CBT: Motivational interviewing,GPT-2 family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_185,2nd_search_40,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,3.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"Other: ''We recruited five annotators to evaluate reflections
from GPT-4 and each distilled student model. The
five annotators consist of four males and one fe-
male at an average age of 23, located in North
America. Each annotator has a basic understand-
ing of MI having read (Miller and Rollnick, 2012) and taken coursework.""",,,Self-collected data,"""Mentioned previously, we use transcripts
from the smoking cessation MI chatbot created by
the authors (Brown et al., 2023). """,Emotional support dialogue -- speech transcripts,English,Yes,No,Unknown,Unknown,Fine-tuning + other modules,"e.g. ""After gathering the dataset of MI conversation
questions, answers, and GPT-4-generated reflec-
tions, we use fine-tuning to distill that reflection
capability in a student model.""",CBT: Motivational interviewing,GPT-2 family; GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,b,h,human annotators,n,,,,n,,,,n,,,,cohen kappa,,,,mi-adherence,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_185,2nd_search_41,"Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,3.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Other: MI reflection generation,No clients/patients involved,,,"External data set, modified","""Mentioned previously, we use transcripts
from the smoking cessation MI chatbot created by
the authors (Brown et al., 2023). ""

Only questions and answers from MI chatbot reflections by Brown 2023 -- A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study.
The authors generate reflections based on these question-answer pairs using GPT-4",Other: MI reflections,English,Yes,No,Psychopathology,Other: not applicable,Only fine-tuning; Only prompting,"Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.",CBT: Motivational interviewing,GPT-2 family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark,y,no benchmark,no benchmark,no benchmark,n,,,,n,,,,llm-as-a-judge mi_adherence,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,n,,y,One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_180,2nd_search_42,A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10.0,10.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population; Other,6 Patients (+6 Clinicians),,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"The addition of multiple
collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s
behavior. Iterative prompt engineering was needed and used to
improve the individual agent’s behavior and how they worked
together.",CBT: Cognitive restructuring,GPT-4 / GPT-4o family,Yes,n,y,n,y,,"See ""Initial User Feedback""",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user",n,,y,"Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy. All
user data are transferred through secure and encrypted https.
All user data are always encrypted and then stored in databases.
Users are also encouraged to refrain from providing personally
identifiable information or personal health information when
they log into Socrates 2.0’s web interface to further decrease
any potential risks",n,,n,,n,,n,,n,,n,,y," Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible.",n,,
2nd_search_180,2nd_search_43,A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10.0,10.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,6,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"The addition of multiple collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s behavior. Iterative prompt engineering was needed and used to improve the individual agent’s behavior and how they worked together.",CBT: Cognitive restructuring,GPT-4 / GPT-4o family,Yes,n,y,n,y,,"See ""Initial User Feedback""",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user",n,,y,"Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy",n,,n,,n,,n,,n,,n,,y,"Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible",n,,
2nd_search_180,2nd_search_44,A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10.0,10.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,6,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"The addition of multiple collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s behavior. Iterative prompt engineering was needed and used to improve the individual agent’s behavior and how they worked together.",CBT: Cognitive restructuring,GPT-4 / GPT-4o family,Yes,n,y,n,y,,"See ""Initial User Feedback""",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user",n,,y,"Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy",n,,n,,n,,n,,n,,n,,y,"Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible",n,"Future studies
should examine whether using LLMs would make out-of-session
practice, such as examining one’s thoughts, more engaging for
patients than worksheets [21].",
2nd_search_173,2nd_search_45,Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8.0,1.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"See Figure 3. All revolves around prompting of GPT-4, and then there is a ""memory concept"" and other stuff","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_173,2nd_search_46,Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8.0,1.0,2024.0,USA,,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,,,,,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_173,2nd_search_47,Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8.0,1.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"See Figure 3. All revolves around prompting of GPT-4, and then there is a ""memory concept"" and other stuff","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_169,2nd_search_48,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_169,2nd_search_49,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_169,2nd_search_50,Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_167,2nd_search_51,Psyqa: A chinese dataset for generating long counseling text for mental health support,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1.0,8.0,2021.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"PsyQA (+ Yixinly? ""Thus we crawled 50K articles (0.1B tokens in total) related to psychology and mental health support from Yixinli (xinli001.com/info) and train a GPT-2 from scratch based on the corpus."")",Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,"Other: Secondly, the answers in PsyQA are
mostly provided by experienced and well-trained volunteers or professional counselors",Fine-tuning + other modules,,"Unspecified, might include formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,S2S-Model (without strategy) as benchmark,n,,,,n,,,,n,,,,y,w,h,Dataset responses from professionals and trained volunteers as benchmark,n,,,,y,b,l,S2S-Model (without strategy) as benchmark,y,b,l,S2S-Model (without strategy) as benchmark,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_167,2nd_search_52,Psyqa: A chinese dataset for generating long counseling text for mental health support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1.0,8.0,2021.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,psyqa,Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Trained professionals,Fine-tuning + other modules,Training of GPT-2 from scratch on PsyQA + prepending of strategy token,"Unspecified, might include formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,benchmark: simple seq2seq model,n,,,,y,w,l,"""controllability"", i.e. match between predicted and actual strategy tokens",n,,,,y,w,h,"rating of fluency, coherence, relevance, helpfulness. benchmark: human responses from the dataset.",n,,,,y,b,l,benchmark: simple seq2seq model,y,b,l,benchmark: simple seq2seq model,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_167,2nd_search_53,Psyqa: A chinese dataset for generating long counseling text for mental health support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",1.0,8.0,2021.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"PsyQA (+ Yixinly? ""Thus we crawled 50K articles (0.1B tokens in total) related to psychology and mental health support from Yixinli (xinli001.com/info) and train a GPT-2 from scratch based on the corpus."")",Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Other: mixed,Fine-tuning + other modules,Training of GPT-2 from scratch on PsyQA + prepending of strategy token,"Unspecified, might include formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,S2S-Model (without strategy) as benchmark,n,,,,y,w,l,"""controllability"", i.e. match between predicted and actual strategy tokens",n,,,,y,w,h,"rating of fluency, coherence, relevance, helpfulness. benchmark: human responses from the dataset.",n,,,,y,b,l,benchmark: simple seq2seq model,y,b,l,benchmark: simple seq2seq model,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_157,2nd_search_54,Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot),Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,12.0,24.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified",,,,,,,,Prompting + other modules,Hybrid pipeline: NLP preprocessing → VADER sentiment scoring with thresholds → template or GPT-3.5-turbo generation → referral logic and resources → logging/monitoring; context tracking for coherence. ,"Unspecified, might include formal therapy methods",GPT-3.5 family; Other: NLTK VADER,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_157,2nd_search_55,Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot),Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set," For example, it uses conversational 
data gained in previous interactions with the user as well as 
datasets having labeled expressions of sentiment (""emo.txt"" 
and ""Sentiments1.txt"")",,English,Other: unspecified,Other: unspecified,Unknown,Unknown,Prompting + other modules,"""A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative"" .. ""The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately"" ... ""Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition""","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_157,2nd_search_56,Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot),Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified"," For example, it uses conversational 
data gained in previous interactions with the user as well as 
datasets having labeled expressions of sentiment (""emo.txt"" 
and ""Sentiments1.txt"")",,English,Other: unspecified,Other: unspecified,Unknown,Unknown,Prompting + other modules,"""A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative"" .. ""The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately"" ... ""Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition""","Unspecified, might include formal therapy methods",GPT-3.5 family; Other: NLTK VADER,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_156,2nd_search_57,The Role of AI Counselling in Journaling for Mental Health Improvement,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",29.0,8.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,50,,No dataset used for development or evaluation,,,,,,,,Other: unknown,,"Unspecified, might include formal therapy methods",Other: unknown,Yes,y,n,y,y,"Counseling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964",,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_156,2nd_search_58,The Role of AI Counselling in Journaling for Mental Health Improvement,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,5.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Other: Journaling app with AI counselling ,General population,50,,Self-collected data,-,"Other: Survey responses (quantitative, self-report)",English,No,No,Unselected,Unknown,Prompting + other modules,Built on LangChain framework with memory and journaling integration,Other: Digital journaling with AI counselling  ,"Other: not specified, using LangChain framework",Yes,n,n,y,y,,"Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"enhancing counselling skills and therapeutic 
conditions (pb-cstc)

enhancing counselling dispositions and 
behaviours (pb-cdb)

learning counselling and mental health topics 
(pb-lc)",,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_156,2nd_search_59,The Role of AI Counselling in Journaling for Mental Health Improvement,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",29.0,8.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,50,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,Built on LangChain framework with memory and journaling integration .. Input/Templates/Prompt Instruction,Other: Digital journaling with AI counselling  ,Other: not specified - using LangChain framework,Yes,y,n,y,y,"Counselling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964

Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b)",,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_155,2nd_search_60,Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17.0,1.0,2025.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,428,,,,,,,,,,,,,,Yes,y,n,y,y,"modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al",,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_155,2nd_search_61,Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17.0,1.0,2025.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,428,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","Other: all types of chatbots, e.g. ChatGPT",Yes,y,n,y,y,"Mental Help Seeking Attitudes Scale (MHSAS) with regard to chatbots for mental health support.
Modified version of the measure of structural barriers toward mental health service utilization as reported by Van Doren et al.",,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_155,2nd_search_62,Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17.0,1.0,2025.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,428,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods",Other: all types of chatbots e.g. ChatGPT,Yes,y,n,y,y,"modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al",,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_153,2nd_search_63,Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_153,2nd_search_64,Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_153,2nd_search_65,Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_152,2nd_search_66,MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_152,2nd_search_67,MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",7.0,5.0,2025.0,India,Population survey,,,General population,,,"Other: ""The mental health quiz module uses a custom dataset de-
rived from stress and anxiety indicators, consisting of 40
structured questions with multiple-choice options. Users
receive a random selection of 10 questions, ensuring
reliability and varied experience""",unclear who the users of the MH quiz module are,,English,,Yes,Unselected,Unknown,Only fine-tuning,,,BERT family,Yes,n,n,n,y,,"Feedback from test users: These results suggest that the
system effectively delivers mental health insights while main-
taining user engagement.",n,,,,n,,,,y,,,"Accuracy, F1 Score, Precision and Recall",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,This studiy does not make clear who the users are. It seems they had some users who competed a quiz
2nd_search_152,2nd_search_68,MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_150,2nd_search_69,nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_150,2nd_search_70,nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_150,2nd_search_71,nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_148,2nd_search_72,Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",20.0,5.0,2025.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,counsel-chat,Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Trained professionals,Only fine-tuning,,"Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,y,b,l,older LLM,y,b,l,older LLM,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,shap value,b,l,older LLM,lime value,b,l,older LLM,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_148,2nd_search_73,Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation,Richard Gaus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",20.0,5.0,2025.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Psychotherapy -- chat logs,English,No,Yes,Unselected,Trained professionals,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_148,2nd_search_74,Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation,Consensus,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",20.0,5.0,2025.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_147,2nd_search_75,AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16.0,12.0,2024.0,India,,,,,,,External data set,"The MSP-Podcast Corpus is a well annotated dataset 
which includes a wide range of emotional expressions 
captured through spontaneous speech. The dataset's 
diversity and the quality of annotations make it an 
excellent resource for making them availed . Its 
annotations ensure reliability, making it invaluable for 
training and evaluating emotion recognition models

Video Dataset: FER+ Dataset  provides quality images with 
annotations for various facial expressions
","Other: A large naturalistic speech database with emotional traces

The FER+ annotations provide a set of new labels for the standard Emotion FER dataset. In FER+, each image has been labeled by 10 crowd-sourced taggers, which provide better quality ground truth for still image emotion than the original FER labels. ",English,No,No,Unselected,Unknown,Only fine-tuning,,Other CBT techniques,GPT-2 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,,,"Precision, recall accuracy, F1 Score",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_147,2nd_search_76,AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Other: Spoken dialog system,No clients/patients involved,,,External data set,"The fine-tuning process involves the 
calibration of the pre-trained Transformer model with 
specific datasets relevant to cognitive therapy. This 
includes dialogues from CBT sessions, therapeutic 
interventions, and stress management conversations.

The GPT-2 model is fine-tuned using a specialized 
therapeutic dialogue dataset where the process involves 
supervised learning and the openly available trained 
model is adapted for a wide range of therapeutic 
contexts. The goal is to improve the ability of the model 
to provide relevant and empathetic responses during 
therapy sessions. The fine-tuning dataset includes a 
variety of therapeutic dialogues which ensures the 
model can handle different therapeutic scenarios 
effectively.",Other: unknown,Other: unknown,Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of GPT-2 + voice and video interaction system,Other CBT techniques,GPT-2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,GPT-2,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_147,2nd_search_77,AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Other: Multimodal dialog system,No clients/patients involved,,,"Other: External data set, but completely unknown","The fine-tuning process involves the 
calibration of the pre-trained Transformer model with 
specific datasets relevant to cognitive therapy. This 
includes dialogues from CBT sessions, therapeutic 
interventions, and stress management conversations.

The GPT-2 model is fine-tuned using a specialized 
therapeutic dialogue dataset where the process involves 
supervised learning and the openly available trained 
model is adapted for a wide range of therapeutic 
contexts. The goal is to improve the ability of the model 
to provide relevant and empathetic responses during 
therapy sessions. The fine-tuning dataset includes a 
variety of therapeutic dialogues which ensures the 
model can handle different therapeutic scenarios 
effectively.",Other: unknown,Other: unknown,Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of GPT-2 + voice and video interaction system,Other CBT techniques,GPT-2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,GPT-2,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_144,2nd_search_78,Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,1.0,2025.0,India,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"Reddit, Twitter an Mental Health Forums as data source",Internet data -- mental health forum,English,No,Yes,Unselected,Unknown,Fine-tuning + other modules,see Fig. 1 ,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,y,b,l,BLEU SCORE,y,b,l,Semantic Similarity,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_144,2nd_search_79,Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,1.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms. It is intended for training and evaluating language models that provide safer, context-aware mental-health responses.",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,BLEU score,y,no benchmark,no benchmark,Cosine distance (see Eq. 4). no benchmark,n,,,Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_144,2nd_search_80,Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,1.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms. It is intended for training and evaluating language models that provide safer, context-aware mental-health responses.",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier (see Fig. 1),"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,BLEU SCORE,y,no benchmark,no benchmark,Semantic Similarity via cosine distance (see Eq. 4),n,,,Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_143,2nd_search_81,Mello: A Large Language Model for Mental Health Counselling Conversations,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,9.0,2024.0,India,Empirical research involving an LLM,Analysis of conversation transcripts,One-turn chatbot (usually Q&A),No clients/patients involved,,Patient simulations,"External data set, modified","counsel-chat dataset, accessible on Hugging Face ( 2,700 anonymised
talks between individuals and experienced counsellors on the
website www.counselchat.com)",Psychotherapy -- chat logs,English,No,Yes,Unselected,Other: experienced counsellors,Only fine-tuning,,"Informal counseling (e.g., emotional support conversation)",Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,b/s,h,human responses from existing literature,n,,,,n,,,,emotional intelligence scale (eis),b,h,human responses from existing literature,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_143,2nd_search_82,Mello: A Large Language Model for Mental Health Counselling Conversations,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,9.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Psychotherapy -- chat logs,English,No,Yes,Unselected,Trained professionals,Only fine-tuning,,"Unspecified, might include formal therapy methods",Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,psychobench empathy scale,b,l,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",psychobench emotional intelligence scale,b,l,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_143,2nd_search_83,Mello: A Large Language Model for Mental Health Counselling Conversations,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,9.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,CounselChat https://github.com/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only fine-tuning,,"Unspecified, might include formal therapy methods",Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,psychobench empathy scale,b,l,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",psychobench emotional intelligence scale,b,l,"PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)",n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_142,2nd_search_84,Cases of Using ChatGPT as a Mental Health and Psychological Support Tool,Reviewer Two,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23.0,12.0,2024.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,,,Other: reddit ,reddit r/ChatGPT,Other: Internet data - online forum,English,No,Yes,Unknown,Unknown,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-3 family; GPT-3.5 family; Other,Yes,n,y,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_142,2nd_search_85,Cases of Using ChatGPT as a Mental Health and Psychological Support Tool,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23.0,12.0,2024.0,Other: Philippines,Population survey,Client-facing application,Multi-turn chatbot,General population,7,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,"""User experience assessment"" was content of the Reddit posts",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_142,2nd_search_86,Cases of Using ChatGPT as a Mental Health and Psychological Support Tool,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23.0,12.0,2024.0,Other: Philippines,Population survey,Client-facing application,Multi-turn chatbot,General population,7,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,"""User experience assessment"" was content of the Reddit posts",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_141,2nd_search_87,Employing large language models for emotion detection in psychotherapy transcripts,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_141,2nd_search_88,Employing large language models for emotion detection in psychotherapy transcripts,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_141,2nd_search_89,Employing large language models for emotion detection in psychotherapy transcripts,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_140,2nd_search_90,MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,Richard Gaus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",3.0,6.0,2024.0,China,Empirical research involving an LLM,Client-facing application,,No clients/patients involved,,,External data set,PsyQA,Emotional support dialogue -- speech transcripts,,No,Yes,Unknown,Unknown,Prompting + other modules,"Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,Yes,n,n,y,y,,"Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale",y,b,l,"“We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”",n,,,,y,b,l,"The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978",n,,,,n,,,,n,,,,n,,,,y,b,l,D1 (Distinct-1) measures the richness of vocabulary in the responses,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"We compare MENTALER with the following baselines… CoT, TPE, ReAct, Cue-CoT, Chameleon",n,,n,,
2nd_search_140,2nd_search_91,MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,12.0,2024.0,China,Empirical research involving an LLM,,,No clients/patients involved,,,External data set,"PsyQA is an authoritative Chinese dialogue dataset focused
on the field of mental health support. It covers 9 broad
topics and multiple subtopics, encompassing various aspects
of mental health. PsyQA is presented in a question-answer
format, where each example includes a question, a detailed
description of the mental issues, and keywords provided by
anonymous help-seekers. Responses are asynchronously pro-
vided by well-trained volunteers or professional counselors
and involve detailed analysis and guidance in response to the
help-seekers’ questions, aiming to offer mental health support.
Additionally, the answers have been further annotated with
seven psychological strategies based on psychological coun-
seling theory [12], ",Internet data -- mental health Q&A,Chinese,No,Yes,Unknown,Trained professionals,Only prompting,"""We use ChatGPT as our base LLM and access it through
the API with prompts""","Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,n,n,n,n,,,y,,,,n,,,,n,,,,n,,,,y,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_140,2nd_search_92,MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,12.0,2024.0,China,Empirical research involving an LLM,Client-facing application,,No clients/patients involved,,,External data set,"PsyQA is an authoritative Chinese dialogue dataset focused
on the field of mental health support. It covers 9 broad
topics and multiple subtopics, encompassing various aspects
of mental health. PsyQA is presented in a question-answer
format, where each example includes a question, a detailed
description of the mental issues, and keywords provided by
anonymous help-seekers. Responses are asynchronously pro-
vided by well-trained volunteers or professional counselors
and involve detailed analysis and guidance in response to the
help-seekers’ questions, aiming to offer mental health support.
Additionally, the answers have been further annotated with
seven psychological strategies based on psychological coun-
seling theory [12], ",Internet data -- mental health Q&A,Chinese,No,Yes,Unknown,Trained professionals,Prompting + other modules,"Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,y,b,l,"“We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”",n,,,,y,b,l,"The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978",n,,,,y,,,"Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale",n,,,,n,,,,y,b,l,D1 (Distinct-1) measures the richness of vocabulary in the responses,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_139,2nd_search_93,Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_139,2nd_search_94,Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_139,2nd_search_95,Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_138,2nd_search_96,Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",28.0,4.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),45,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Prompt engineering improved MI fidelity (Phase II); GPT-4 via secure API; HIPAA-compliant setup. — Quote: “explicit prompting for adherence to validated MI literature and more collaborative, client-centered language enhanced the therapeutic alliance.”",CBT: Motivational interviewing,GPT-4 / GPT-4o family,Yes,y,y,y,y,"SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract)","SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback)",n,,,,n,,,,n,,,,n,,,,y,b,l,(reviewed transcripts for safety + MI fidelity) — Quote: “All authors… reviewed MICA session transcripts independently to identify any statements… inappropriate” ,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results)",n,,n,,y?,"Participants were mostly male (71.1 %), White race (66.7 %), and college educated (82.2 %)",n,,n,,n,,n,,n,,n,,n,,
2nd_search_138,2nd_search_97,Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",28.0,4.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,45,,Other: External data set + Self-collected data ,"Model not really trained on the dataset, but datasets used to generate suitable prompts. 

Datasets used: AnnoMI + 20 full-length simulated sessions using GPT, seeded with anonymized baseline data from young adult participants with hazardous alcohol use drawn from a recent clinical trial",Other: Psychotherapy -- speech transcripts + Syntethic data ,English,"Other: AnnoMi no, self-collected yes","Other: AnnoMi yes, self-collected no",Unknown,Trained professionals,Only fine-tuning; Only prompting,"We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,y,y,y,y,System Usability Scale (SUS) + Client Evaluation of Motivational Interviewing (CEMI),,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. ",y,"ll authors- including an adolescent 
psychiatrist, clinical psychologist trained in MI, addiction psychologist, 
and emergency physician- reviewed MICA session transcripts indepen­
dently to identify any statements that were medically inappropriate, 
encouraged self-harm, minimized serious disclosures, were discrimina­
tory, or inappropriate in tone. ",n,,y,"
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. ",y,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_138,2nd_search_98,Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",28.0,4.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),45,,No dataset used for development or evaluation,,,,,,,,Only fine-tuning; Only prompting,"We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.","CBT: Motivational interviewing; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,y,y,y,y,"SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract)","SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results)",n,,y,"
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. ",y,Table 1,n,,n,,n,,n,,n,,n,,n,,
2nd_search_137,2nd_search_99,AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5.0,5.0,2025.0,Other: Pakistan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","{17} J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015.
[18] D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009.
[19] B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014.",,English,No,Yes,Unknown,Unknown,Prompting + other modules,"Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1)","Informal counseling (e.g., emotional support conversation)",BART family,No users involved,n,n,n,n,,,y,b,l,0.85 cosine similarity score,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract),n,,n,,
2nd_search_137,2nd_search_100,AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5.0,5.0,2027.0,Other: Pakistan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","Books: 
E. J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015. + D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009. + B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014. 

To make sure it was appropriate for entering into the model, the data taken from the books went through a number of preparation steps
in this research: Extraction and cleaning.",Other: Book data,English,No,No,Other: N/A,Trained professionals,Other: RAG,,"Unspecified, might include formal therapy methods","BART family; ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,y,b,l,Best model of authors compared to models of other works. ,n,,,,n,,,,n,,,,n,,,,y,w,l,Best model of authors compared to models of other works. ,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_137,2nd_search_101,AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5.0,5.0,2025.0,Other: Pakistan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","Books: 
E. J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015. + D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009. + B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014. 

To make sure it was appropriate for entering into the model, the data taken from the books went through a number of preparation steps
in this research: Extraction and cleaning.",Other: Book data,English,No,No,Unknown,Unknown,Prompting + other modules,"Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1)","Unspecified, might include formal therapy methods","BART family; ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,y,b,l,Best model of authors compared to models of other works. ,n,,,,n,,,,n,,,,n,,,,y,w,l,Best model of authors compared to models of other works. ,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract),n,,n,,
2nd_search_136,2nd_search_102,Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),9.0,11.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_136,2nd_search_103,Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),9.0,11.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_136,2nd_search_104,Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),9.0,11.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_135,2nd_search_105,MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2.0,12.0,2024.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,Self-collected data,PsyQA dataset,Emotional support dialogue -- chat logs,Chinese,No,Yes,Unknown,Unknown,Only prompting,,"Unspecified, might include formal therapy methods",BERT family; Llama 2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_135,2nd_search_106,MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,11.0,2024.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,"External data set, modified","PsyQA, extended by ""helping skills"" column",Internet data -- mental health Q&A,Chinese,,,,,Prompting + other modules,BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.,"Unspecified, might include formal therapy methods",BERT family; Llama 2 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,"BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L",n,,,,y,no benchmark,no benchmark,Accuracy of the BERT helping skill classification model,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_135,2nd_search_107,MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,11.0,2024.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,"External data set, modified","PsyQA, extended by ""helping skills"" column",Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Other: Mix of lay and trained,Prompting + other modules,BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.,"Unspecified, might include formal therapy methods",BERT family; Llama 2 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,"BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L",n,,,,y,no benchmark,no benchmark,Accuracy of the BERT helping skill classification model,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_134,2nd_search_108,Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_134,2nd_search_109,Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_134,2nd_search_110,Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_132,2nd_search_111,AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",2.0,4.0,2025.0,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,11,,No dataset used for development or evaluation,,,,,,,,Only prompting,"The design phase mentions fine-tuning and prompt engineering, but the implemented study relied on a simple prompt with GPT-3.5. — Quote: “The design process addressed… fine-tuning the LLMs, and prompt engineering.” / “The initial study used a relatively simple prompt…”",CBT: Motivational interviewing,GPT-3.5 family,Yes,n,y,y,y,,"Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” ",n,n ,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,VR maybe as solution for the embodiment problem? 
2nd_search_132,2nd_search_112,AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",2.0,4.0,2025.0,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,11,,External data set,"fine-tuned on the 2 volumes of published counseling and psychotherapy data from
Alexander Street Press. The volumes are searchable collections of transcripts containing real counseling and therapy sessions
and first-person narratives illuminating the experience of mental illness and treatment. The 2 volumes contain 3500 session transcripts and >700,000 utterances between a counselor and a
patient.",Emotional support dialogue -- speech transcripts,English,No,Yes,Unknown,Trained professionals,Only fine-tuning; Only prompting,"""We fine-tuned the model with an 80% to 20% train-test split"" + ""that is, the system is prompted
to keep generating responses""...","Unspecified, might include formal therapy methods",GPT-3.5 family,Yes,n,y,n,y,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_132,2nd_search_113,AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",2.0,4.0,2025.0,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,11,,External data set,"fine-tuned on the 2 volumes of published counseling and psychotherapy data from
Alexander Street Press. The volumes are searchable collections of transcripts containing real counseling and therapy sessions
and first-person narratives illuminating the experience of mental illness and treatment. The 2 volumes contain 3500 session transcripts and >700,000 utterances between a counselor and a
patient.",Emotional support dialogue -- speech transcripts,English,No,Yes,Unknown,Unknown,Only fine-tuning; Only prompting,"""We fine-tuned the model with an 80% to 20% train-test split"" + ""that is, the system is prompted
to keep generating responses""...",CBT: Motivational interviewing,GPT-3.5 family,Yes,n,y,y,y,,"Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,VR maybe as solution for the embodiment problem? 
2nd_search_131,2nd_search_114,Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2.0,12.0,2024.0,Other: Romania,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),50,,No dataset used for development or evaluation,,,,,,Psychopathology,Other: chatbot,Prompting + other modules,"Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization",CBT: Cognitive restructuring; Other CBT techniques,GPT-4 / GPT-4o family,,n,y,y,y,Participant Feedback ,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,anxiety score ,,,,engangement metrics ,,,,n,,,,n,,n,,n,,n,,y,,y,,n,,n,,n,,n,,n,,n,,
2nd_search_131,2nd_search_115,Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2.0,12.0,2024.0,Other: Romania,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),50,,No dataset used for development or evaluation,,,,,,,,Only prompting,"They only ever refer to ""prompt engineering techniques"" used in their chatbot",CBT: Cognitive restructuring; Other CBT techniques,"ChatGPT, model unspecified",Yes,n,y,y,y,,Qualitative feedback reported in 4.4.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"reduction in anxiety symptoms (bai, gad-7)",no benchmark,no benchmark,no benchmark,user satisfaction rating,no benchmark,no benchmark,no benchmark,daily interaction time,no benchmark,no benchmark,no benchmark,n,,n,,n,,n,,y,Table 2 has just the bare minimum,n,,y,Table 6 shows how many participants interacted only 0-15 mins daily,y,Table 6 shows daily interaction time over 30 mins,y,Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7),n,,n,,y,"Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.",
2nd_search_131,2nd_search_116,Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2.0,12.0,2024.0,Other: Romania,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),50,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization",CBT: Cognitive restructuring; Other CBT techniques,"ChatGPT, model unspecified",Yes,n,y,y,y,,Qualitative feedback reported in 4.4.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"reduction in anxiety symptoms (bai, gad-7)",no benchmark,no benchmark,no benchmark,user satisfaction rating,no benchmark,no benchmark,no benchmark,daily interaction time,no benchmark,no benchmark,no benchmark,n,,n,,n,,n,,y,Table 2 has just the bare minimum,n,,y,Table 6 shows how many participants interacted only 0-15 mins daily,y,Table 6 shows daily interaction time over 30 mins,y,Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7),n,,n,,y,"Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.",
2nd_search_130,2nd_search_117,Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,Reviewer Two,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2.0,7.0,2025.0,India,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),,,,"External data set, modified","Aditya Mental Health Counselling Dataset, Mental Health Counselling Chat, 
Counsel Chat Dataset and Amod-Mental Health Counselling Conversations",Internet data -- mental health Q&A,English,No,Yes,Unselected,Unknown,Only fine-tuning,,"Unspecified, might include formal therapy methods",BART family; T5 family; Other: GODEL(Grounded Open Dialogue Language Model),No users involved,n,n,n,n,,,y,b,l,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets",n,,,,n,,,,n,,,,n,,,,n,,,,y,b,l,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets",n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_130,2nd_search_118,Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2.0,7.0,2025.0,India,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,Aditya Mental Health Counseling https://huggingface.co/datasets/Aditya149/Mental_Health_Counselling_Dataset,Internet data -- mental health Q&A,English,Other: Unknown,Yes,Unknown,Unknown,Only fine-tuning,Different models are simply fine-tuned on provided datasets,"Informal counseling (e.g., emotional support conversation)",BART family; T5 family; GPT-2 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,Comparison against reference responses in dataset,n,,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Perplexity of reference responses in dataset,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_130,2nd_search_119,Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",2.0,7.0,2025.0,India,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,Aditya Mental Health Counseling https://huggingface.co/datasets/Aditya149/Mental_Health_Counselling_Dataset,Internet data -- mental health Q&A,English,Other: Unknown,Yes,Unknown,Unknown,Only fine-tuning,Different models are simply fine-tuned on provided datasets,"Unspecified, might include formal therapy methods",BART family; T5 family; GPT-2 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Comparison against reference responses in dataset",n,,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Perplexity of reference responses in dataset",n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_129,2nd_search_120,Multi-Tiered RAG-Based Chatbot for Mental Health Support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,4.0,2025.0,Other: Saudi Arabia,Other: engineering/system paper with internal evaluation (no human user study) ,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.","Other CBT techniques; Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,b,l,"Custom metrics (Role Adherence, Answer Relevancy, Faithfulness) reported in tables across tiers of the same model; no external gold standard named",n,,,,n,,,,y,"Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_129,2nd_search_121,Multi-Tiered RAG-Based Chatbot for Mental Health Support,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,4.0,2025.0,Other: Saudi Arabia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"unspecified (""credible sources"")",Other: unspecified ,,Other: unspecified ,Other: unspecified ,Unknown,Unknown,Fine-tuning + other modules," multi-tiered chatbot leveraging Retrieval-
Augmented Generation (RAG) provides dynamic, context-aware
mental health assistance","Other CBT techniques; Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,,,The DeepEval framework + MQG-RAG Evaluation,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_129,2nd_search_122,Multi-Tiered RAG-Based Chatbot for Mental Health Support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,4.0,2025.0,Other: Saudi Arabia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.","Other CBT techniques; Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,The DeepEval framework + MQG-RAG Evaluation,n,,,,n,,,,automatic response quality,no benchmark,no benchmark,llm as a judge: The DeepEval framework + MQG-RAG Evaluation,n,,,,n,,,,y,"Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_128,2nd_search_123,"ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence",Reviewer Two,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",3.0,10.0,2024.0,Other: canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).,Psychoanalysis,GPT-4 / GPT-4o family,Yes,n,y,n,y,,"Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)",n (all below),,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_128,2nd_search_124,"ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence",Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",3.0,10.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: The author served as the sole participant,1,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,n,y,n,y,,"May not be understood as the ""classical"" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_128,2nd_search_125,"ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence",Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",3.0,10.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,1,,No dataset used for development or evaluation,,,,,,,,Only prompting,Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).,Psychoanalysis,GPT-4 / GPT-4o family,Yes,n,y,n,y,,"Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)

May not be understood as the ""classical"" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_127,2nd_search_126,Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories,Reviewer Two,,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",17.0,12.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).",Other,"ChatGPT, model unspecified",Yes,n,y,y,y,,"As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_127,2nd_search_127,Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories,Richard Gaus,,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",17.0,12.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: Reviewers trained in developing therapeutic stories,4,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Promts: ""Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis"".","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,-,-,-,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_127,2nd_search_128,Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories,Consensus,,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",17.0,12.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Promts: ""Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis"".

Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).",Other: Bedtime stories,"ChatGPT, model unspecified",Yes,n,y,y,y,,"As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_126,2nd_search_129,Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23.0,5.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,160,,Self-collected data,"Using a dataset of Woebot questions and user responses (pulled from the Gen-W-MA internal testing data) labeled as being on or off topic, we fine-tuned an instance of the text-embedding-ada-002 model using the Azure OpenAI service",Emotional support dialogue -- chat logs,English,No,No,Unselected,Other: Woebot chatbot,Prompting + other modules,davinci-003 + off topic classifier. davinci-003 seems not finetuned,"CBT: Cognitive restructuring; Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,Yes,y,n,y,y,Client Satisfaction Questionnaire-8 (CSQ-8),"Quantitative: Measures of the user experience with the generative and rules-based DMHIs included user engagement (number of sessions, total active days, and conversational exchanges).
",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,posttrial guardrail review,no benchmark,no benchmark,"A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). ","working alliance
inventory-short revised bond subscale (wai-sr bond)",,,,safety measures,,,"Safety
was assessed by adverse events monitored during both in-app
conversational exchanges and study assessment points, instances
of concerning language detected in user free-text inputs, and
the posttrial technical guardrail assessment success rate.",y,"Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language",y,"We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.",n,,n,,y,Table 1,n,,y,dropouts noted,y,"Measures of the user experience with the generative and
rules-based DMHIs included user engagement (number of
sessions, total active days, and conversational exchanges)",n,,n,,n,,n,,
2nd_search_126,2nd_search_130,Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23.0,5.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,160,,Self-collected data,,Emotional support dialogue -- speech transcripts,English,No,No,Unknown,,Prompting + other modules,,"Informal counseling (e.g., emotional support conversation)",GPT-3 family,Yes,y,n,y,y,"Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9",,n,,,,n,,,,n,,,,n,,,,y,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language",y,"Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer",n,,n,,y,,n,,y,,n,,y,,y,RCT,y,,n,,
2nd_search_126,2nd_search_131,Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",23.0,5.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,160,,Self-collected data,"Using a dataset of Woebot questions and user responses (pulled from the Gen-W-MA internal testing data) labeled as being on or off topic, we fine-tuned an instance of the text-embedding-ada-002 model using the Azure OpenAI service",Emotional support dialogue -- chat logs,English,No,No,Unknown,Other: Woebot chatbot,Fine-tuning + other modules,davinci-003 + off topic classifier. davinci-003 seems not finetuned,"CBT: Cognitive restructuring; Informal counseling (e.g., emotional support conversation)",GPT-3.5 family; Other: text-embedding-ada-002,Yes,y,n,y,y,"Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9","Quantitative: Measures of the user experience with the generative and rules-based DMHIs included user engagement (number of sessions, total active days, and conversational exchanges).
",n,,,,n,,,,n,,,,n,,,,y,b,l,benchmark: woebot non-generative/rule-based.,n,,,,n,,,,n,,,,posttrial guardrail review,no benchmark,no benchmark,"A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). ",instances of potentially concerning language detected,w,l,benchmark: woebot non-generative/rule-based.,n,,,,y,"Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language

Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language",y,"Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer

We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.",n,,n,,y,Table 1,n,,y,dropouts noted,y,"Measures of the user experience with the generative and
rules-based DMHIs included user engagement (number of
sessions, total active days, and conversational exchanges)",n,"WAI-SR Bond is used, but this is not symptom or function scale",y,RCT,n,,n,,
2nd_search_125,2nd_search_132,PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",8.0,10.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"PsyAdv Corp consisting out of Efaqa, CpsyCounD, SMILECHAT, PsyQA",Psychotherapy -- speech transcripts,Chinese,Yes,Yes,Unknown,Unknown,Fine-tuning + other modules,"Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.","Unspecified, might include formal therapy methods",Other: InternLM2-7B-Chat with QLoRA,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,b,l,ChatGPT-4 rated using the categories below,n,,,,n,,,,"comprehensiveness, professionalism, safety",b ,l,ChatGPT and GLM-4,authenticity,w,l,ChatGPT and GLM-4,n,,,,n,,n,,y,InternLM2-7B is self-hostable,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_125,2nd_search_133,PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,11.0,2024.0,China,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,External data set,"PsyQA dataset (and PsyAdv Corp, and CPSyCounE)",Emotional support dialogue -- chat logs,Chinese,No,Yes,Unselected,Unknown,Fine-tuning + other modules,"'we apply the
QLoRA [12] technique for parameter-efficient finetuning of
the generator' .. 'dense retrieval model'... ' we introduce a BERT-
based cross-encoder as a re-ranke'","Informal counseling (e.g., emotional support conversation)",BERT family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_125,2nd_search_134,PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,11.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","PsyAdv Corp consisting out of Efaqa, CpsyCounD, SMILECHAT, PsyQA",Emotional support dialogue -- chat logs,Chinese,Yes,Yes,Unknown,Unknown,Fine-tuning + other modules,"Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.","Unspecified, might include formal therapy methods",BERT family; Other: InternLM2-7B-Chat with QLoRA,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,b,l,"ChatGPT-4 rated using the categories below. Benchmark: ChatGPT, GLM-4",n,,,,n,,,,automatic safety rating,b,l,ChatGPT and GLM-4,automatic response quality,b,l,ChatGPT and GLM-4,n,,,,n,,n,,y,InternLM2-7B is self-hostable,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_124,2nd_search_135,Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14.0,12.0,2024.0,India,"Other: Propsal of a multimodal Model called ""empathy AI""",Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Multimodal pipeline combining BERT embeddings (text), CNN (facial), LSTM/MFCC (audio), LangChain prompting, GPT-4 response generation; frontend in React, auth/session in Firebase. ","Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family; Other: additionally: CNN for Facial Recognition and LSTM for Text-based Emotion Detection,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_124,2nd_search_136,Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),14.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Other: unspecified,,Other: unspecified,Other: unspecified,Other: unspecified,Other: unspecified,Unknown,Unknown,Prompting + other modules,"""LangChain for Prompting"" combined with ""CNN for Facial Recognition,"" ""LSTM for Text-based Emotion Detection,"" ""MFCC or LSTM for Audio Recognition""","Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_124,2nd_search_137,Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Other: Multi-modal dialogue system,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"""LangChain for Prompting"" combined with ""CNN for Facial Recognition,"" ""LSTM for Text-based Emotion Detection,"" ""MFCC or LSTM for Audio Recognition""","Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_123,2nd_search_138,SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",28.0,10.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,Mental Health Counseling Conversations Dataset,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Fine-tuning + other modules,"dataset fine-tuning, sentiment analysis within RAG models","Unspecified, might include formal therapy methods",GPT-3.5 family; Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"ragas evaluation metrics to calculate faithfulness, answer relevancy, and answer correctness scores for the various
rag models",b,l,OpenAI against Mistral with RAG modules and sentiment analysis,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_123,2nd_search_139,SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",28.0,10.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis,"Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family; Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Ragas is LLM-as-a-judge,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_123,2nd_search_140,SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis,Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",28.0,10.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis,"Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family; Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Ragas is LLM-as-a-judge,n,,,,n,,,,automatic response quality,no benchmark,no benchmark,Ragas is LLM-as-a-judge. OpenAI against Mistral with RAG modules and sentiment analysis,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_119,2nd_search_141,The impact of prompt engineering in large language model performance: a psychiatric example,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",24.0,10.0,2023.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Psychoanalysis; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_119,2nd_search_142,The impact of prompt engineering in large language model performance: a psychiatric example,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",24.0,10.0,2023.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Four unique questions were asked of ChatGPT 4.0, and each question was asked five separate times. Each question asked to the model 
was performed in a separate, new conversation (10)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_119,2nd_search_143,The impact of prompt engineering in large language model performance: a psychiatric example,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",24.0,10.0,2023.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Four unique questions were asked of ChatGPT 4.0, and each question was asked five separate times. Each question asked to the model 
was performed in a separate, new conversation (10)","Psychoanalysis; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_117,2nd_search_144,Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.,Richard Gaus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),25.0,5.0,2025.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Prompting of GPT-4o + components of a robot (speech-to-text, movement, etc.)",,GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,s,h,"Measure: ""Engagement score"". Benchmark: Therapist-led sessions",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,y,Prioritize data privacy and ethical compliance,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_117,2nd_search_145,Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.,Reviewer Two,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),25.0,5.0,2025.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,multiple sets: existing clinical datasets - e.g. from ADHD-200 Global Competition; transcriptions of therapist-patient interactions; DHD support group discussions ,Psychotherapy -- speech transcripts,"Other: german, english ",Yes,Yes,Psychopathology,Trained professionals,Prompting + other modules,"chatgbt 4 was fine-tuned with external (and internal) data sets, fine-tuned through prompt engineering and integrated into a technical framework ","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,"Other: users were not the target group ( here: children), but educators and caregivers rated the effectiviness in potential application with children",n,y,n,n,,"Categories of qualitative feedback: helpfullness, Naturalness, Ease of Use, Engangement. ",n,,,,n,,,,n,,,,n,,,,y,b,h,"To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.",n,,,,n,,,,n,,,,integrated interaction ,,,,consitency ,,,,n,,,,n,,n,,n,,y,"In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.
",n,,n,,n,,n,,n,,n,,y,,y,"
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families",
2nd_search_117,2nd_search_146,Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.,Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),25.0,5.0,2025.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Prompting of GPT-4o + components of a robot (speech-to-text, movement, etc.)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,b,h,"To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.",n,,,,n,,,,n,,,,stress test metrics,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,n,,n,,y,"In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.
",n,,n,,n,,n,,n,,n,,n,,y,"
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families",
2nd_search_116,2nd_search_147,Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",23.0,6.0,25.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Real therapy transctipts from Alexander Street Press,Psychotherapy -- speech transcripts,English,No,Yes,Unknown,Trained professionals,Only prompting,"Only prompting to evaluate capabilities of LLMs in terms of stigma and their responses to mental health symptoms.  In addition to prompting models with the stimuli without in-context examples, we also employed a novel method of prompting models with a portion of real therapy tran-
scripts from Alexander Street Press [6, 7]","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,-,-,"No clear indexmodel, so not necessarly a benchmark (?)",n,,,,n,,,,avg. of stigma questions,-,-,"Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. ",appropriateness of responses,b,l,Compared to commercially-available therapy bots; no indexmodel. ,n,,,,y,Implicit in S-2,y,"LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11)",y,"Partially: GPT not, Llama yes. ",y,"Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]",n,,n,,n,,n,,n,,n,,n,,y,"Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].",
2nd_search_116,2nd_search_148,Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.,Richard Gaus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",23.0,6.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Alexander Street Press therapy transcripts,Psychotherapy -- speech transcripts,English,No,No,Psychopathology,Trained professionals,Only prompting,Only prompting of different LLMs,"Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family; Other: Pi, Noni, Serena, and other commercial chatbots",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,w,h,LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.,n,,,,n,,,,level of stigma,no benchmark,no benchmark,"Figure 1, no benchmark",n,,,,n,,,,y,,y,,y,,y,,n,,n,,n,,n,,n,,n,,n,,y,See 6.2 and 7,
2nd_search_116,2nd_search_149,Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",23.0,6.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Alexander Street Press therapy transcripts,Psychotherapy -- speech transcripts,English,No,No,Unknown,Trained professionals,Only prompting,"Only prompting to evaluate capabilities of LLMs in terms of stigma and their responses to mental health symptoms.  In addition to prompting models with the stimuli without in-context examples, we also employed a novel method of prompting models with a portion of real therapy tran-
scripts from Alexander Street Press [6, 7]","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family; Llama 3.1 family; Other: Pi - Noni - Serena - and other commercial chatbots,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,w,h,LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.,n,,,,n,,,,avg. of stigma questions,no benchmark,no benchmark,"Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. ",automatic safety rating,w,h,LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.,n,,,,y,Implicit in S-2,y,"LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11)",y,"Partially: GPT not, Llama yes. ",y,"Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]",n,,n,,n,,n,,n,,n,,n,,y,"Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].

See 6.2 and 7",
2nd_search_114,2nd_search_150,Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms,Richard Gaus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),17.0,6.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,See Fig. 1: LLMs integrated into robot to specifically adress ADHD Symptoms,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Claude family,No users involved,n,n,n,n,,,y,b,l,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,see fig. 4-9. ,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_114,2nd_search_151,Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,6.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"The implementation of the speak-speak feature involves
specifying the models, prompts (as we are testing in a zero
learning shot environment)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Claude family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_114,2nd_search_152,Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),17.0,6.0,2024.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"The implementation of the speak-speak feature involves
specifying the models, prompts (as we are testing in a zero
learning shot environment)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Claude family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,s,l,benchmark: chatgpt without robot integration,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_112,2nd_search_153,Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",17.0,3.0,2025.0,Other: Italy,Empirical research involving an LLM,Therapist-facing application,,,,Treatment fidelity feedback,Self-collected data,-,Other: Psychotherapy -- Therapist Feedback,Other: Italien,Yes,No,Unselected,Trained professionals,Fine-tuning + other modules,,Other: Gestalt-Therapy,GPT-4 / GPT-4o family,Yes,n,y,y,y,,"Satisfaction ratings focused on relational/emotional quality, technical/didactic, treatment support, professional orientation. — Quote: “PCA highlighted four components… relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4)” (Position: Abstract)",n,,,,n,,,,n,,,,n,,,,y,b,h,valuated by Gestalt psychotherapy trainees,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract)",y,Yes (trainee evaluations of AI vs. human) — Quote: “evaluated by Gestalt psychotherapy trainees using a Likert scale rating” (Position: Abstract),n,,
2nd_search_112,2nd_search_154,Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,3.0,2025.0,Other: Italy,Empirical research involving an LLM,Therapist-facing application,,,,Treatment fidelity feedback,No dataset used for development or evaluation,,,,,,,,Only prompting,"in a further chat interface (test 2), we
introduced the scope of expertise using a prompt as a pretraining tool, before proceeding
with the presentation of the case. This prompt was generated using ChatGPT, and its
reliability as a pretraining tool for the chat session was verified as a good alternative to
a longer and more complex training process in a previous study we conducted [12]","Unspecified, might include formal therapy methods; Other: Gestalt Therapy techniques, Gestalt supervision",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,b,h,qualified human supervisor,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_112,2nd_search_155,Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT,Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),17.0,3.0,2025.0,Other: Italy,Empirical research involving an LLM,Therapist-facing application,,,,Treatment fidelity feedback,No dataset used for development or evaluation,,,,,,,,Only prompting,"in a further chat interface (test 2), we
introduced the scope of expertise using a prompt as a pretraining tool, before proceeding
with the presentation of the case. This prompt was generated using ChatGPT, and its
reliability as a pretraining tool for the chat session was verified as a good alternative to
a longer and more complex training process in a previous study we conducted [12]","Unspecified, might include formal therapy methods; Other: Gestalt Therapy techniques, Gestalt supervision",GPT-4 / GPT-4o family,Yes,n,y,y,y,,"Satisfaction ratings focused on relational/emotional quality, technical/didactic, treatment support, professional orientation. — Quote: “PCA highlighted four components… relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4)” (Position: Abstract)",n,,,,n,,,,n,,,,n,,,,y,b,h,valuated by Gestalt psychotherapy trainees,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract)",n,,n,,
2nd_search_111,2nd_search_156,Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24.0,3.0,2025.0,Other: Korea,Empirical research involving an LLM,Therapist-facing application,,No clients/patients involved,,Treatment fidelity feedback,External data set," Korean Children Voice Records from
AI-Hub","Other: Speech data from children,  child
counseling service provided by the Professor Youjin Han’s
research lab",Other: Korean,No,Yes,Unselected,Trained professionals,Fine-tuning + other modules,,,,Yes,y,y,y,y,SEM,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_111,2nd_search_157,Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),24.0,3.0,2025.0,Other: Korea,Empirical research involving an LLM,Therapist-facing application,,"Other: ""Child counseling experts""",48,Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Prompting of different LLMs + other modules (speech-to-text, editable dashboard, etc.)","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,Yes,n,y,y,y,,User experience assessment of therapists using the tool. There is a quantiative (Likert scale questionnaires) and qualitative (open-ended user questions) assessment,n,,,,n,,,,n,,,,n,,,,y,b,l,"Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_111,2nd_search_158,Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),24.0,3.0,2025.0,Other: Korea,Empirical research involving an LLM,Therapist-facing application,,"Other: ""Child counseling experts""",48,Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Prompting of different LLMs + other modules (speech-to-text, editable dashboard, etc.)","Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family,Yes,n,y,y,y,,User experience assessment of therapists using the tool. There is a quantiative (Likert scale questionnaires) and qualitative (open-ended user questions) assessment,n,,,,n,,,,n,,,,n,,,,y,b,l,"Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_109,2nd_search_159,Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),11.0,9.0,2025.0,Other: Finland,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,70,,No dataset used for development or evaluation,,,,,,,,Only prompting,"GPT-4 outputs compared with curated expert book excerpts; randomized and blinded survey design. — Quote: “participants were blinded in terms of which advice was book-based and which was ChatGPT generated.” (Position: Methods, p. 36)","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,n,y,y,,"72.86% rated GPT-4 advice practical … 87.94% well-explained … statistically significant” (Position: Results, p. 41–42)

GPT-4 advice perceived as more practical and clearer; effect sizes small-to-medium. — Quote: “differences … statistically significant … d ≈ 0.34 … d ≈ 0.53 …” (Position: Results, p. 41–42)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_109,2nd_search_160,Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2.0,3.0,2025.0,Other: Finnland,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,70,,No dataset used for development or evaluation,,,,,,,,Only prompting,"For the ChatGPT-generated advice, version GPT-4 was used. On October 12, 2023, 
the following question was asked on a fresh chatbot session and the response was saved:
[...] Since the study’s objective was to evaluate performance of ChatGPT-4 with-
out any refinement or customization, we deliberately refrained from using any prompt 
engineering techniques in our design and opted to use the afore mentioned one-time 
instruction to the LLM. ","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,lay rating,b,h (?),Book written by a human expert.,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"„Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“",
2nd_search_109,2nd_search_161,Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",2.0,9.0,2024.0,Other: Finland,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,70,,No dataset used for development or evaluation,,,,,,,,Only prompting,"For the ChatGPT-generated advice, version GPT-4 was used. On October 12, 2023, 
the following question was asked on a fresh chatbot session and the response was saved:
[...] Since the study’s objective was to evaluate performance of ChatGPT-4 with-
out any refinement or customization, we deliberately refrained from using any prompt 
engineering techniques in our design and opted to use the afore mentioned one-time 
instruction to the LLM. ","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,n,y,y,,"72.86% rated GPT-4 advice practical … 87.94% well-explained … statistically significant” (Position: Results, p. 41–42)

GPT-4 advice perceived as more practical and clearer; effect sizes small-to-medium. — Quote: “differences … statistically significant … d ≈ 0.34 … d ≈ 0.53 …” (Position: Results, p. 41–42)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,lay rating,b,h,Book written by a human expert.,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"„Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“",
2nd_search_108,2nd_search_162,Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,12.0,2024.0,Other: SriLanka,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),unknown,,External data set,,Psychotherapy -- speech transcripts,Other: indian,Yes,No,Unselected,Trained professionals,Only fine-tuning,,CBT: Cognitive restructuring,GPT-3.5 family,No,n,n,n,n,,,n,,,,n,,,,y,,,no benchmark,n,,,,y,,,no benchmark,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_108,2nd_search_163,Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,12.0,2024.0,Other: Sri Lanka,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,cognitive distortion identification dataset,Other: Labeled examples of cognitive distortions,Other: unknown,Yes,No,Unknown,Other: not applicable,Prompting + other modules,GPT-3.5 is only prompted but there is also an intent classifier and cognitive distortion identifier,CBT: Cognitive restructuring,GPT-3.5 family; Other: text-embedding-3-small,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.",n,,,,y,no benchmark,no benchmark,Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_108,2nd_search_164,Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,12.0,2024.0,Other: Sri Lanka,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,cognitive distortion identification dataset,Other: Labeled examples of cognitive distortions,Other: unknown,Yes,No,Unknown,Other: not applicable,Prompting + other modules,GPT-3.5 is only prompted but there is also an intent classifier and cognitive distortion identifier,CBT: Cognitive restructuring,GPT-3.5 family; Other: text-embedding-3-small,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.",n,,,,y,no benchmark,no benchmark,Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_107,2nd_search_165,Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Other: unknown,Some documents for their RAG database but no further information is given.,,,,,,,Prompting + other modules,"Llama 2 is prompted, but there is also a RAG component","Unspecified, might include formal therapy methods",Llama 2 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.,n,,,,y,no benchmark,no benchmark,"Various classification metrics (accuracy, precision, recall, f1) for checking how ""relevant"" the responses are. No information is given on how ""relevance"" is measured here.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_107,2nd_search_166,Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),23.0,1.0,2024.0,India,Population survey,Client-facing application,Other: unclear,Other: unclear,unclear,,Other: unclear,,,,,,,,Prompting + other modules,,"Unspecified, might include formal therapy methods",Llama 2 family,Yes,n,y,y,n,,,y,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_107,2nd_search_167,Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Other: unclear,,,,,,,,Prompting + other modules,"Llama 2 is prompted, but there is also a RAG component","Unspecified, might include formal therapy methods",Llama 2 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.,n,,,,n,,,"Various classification metrics (accuracy, precision, recall, f1) for checking how ""relevant"" the responses are. No information is given on how ""relevance"" is measured here.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_106,2nd_search_168,Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),15.0,9.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Primate2022 https://github.com/primate-mh/Primate2022,Internet data -- mental health forum,English,No,Yes,Unselected,Other: not applicable,Prompting + other modules,Prompt engineering + RAG,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,"BLEU, ROUGE with counselor responses as reference (CounselChat)",y,no benchmark,no benchmark,Wonky measurement: embedding similarity between chatbot responses and patient inputs,n,,,,y,no benchmark,no benchmark,Comparison of chatbot-inferred PHQ-9 scores with human scorers via ICC and Cohen's kappa,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_106,2nd_search_169,Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),15.0,9.0,2024.0,Other: Canada ,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,"Evalution with Primate2022 dataset, but not for development ",,,,,,,Only fine-tuning,,"Unspecified, might include formal therapy methods",GPT-3.5 family,,n,n,n,n,,,y,,," BLEU [57] and ROUGE
[58] metrics",y,,,on average 0.93 (±0.03) similarity in the embedding space.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_106,2nd_search_170,Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),15.0,9.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Primate2022 https://github.com/primate-mh/Primate2022 (but only for evaluation),Internet data -- mental health forum,English,No,Yes,Unselected,"Other: not applicable (only initial posts, no responses)",Prompting + other modules,Prompt engineering + RAG,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,"BLEU, ROUGE with counselor responses as reference (CounselChat)",y,no benchmark,no benchmark,"on average 0.93 (±0.03) similarity in the embedding space.

Wonky measurement: embedding similarity between chatbot responses and patient inputs",n,,,,y,no benchmark,no benchmark,Comparison of chatbot-inferred PHQ-9 scores with human scorers via ICC and Cohen's kappa,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_104,2nd_search_171,Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,4.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,"The chatbot’s corpus comprises 200 manually curated context-response pairs that cover themes like stress, anxiety, depression, and motivation","Other: ""context-response pairs"" -- what is that?",Other: unknown,Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of all-MiniLM-L6-v2 for relevance detection + response generation by Gemini 1.5 Flash,"Unspecified, might include formal therapy methods",BERT family; Gemini / Bard family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,Classification into relevant/irrelevant via all-MiniLM-L6-v2,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,4) Crisis Intervention and Ethical Safeguards,n,,n,Gemini 1.5 Flash,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_104,2nd_search_172,Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,4.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,"The chatbot’s corpus comprises 200 manually curated
context-response pairs that cover themes like stress, anxiety, 
depression, and motivation",Other: Self curated ,,Yes,No,Other: no users in study ,,Prompting + other modules,"The proposed chatbot system aims to enhance mental health 
assistance through generative AI and embedding-based 
relevance detection. Unlike rule-based chatbots with 
predefined responses, our chatbot responds dynamically to 
diverse user inputs. The core innovations include few-shot 
learning for personalized conversation, semantic similarity 
filtering to ensure relevance, and generative response 
generation using Google Gemini API. ","Unspecified, might include formal therapy methods",Gemini / Bard family,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,"Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries) ",n,,,,n,,,,n,,,,n,,,,n,,,,"response quality assessments (evaluating coherence,
empathy, tone) ",,,,"user interaction studies (questionnaires and session 
reviews)",,,,n,,,,y,"The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response",y,"The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response1",n,,y,"To ensure ethical deployment, the system avoids medical 
advice, doesn’t store personal data, and complies with data 
protection standards. Sensitive inputs are not logged or 
reused. ",n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_104,2nd_search_173,Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",17.0,4.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,"The chatbot’s corpus comprises 200 manually curated context-response pairs that cover themes like stress, anxiety, depression, and motivation","Other: unclear (authors call it ""context-response pairs""",Other: unknown,Other: unknown,No,Unknown,Unknown,Prompting + other modules,"The proposed chatbot system aims to enhance mental health 
assistance through generative AI and embedding-based 
relevance detection. Unlike rule-based chatbots with 
predefined responses, our chatbot responds dynamically to 
diverse user inputs. The core innovations include few-shot 
learning for personalized conversation, semantic similarity 
filtering to ensure relevance, and generative response 
generation using Google Gemini API. ","Unspecified, might include formal therapy methods",BERT family; Gemini / Bard family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,"Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries)

Classification into relevant/irrelevant via all-MiniLM-L6-v2",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response

4) Crisis Intervention and Ethical Safeguards",n,,n,Gemini 1.5 Flash,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_103,2nd_search_174,Addressing the Challenges of Mental Health Conversations with Large Language Models,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",28.0,4.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,HOPE (Mental Health cOunselling of PatiEnts),Emotional support dialogue -- speech transcripts,English,No,Yes,Unselected,Unknown,Fine-tuning + other modules,"The authors adapted ChatMGL, a GPT-2–based model originally trained on STEM Q&A with supervised fine-tuning and PPO, for mental health dialogue. They dropped PPO, focusing instead on supervised fine-tuning with Hugging Face’s SFTT and Delta Tuning (a parameter-efficient method that updates only part of the weights). ","Unspecified, might include formal therapy methods",BERT family; GPT-2 family,No users involved,n,n,n,n,,,y,b,l,"METEOR and ROUGE tested, but Strawman models",y,b,l,,y,b,l,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_103,2nd_search_175,Addressing the Challenges of Mental Health Conversations with Large Language Models,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",28.0,5.0,2025.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,External data set,"We use the HOPE (Mental Health cOunselling of PatiEnts) [8]
dataset for our experiments. The dataset is specifically designed
for mental health counseling and dialogue-act classification tasks.
The HOPE dataset includes approximately 12,800 utterances ex-
tracted from 212 mental health counseling sessions, sourced from
publicly available YouTube videos. ",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,"To this end, we propose a set of mod-
ifications: supervised fine-tuning on a carefully designed mental
health dataset and incorporating dialogue-act labels that capture
the unique structure and emotional undercurrents of therapeutic
sessions. By evaluating these adaptations, we show how ChatMGL
can be transformed into a more context-aware and empathetic
system, capable of generating clinically relevant and supportive
responses","Informal counseling (e.g., emotional support conversation)",GPT-2 family,No users involved,n,n,n,n,,,y,b,l,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_103,2nd_search_176,Addressing the Challenges of Mental Health Conversations with Large Language Models,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",28.0,4.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"We use the HOPE (Mental Health cOunselling of PatiEnts) [8]
dataset for our experiments. The dataset is specifically designed
for mental health counseling and dialogue-act classification tasks.
The HOPE dataset includes approximately 12,800 utterances ex-
tracted from 212 mental health counseling sessions, sourced from
publicly available YouTube videos. ",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,"The authors adapted ChatMGL, a GPT-2–based model originally trained on STEM Q&A with supervised fine-tuning and PPO, for mental health dialogue. They dropped PPO, focusing instead on supervised fine-tuning with Hugging Face’s SFTT and Delta Tuning (a parameter-efficient method that updates only part of the weights). ","Unspecified, might include formal therapy methods",BERT family; GPT-2 family,No users involved,n,n,n,n,,,y,b,l,"METEOR and ROUGE tested, but Strawman models",y,b,l,,y,b,l,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_102,2nd_search_177,Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",10.0,3.0,2025.0,USA,Empirical research involving an LLM,Therapist-facing application,Multi-turn chatbot,,,Utterance suggestions,No dataset used for development or evaluation,,,,,,,,Only prompting,"Prompts were comprised of (1) context for the request, (2) how the model was meant to respond, 3) the specific request, and (4) the output format ",Other CBT techniques,GPT-4 / GPT-4o family,Yes,n,n,y,y,,"Two staff psychologists (EB, AJ) each reviewed all 
ChatGPT responses and evaluated them along the 
following dimensions: (1) task completion and (2 
degree to which input information was incorporated in the output.",n,,,,n,,,,n,,,,n,,,,y,w,h,human therapists,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_102,2nd_search_178,Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",10.0,3.0,2025.0,USA,Empirical research involving an LLM,Therapist-facing application,,No clients/patients involved,,Other: OCD exposure hierarchy generation,No dataset used for development or evaluation,,,,,,,,Only prompting,,Other CBT techniques,GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,w,h,benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_102,2nd_search_179,Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",10.0,3.0,2025.0,USA,Empirical research involving an LLM,Therapist-facing application,,,,Other: OCD exposure hierarchy generation (therapy material generation),No dataset used for development or evaluation,,,,,,,,Only prompting,"Prompts were comprised of (1) context for the request, (2) how the model was meant to respond, 3) the specific request, and (4) the output format ",Other CBT techniques,GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,w,h,benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_101,2nd_search_180,A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",21.0,5.0,2025.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,Used scripted prompts to elicit chatbot interactions and compared with therapists; coded with MULTI; non-naturalistic brief interactions. — Quote: “We also only examined brief interactions with scripted prompts… The interaction logs … were coded using the Multitheoretical List of Therapeutic Interventions” (Position: p. 12; PubMed Methods).,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified; Other: Pi, and Replika",No,n,n,n,n,,,n,,,,n,,,,y,-,h,“coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” ,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"significant gaps in chatbots’ crisis management abilities, including the absence of risk assessment and failure to refer users to lifesaving crisis hotlines” (Position: p. 12).",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_101,2nd_search_181,A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",21.0,5.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,,Other: Therapists,17,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Each chatbot was prompted in the most up-to-date
version...","Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Other: Chatbots Pi & Replika, Modell behind unclear ",No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,-,-,-,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.",y,"See e.g., ""Strong Framing of Opinions and Suggestions""",n,,n,,y,Table 1,n,,n,,n,,n,,n,,n,,n,,
2nd_search_101,2nd_search_182,A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",21.0,5.0,2025.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved; Other: Therapists,,,No dataset used for development or evaluation,,,,,,,,Only prompting,Used scripted prompts to elicit chatbot interactions and compared with therapists; coded with MULTI; non-naturalistic brief interactions. — Quote: “We also only examined brief interactions with scripted prompts… The interaction logs … were coded using the Multitheoretical List of Therapeutic Interventions” (Position: p. 12; PubMed Methods).,"Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Other: Pi, Replika",No,n,n,n,n,,,n,,,,n,,,,y,-,h,“coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” ,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.",y,"See e.g., ""Strong Framing of Opinions and Suggestions""",n,,n,,y,Table 1,n,,n,,n,,n,,n,,n,,n,,
2nd_search_100,2nd_search_183,"""Shaping ChatGPT into my Digital Therapist"": A thematic analysis of social media discourse on using generative artificial intelligence for mental health",Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10.0,7.0,2025.0,USA,Other: thematic analyses of social media data,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,,Internet data -- mental health forum,English,No,No,Unknown,Unknown,,,,,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_100,2nd_search_184,"""Shaping ChatGPT into my Digital Therapist"": A thematic analysis of social media discourse on using generative artificial intelligence for mental health",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10.0,7.0,2025.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,87 Reddit posts,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,"Qualitative analysis:
This approach involves six main steps: getting familiar with the data; generating initial codes; searching for themes; reviewing themes; defining and naming themes; and producing the report. The analysis process was iterative and recursive, allowing the researchers to move back and forth between these steps as needed to ensure thoroughness and validity. ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_100,2nd_search_185,"""Shaping ChatGPT into my Digital Therapist"": A thematic analysis of social media discourse on using generative artificial intelligence for mental health",Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10.0,7.0,2025.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,87 Reddit posts,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,"Qualitative analysis:
This approach involves six main steps: getting familiar with the data; generating initial codes; searching for themes; reviewing themes; defining and naming themes; and producing the report. The analysis process was iterative and recursive, allowing the researchers to move back and forth between these steps as needed to ensure thoroughness and validity. ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_99,2nd_search_186,The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",8.0,1.0,2025.0,USA,Empirical research involving an LLM,Therapist-facing application,Multi-turn chatbot,,,Patient simulations,Self-collected data,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,b,l,older and other LLM,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_99,2nd_search_187,The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",8.0,1.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family; Other: Microsoft Copilot,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,7 different safety questions,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,entire study is about this,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_99,2nd_search_188,The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",8.0,1.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Gemini / Bard family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,7 different safety questions,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,entire study is about this,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_98,2nd_search_189,Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),9.0,10.0,2024.0,Other: Indonesia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,58,,,data from Counsel Chat repository and Indonesian mental health articels,Emotional support dialogue -- chat logs,Other: English and Indonesian ,No,Yes,Unselected,Trained professionals,Only fine-tuning; Only prompting,"In this project, prompts were crafted based on common mental health issues such as  anxiety, stress, and academic pressure. 
Initially trained on a general mental health dataset, the model gained a basic understanding of mental health issues such as anxiety and depression. However, to address local language and cultural nuances, additional data from Indonesian sources, including articles and forums, were integrated.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,n,y,y,,"10 questions regarding user interface, ease of use, chatbot responsiveness, 
and overall satisfaction. might include standard instrument, but not described.",n,,,,n,,,,y,,,Accuracy,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_98,2nd_search_190,Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9.0,10.0,2024.0,Other: Indonesia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,58,,External data set,"Additionally, local data from Indonesian mental health articles and discussion forums fine-tuned the model.","Other: ""mental health articles and discussion forums""",Other: Indonesian,Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of GPT-4 + RAG (see Fig. 1) and user interface,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,n,y,y,,"A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.",n,,,,n,,,,y,no benchmark,no benchmark,"""training accuracy"" of generative LLM. Not sure what this accurac is measuring.",n,,,,n,,,,n,,,,n,,,,n,,,,user experience rating,no benchmark,no benchmark,"A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.",n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_98,2nd_search_191,Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9.0,10.0,2024.0,Other: Indonesia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,58,,External data set,"Additionally, local data from Indonesian mental health articles and discussion forums fine-tuned the model.","Other: ""mental health articles and discussion forums""",Other: Indonesian,No,No,Unknown,Unknown,Fine-tuning + other modules; Prompting + other modules,Fine-tuning of GPT-4 + RAG (see Fig. 1) and user interface,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,n,y,y,,"10 questions regarding user interface, ease of use, chatbot responsiveness, 
and overall satisfaction. might include standard instrument, but not described.",n,,,,n,,,,y,no benchmark,no benchmark,"""training accuracy"" of generative LLM. Not sure what this accurac is measuring.",n,,,,n,,,,n,,,,n,,,,n,,,,user experience rating,no benchmark,no benchmark,"A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.",n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_97,2nd_search_192,A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8.0,8.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,16,Utterance suggestions,Self-collected data,"collected a substantial dataset comprising 21,924 records
Fig. 2. Volunteer Screening Chart
through five distinct avenues: instances, media, literature,
guidelines, and databases. To ensure the integrity and rele-
vance of our data, we undertook a meticulous cleaning pro-
cess spearheaded by three experienced quality controllers.
Each controller boasts broad expertise in mental health
counseling. Initially, we eliminated duplicates and linguistic
inaccuracies. Subsequently, we focused on screening data
for key terms such as “sleep”, “dream”, “evening”, “night”,
“bed” and related expressions. This step aimed to sift out
conversational data irrelevant to our study’s objectives, en-
suring that the textual content was pertinent to the context
of potential sleep disorders. We further conducted dialog
integrity screening. Finally, through manual inspection on a
case-by-case basis, we excluded dialogue data that contra-
dicted universal core values. This rigorous curation process
resulted in the selection of 764 dialogues",Emotional support dialogue -- chat logs,Other: unknown,No,No,Unknown,Unknown,Fine-tuning + other modules,"ChatGLM model with LoRA fine-tuning, optimized using AdamW for 450 epochs. — Quote: “We optimized the model parameters using the AdamW optimizer… training process spanned 450 epochs.” (Position: Model Construction)","Unspecified, might include formal therapy methods",Other:  ChatGLM-LoRA,Yes,n,n,y,y,,High satisfaction (>80% smooth design); moderate effectiveness (~50% improved sleep); high acceptance (75% continued use). — Quote: “More than 80% of the volunteers think that our app is well designed and smooth to use” (Position: Application Utilization),y,b,l,metrics… including BLEU-4 and ROUGE” (Position: Model Evaluation),n,,,,y,b,l,evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,ChatGLM is open-source/self-hostable,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one."
2nd_search_97,2nd_search_193,A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8.0,8.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,16,,"External data set, modified","Between November 28 and December 14, 2023, our study collected a substantial dataset comprising 21,924 records through five distinct avenues: instances, media, literature,
guidelines, and database.",Other: Mixed,Other: unknown,No,No,Unknown,Unknown,Only fine-tuning,LoRA,"Unspecified, might include formal therapy methods",Other: ChatGLM,Yes,n,n,y,y,,See Application Utilization,y,-,-,-,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_97,2nd_search_194,A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8.0,8.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,16,,Self-collected data,"collected a substantial dataset comprising 21,924 records
Fig. 2. Volunteer Screening Chart
through five distinct avenues: instances, media, literature,
guidelines, and databases. To ensure the integrity and rele-
vance of our data, we undertook a meticulous cleaning pro-
cess spearheaded by three experienced quality controllers.
Each controller boasts broad expertise in mental health
counseling. Initially, we eliminated duplicates and linguistic
inaccuracies. Subsequently, we focused on screening data
for key terms such as “sleep”, “dream”, “evening”, “night”,
“bed” and related expressions. This step aimed to sift out
conversational data irrelevant to our study’s objectives, en-
suring that the textual content was pertinent to the context
of potential sleep disorders. We further conducted dialog
integrity screening. Finally, through manual inspection on a
case-by-case basis, we excluded dialogue data that contra-
dicted universal core values. This rigorous curation process
resulted in the selection of 764 dialogues",Other: Mixed,Other: unknown,No,No,Unknown,Unknown,Only fine-tuning,LoRA,"Unspecified, might include formal therapy methods",Other: ChatGLM,Yes,n,n,y,y,,High satisfaction (>80% smooth design); moderate effectiveness (~50% improved sleep); high acceptance (75% continued use). — Quote: “More than 80% of the volunteers think that our app is well designed and smooth to use” (Position: Application Utilization),y,-,-,-,n,,,,y,b,l,evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,ChatGLM is open-source/self-hostable,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one."
2nd_search_96,2nd_search_195,Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12.0,12.0,24.0,Other: Slovenia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: Convenience sample,42,,Self-collected data,"First, we introduce a novel, golden standard dataset, comprising panel 
data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires 
and qualitative daily diary entries. ",,Other: unknown,No,No,Unknown,Unknown,Fine-tuning + other modules,"The system uses a cognitive architecture that combines NLP, a Theory of Mind module with user modeling and forecasting, CBT-based expert knowledge, strategy control, and natural language generation.","Other CBT techniques; Unspecified, might include formal therapy methods; Other: A strategy is selected and adapted according to the metrics, determined in the previous part. The text serves to mitigate the user’s mental health problems based mostly on CBT, and, if the forecasted trend is negative, to try to break 
that trend. To ensure that the user follows the selected strategy, the text on CBT is wrapped in a persuasion strategy.","Other: Long-short term memory 
network (LSTM)?",Yes,y,n,y,y,"One of the ICAs on the 
7-point Likert scale using two measures from UEQ"," “Our system” (M = 5.368) found this work’s system statistically significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).",n,,,,n,,,,y,b,l,Woebot,y,b,l,Woebot,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.",y,"Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.",n,,n,,n,,n,,n,,n,,y,"Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair",n,,n,,n,,
2nd_search_96,2nd_search_196,Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12.0,12.0,2024.0,Other: Slovenia,Empirical research involving an LLM,,Multi-turn chatbot,General population,42,,Self-collected data,"495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores … and qualitative daily diary entries",Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) ,Other: unknown,No,No,Other: convenience sample,Unknown,Fine-tuning + other modules,"“cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies” (Position: Methods)","Unspecified, might include formal therapy methods","Other: highest accuracy: 91.41% using k-nearest neighbors (kNN),highest accuracy of other systems: 84% using … long-short term memory network (LSTM)",Yes,y,n,y,y,UEQ," “Our system” (M = 5.368) found this work’s system statistically 
significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the 
measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).",n,,,,n,,,,y,b,l,"Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation)",y,b,l,"Table 6 Accuracy of machine learning models in forecasting 7-day 
SAD (stress, anxiety, and depression) levels based on single text 
entries against extrapolation",n,,,,n,,,,n,,,,n,,,,"stress, anxiety, depression",b,l ,Woebot,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,comparison with Woebot — Quote: “system outperformed Woebot … in reducing stress … and anxiety levels” (Position: Results),n,,n,,
2nd_search_96,2nd_search_197,Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12.0,12.0,2024.0,Other: Slovenia,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,42,,Self-collected data,"First, we introduce a novel, golden standard dataset, comprising panel 
data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires 
and qualitative daily diary entries. ",Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) ,Other: unknown,No,No,Unknown,Other: n.a.,Fine-tuning + other modules,"“cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies” (Position: Methods)","Other CBT techniques; Unspecified, might include formal therapy methods","GPT-3 family; Other: GPT-Neo, GPT-J, AI21 Jurassic-1",Yes,y,n,y,y,"One of the ICAs on the 
7-point Likert scale using two measures from UEQ"," “Our system” (M = 5.368) found this work’s system statistically significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).",n,,,,n,,,,y,b,l,"Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation)",y,b,l,"Table 6 Accuracy of machine learning models in forecasting 7-day 
SAD (stress, anxiety, and depression) levels based on single text 
entries against extrapolation",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.",y,"Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.",n,,n,,n,,n,,n,,n,,y,"Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair",y,,n,,n,,
2nd_search_94,2nd_search_198,"LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour",Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",30.0,4.0,2025.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules",CBT: Motivational interviewing,GPT-4 / GPT-4o family,Yes,n,y,y,y,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"client evaluation of motivational interviewing scale (cemi) (madson et al., 2013, 2015, 2016)",s,l,without MI fine-tuning,"working alliance inventory - short
revised (wai-sr) ",s,l,without MI fine-tuning,𝛥 readiness to change,s,l,without MI fine-tuning,y,"We identify harmful outputs in two ways: First, we take participant’s
ratings of generated turns into account, specifically with respect to
turns rated as offensive/harmful. Secondly, we draw on MI-literature as
well as empathy research and ethical academic discussions about LLMs
outlined below to identify behaviours which are unsuited specifically
in the context of LLM-delivered MI-conversations. Based on this, we
create an annotation scheme to be applied to the logged conversations after data collection. This annotation scheme goes beyond the
identification of MI-Inadherent behaviours defined in the MISC (listed
in Table 1). While such behaviours are not suited for MI interactions
in general, we lay special emphasis on such actions that would be
suitable in MI-interactions with a human counsellor, but should be
avoided when responses are generated by LLMs. We focus specifically
on significant concerns around anthropomorphism and the unreliable
factual correctness of LLM outputs",y,"To test the validity of the annotation scheme, the first author
annotated each GPT-4 generated turn collected in the user study along
the 4 dimensions of undesired behaviour outlined above, allowing
multiple labels per turn. Based on the annotation scheme, GPT-outputs
containing advice, factual information, sharing personal information, or
revealing emotions, as well as outputs with directive or confrontational
wording were marked as harmful. Following this, a second annotator
(not an author) was employed to annotate a share of the turns along
the four axes. 50 randomly sampled turns, along with the guidelines,
were given to the annotator for annotation in a training round.",n,,y,in the schema,n,,n,,n,,n,,n,,y,"either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle",n,,n,,
2nd_search_94,2nd_search_199,"LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour",Richard Gaus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),30.0,4.0,2025.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.",CBT: Motivational interviewing,GPT-4 / GPT-4o family,Yes,y,y,y,y,German version of the Working Alliance Inventory - short revised (WAI-SR),"Also measuring undesirable output via annotation.
For qualitative results, see 4.3.3.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"client evaluation of motivational interviewing scale (cemi), readiness to change delta",s,l,Measures MI adherence of clients. Benchmark: GPT-4 out of the box,number of harmful outputs (user-rated),b,l,Benchmark: GPT-4 out of the box,german version of the working alliance inventory - short revised (wai-sr),s,l,Benchmark: GPT-4 out of the box,n,,n,,n,,n,,y,,n,,n,,n,,y,,y,Control: GPT-4 out-of-the-box,n,,n,,
2nd_search_94,2nd_search_200,"LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour",Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),30.0,4.0,2025.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,159,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.

MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules",CBT: Motivational interviewing,GPT-4 / GPT-4o family,Yes,n,y,y,y,,"Also measuring undesirable output via annotation.
For qualitative results, see 4.3.3.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"client evaluation of motivational interviewing scale (cemi) (madson et al., 2013, 2015, 2016); readiness to change delta",s,l,Measures MI adherence of clients. Benchmark: GPT-4 out of the box without MI fine-tuning,number of harmful outputs (user-rated),b,l,Benchmark: GPT-4 out of the box without MI fine-tuning,german version of the working alliance inventory - short revised (wai-sr),s,l,Benchmark: GPT-4 out of the box without MI fine-tuning,n,,n,,n,,n,,y,,n,,n,,n,,n,WAI-SR but is neither symptom nor function scale,y,"either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle

Control: GPT-4 out-of-the-box",n,,n,,
2nd_search_93,2nd_search_201,Early Detection and Personalized Intervention in Mental Health,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16.0,4.0,2025.0,India,Empirical research involving an LLM,Analysis of conversation transcripts,Multi-turn chatbot,No clients/patients involved,,,External data set,epsilon3/cbt-cognitive-distortions-analysis,Psychotherapy -- chat logs,English,Yes,Yes,Unknown,,Only fine-tuning,,Other CBT techniques,GPT-2 family,No users involved,n,n,n,n,,,y,unsure,l,Bleu score,n,,,,y,unsure,l,,n,,,,n,,,,n,,,,y,w,l,,n,,,,empathy score,b,l,,human helpfulness,b,l,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_93,2nd_search_202,Early Detection and Personalized Intervention in Mental Health,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14.0,2.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"epsilon3/cbt-cognitive-distortions-analysis dataset
https://huggingface.co/datasets/epsilon3/cbt-cognitive-distortions-analysis
",Internet data -- mental health Q&A,English,Yes,Yes,Unknown,Other: not applicable,Fine-tuning + other modules,Fine-tuning of GPT-2 + other modules such as BERT-based cognitive distortion classification,CBT: Cognitive restructuring,BERT family; GPT-2 family,No users involved,n,n,n,n,,Only quantitative scoring of ,y,b,l,BLEU against human responses. Benchmark: non fine-tuned GPT-2,n,,,,n,,,Performance of cognitive distortion classification of non-generative BERT model is given (but it's non-generative),n,,,,n,,,,n,,,,y,b,l,Benchmark: non fine-tuned GPT-2,n,,,,user rating,b,l,"User scores of ""helpfulness"" and ""empathy"". Benchmark: non fine-tuned GPT-2.",n,,,,n,,,,y,"The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.",n,,y,GPT-2,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_93,2nd_search_203,Early Detection and Personalized Intervention in Mental Health,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",14.0,2.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"epsilon3/cbt-cognitive-distortions-analysis dataset
https://huggingface.co/datasets/epsilon3/cbt-cognitive-distortions-analysis
",Internet data -- mental health Q&A,English,Yes,Yes,Unknown,Other: not applicable,Fine-tuning + other modules,Fine-tuning of GPT-2 + other modules such as BERT-based cognitive distortion classification,CBT: Cognitive restructuring,BERT family; GPT-2 family,No users involved,n,n,n,n,,Only quantitative scoring of ,y,b,l,BLEU against human responses. Benchmark: non fine-tuned GPT-2,n,,,,y,no benchmark,no benchmark,Performance of cognitive distortion classification of non-generative BERT model is given (but it's non-generative),n,,,,n,,,,n,,,,y,b,l,Benchmark: non fine-tuned GPT-2,n,,,,user rating,b,l,"User scores of ""helpfulness"" and ""empathy"". Benchmark: non fine-tuned GPT-2.",n,,,,n,,,,y,"The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.",n,,y,GPT-2,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_92,2nd_search_204,Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",2.0,7.0,2025.0,USA,Population survey,Client-facing application,Multi-turn chatbot,No clients/patients involved,0,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Evaluated existing GPT-based chatbots via persona-based simulated interactions, scored with CAPE framework. ","Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,n,y,n,??CAPE Framework — Quote: “The CAPE framework lays a foundation for future quality assessments” (Position: Discussion),"Strengths = accessibility, safety in most cases; Weaknesses = occasional advertising, reading level too high, missing referral for severe depression. ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y?,"(suicidality referral present, severe depression detection failed) — Quote: “the ability of the chatbots to always recommend connecting to another person for users expressing suicidality … frequent failure to connect personas exhibiting severe depression to a human",y,we did not find any unsafe recommendations for managing depression or wellness,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots."
2nd_search_92,2nd_search_205,Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",2.0,7.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: Researchers,2,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.",y,"Chatbots mostly preserved privacy and avoided harmful
content. However",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_92,2nd_search_206,Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",2.0,7.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Evaluated existing GPT-based chatbots via persona-based simulated interactions, scored with CAPE framework. ","Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"CAPE category ratings, no benchmark",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.",y,"Chatbots mostly preserved privacy and avoided harmful
content. ",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots."
2nd_search_91,2nd_search_207,“Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26.0,4.0,2025.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),30,,No dataset used for development or evaluation,,,,,,,,Fine-tuning + other modules,"Used fine-tuning and modular architecture (context, expression, feedback generators) to simulate different therapeutic styles.
Three key technical components were employed: the Context Generator, the Expression Expander, and the Feedback Generator","Informal counseling (e.g., emotional support conversation)","Other: MiniMax 6.5s (245K), Doubao Function Call model (32K) ",Yes,y,y,y,y,SAM-Scale,"emotional valence and dominance improved; deliberate style increased happiness, rapid style increased surprise. ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,y?,Sam Scale?,n,,n,,n,,
2nd_search_91,2nd_search_208,“Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26.0,4.0,2025.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,30,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Despited being labeled as fine-tuning, I suppose it is more about promtpting and other modules (Context Generator, Expression Expander, and the Feedback Generator)?","Unspecified, might include formal therapy methods",Other:  MiniMax 6.5s(245K),No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,y,Self-Assessment Manikin Scale + heart rate variability,n,,n,,n,,
2nd_search_91,2nd_search_209,“Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),26.0,4.0,2025.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility; People with some symptoms but not disorder (determined by symptom scales or questionnaires),30,,No dataset used for development or evaluation,,,,,,,,Fine-tuning + other modules,"Used fine-tuning and modular architecture (context, expression, feedback generators) to simulate different therapeutic styles.
Three key technical components were employed: the Context Generator, the Expression Expander, and the Feedback Generator.

Not clear if they really fine-tuned.","Unspecified, might include formal therapy methods","Other: MiniMax 6.5s (245K), Doubao Function Call model (32K) ",No,n,n,n,n,SAM-Scale,"emotional valence and dominance improved; deliberate style increased happiness, rapid style increased surprise. ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,y,(Self-Assessment Manikin Scale)+ Heart Rate variability,n,,n,,n,,
2nd_search_90,2nd_search_210,"Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19.0,1.0,2025.0,Other: Iran,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients with disorder explicitly based on ICD or DSM,9,,External data set,"ArmanEmo dataset, a Persian emotion detection dataset",Emotional support dialogue -- speech transcripts,Other: Persian,No,Yes,Unknown,Unknown,Prompting + other modules,"the proposed system integrates several modules: emotion detection, disorder identification, and language model validation see figure 1 ","Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods",BERT family,No,n,n,n,n,,,n,,,,n,,,,y?,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,see 4.1,y,"Responses are validated for non-toxicity before being delivered to
the user.",y,"ParsBERT, XLM-R are open-source",n,,n,,n,,n,,n,,n,,n,,n,,n,,"This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy."
2nd_search_90,2nd_search_211,"Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19.0,1.0,2025.0,Other: Iran,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,9,,Other: External data set + a modified (translated) data set,ArmanEmo & Jigsaw Toxic Comment Classification translated into persian,Other: Emotion detection dataset and classification dataset,Other: Persian,No,Yes,Unknown,Other: No responders,Only fine-tuning; Prompting + other modules,Trained on ArmanEmo dataset & Jigsaw Toxic Comment Classification translated into persian + user message validiation module. ,"Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family,Yes,-,y,y,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users",n,,n,,n,,n,,n,,n,,y,,n,,n,,n,,
2nd_search_90,2nd_search_212,"Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",19.0,1.0,2025.0,Other: Iran,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),9,,External data set,ArmanEmo,Internet data -- mental health forum,Other: Persian,No,Yes,Unknown,Lay people,Only fine-tuning; Prompting + other modules,Trained on ArmanEmo dataset & Jigsaw Toxic Comment Classification translated into persian + user message validiation module. ,"Unspecified, might include formal therapy methods",BERT family; GPT-4 / GPT-4o family,Yes,-,y,y,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,see 4.1,y,"In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users",n,,n,,n,,n,,n,,n,,y,PHQ-9,n,,n,,n,,"This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy."
2nd_search_89,2nd_search_213,Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21.0,6.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Designed prompt phrases, iteratively refined; three prompting regimes (zero-shot, few-shot, CoT); modeled counselor reasoning style; two LLM families evaluated (ChatGLM, ERNIE Bot; also Qianwen)","Unspecified, might include formal therapy methods","Qwen family; Other: ChatGLM, ERNIE Bot, Qianwen (as model and as judge)",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,b,l,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,"	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.)",n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_89,2nd_search_214,Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21.0,6.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,Patient simulations,Self-collected data,"We gathered doctor-
patient conversations from online medical consultation web-
sites and identified representative psychological issues from
the experiences of people around us and public platforms
such as Weibo and Zhihu. Our dataset encompasses the most
common mental health concerns and A substantial portion of
the themes is centered on stress and interpersonal relationships,
as these two are important contributing factors to mental
disorders. We also considered the perspectives of the LGBTQ
community and diverse cultural groups.Utilizing this data, we
crafted 31 unique questions.",,Chinese,No,No,Unselected,Unknown,Only prompting,"Three different prompt methods: Zero-Shot, Few-Shot, Chain of Thought (CoT)","Informal counseling (e.g., emotional support conversation)","Other: ERNIE Bot, Qianwen, and ChatGLM",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_89,2nd_search_215,Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21.0,6.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,Patient simulations,No dataset used for development or evaluation,,,,,,,,Only prompting,"Designed prompt phrases, iteratively refined; three prompting regimes (zero-shot, few-shot, CoT); modeled counselor reasoning style; two LLM families evaluated (ChatGLM, ERNIE Bot; also Qianwen)","Unspecified, might include formal therapy methods","Qwen family; Other: ChatGLM, ERNIE Bot, Qianwen (as model and as judge)",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,b,l,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),n,,,,n,,,,llm-as-a-judge emotional resonance and understanding,b,l,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),llm-as-a-judge professionalism etc.,b,l,Qianwen to automatically evaluate the counseling dialogue (esction: C. ),n,,,,n,,n,,y,"	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.)",n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_88,2nd_search_216,The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",26.0,5.0,2025.0,Other: Iran,Other: Case study,Client-facing application,Multi-turn chatbot,Patients with disorder explicitly based on ICD or DSM,1,,No dataset used for development or evaluation,,,,,,,,Other: None,,"Unspecified, might include formal therapy methods",Other: unspecified,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_88,2nd_search_217,The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",26.0,5.0,2025.0,Other: Iran,Other: Case study,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),1,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,"This article is a case study with loose, qualitative descriptions",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_88,2nd_search_218,The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",26.0,5.0,2025.0,Other: Iran,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),1,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,"This article is a case study with loose, qualitative descriptions",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_87,2nd_search_219,Evaluating Language Models for Assessing Counselor Reflections,Reviewer Two,,,30.0,12.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,,,,Utterance suggestions,Other: both self-collected and external data set ,,Psychotherapy -- speech transcripts,English,Yes,No,Unknown,Trained professionals,Fine-tuning + other modules; Prompting + other modules,,CBT: Motivational interviewing,BERT family; GPT-3.5 family; Mistral family; Other: Flan ,No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,Recall,y,s,l,"Pearson, Spearman, Kendall’s Tau",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_87,2nd_search_220,Evaluating Language Models for Assessing Counselor Reflections,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,12.0,2024.0,USA,Empirical research involving an LLM,Therapist-facing application,,,,Treatment fidelity feedback,"External data set, modified","Original dataset: Perez-Rosas motivational interviewing dataset.
Modification: Added custom annnotations
Also extended with their own MI prompts.
We thus obtained 4,365 client prompt-counselor reflection pairs, including 2,429 prompt-CR and 1,636 prompt-SR pairs.

public under https://lit.eecs.umich.edu/downloads.html#PAIR",Other: MI reflections (client prompt - counselor reflection pairs),English,Yes,Yes,Unknown,Trained professionals,Only prompting,"They fine-tune RoBERTa but only prompt the generative models (Flan-t5, Mistral, GPT-3.5) that they use",CBT: Motivational interviewing,BERT family; T5 family; GPT-3.5 family; Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,yes for pair but this is not genAI,y,w,l,"Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_87,2nd_search_221,Evaluating Language Models for Assessing Counselor Reflections,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,12.0,2024.0,USA,Empirical research involving an LLM,Therapist-facing application,,,,Treatment fidelity feedback,"External data set, modified","Original dataset: Perez-Rosas motivational interviewing dataset.
Modification: Added custom annnotations
Also extended with their own MI prompts.
We thus obtained 4,365 client prompt-counselor reflection pairs, including 2,429 prompt-CR and 1,636 prompt-SR pairs.

public under https://lit.eecs.umich.edu/downloads.html#PAIR",Other: MI reflections (client prompt - counselor reflection pairs),English,Yes,No,Unknown,Trained professionals,Only prompting,"They fine-tune RoBERTa but only prompt the generative models (Flan-t5, Mistral, GPT-3.5) that they use",CBT: Motivational interviewing,BERT family; T5 family; GPT-3.5 family; Mistral family,No users involved,n,n,n,n,,,n,,,,n,,,,y,w,l,Recall@1 (Tables 3 and 4),y,w,l,"Pearson, Spearman, Kendall’s Tau. Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_85,2nd_search_222,Can AI relate: Testing large language model response for mental health support,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),12.0,11.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,not mentioned ,,External data set," Reddit with of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.",Other: Reddit Posts,English,No,No,Unknown,Unknown,Only prompting,,"Peer support conversation; Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,s,h,,n,,,,n,,,,n,,,,epit-one framework,b,h,,miti,w,h,,empathy metrics ,s,h,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"used metrics: EPIT-ONE framework, MITI, RoVERTa"
2nd_search_85,2nd_search_223,Can AI relate: Testing large language model response for mental health support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,11.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"No specific name: ""We use the Reddit API to collect
a new dataset of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.""

Available under https://github.com/skgabriel/mh-eval",Internet data -- mental health forum,English,No,Yes,Unselected,Lay people,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,b,h,"Measures: Likert empathy rating, MITI global score. Benchmark: human peer supporter from Reddit post.",n,,,,n,,,,n,,,,empathy rating unequality,no benchmark,no benchmark,no benchmark. Measure: difference in automated empathy ratings between different ethnic groups,n,,,,n,,,,n,,n,,n,,n,,y,Appendix Table 2,y,,n,,n,,n,,n,,n,,n,,
2nd_search_85,2nd_search_224,Can AI relate: Testing large language model response for mental health support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",12.0,11.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"No specific name: ""We use the Reddit API to collect
a new dataset of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits.""

Available under https://github.com/skgabriel/mh-eval",Internet data -- mental health forum,English,No,Yes,Unselected,Lay people,Only prompting,,"Peer support conversation; Unspecified, might include formal therapy methods",GPT-3.5 family; GPT-4 / GPT-4o family; Llama 2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,b,h,"Measures: Likert empathy rating, MITI global score. Benchmark: human peer supporter from Reddit post.",n,,,,n,,,,n,,,,empathy rating unequality,unknown,l,"Table 1: mechanical turk worker ratings (""Human"") vs. GPT-4",n,,,,n,,,,n,,n,,n,,n,,y,Appendix Table 2,y,,n,,n,,n,,n,,n,,n,,"used metrics: EPIT-ONE framework, MITI, RoVERTa"
2nd_search_83,2nd_search_225,Multimodal Framework for Therapeutic Consultations,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_83,2nd_search_226,Multimodal Framework for Therapeutic Consultations,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_83,2nd_search_227,Multimodal Framework for Therapeutic Consultations,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_82,2nd_search_228,Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",20.0,11.0,2024.0,Other: Turkey,Empirical research involving an LLM,Client-facing application,Other: Emotional reflection generation,Other: unknown recruitment criteria,200,,No dataset used for development or evaluation,,,,,,,,Only prompting,"The model was trained using one-shot 
learning, meaning that it was given a client statement and 
a counselor’s emotional reflection response as input, and 
then used that information to generate its own emotional 
reflection response.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,w,h,likert-scale appropriateness rating of emotional reflection responses. compared GPT-4 against human counselor responses.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_82,2nd_search_229,Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",20.0,11.0,2024.0,Other: Turkey,Empirical research involving an LLM,Analysis of conversation transcripts,One-turn chatbot (usually Q&A),No clients/patients involved,,,Other: emotional expressions from psychological counseling sessions,,Psychotherapy -- speech transcripts,,No,No,Unknown,Trained professionals,Only prompting,"no development, only prompting ","Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"average rating of emotional reflections 
",w,h,human therapists ,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_82,2nd_search_230,Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",20.0,11.0,2024.0,Other: Turkey,Empirical research involving an LLM,Client-facing application,Other: Emotional reflection generation,No clients/patients involved,,,Self-collected data,"In this study, 200 real counseling ses
sions were recorded and transcribed to capture authentic 
client interactions. From these, 50 client statements were 
identified that best represented a range of emotions such as 
sadness, guilt, anxiety, determination, and anger, based on 
the PANAS Scale, along with helplessness, love, trust, lone
liness, and doubt from existing literature, aiming to ensure

emotional expressions from psychological counseling sessions",Psychotherapy -- speech transcripts,Other: unknown,No,No,Other: unknown,Trained professionals,Only prompting,"The model was trained using one-shot 
learning, meaning that it was given a client statement and 
a counselor’s emotional reflection response as input, and 
then used that information to generate its own emotional 
reflection response.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,w,h,likert-scale appropriateness rating of emotional reflection responses. compared GPT-4 against human counselor responses.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_81,2nd_search_231,Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,Richard Gaus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3.0,12.0,2024.0,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_81,2nd_search_232,Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,Reviewer Two,Conference paper,,3.0,6.0,2024.0,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,Not specified,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Employs an ""Empathetic Meta-Chain (EMC) learning method"" that combines meta-learning, Chain of Thought prompting, and counseling strategies. Multiple approaches were compared including zero-shot, few-shot, CoT, meta-learning, and EMC.","Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,,,"We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_81,2nd_search_233,Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques,Consensus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3.0,12.0,2024.0,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,"We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance

Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_80,2nd_search_234,Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*,Reviewer Two,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",7.0,12.0,2023.0,Other: Spain,Empirical research involving an LLM,"Other: question-answering system based on
retrieval augmented generation — this approach allows the
system to generate answers based on a corpus of documents
curated by psychologists and psychiatrists",One-turn chatbot (usually Q&A),No clients/patients involved,,,Self-collected data,,Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists,Other: spanish,No,No,,,Fine-tuning + other modules,Retrieval Augmented Generation (RAG) kombiniert mit prompting,"Unspecified, might include formal therapy methods",BERT family,No,n,n,n,n,,,n,,,,y,b,l,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_80,2nd_search_235,Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*,Richard Gaus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",7.0,12.0,2023.0,Other: Spain,Empirical research involving an LLM,,,,,,External data set,"The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG",Other: Mental health education documents,Other: Spanish,No,No,Unknown,Unknown,,,,BERT family; GPT-2 family; Other: BLOOMZ,No users involved,n,n,n,n,,,n,,,,y,no benchmark,no benchmark,Distance between embeddings of reference and output.,y,no benchmark,no benchmark,"F1 score, not quiet clear what of.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_80,2nd_search_236,Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*,Consensus,Conference paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",7.0,12.0,2023.0,Other: Spain,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists

The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG",Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists,Other: Spanish,No,No,Other: not applicable,Other: not applicable,Prompting + other modules,Retrieval Augmented Generation (RAG) kombiniert mit prompting,"Unspecified, might include formal therapy methods",BERT family; GPT-2 family; Other: BLOOMZ,No users involved,n,n,n,n,,,n,,,,y,no benchmark,no benchmark,Distance between embeddings of reference and output.,y,no benchmark,no benchmark,"F1 score, not quiet clear what of.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_75,2nd_search_237,Conversational ai for mental health support,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",25.0,4.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,data from Kaggle,Emotional support dialogue -- chat logs,Other: Unsure,No,Yes,Unknown,Unknown,Only fine-tuning,,"Unspecified, might include formal therapy methods",BERT family; Other: LTSM,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,,
2nd_search_75,2nd_search_238,Conversational ai for mental health support,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25.0,4.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"Data from various sources (""Simple conversations, psychological questions, information on classic therapy sessions, and general advice for people with anxiety and depression. This data can be used to train a chatbot model that can act as a therapist to assist patients with anxiety and depression. The dataset includes intents. An ""intent"" is the purpose of a user's message. There are a total of 89 such intents, some of which are “greeting, sad, stressed, worthless, depressed, happy, anxious, sleep, scared, hate, problem” etc."").

Seemingly for intent classification.",Other: unknown,Other: unknown,Other: unknown,No,Unknown,Unknown,Only fine-tuning,"Fine-tuning of BERT, likely for intent classification. Not clear how responses are generated -- via LSTM?","Unspecified, might include formal therapy methods",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,Confusion matrices for the BERT and LSTM models (Fig. 7 and 8),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_75,2nd_search_239,Conversational ai for mental health support,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25.0,4.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"Data from various sources (""Simple conversations, psychological questions, information on classic therapy sessions, and general advice for people with anxiety and depression. This data can be used to train a chatbot model that can act as a therapist to assist patients with anxiety and depression. The dataset includes intents. An ""intent"" is the purpose of a user's message. There are a total of 89 such intents, some of which are “greeting, sad, stressed, worthless, depressed, happy, anxious, sleep, scared, hate, problem” etc."").

Seemingly for intent classification.",,,,,Unknown,Unknown,Only fine-tuning,,"Unspecified, might include formal therapy methods",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_72,2nd_search_240,AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",28.0,2.0,2025.0,China,Other: Qualitative Study ,Client-facing application,Multi-turn chatbot,General population,69,,Self-collected data,qualitative data from 69 students,Other: qualitative interviews,Chinese,No,No,Unselected,,,,"Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,n,y,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_72,2nd_search_241,AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28.0,2.0,2025.0,China,Population survey,Client-facing application,Multi-turn chatbot,General population,69,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods",Other: various GenAI chatbots,Yes,n,y,n,y,,"This study employed a deductive thematic analysis, using the conceptual framework of Grodniewicz and Hohol’s (2023) three challenges in AI psychotherapy as a general guide",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_72,2nd_search_242,AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28.0,2.0,2025.0,China,Population survey,Client-facing application,Multi-turn chatbot,General population,69,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Other: various GenAI chatbots,Yes,n,y,n,y,,"This study employed a deductive thematic analysis, using the conceptual framework of Grodniewicz and Hohol’s (2023) three challenges in AI psychotherapy as a general guide",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_71,2nd_search_243,Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6.0,6.0,2025.0,Other: Switzerland,Empirical research involving an LLM,Therapist-facing application,Multi-turn chatbot,,20,Patient simulations,No dataset used for development or evaluation,,,,,,Unselected,Trained professionals,Only prompting,prompted to simulate a psychiatric first-responder chatbot,"Unspecified, might include formal therapy methods",GPT-3.5 family,Yes,n,n,y,y,,"10-point Likert scale rating the overall experience, the pleasantness of the chat, the appropriateness of the responses, the realism of the chatbot, and the helpfulness of the advice
Yes/No/ dont know:
1. It would be used by patients. (Hypothesized patients’ subjective norm)
2. It would be recommended by outpatient psychiatrists (Subjective norm)
3. It would relieve the psychiatric emergency department. (Anticipated
benefit)
4. It would be helpful for patients in
a crisis situation. (Expectancy of
usefulness)",n,,,,n,,,,n,,,,n,,,,y,b,l,compared to chatbots in custumer service,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,y,"Table 1 Study population characteristics
Male 
Total 5 
Age (26–30 years) 4;
(31–35 years) 1
Years of training 2.6 years (range:
1–3 years)
Swiss nationality/other nationality (EU) 0/5 
Psychiatry/other specialty/psychologist 5/0/0 
Female
Total 15
(26–30 years) 1; (31–35 years) 8;
(36–40 years) 5; (> 40 years) 1
2.4 years (range: 1–7 years)
Swiss nationality/other nationality (EU) 7/8
Psychiatry/other specialty/psychologist 4/4/7",n,,n,,n,,n,,n,,n,,y,"Primarily, as text-based systems, LLMs
cannot interpret crucial nonverbal cues
and they lack cultural sensitivity, which
is essential for a comprehensive mental
health assessment [2]. Secondly, research
shows that users tend to over-rely on the
accuracy of information delivered by LLMs,
which may discourage patients from seeking timely professional mental health assistance rather than facilitating it [5, 24].
Thirdly, technical challenges, such as AI
hallucination and insufficient technological literacy among users or connectivity,
further compromise the accessibility and
reliability of these systems. Fourthly, given
thelimited regulationof AI, the storageand
potential use of sensitive mental health
data for LLM training pose significant data
privacy concerns, underscoring the need
for robust legal and ethical frameworks. Finally, LLMs lack real-time safety protocols,
such as direct emergency service contact,
presenting a critical limitation when users
are in immediate danger.",
2nd_search_71,2nd_search_244,Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6.0,6.0,2025.0,Other: Switzerland,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_71,2nd_search_245,Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",6.0,6.0,2025.0,Other: Switzerland,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,prompted to simulate a psychiatric first-responder chatbot,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_69,2nd_search_246,Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",4.0,11.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,,,,Self-collected data,,Internet data -- mental health forum,English,No,No,Unknown,Unknown,Fine-tuning + other modules,fine tuning + sentiment analysis module,CBT: Cognitive restructuring,GPT-3.5 family,Yes,n,n,y,y,,,n,,,,n,,,,y,b,l,"between the rule-based sentiment analysis module and the
sentiment analysis capabilities of the GPT-powered model",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.",y," Conducted thorough testing of the
chatbot’s responses (Fig. 6) to various user queries and
scenarios to assess its ability to uphold ethical standards
and protect user privacy. This testing involved simulating
user interactions with the chatbot and evaluating its
responses for any instances of inappropriate or unethical behavior, such as providing inaccurate information,
breaching confidentiality, or engaging in discriminatory
practices. Any identified issues were promptly addressed
and remediated to ensure that the chatbot consistently
adheres to ethical guidelines and respects user privacy.",n,,y," Conducted thorough testing of the
chatbot’s responses (Fig. 6) to various user queries and
scenarios to assess its ability to uphold ethical standards
and protect user privacy. This testing involved simulating
user interactions with the chatbot and evaluating its
responses for any instances of inappropriate or unethical behavior, such as providing inaccurate information,
breaching confidentiality, or engaging in discriminatory
practices. Any identified issues were promptly addressed
and remediated to ensure that the chatbot consistently
adheres to ethical guidelines and respects user privacy.",n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_69,2nd_search_247,Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",4.0,11.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","Data Preparation: The sentiment analysis model is
trained on an annotated text classification dataset after
undergoing extensive cleaning, normalization, and tok-
enization processes to ensure the quality and integrity of
the data. The datasets used are compiled from a multitude
of sources, including a text classification repository of so-
cial media tweets, restaurant reviews, and airline reviews.
Data was also extracted from the Reddit API across vari-
ous forums such as those focused on CBT, mental health,
therapy, anxiety, depression, and CPTSD. Initially, the
dataset comprised 3,000 data points, with a text column
serving as the input and a target column providing labeled
sentiment. Recognizing the importance of data volume
in enhancing the accuracy and robustness of machine
learning models, we expand the dataset (TABLE I) to encompass 12,221 data points

for RAG",Other: Mixed,English,Other: Unknown,No,Other: n.a.,Other: n.a.,Fine-tuning + other modules,It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot,Other CBT techniques,GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,y,no benchmark,no benchmark,Performance of the sentiment classification component.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_69,2nd_search_248,Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",4.0,11.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,unknown,,"External data set, modified","Data Preparation: The sentiment analysis model is
trained on an annotated text classification dataset after
undergoing extensive cleaning, normalization, and tok-
enization processes to ensure the quality and integrity of
the data. The datasets used are compiled from a multitude
of sources, including a text classification repository of so-
cial media tweets, restaurant reviews, and airline reviews.
Data was also extracted from the Reddit API across vari-
ous forums such as those focused on CBT, mental health,
therapy, anxiety, depression, and CPTSD. Initially, the
dataset comprised 3,000 data points, with a text column
serving as the input and a target column providing labeled
sentiment. Recognizing the importance of data volume
in enhancing the accuracy and robustness of machine
learning models, we expand the dataset (TABLE I) to encompass 12,221 data points

for RAG","Other: Other: statements, labeled with sentiments (sentiment analysis data)",English,No,No,Other: n.a.,Other: n.a.,Fine-tuning + other modules,"It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot

fine tuning + sentiment analysis module",CBT: Cognitive restructuring,GPT-3.5 family,Yes,n,n,y,y,,See V.B.,n,,,,n,,,,y,b,l,"between the rule-based sentiment analysis module and the
sentiment analysis capabilities of the GPT-powered model",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_68,2nd_search_249,Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",18.0,10.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Other: Development of new Framework,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Other: Assessment of OCD-Coach,FAITA-mental health chatbot was designed to classify OCD-Coach's responses,Other CBT techniques,"ChatGPT, model unspecified",Yes,y,y,y,y,FAITA-Mental Health,"Readability high (Flesch-Kincaid 10.8), messages long; insufficient cultural tailoring; transparency reactive not proactive; crisis responses generic (no local numbers) — Quote: “10.8 on the Flesch-Kincaid …”; “did not make an attempt to ask Sam about her cultural identity variables”; “grant … a score of 1 … provides details upon request, [but] does not offer full, immediate … disclosure”; “no specific numbers or links were provided … score of 1 … general recommendations but no local resources” (Position: p. 8–9)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"“After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9)",y,“OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9),n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"Future research should explore how this tool can be integrated into the decision-making processes of mental health professionals, health care organizations, and technology developers.” (Position: p. 10)",
2nd_search_68,2nd_search_250,Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",18.0,10.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,Study describes FAITA Mental Health Framework and evaluates OCD Coach as an existing LLM-based Tool,,,,,,,Other: N/A,,"Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"With regard to crisis response, when Sam reported suicidal
ideation, OCD Coach directed her to emergency services or a crisis hotline “in [her] area” (Subdomain 2: Interactivity Quality)",y,"Subdomain 2: Evidence-Based Content
",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_68,2nd_search_251,Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",18.0,10.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,Study describes FAITA Mental Health Framework and evaluates OCD Coach as an existing LLM-based Tool,,,,,,,Only prompting,FAITA-mental health chatbot was designed to classify OCD-Coach's responses,"Other CBT techniques; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"“After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9)",y,“OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9),n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_66,2nd_search_252,Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_66,2nd_search_253,Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_66,2nd_search_254,Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_64,2nd_search_255,"Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study",Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",25.0,2.0,2025.0,Other: japan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,5,,,,,,,,,,Only prompting,"two age-appropriate AI chatbots, leveraging GPT-4, were developed to provide natural, empathetic conversations","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,somewhat,somewhat,y,,"All participants noted improved treatment motivation … 80% disclosing personal concerns … 24/7 availability particularly benefited patients
Four out of five participants reported significant reductions in anxiety and stress levels post-intervention” (Results, p. 1)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_64,2nd_search_256,"Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",25.0,2.0,2025.0,Other: Japan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"Other: Pediatric and adolescent and
young adult (AYA) patients with cancer",5,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,y,y,y,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.",y,"Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.",n,,n,,y,Table 2,n,,n,,n,,n,,n,,n,,n,,
2nd_search_64,2nd_search_257,"Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study",Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",25.0,2.0,2025.0,Other: Japan,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,5,,No dataset used for development or evaluation,,,,,,,,Only prompting,"two age-appropriate AI chatbots, leveraging GPT-4, were developed to provide natural, empathetic conversations","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,y,y,y,,"All participants noted improved treatment motivation … 80% disclosing personal concerns … 24/7 availability particularly benefited patients
Four out of five participants reported significant reductions in anxiety and stress levels post-intervention” (Results, p. 1)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.",y,"Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.",n,,n,,y,Table 2,n,,n,,n,,n,,n,,n,,n,,
2nd_search_63,2nd_search_258,Developing a single-session outcome measure using natural language processing on digital mental health transcripts,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",30.0,5.0,2024.0,UK,Empirical research involving an LLM,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_63,2nd_search_259,Developing a single-session outcome measure using natural language processing on digital mental health transcripts,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",30.0,5.0,2024.0,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"The data were provided and 
anonymised by Qwell",Emotional support dialogue -- chat logs,English,No,No,Unknown,Other: Mixed,Other: None of these ,They used a pre-trained RoBERTa model via Hugging Face to analyze text patterns without mentioning any fine-tuning or prompt engineering. The model was applied directly to classify and interpret data.,"Unspecified, might include formal therapy methods",BERT family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,content validity index,-,-,-,n,,,,n,,,,n,,n,,y,RoBERTa should be on-premise-capable,n,,n,,n,,n,,n,,n,,n,,y,content validity with clinicians and SUs to improve the relevance and clarity of the items,n,,
2nd_search_63,2nd_search_260,Developing a single-session outcome measure using natural language processing on digital mental health transcripts,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",30.0,5.0,2024.0,UK,Empirical research involving an LLM,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_62,2nd_search_261,Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories,Richard Gaus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24.0,7.0,2024.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,"External data set, modified","1,000 QA pairs from the PsyQA dataset.  two human annotators inspected and labeled the responses of professional supporters in the PsyQA dataset with the corresponding professional psychological counseling theory categories (Cognitive-Behavioral Therapy labeled as 1, Dialectical Behavior Therapy as 2, Person-Centered Therapy as 3, and Reality Therapy as 4). The original data had quantities of 189, 317, 196, and 298 for categories 1, 2, 3, and 4, respectively.",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Other: Mix of trained and lay,Fine-tuning + other modules,"Prompting of general purpose LLMs (stage 1, probably ChatGPT) + fine-tuning of BERT (stage 2) + some type of RAG based on SimCSE (stage 3)",Mix of formal therapy methods,"BERT family; ChatGPT, model unspecified",No users involved,n,n,n,n,,,y,b,l,"Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.",y,b,l,"PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.",n,,,,n,,,,y,b,l,"Fluency, helpfulness, relevance, empathy, professionalism, evaluated by psychology graduate students on a 5-Likert scale. Benchmark are other LLM-based methods.",n,,,,n,,,,y,s,l,Dist-2. Benchmark are other LLM-based methods.,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_62,2nd_search_262,Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories,Reviewer Two,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24.0,7.0,2024.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"PsyQA dataset (Sun et al., 2021). ",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Other: Well-trained volunteers or professional counselors = mixed,Fine-tuning + other modules; Prompting + other modules,Prompting for emotional analysis + fine-tuning BERT for therapy classification + semantic retrieval with SimCSE for relevant case examples.,"Unspecified, might include formal therapy methods","BERT family; ChatGPT, model unspecified",No users involved,n,n,n,n,,,y,b,l,"Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",y,b,l,"Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",n,,,,n,,,,y,b,h,"To evaluate the quality of generated supportive responses, we
also conduct a human evaluation. We recruit 12 graduate stu-
dents, major in psychology, to annotate the responses.",n,,,,n,,,,y,"b
",l,"Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_62,2nd_search_263,Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories,Consensus,Conference paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24.0,7.0,2024.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,"External data set, modified","1,000 QA pairs from the PsyQA dataset.  two human annotators inspected and labeled the responses of professional supporters in the PsyQA dataset with the corresponding professional psychological counseling theory categories (Cognitive-Behavioral Therapy labeled as 1, Dialectical Behavior Therapy as 2, Person-Centered Therapy as 3, and Reality Therapy as 4). The original data had quantities of 189, 317, 196, and 298 for categories 1, 2, 3, and 4, respectively.",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Other: Mix of trained and lay,Fine-tuning + other modules; Prompting + other modules,"Prompting of general purpose LLMs (stage 1, probably ChatGPT) + fine-tuning of BERT (stage 2) + some type of RAG based on SimCSE (stage 3)",Mix of formal therapy methods,"BERT family; ChatGPT, model unspecified",No users involved,n,n,n,n,,,y,b,l,"Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",y,b,l,"PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",n,,,,n,,,,y,b,l,"Fluency, helpfulness, relevance, empathy, professionalism, evaluated by psychology graduate students on a 5-Likert scale. Benchmark are other LLM-based methods.",n,,,,n,,,,y,s,l,"Dist-2. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).",n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_60,2nd_search_264,Conversational Agents for Dementia using Large Language Models,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,9.0,2023.0,Other: mexico,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Informal counseling (e.g., emotional support conversation)",Other: unclear,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_60,2nd_search_265,Conversational Agents for Dementia using Large Language Models,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,9.0,2023.0,Other: Mexico,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,Prompt engineering to shape ChatGPT’s dialogue,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_60,2nd_search_266,Conversational Agents for Dementia using Large Language Models,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,9.0,2023.0,Other: Mexico,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,Prompt engineering to shape ChatGPT’s dialogue,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_58,2nd_search_267,Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",24.0,10.0,2024.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,"Other: Digitial Standardized Patients- no collection of data, only creation via GPT 3.5","By using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
These characteristics encompassed a spectrum of ages (40 and
80 years old), genders (male, female, transgender male, and
transgender female), races (non-Hispanic black, non-Hispanic
white, and Hispanic), and annual income ($25,000, $50,000,
$500,000, and $1,000,000). We also generated a control group
without assigned identities. Each DSP represented a unique
mix of these demographic characteristics.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses",Psychotherapy -- chat logs,English,Yes,No,Psychopathology,Trained professionals,Only prompting,"o systematically test
the conversational agent’s responses for potential bias, we
developed an experimental setup where the conversational
agent interacted with digital standardized patients (DSPs)
exhibiting symptoms of anxiety or depression. DSPs were
GPT-3.5-enabled chatbots prompted to emulate a patient
with depression and anxiety seeking professional help from
a therapist. This was achieved by prompting GPT-3.5 to:
“Pretend that you are [first_name], a human client speaking to
a therapist. You are NOT the therapist; you are the clienBy using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses",Other CBT techniques,GPT-3.5 family; GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,average tone,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_58,2nd_search_268,Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",24.0,10.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Other: Spoken dialog system,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Their intervention: The AI therapist tested in this study is an LLM-based pro- gram previously developed and tested at Cedars-Sinai Medical Center, with the goal of offering AI-enabled, self-
administered, mental health support within immersive virtual
reality environments.18 The user interacts with an AI conver-
sational agent designed for history-taking and therapeutic sup-
port for anxiety and depression. The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
The agent facilitates turn-based conversations, summarizing
and concluding each session, and was found to be an accepta-
ble and feasible support modality for patients with mild-to-
moderate anxiety or depression.18 Here, we focus on whether
there is evidence of any sociodemographic bias exhibited by
the conversational agent.",Other CBT techniques,GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,liwc score domains and linguistic marker scores,no benchmark,no benchmark,no benchmark,n,,,,n,,,,y,"from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.",n,,n,,y,"The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).",y,"yes, in simulated DSPs (digital standardized patient)",y,"yes, in simulated DSPs (digital standardized patient)",n,,n,,n,,n,,n,,n,,
2nd_search_58,2nd_search_269,Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",24.0,10.0,2024.0,USA,Empirical research involving an LLM,Client-facing application,Other: Spoken dialog system,No clients/patients involved,,,No dataset used for development or evaluation,"By using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
These characteristics encompassed a spectrum of ages (40 and
80 years old), genders (male, female, transgender male, and
transgender female), races (non-Hispanic black, non-Hispanic
white, and Hispanic), and annual income ($25,000, $50,000,
$500,000, and $1,000,000). We also generated a control group
without assigned identities. Each DSP represented a unique
mix of these demographic characteristics.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses",,,,,,,Prompting + other modules,"Their intervention: The AI therapist tested in this study is an LLM-based pro- gram previously developed and tested at Cedars-Sinai Medical Center, with the goal of offering AI-enabled, self-
administered, mental health support within immersive virtual
reality environments.18 The user interacts with an AI conver-
sational agent designed for history-taking and therapeutic sup-
port for anxiety and depression. The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
The agent facilitates turn-based conversations, summarizing
and concluding each session, and was found to be an accepta-
ble and feasible support modality for patients with mild-to-
moderate anxiety or depression.18 Here, we focus on whether
there is evidence of any sociodemographic bias exhibited by
the conversational agent.",Other CBT techniques,GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,liwc score domains and linguistic marker scores,no benchmark,no benchmark,no benchmark,average tone (sentiment score),no benchmark,no benchmark,no benchmark,n,,,,y,"from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.",y,"The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses",n,,y,"The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).",y,"yes, in simulated DSPs (digital standardized patient)",y,"yes, in simulated DSPs (digital standardized patient)",n,,n,,n,,n,,n,,n,,
2nd_search_57,2nd_search_270,AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28.0,3.0,2025.0,Other: korea,Empirical research involving an LLM,Therapist-facing application,,,,Other: self efficacy enhancement,Self-collected data,,,,No,No,Unselected,Lay people,Only prompting,ChatGPT played the role of a client and optionally provided AI-generated feedback to trainees,"Informal counseling (e.g., emotional support conversation); Other: counselor-training simulation",GPT-4 / GPT-4o family,Yes,n,y,y,y,,Self-efficacy improved across sessions; anxiety reduced mainly between Session 1 and 2. — Quote: same as quantitative assessment line,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_57,2nd_search_271,AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction,Richard Gaus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28.0,3.0,2025.0,Other: Korea,Empirical research involving an LLM,Therapist-facing application,,,,Treatment fidelity feedback,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,y,y,y,y,CSQ and PU,"Qualitative assessment only implicitly mentioned in the discussion section, where user experiences are reported.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"Although the risk was minimal in the present study because
the client was fictional, it may not be completely credible in real
situations involving real clients. Counselor educators would thus
have to be cautious about using AI to directly generate feedback
for the trainees.",n,,n,,n,,n,,n,,n,,n,,y,CASES & STAI,y,"The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.",n,,n,,
2nd_search_57,2nd_search_272,AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",28.0,3.0,2025.0,Other: Korea,Empirical research involving an LLM,Therapist-facing application,,,,Treatment fidelity feedback,No dataset used for development or evaluation,,,,,,,,Only prompting,ChatGPT played the role of a client and optionally provided AI-generated feedback to trainees,"Unspecified, might include formal therapy methods; Other: counselor-training simulation",GPT-4 / GPT-4o family,Yes,y,y,y,y,CSQ and PU,Self-efficacy improved across sessions; anxiety reduced mainly between Session 1 and 2. — Quote: same as quantitative assessment line,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,(CASES &) STAI,n,"The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.",n,,n,,
2nd_search_54,2nd_search_273,"""I Don't Understand It, but Okay"": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models",Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",21.0,3.0,2025.0,USA,Population survey,Therapist-facing application,Multi-turn chatbot,Other: Mental health practitioners,21,Other: No specific application,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,"This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_54,2nd_search_274,"""I Don't Understand It, but Okay"": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models",Reviewer Two,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),21.0,3.0,2025.0,USA,Population survey,Therapist-facing application,,,,Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,"Both, user experience assessment and participant attitude assessment. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview. ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Despite it being labeled an ""empirical study"", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. "
2nd_search_54,2nd_search_275,"""I Don't Understand It, but Okay"": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models",Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),21.0,3.0,2025.0,USA,Population survey,Therapist-facing application,,Other: Mental health practitioners,21,Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs. No one particular application type.,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,"Both, user experience assessment and participant attitude assessment.

This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Despite it being labeled an ""empirical study"", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. "
2nd_search_51,2nd_search_276,Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",25.0,1.0,2025.0,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues,Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Lay people,Fine-tuning + other modules,"BERT and GPT-3.5 fine-tuned, interacting with each other","Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,n,n,n,n,,,y,s,l,"BLEU, ROUGE against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.",y,s,l,BLEUScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.,y,b,l,Various classification metrics of BERT emotional distress detection. Benchmark: Bi-LSTM,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_51,2nd_search_277,Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25.0,1.0,2025.0,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"Multiple external datasets used: 
a) From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger. 
b) From Bertagnolli [53], including high-quality therapist responses to real patient´s mental health questions.","Other: a) Crowdsourced dialogues
b) Internet data -- mental health Q&A",English,No,Yes,Unknown,"Other: a) Unknown 
b) Trained professionals (""verified therapists"")",Fine-tuning + other modules; Prompting + other modules,"The system integrates two main components: a BERT-based emotional distress detection module and a fine-tuned GPT-3.5 model for Psychological First Aid (PFA) response generation.
First, BERT analyzes the user’s input to detect their emotional state and distress level. These outputs are then passed, along with the original user input, into a custom prompt designed for GPT-3.5. GPT-3.5 was fine-tuned on therapy transcripts to improve its ability to produce empathetic, context-aware PFA responses.","Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,n,n,n,n,,,y,s,l,Against chatGPT-3.5,y,b,l,Against chatGPT-3.5,y,b,l,Bi-LSTM for emotional distress detection,n,,,,n,,,,n,,,,n,,,,n,,,,"relevance, personalisation, pfa (advice and guidance)professional referral, and validation and empathy",b,?,In the text there is no statement on how or by whom this was evaluated. ,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_51,2nd_search_278,Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",25.0,1.0,2025.0,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues

From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger.",Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Lay people,Fine-tuning + other modules; Prompting + other modules,"The system integrates two main components: a BERT-based emotional distress detection module and a fine-tuned GPT-3.5 model for Psychological First Aid (PFA) response generation.
First, BERT analyzes the user’s input to detect their emotional state and distress level. These outputs are then passed, along with the original user input, into a custom prompt designed for GPT-3.5. GPT-3.5 was fine-tuned on therapy transcripts to improve its ability to produce empathetic, context-aware PFA responses.","Unspecified, might include formal therapy methods",BERT family; GPT-3.5 family,No users involved,n,n,n,n,,,y,s,l,"BLEU, ROUGE against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.",y,s,l,BERTScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.,y,b,l,Various classification metrics of BERT emotional distress detection. Benchmark: Bi-LSTM,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_47,2nd_search_279,Digital Risk Considerations Across Generative AI-based Mental Health Apps,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),10.0,12.0,2023.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,1000000 user reviews,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods",Other: various LLMs,Yes,n,y,n,y,,"BERTopic analysis of app reviews, qualitative only",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_47,2nd_search_280,Digital Risk Considerations Across Generative AI-based Mental Health Apps,Reviewer Two,Other: Workshop paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),10.0,12.0,2023.0,USA,Other: Observational study (user-generated reviews) with unsupervised text mining,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods",Other: unspecified,Yes,n,y,n,y,,Qualitative analysis of user reviews focusing on risks,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_47,2nd_search_281,Digital Risk Considerations Across Generative AI-based Mental Health Apps,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),10.0,12.0,2023.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,1000000 user reviews,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods",Other: various LLMs,Yes,n,y,n,y,,Qualitative analysis of user reviews focusing on risks using BERTopic,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_46,2nd_search_282,"Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,3.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. ",Psychotherapy -- speech transcripts,English,No,Yes,Unknown,Trained professionals,Prompting + other modules,Retrieval + hierarchical fusion + attention + lexicon-enriched embeddings used to prompt LLMs.,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Llama 2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"empathy
 coherence 
informativeness 
fluency",b,l,,n,,,,n,,,,n,,n,,y (some),Llama and DeepSeek are,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_46,2nd_search_283,"Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,3.0,2025.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,No clients/patients involved,,,External data set,"The primary dataset used in this study consists of psychotherapy transcripts from
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. ",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,"''We introduce Emotion-Aware Embedding Fusion, a novel framework integrating hierarchical fusion and attention mechanisms'' ... ","Unspecified, might include formal therapy methods",T5 family; GPT-4 / GPT-4o family; Llama 2 family; Other: DeepSeek-R1,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_46,2nd_search_284,"Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation",Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",13.0,3.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"The primary dataset used in this study consists of psychotherapy transcripts from
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. ",Psychotherapy -- speech transcripts,English,No,Yes,Psychopathology,Trained professionals,Fine-tuning + other modules,Retrieval + hierarchical fusion + attention + lexicon-enriched embeddings used to prompt LLMs.,"Unspecified, might include formal therapy methods",T5 family; GPT-4 / GPT-4o family; Llama 2 family; Other: DeepSeek-R1,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,automatic empathy rating,b,l,they study integration of empathy lexicon into different LLMs. benchmark are those LLMs without empathy lexicon integration.,"coherence, informativeness, fluency",w,l,they study integration of empathy lexicon into different LLMs. benchmark are those LLMs without empathy lexicon integration.,n,,,,n,,n,,y,Llama and DeepSeek are,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_45,2nd_search_285,Expert Patient Interaction Language Model (EPILM),Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16.0,12.0,2024.0,India,Empirical research involving an LLM,Analysis of conversation transcripts,Other: Unknown but I assume both multiturn and one turn for the forum answers.,General population,,,External data set,"[COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. 

WebMD, Mayo Clinic and Heatlhline.com 
- One text exchange between a patient and their therapist is included in this dataset. The dataset was assembled from online FAQs, WebMD, Mayo Clinic, and HealthLine, among other well-known healthcare blogs",Other: Emotional support dialogue and Internet data,English,No,Other: Unknown as the datasets are not fully described,Unknown,Trained professionals,Fine-tuning + other modules,"The proposed method involves enhancing the 
Falcon-7B model through the application of quantization low rank adaptation (Q-LoRA). ","Unspecified, might include formal therapy methods",BERT family,No users involved,n,n,n,n,,,n,,,,y,b,l,"BERT Score, Sentence Transformer score",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_45,2nd_search_286,Expert Patient Interaction Language Model (EPILM),Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,,,,External data set,CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat,Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only fine-tuning,Q-LoRA fine-tuning,"Unspecified, might include formal therapy methods",Other: Falcon-7B,No users involved,n,n,n,n,,,n,,,,y,b,l,benchmark is GPT-3.5. measure is BERTScore and sentence transformer score with dataset expert responses as reference,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,falcon,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_45,2nd_search_287,Expert Patient Interaction Language Model (EPILM),Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",16.0,12.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat

[COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. ",Internet data -- mental health Q&A,English,No,Yes,Unselected,Trained professionals,Only fine-tuning,Q-LoRA fine-tuning,"Unspecified, might include formal therapy methods",Other: Falcon-7B,No users involved,n,n,n,n,,,n,,,,y,b,l,benchmark is GPT-3.5. measure is BERTScore and sentence transformer score with dataset expert responses as reference,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,falcon,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_43,2nd_search_288,Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",27.0,11.0,2024.0,Other: Iraq,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,Alzheimer's Q&A dataset https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning of GPT-2 + memory mechanism (storing previous embeddings in an array),"Unspecified, might include formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,BLEU with reference,n,,,,n,,,,y,no benchmark,no benchmark,Train loss,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_43,2nd_search_289,Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model,Reviewer Two,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",27.0,11.0,2024.0,Other: Iraq,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately! ",Internet data -- mental health Q&A,English,No,"Other: probably yes, but name not specified. ",Unknown,Other: Probably mixed,Fine-tuning + other modules; Prompting + other modules,"Developed by fine-tuning a GPT-2 model on a domain-specific dataset and integrating it into a larger architecture that included array-based context storage for multi-turn dialogue, Top-K/Top-P sampling for controlled text generation, and response ranking modules using BLEU scores and cosine similarity to select the most relevant and coherent reply.","Unspecified, might include formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,-,-,-,y,-,-,-,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,computational performance/ training metrics,-,-,-,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n ,,
2nd_search_43,2nd_search_290,Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",27.0,11.0,2024.0,Other: Iraq,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately!

But user has other dataset that fits description: https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Fine-tuning + other modules,"Developed by fine-tuning a GPT-2 model on a domain-specific dataset and integrating it into a larger architecture that included array-based context storage for multi-turn dialogue, Top-K/Top-P sampling for controlled text generation, and response ranking modules using BLEU scores and cosine similarity to select the most relevant and coherent reply.","Unspecified, might include formal therapy methods",GPT-2 family,No users involved,n,n,n,n,,,y,no benchmark,no benchmark,BLEU with reference,n,,,,n,,,,y,no benchmark,no benchmark,Train loss,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n ,,
2nd_search_42,2nd_search_291,"The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4.0,2.0,25.0,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,1,,No dataset used for development or evaluation,,,,,,,,Only prompting,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,,n,y,n,y,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc",y,"This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_42,2nd_search_292,"The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis",Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4.0,2.0,2025.0,India,"Other: theoretical,clinical, and ethical analysis",Client-facing application,Other: image generator,No clients/patients involved,,,,,,,,,,,Only prompting,"The research team, consisting of an AI product manager and
prompt engineer (OP), several expert psychologists (YH, KG, ZE,
and DHS), and a music therapist (TA), jointly developed the two
AI tools from December 2023 to January 2024. Initially, a
primary prompt was created by YH. This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests
and added operational instructions to ensure safety and ethics.
Subsequently, group members with clinical backgrounds (YH,
KG, ZE, TA) tested the tool on themselves and shared their
experiences and suggestions for improvements. Throughout this
process, the team realized that due to the tools’ depth and their
ability to reflect on unconscious internal parts, the presence of a
mental health care professional would be essential. An expert in
social psychology, DHS, addressed cultural issues, which were
also incorporated into the prompt. Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications.","Psychodynamic psychotherapy; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,y,n,y,,"Results reported: Yes (case-level outcomes) — Quote: “the AI successfully managed to … become more experientially aligned with the patient” (Position: Clinical section, p. 11)

Empathic failures, uncanny feelings, corrective alignment described — Quote: “empathic failure occurred … however, the ability to overcome empathic failures … holds significant therapeutic value” (Position: Clinical section, p. 11)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"Y (SAFE-AI protocol includes screening, monitoring, and evaluation) — Quote: “SAFE-AI protocol (Screening, Alignment, Facilitation, and Evaluation of AI-Enhanced Interventions) provides systematic guidelines … including consent, alliance monitoring, empathic failures” (Position: Section 4.7, p. 13)",y,"(recommend therapist-side implementation) — Quote: “Therefore, these tools should be used on the therapist’s computer … ensuring anonymity for the patient.” ",y,"Quote: “Patients must be fully informed about how their data will be used and must provide explicit consent … robust data protection measures are paramount” (Position: Section 4.2, p. 12)",n,,n,,n,,n,,n,,n,,somewhat," Quote: “therapist, patient, and AI agent … proof-of-concept in this article illustrates … therapeutic value” (Position: Clinical section, p. 11)",somewhat,"“SAFE-AI protocol … provides systematic guidelines for implementing AI-based externalization techniques … while ensuring cultural sensitivity, gender representation, and therapeutic authenticity.”",
2nd_search_42,2nd_search_293,"The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis",Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",4.0,2.0,2025.0,Other: Israel,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: unknown,1,,No dataset used for development or evaluation,,,,,,,,Only prompting,"The research team, consisting of an AI product manager and
prompt engineer (OP), several expert psychologists (YH, KG, ZE,
and DHS), and a music therapist (TA), jointly developed the two
AI tools from December 2023 to January 2024. Initially, a
primary prompt was created by YH. This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests
and added operational instructions to ensure safety and ethics.
Subsequently, group members with clinical backgrounds (YH,
KG, ZE, TA) tested the tool on themselves and shared their
experiences and suggestions for improvements. Throughout this
process, the team realized that due to the tools’ depth and their
ability to reflect on unconscious internal parts, the presence of a
mental health care professional would be essential. An expert in
social psychology, DHS, addressed cultural issues, which were
also incorporated into the prompt. Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications.","Psychodynamic psychotherapy; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,y,n,y,,"Results reported: Yes (case-level outcomes) — Quote: “the AI successfully managed to … become more experientially aligned with the patient” (Position: Clinical section, p. 11)

Empathic failures, uncanny feelings, corrective alignment described — Quote: “empathic failure occurred … however, the ability to overcome empathic failures … holds significant therapeutic value” (Position: Clinical section, p. 11)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_40,2nd_search_294,"Multimodal Integration, Fine Tuning of Large Language Model for Autism Support",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",,,,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,"The research begins by curating a specialized User and
Counsellor Interaction CSV dataset by querying an autism
counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source
for reducing bias and adding a human touch. Alongside the
primary dataset, the additional data points ensure a compre-
hensive understanding of the domain.",Other: unclear,,,,,,Fine-tuning + other modules,Fine-tuned LLM + integration of other data modalities (vision),"Other: ""Autism counselling""",Other: Falcon-7B,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_40,2nd_search_295,"Multimodal Integration, Fine Tuning of Large Language Model for Autism Support",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,1.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,,"Other: The research begins by curating a specialized User and Counsellor Interaction CSV dataset by querying an autism counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source for reducing bias and adding a human touch. ","Other: unknown, probably english or hindi",No,No,Unknown,Trained professionals,Only fine-tuning,Fine-tuned with PEFT-LoRA,"Unspecified, might include formal therapy methods",Other: Falcon7B,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,Falcon7B ,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_40,2nd_search_296,"Multimodal Integration, Fine Tuning of Large Language Model for Autism Support",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",18.0,1.0,2024.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,"The research begins by curating a specialized User and
Counsellor Interaction CSV dataset by querying an autism
counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source
for reducing bias and adding a human touch. Alongside the
primary dataset, the additional data points ensure a compre-
hensive understanding of the domain.",Other: unclear,"Other: unknown, probably english or hindi",Other: unknown,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuned LLM + integration of other data modalities (vision),"Unspecified, might include formal therapy methods",Other: Falcon-7B,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,Falcon7B ,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_39,2nd_search_297,Use of AI in mental health care: Community and mental health professionals survey,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",11.0,10.0,2024.0,Other: Australia,Population survey,Other: Client-facing AND therapist-facing,Other: No specific subtype (AI use in general),General population,107 community members AND 86 mental health providers,Other: No specific subtype (AI use in general),No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","Other: AI in general, mostly ChatGPT",Yes,y,y,y,y,AI attitude scale (AIAS-4): doi: 10.3389/fpsyg.2023.1191628,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_39,2nd_search_298,Use of AI in mental health care: Community and mental health professionals survey,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",,,,Other: Australia,Population survey,Other: survey,,General population,193 — Quote: “final sample consisted of 107 CMs and 86 MHPs”,,Self-collected data,,,English,No,No,Unselected,Other: both lay and pros,,,Other: no intervention,Other: none,Yes,n,y,y,y,,"See section: Technology Comfort, AI Attitudes, and AI
Use Intention",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,see ux,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,,y,,
2nd_search_39,2nd_search_299,Use of AI in mental health care: Community and mental health professionals survey,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",11.0,10.0,2024.0,Other: Australia,Population survey,Other: Client-facing AND therapist-facing,Other: No specific subtype (AI use in general),General population,193 — Quote: “final sample consisted of 107 CMs and 86 MHPs”,Other: No specific subtype (AI use in general),No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","Other: AI in general, mostly ChatGPT",Yes,y,y,y,y,AI attitude scale (AIAS-4): doi: 10.3389/fpsyg.2023.1191628,"See section: Technology Comfort, AI Attitudes, and AI
Use Intention",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,both CMs and MHPs were surveyed,n,,
2nd_search_34,2nd_search_300,Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21.0,3.0,2025.0,China,Empirical research involving an LLM,Analysis of conversation transcripts,Multi-turn chatbot,No clients/patients involved,,,External data set,EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed),Psychotherapy -- speech transcripts,Chinese,,Yes,,Trained professionals,Any reinforcement learning; Other: proposed a dynamic adversarial test method based on cross-variation” (Position: Abstract in excerpt),"Cross-mutation/crossover to swap context & emotion while fixing behavior; compute BSI & CR; chi-square tests — Quote: “behavior-anchored crossover-mutation… fixes the behavior dimension, swaps context and emotion… Core metrics: Behavior Sensitivity Index (BSI)… Consistency Rate (CR)… Chi-square test”","Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Claude family; Other: selected four widely-used LLMs … DeepSeek, GPT-4, Claude, Wenxin-Yiyan",No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,Models classified cases requiring intervention; BSI>1 and higher CR indicate better behavior sensitivity/consistency; DeepSeek/Wenxin/Claude > GPT-4. — Quote: “BSI and CR … DeepSeek 1.0662 / 0.8985 … GPT-4 1.0415 / 0.8250,n,,,,n,,,,y,w,l,Each LLM’s own “intervention decision” used to compute metrics. — Quote: “intervention decisions recorded and results labeled … quantitative analysis uses BSI and CR”,n,,,,n,,,,n,,,,n,,,,n,,,,y,detect the ability of LLMs to recognize extreme behaviors … behaviors covering high-risk actions,y,evaluate … ability to intervene ethically … provide data-driven insights for optimizing safety mechanisms,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_34,2nd_search_301,Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21.0,3.0,2025.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified",selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.,Other: Roleplay and sharing/microblogging data,Chinese,No,No,Unknown,Unknown,Only prompting,"The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice.",Other,GPT-4 / GPT-4o family; Claude family; Other: Deepseek & Wenxin - Yiyan,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,behavior sensitivity index,-,l,No Indexmodel; comparison of multiple LLMs,consistency ratio,-,l,No Indexmodel; comparison of multiple LLMs,n,,,,n,,n,,y & n ,"Some are, others don´t",n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_34,2nd_search_302,Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",21.0,3.0,2025.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.

EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed)","Other: test cases for ""psychological counseling scenarios"" involving extreme behaviors",Chinese,No,No,Unselected,Unknown,Only prompting,"The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice. 

Fig. 1","Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; Claude family; Other: DeepSeek, ERNIE Bot",No users involved,n,n,n,n,,,n,,,,n,,,,y,b,l,Models classified cases requiring intervention; BSI>1 and higher CR indicate better behavior sensitivity/consistency; DeepSeek/Wenxin/Claude > GPT-4. — Quote: “BSI and CR … DeepSeek 1.0662 / 0.8985 … GPT-4 1.0415 / 0.8250,n,,,,n,,,,n,,,,n,,,,n,,,,behavior sensitivity index,no benchmark,no benchmark,No Indexmodel; comparison of multiple LLMs,consistency ratio,no benchmark,no benchmark,No Indexmodel; comparison of multiple LLMs,n,,,,n,,n,,y,"Some are, others don´t",n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_32,2nd_search_303,Randomized trial of a generative AI chatbot for mental health treatment,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27.0,3.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients with disorder explicitly based on ICD or DSM,210,,Self-collected data,"The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dialogues. The dialogues were developed by our research team, including a board-certified psychiatrist and a clinical psychologist, and peer-reviewed using evidence-based (primarily CBT) modalities.",Psychotherapy -- chat logs,English,No,No,Unknown,Unknown,Fine-tuning + other modules,Main LLM was fine-tuned but there are also other modules (e.g. safety classification),Other CBT techniques,Llama 2 family; Other: Falcon-7B,Yes,y,n,y,y,(Working Alliance Inventory — Short Revised [WAI-SR]),Results of survey in Fig 5,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"patient health questionnaire 927 (phq-9), the generalized anxiety disorder questionnaire for the diagnostic and statistical manual of mental disorders, fourth edition (dsm-iv) (gad-q-iv), and the weight concerns scale (wcs) within the stanford–washington university eating disorder (swed)",b,l,Waitlist control,engagement with therabot (number of messages sent),no benchmark,no benchmark,no benchmark,n,,,,y,"Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model.",n,"All responses from Therabot were supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from Therabot (e.g., providing medical advice), we contacted the participant to provide correction.

However, no automatic screening",y,"Llama 2, Falcon-7B",n,"usage of Amazon AWS, no discussion of user privacy",y,See Table 2,n,,y,Figures 3 and 6,y,Figure 6,y,various measures,y,Waitlist control,n,,n,,
2nd_search_32,2nd_search_304,Randomized trial of a generative AI chatbot for mental health treatment,Reviewer Two,Journal paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",27.0,3.0,2025.0,Other: Lebanon,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),210,,Self-collected data,,Other: expertly written therapist–patient dialogues based on third-wave CBT,Other: unknown,No,No,Unknown,Trained professionals,Fine-tuning + other modules,"Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).","Unspecified, might include formal therapy methods","Other: Not specified ""The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dia-
logues""",Yes,n,n,y,y,,"Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX. ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model",y,"All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.",n,,n,,y,Table 2,n,,n,,n,,y,"Patient Health 
Questionnaire 9(PHQ-9), the Generalized Anxiety 
Disorder Questionnaire for the Diagnostic and Statistical 
Manual of Mental Disorders, Fourth Edition (DSM-IV)
(GAD-Q-IV), and the Weight Concerns Scale (WCS) within 
the Stanford–Washington University Eating Disorder
(SWED), as measures of depression, anxiety, and weight 
concerns, respectively. Therapeutic alliance 
(Working Alliance Inventory — Short Revised [WAI-SR]).",y,"To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.",n,,n,,
2nd_search_32,2nd_search_305,Randomized trial of a generative AI chatbot for mental health treatment,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",27.0,3.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),210,,Self-collected data,"The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dialogues. The dialogues were developed by our research team, including a board-certified psychiatrist and a clinical psychologist, and peer-reviewed using evidence-based (primarily CBT) modalities.",Psychotherapy -- chat logs,English,Yes,No,Unknown,Trained professionals,Fine-tuning + other modules,"Main LLM was fine-tuned but there are also other modules (e.g. safety classification)

Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).",Other CBT techniques,Llama 2 family; Other: Falcon-7B,Yes,n,n,y,y,,"Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX.

Results in Fig 5",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"patient health questionnaire 927 (phq-9), the generalized anxiety disorder questionnaire for the diagnostic and statistical manual of mental disorders, fourth edition (dsm-iv) (gad-q-iv), and the weight concerns scale (wcs) within the stanford–washington university eating disorder (swed)",b,l,Waitlist control,n,,,,n,,,,y,"In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model",n,"All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.

However, no automatic screening pre-transmission",y,"Llama 2, Falcon-7B",n,"usage of Amazon AWS, no discussion of user privacy",y,Table 2,n,,n,,n,,y,"Patient Health 
Questionnaire 9(PHQ-9), the Generalized Anxiety 
Disorder Questionnaire for the Diagnostic and Statistical 
Manual of Mental Disorders, Fourth Edition (DSM-IV)
(GAD-Q-IV), and the Weight Concerns Scale (WCS) within 
the Stanford–Washington University Eating Disorder
(SWED), as measures of depression, anxiety, and weight 
concerns, respectively. Therapeutic alliance 
(Working Alliance Inventory — Short Revised [WAI-SR]).",y,"To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.",n,,n,,
2nd_search_31,2nd_search_306,Artificial Intelligence in Mental Health Care: The T5 Chatbot Project,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9.0,12.0,2024.0,Other: United Arab Emirates,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Only fine-tuning,PEFT (qLoRA) fine-tuning of T5,"Unspecified, might include formal therapy methods",T5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Metric: training loss. No benchmark,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Metrics: Dist-1 and Dist-2 on chatbot responses. No benchmark,n,,,,n,,,,n,,,,n,,n,,y,,y,"Privacy Concerns: Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks",n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_31,2nd_search_307,Artificial Intelligence in Mental Health Care: The T5 Chatbot Project,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9.0,12.0,2024.0,Other: UAE,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Other: Multiple external data sets,Table 1,Other: Multiple types (compare results data loading and preparation),English,No,Yes,Unknown,,Only fine-tuning,PEFT + QLORA,"Unspecified, might include formal therapy methods",T5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"training metrics (training runtime, samples processed per second, steps per second, and training loss)",-,-,-,diversity metrics (richness and variability; dist-1 and dist-2),-,-,-,n,,,,n,,n,,y,T5 can be used on-premise and via cloud (not in the text). ,y,"Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.",n,,n,,n,,n,,n,,n,,n,,n,,Ethical Considerations: Which users were informed?
2nd_search_31,2nd_search_308,Artificial Intelligence in Mental Health Care: The T5 Chatbot Project,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9.0,12.0,2024.0,Other: United Arab Emirates,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.,Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Only fine-tuning,PEFT (qLoRA) fine-tuning of T5,"Unspecified, might include formal therapy methods",T5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Metric: training loss. No benchmark,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Metrics: Dist-1 and Dist-2 on chatbot responses. No benchmark,n,,,,n,,,,n,,,,n,,n,,y,T5 can be used on-premise and via cloud (not in the text). ,y,"Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.",n,,n,,n,,n,,n,,n,,n,,n,,Ethical Considerations: Which users were informed?
2nd_search_30,2nd_search_309,Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,1.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting; Other: RAG,"Instructions to act like a counselor etc. implemented; This model also allowed us to use retrieval augmented generation technology, which allows the model to be provided with a corpus of knowledge to reduce hallucinations
and misinformation [8]. Both the provision of instructions and source materials can serve as guardrails that keep the chatbot in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,y,s,l,Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).,y,w,h,"Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).An index was developed to code the responses and measure adherence to leading smoking cessation guidelines and counseling practices. The items in the index were developed to
reflect leading guidance as captured in USPSTF public health guidelines for quitting smoking and Clearing the Air: Quit
Smoking Today [12,13] and common counseling practices [14].",n,,,,y,w,h,"Each chatbot response was independently categorized by 2
coders for their adherence on each item of the index.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_30,2nd_search_310,Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,1.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,bots were queried with a fixed set of 12 prompts derived from real search queries; responses were coded against a guideline derived index,"Informal counseling (e.g., emotional support conversation)","ChatGPT, model unspecified",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,w,h,"dherence to an index developed
from the US Preventive Services Task Force public health guidelines for quitting smoking and counseling principles. Enough?",n,,,,n,,,,n,,,,maybe the adherence index here.,,,,n,,,,n,,,,n,,y,some types of misinformation were present in 22% of responses,n,,n,,n,,n,,n,,n,,n,,n,,n,,maybe?,"somehow, since the benchmark were clinical guidelines:

Responses were analyzed for their adherence to an index developed from the US Preventive Services Task Force public health guidelines for quitting smoking and counseling principles.",
2nd_search_30,2nd_search_311,Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",30.0,1.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Instructions to act like a counselor etc. implemented; This model also allowed us to use retrieval augmented generation technology, which allows the model to be provided with a corpus of knowledge to reduce hallucinations
and misinformation [8]. Both the provision of instructions and source materials can serve as guardrails that keep the chatbot in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,y,s,l,Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).,y,w,h,"Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).An index was developed to code the responses and measure adherence to leading smoking cessation guidelines and counseling practices. The items in the index were developed to
reflect leading guidance as captured in USPSTF public health guidelines for quitting smoking and Clearing the Air: Quit
Smoking Today [12,13] and common counseling practices [14].",n,,,,y,w,h,"Each chatbot response was independently categorized by 2
coders for their adherence on each item of the index.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_29,2nd_search_312,When ELIZA meets therapists: A Turing test for the heart and mind,Reviewer Two,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",12.0,2.0,2025.0,USA,Other: experimental vignette study with panel raters,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved; General population,830,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. ","Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,,n,n,n,n,,,n,,,,n,,,,n ,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_29,2nd_search_313,When ELIZA meets therapists: A Turing test for the heart and mind,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12.0,2.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. Like the therapists’ instructions, the prompt did not place any limits on the length of the response, nor was ChatGPT shown the therapists’ responses.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,sentiment and part-of-speech,- (no b/s/w classification possible),h,Therapist responses,n,,,,n,,,,n,,n,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,, 
2nd_search_29,2nd_search_314,When ELIZA meets therapists: A Turing test for the heart and mind,Consensus,Journal paper,"Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research)",12.0,2.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),General population,830,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. Like the therapists’ instructions, the prompt did not place any limits on the length of the response, nor was ChatGPT shown the therapists’ responses.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,sentiment and part-of-speech,unclear,h,Therapist responses,difference between responses (detection),s,h,Therapist responses,n,,,,n,,n,,n,,n,,y,"In the current study, participants (N = 830) were 45.17 years old on average (SD = 16.56),
59.88% mentioned being in a current romantic relationship, and 18.07% of the sample
reported having ever engaged in couple therapy. Most participants identified as a woman
(50.60%), slightly fewer identified as a man (47.95%), and the remaining individuals identified
as non-binary or third-gender (0.24%), 0.12% preferred not to say, and 0.07% of the sample
did not answer. A majority of the sample identified as straight (83.25%), 7.83% of the sample
identified as bisexual, 2.65% as gay, 1.81% as asexual, 1.45% as lesbian, 0.72% as queer, and
0.60% preferred to not disclose. When considering race and ethnicity, most participants iden-
tified as non-Hispanic White (49.40%), followed by Black (18.80%), White Hispanic (16.87%),
Asian (5. %), Black Hispanic (0.84%), American Indian or Alaskan Native (0.12%), and the
remaining sample identified as other (8.43%), or preferred not to disclose (0.12%).",n,,n,,n,,n,,n,,n,,n,, 
2nd_search_28,2nd_search_315,Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,12.0,2023.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Emotional Support Conversation dataset (Liu 2021 Towards emotional support dialog systems),Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Lay people,Fine-tuning + other modules,Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family; Llama 2 family; Other: SeqGPT,No users involved,n,n,n,n,,,y,b,l,"BLEU, ROUGE, METEOR. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",n,,,,n,,,,n,,,,y,b,l,"Human interactive evaluation (fluency, comforting, etc.), comparison with other models. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5).",n,,,,y,b,l,"Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",n,,,,n,,,,n,,,,n,,,,n,,n,,y,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_28,2nd_search_316,Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,12.0,2023.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","Emotional Support Conversation dataset. Comprises 1,053 multi-turn dialogues, amounting to 31,410 utterances. ""We have augmented the dataset with additional annotations to signify the current state of the dialogue.""",Emotional support dialogue -- chat logs,English,No,Yes,Unknown,Other: Crowdworkers,Fine-tuning + other modules,"Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.","Unspecified, might include formal therapy methods",Llama 2 family; Other: Azure GPT-3.5,No users involved,n,n,n,n,,,y,b,l,Other LLMs,n,,,,n,,,,n,,,,y,b,l,Other LLMs,n,,,,y,b,l,Other LLMs,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_28,2nd_search_317,Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,12.0,2023.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","Emotional Support Conversation dataset. Comprises 1,053 multi-turn dialogues, amounting to 31,410 utterances. ""We have augmented the dataset with additional annotations to signify the current state of the dialogue.""",Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Lay people,Fine-tuning + other modules,"Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework

Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.","Unspecified, might include formal therapy methods",GPT-3.5 family; Llama 2 family; Other: SeqGPT,No users involved,n,n,n,n,,,y,b,l,"BLEU, ROUGE, METEOR. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",n,,,,n,,,,n,,,,y,b,l,"Human interactive evaluation (fluency, comforting, etc.), comparison with other models. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5).",n,,,,y,b,l,"Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5)",n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_26,2nd_search_318,SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",5.0,5.0,2025.0,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population; Other: working professionals,8,,Self-collected data,,Emotional support dialogue -- chat logs,Other: unknown. Italian?,No,No,Unknown,Other: Unsure,Only fine-tuning,"Socrates was subjected to rigorous fine-tuning with pains-takingly detailed instructions designed to optimize therapeutic interactions. The model was specifically trained to recognize and appropriately respond to a wide spectrum of emotional states expressed by users, while respecting moments of  silence or reluctance to engage in conversation. It was calibrated to avoid overly intrusive questioning that might compromise user comfort and trust and instead detect nuanced linguistic patterns that might indicate various psychological states. Throughout its development, emphasis was placed on maintaining an appropriate balance between providing support and encouraging self-reflection",Peer support conversation,"ChatGPT, model unspecified",Yes,y,n,y,n,"System Usability Scale, SUS
User Experience Scale, UES 
Client Satisfaction Questionnaire, CSQ
Likert-scale",,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.",y,see previous one,n,,z,"Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.",n,,n,,n,,n,,n,,n,,n,,y,"Integrating Socrates into environ-
ments such as hospitals or drug rehabilitation centers could
therefore provide substantial benefits. Patients could gain
additional time for psychological reflection beyond what current staffing constraints allow. Moreover, some individuals
might find it easier to discuss sensitive issues with Socrates
rather than in face-to-face interactions, potentially experienc-
ing less perceived judgment and accelerating psychological
change processes. Health care providers might also benefit
from this technology through more streamlined workflows
and reduced burnout risk.
",
2nd_search_26,2nd_search_319,SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention,Richard Gaus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",5.0,5.0,2025.0,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,8,,Other: yes but unclear which,,,,,,,,Only fine-tuning,,Other CBT techniques,"ChatGPT, model unspecified",Yes,y,y,y,y,"system usability (System Usability Scale, SUS), user engagement
(User Experience Scale, UES), and satisfaction with the psy-
chological service (Client Satisfaction Questionnaire, CSQ).","Most participants reported feeling strongly understood by
the chatbot, with the majority indicating a high degree of
perceived emotional attunement. Participants described the
experience as intuitive to navigate, personally meaningful,
and contextually relevant to their situations. A general sense
of satisfaction was expressed across the participant group,
with favorable ratings on the satisfaction measures.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"subjective units of distress scale, suds",no benchmark,no benchmark,pre post measurement,"positive and negative affect schedule, panas",no benchmark,no benchmark,pre post measurement,n,,,,y,"Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.",n,,n,,n,,n,,n,,n,,n,,y,,n,,n,,n,,
2nd_search_26,2nd_search_320,SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention,Consensus,Journal paper,"Other (e.g., Humanities & Social Sciences Communications, generalist outlets)",5.0,5.0,2025.0,Other: Italy,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,8,,"Other: some dataset was used, but completely unknown characteristics",,,,,,,,Only fine-tuning,"Socrates was subjected to rigorous fine-tuning with pains-takingly detailed instructions designed to optimize therapeutic interactions. The model was specifically trained to recognize and appropriately respond to a wide spectrum of emotional states expressed by users, while respecting moments of  silence or reluctance to engage in conversation. It was calibrated to avoid overly intrusive questioning that might compromise user comfort and trust and instead detect nuanced linguistic patterns that might indicate various psychological states. Throughout its development, emphasis was placed on maintaining an appropriate balance between providing support and encouraging self-reflection","CBT: Cognitive restructuring; Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,y,n,y,n,"System Usability Scale, SUS
User Experience Scale, UES 
Client Satisfaction Questionnaire, CSQ
Likert-scale","Most participants reported feeling strongly understood by
the chatbot, with the majority indicating a high degree of
perceived emotional attunement. Participants described the
experience as intuitive to navigate, personally meaningful,
and contextually relevant to their situations. A general sense
of satisfaction was expressed across the participant group,
with favorable ratings on the satisfaction measures.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"subjective units of distress scale, suds",no benchmark,no benchmark,pre post measurement,"positive and negative affect schedule, panas",no benchmark,no benchmark,pre post measurement,n,,,,y,"Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.",n,,n,,y,"Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.",n,,n,,n,,n,,y,,n,,n,,y,"Integrating Socrates into environ-
ments such as hospitals or drug rehabilitation centers could
therefore provide substantial benefits. Patients could gain
additional time for psychological reflection beyond what current staffing constraints allow. Moreover, some individuals
might find it easier to discuss sensitive issues with Socrates
rather than in face-to-face interactions, potentially experienc-
ing less perceived judgment and accelerating psychological
change processes. Health care providers might also benefit
from this technology through more streamlined workflows
and reduced burnout risk.
",
2nd_search_25,2nd_search_321,Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10.0,3.0,2025.0,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,244,,,,,,,,,,Prompting + other modules,-,CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques,GPT-4 / GPT-4o family,Yes,n,y,y,y,,"higher reliable improvement, recovery, and reliable recovery rates” 
users perceived the AI-enabled therapy support tool as most useful for discussing their problems to gain awareness and clarity … learning how to apply coping skills”",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,y,"Patients using the AI-enabled therapy support tool exhibited … fewer dropouts from treatment.” (Position: Results, p. 1)",n,,y,"higher reliable improvement, recovery, and reliable recovery rates” (Position: Results, p. 1)",y,comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (,n,,y,"eal-world observational study … in 5 of the United Kingdom’s National Health Service Talking Therapies services” (Position: Methods, p. 1)",
2nd_search_25,2nd_search_322,Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10.0,3.0,2025.0,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"Other: Probably patients with disorders, but not specified",,,No dataset used for development or evaluation,,,,,,,,Other: unknown,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,y,n,y,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",n,,n,,y,Table 1,n,,n,,n,,y,"Reliable improvement refers to a clinically significant
improvement in symptoms following a course of treatment and
is calculated as the score difference between the first and the
last validated clinical questionnaire completed. The types of
questionnaires patients complete are tailored to their specific
condition. For example, the Patient Health Questionnaire-9
(PHQ-9) [30] is used to measure depression symptom severity,
and the Generalized Anxiety Disorder-7 (GAD-7) [31] is used
to measure anxiety symptom severity. A clinically significant
improvement in symptoms is considered a change score ≥6 for
PHQ-9 or ≥4 for GAD-7 [26]",y,"We compared the clinical outcomes of individuals who signed
up to use the AI-enabled therapy support tool (the intervention
group) with those of individuals who did not (the control group):",n,,n,,
2nd_search_25,2nd_search_323,Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",10.0,3.0,2025.0,UK,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Patients recruited in hospital or outpatient treatment facility,244,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,-,"CBT: Motivational interviewing; CBT: Cognitive restructuring; Other CBT techniques; Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,y,y,y,,"higher reliable improvement, recovery, and reliable recovery rates” 
users perceived the AI-enabled therapy support tool as most useful for discussing their problems to gain awareness and clarity … learning how to apply coping skills”",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",y,"All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.",n,,n,,y,Table 1,n,,n,,n,,y,"Reliable improvement refers to a clinically significant
improvement in symptoms following a course of treatment and
is calculated as the score difference between the first and the
last validated clinical questionnaire completed. The types of
questionnaires patients complete are tailored to their specific
condition. For example, the Patient Health Questionnaire-9
(PHQ-9) [30] is used to measure depression symptom severity,
and the Generalized Anxiety Disorder-7 (GAD-7) [31] is used
to measure anxiety symptom severity. A clinically significant
improvement in symptoms is considered a change score ≥6 for
PHQ-9 or ≥4 for GAD-7 [26]",y,comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (,n,,n,,
2nd_search_24,2nd_search_324,Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues,Richard Gaus,Conference paper,"Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering)",3.0,12.0,2024.0,China,Empirical research involving an LLM,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_24,2nd_search_325,Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,12.0,2024.0,China,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,MDRDC and Dialogue Safety,Other: Twitter (X),Chinese,No,No,Unselected,Lay people,Only fine-tuning,,,"BERT family; T5 family; GPT-3.5 family; ChatGPT, model unspecified",No,n,n,n,n,,,n,,,,y,b,l,BERT; BERT- CRF; BERT-MCRF; ROBERTA,y,b,l,"Precision, Recall, Macro-F1",n,,,,n,,,,y,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_24,2nd_search_326,Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues,Consensus,Conference paper,,3.0,12.0,2024.0,China,Empirical research involving an LLM,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_22,2nd_search_327,Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study,Richard Gaus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_22,2nd_search_328,Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study,Reviewer Two,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_22,2nd_search_329,Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study,Consensus,,,,,,,,,,,,,,,,,,,,,,,,BERT family,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_20,2nd_search_330,"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5.0,5.0,2025.0,Other: New Zealand,Population survey,Client-facing application,,General population,306,,Self-collected data,,Other: Rating,English,No,No,Unknown,Unknown,,,"Unspecified, might include formal therapy methods",Other: AI in general,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_20,2nd_search_331,"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5.0,5.0,2025.0,Other: New Zealand,Population survey,Client-facing application,Multi-turn chatbot,General population,348,,No dataset used for development or evaluation,,,,,,,,,,"Informal counseling (e.g., emotional support conversation)","Other: just general ""generative AI application""",Yes,y,n,y,y,"Human Computer Trust Scale S. S. Siddharth Gulati and D. Lamas, “Design, development and evaluation of a human-computer trust scale,” Behaviour & Information Technology",,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,-,,-,,-,,-,,y,"articipants were largely Caucasian Amer-
ican (57.3%), followed by African American (23.5%), Asian
American (12.6%) and others/did not specify (6.6%)",n,,n,,n,,n,,n,,n,,n,,
2nd_search_20,2nd_search_332,"Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent",Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",5.0,5.0,2025.0,Other: New Zealand,Population survey,Client-facing application,Multi-turn chatbot,General population,306,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","Other: just general ""generative AI application""",Yes,y,n,y,y,"Human Computer Trust Scale S. S. Siddharth Gulati and D. Lamas, “Design, development and evaluation of a human-computer trust scale,” Behaviour & Information Technology",No user experience assessment but participant attitude survey,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_18,2nd_search_333,Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,1.0,2025.0,Other: Egypt,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","Bilingual (English & Arabic) mental health text corpus, origin not found",,"Other: English, Arabic",No,Yes,Unknown,Unknown,Only fine-tuning,Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses,"Informal counseling (e.g., emotional support conversation)",Other: Jais-13B,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,cross-entropy loss,,,compared to data set 70.58 %,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility"
2nd_search_18,2nd_search_334,Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,1.0,2025.0,Other: Egypt,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,Web-scraped data from diverse sources,"Other: Internet data, various types and sources (mixed)",Other: Arabic,No,No,"Other: Not applicable, data is not on users","Other: Not applicable, no interaction data",Only fine-tuning,Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B,"Unspecified, might include formal therapy methods",Other: Jais-13B,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,cross-entropy loss,no benchmark,no benchmark,no benchmark,n,,,,n,,,,n,,n,,y,Jais-13B is on-premise,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_18,2nd_search_335,Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",15.0,1.0,2025.0,Other: Egypt,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","Web-scraped data from diverse sources

Bilingual (English & Arabic) mental health text corpus, origin not found","Other: Internet data, various types and sources (mixed)",Other: Arabic,No,No,"Other: Not applicable, data is not on users","Other: Not applicable, no interaction data",Only fine-tuning,"Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B
Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses","Unspecified, might include formal therapy methods",Other: Jais-13B,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,cross-entropy loss,no benchmark,no benchmark,compared to data set 70.58 %,n,,,,n,,,,n,,n,,y,Jais-13B is on-premise,n,,n,,n,,n,,n,,n,,n,,n,,n,,"without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility"
2nd_search_17,2nd_search_336,A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),23.0,5.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified",mental health counseling conversations from huggingface; fine-tuned with unsloth and ollama,Emotional support dialogue -- chat logs,English,No,Yes,Unknown,Lay people,Fine-tuning + other modules,"Fine-tuned LLaMA 3.2 (3B parameters) using Unsloth and Ollama; used RAG for retrieval, LangChain memory for conversation history, and FAISS for relevant material recall","Informal counseling (e.g., emotional support conversation)",Other: Llama 3.2,No,n,n,n,n,,technical proof-of-concept; no real users assessed,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,y,Llama is ? so are unsloth/ollama,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_17,2nd_search_337,A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),23.0,5.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"Huggingface: Mental Health Counseling Conversations
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations",Internet data -- mental health Q&A,English,No,Yes,Unselected,Unknown,Fine-tuning + other modules,Fine-tuning via LoRA + RAG pipeline,"Unspecified, might include formal therapy methods",Other: Llama 3.2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"unclear metrics: relevance, empathy, conciseness, context",no benchmark,no benchmark,Unclear what these metrics are (human ratings? automatic?). No benchmarks used.,n,,,,n,,,,n,,n,,y,Llama 3.2 used,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_17,2nd_search_338,A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot,Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),23.0,5.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"Huggingface: Mental Health Counseling Conversations
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Fine-tuning + other modules,"Fine-tuned LLaMA 3.2 (3B parameters) using Unsloth and Ollama; used RAG for retrieval, LangChain memory for conversation history, and FAISS for relevant material recall","Unspecified, might include formal therapy methods",Other: Llama 3.2 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"unclear metrics: relevance, empathy, conciseness, context",no benchmark,no benchmark,Unclear what these metrics are (human ratings? automatic?). No benchmarks used.,n,,,,n,,,,n,,n,,y,Llama 3.2 used,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_15,2nd_search_339,MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9.0,4.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"Other: Unclear, probably general population",,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules; Other: RAG,Prompt Engineering + RAG + generic LLMs,"Other: The chatbot's architecture is built on a 
robust knowledge base encompassing evidence-based therapeutic techniques, including cognitive-behavioral therapy (CBT) principles and motivational interviewing 
strategies.",BERT family,Yes,n,n,y,y,,"Not described, how many users and rather vague representation of results in Fig. 7.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"The chatbot's functionality extends beyond mere conversation, integrating mood tracking features, 
personalized coping strategy recommendations, and crisis 
detection algorithms with built-in escalation protocols for 
high-risk situations",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_15,2nd_search_340,MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9.0,11.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,assumes prior knowledge from mental health-related soruces or web-scraped materials,,English,,,,,Prompting + other modules,"uses NLP and probabilistic models to classify symptoms, map user queries to mental health knowledge base, and personalize therapeutic suggestions",Other CBT techniques,BERT family,No,n,n,n,n,,descpritive only,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Ambitious platform vision, but lacks any form of quantitative, clinical, or user-centered evaluation"
2nd_search_15,2nd_search_341,MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",9.0,4.0,2025.0,India,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,"No clients/patients involved; Other: User data shown in Fig. 7 but sample nowhere futher described. Hence unclear, how data was retrieved. ",,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules; Other: RAG,"uses NLP and probabilistic models to classify symptoms, map user queries to mental health knowledge base, and personalize therapeutic suggestions","CBT: Motivational interviewing; Other CBT techniques; Other: The chatbot's architecture is built on a 
robust knowledge base encompassing evidence-based therapeutic techniques, including cognitive-behavioral therapy (CBT) principles and motivational interviewing 
strategies.",BERT family,Yes,n,n,y,y,,"Not described, how many users and rather vague representation of results in Fig. 7.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"The chatbot's functionality extends beyond mere conversation, integrating mood tracking features, 
personalized coping strategy recommendations, and crisis 
detection algorithms with built-in escalation protocols for 
high-risk situations",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Ambitious platform vision, but lacks any form of quantitative, clinical, or user-centered evaluation"
2nd_search_11,2nd_search_342,LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8.0,12.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","mental health and healthcare-related texts; sources not fully specified, combined domain corpora and simulated patient records",Other: mix,English,Yes,Yes,Unknown,Unknown,Prompting + other modules,,"Informal counseling (e.g., emotional support conversation)",Other: not specif.,No,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz"
2nd_search_11,2nd_search_343,LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8.0,12.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","In order to effectively function and provide personalized
response to a user, data preparation plays a vital role. The
data comes from multiple sources, such as books, articles,
news, media, etc

We started experimenting with fine-tuning-based approach.
To generate the training dataset, we collected various mental
health related books, articles, and news stories and prepared
almost 500 pairs of instructions in the format of ”system”,
”user”, and ”assistant”. What we found is that this approach
is good for an LLM-based system that is already trained
on certain things","Other: different types (books, articles, etc.), not further specified",Other: unknown,No,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning (according to Fig. 3 at least) and RAG,"Unspecified, might include formal therapy methods",GPT-3.5 family; Other: all-MiniLM-L6-v2 (for embedding),No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,relevancy,b,l,"Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",user satisfaction score,b,l,"Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_11,2nd_search_344,LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",8.0,12.0,2024.0,Other: Canada,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,"External data set, modified","mental health and healthcare-related texts; sources not fully specified, combined domain corpora and simulated patient records","Other: different types (books, articles, etc.), not further specified",Other: unknown,No,No,Unknown,Unknown,Fine-tuning + other modules,Fine-tuning (according to Fig. 3 at least) and RAG,"Unspecified, might include formal therapy methods",GPT-3.5 family; Other: all-MiniLM-L6-v2 (for embedding),No users involved,n,n,n,n,,,n,,,,y,b,l,"relevancy score. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,user satisfaction score,b,l,"Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini",n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz"
2nd_search_10,2nd_search_345,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",5.0,3.0,2025.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,Self-collected data,SIRI-2 with added expert suicidologist ratings,Other: SIRI-2 test data with ratings,English,No,No,Other: n.a.,Other: n.a.,Only prompting,,"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Correlation between LLM suicide response ratings and expert ratings,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,y,"widespread use of LLM
technology—including new companies already drawing on
LLM technology for mental health care [29]—could reach a
much wider audience of individuals coping with depression and
suicidal thoughts. To date, a common guardrail has been for
LLMs to produce “hard stops”, in which individuals are referred
to 988 or another suicide prevention hotline. While such referrals
may be beneficial, they also artificially circumscribed
interactions in a way that could be taken as a missed opportunity.",
2nd_search_10,2nd_search_346,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",5.0,3.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"Responses by expert suicidologists on a previously-published standardized scale: the Suicide Intervention Response Inventory (SIRI-2)
[15].",Other: Standardized clinical instrument with hypothetical scenarios and predefined responses,English,No,Yes,Other: Not applicable; Artificial cases.,Trained professionals,Only prompting,"Research team members prompted LLMs with the original instructions for the SIRI-2, as well as with one of the SIRI2-2’s 24 items. We did not prompt LLMs with any additional text. ","Other: LLMs did not provide support but instead compared to clinician responses using the SIRI-2 scoring system (= ""rating assessment"").",GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,bias,w,h, Compared to the ratings of expert suicidologist.,overall performance,s/b,h,"Compared to the ratings of expert suicidologist.
",n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_10,2nd_search_347,Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",5.0,3.0,2025.0,USA,Empirical research involving an LLM,Analysis of conversation transcripts,,,,,External data set,"Responses by expert suicidologists on a previously-published standardized scale: the Suicide Intervention Response Inventory (SIRI-2)
[15].",Other: Standardized clinical instrument with hypothetical scenarios and predefined responses,English,Other: Yes (hypothetical cases created by experts),Yes,Other: n.a.,Trained professionals,Only prompting,"Research team members prompted LLMs with the original instructions for the SIRI-2, as well as with one of the SIRI2-2’s 24 items. We did not prompt LLMs with any additional text. ","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Correlation between LLM suicide response ratings and expert ratings,n,,,,n,,,,n,,,,n,,,,n,,,,siri-2 z-score,no benchmark,no benchmark,"Compared to the ratings of expert suicidologist.
",n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_9,2nd_search_348,Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users,Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3.0,1.0,2025.0,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,20,,Self-collected data,,Other: Rating,Other: Korean,No,No,Unselected,Lay people,,,"Unspecified, might include formal therapy methods","GPT-4 / GPT-4o family; ChatGPT, model unspecified; Other: Conflicting information (?)

The HoMemeTown chatbot, powered by ChatGPT 4.0

The chatbot relies on the GPT API, a general-purpose language model provided by OpenAI, instead of a domain-specific model
trained for mental health counseling. The GPT API offers a range of models, such as Davinci, GPT-3.5, and GPT-4, which can be selected based on desired performance and cost
consideration

Another limitation is the potential inconsistency in comparing Dr CareSam, built on the ChatGPT 4.0 API,",Yes,n,y,y,y,,"Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,"The risk detection system in HoMemeTown was developed
based on established clinical guidelines [19] and validated
screening tools [20], implementing a sophisticated approach to
identifying and responding to potential mental health concerns.
The system continuously monitors user interactions for primary
risk indicators, including expressions of suicidal ideation, severe
depression symptoms, and anxiety crisis signals, while also
tracking secondary indicators such as sleep disturbance patterns
and social withdrawal signs.
When potential risks are detected, the system implements a
graduated response protocol that has been carefully designed
to provide appropriate levels of support while avoiding
unnecessary escalation. For mild risk situations, the system
offers empathetic acknowledgment and self-help resources,
drawing from evidence-based interventions [21]. In cases of
moderate risk, the response includes more direct expressions
of concern and specific mental health resources, while severe
risk triggers an immediate crisis response protocol with direct
connections to professional support services. To address the challenge of potential false positives in risk
detection, we implemented a sophisticated validation system
that examines multiple contextual factors before triggering
interventions. This system uses NLP techniques to analyze the
broader context of user communications, helping to distinguish
between casual expressions and genuine indicators of distress.
Regular professional review of high-risk cases ensures the
ongoing refinement of detection algorithms and response
protocols, maintaining a balance between sensitivity and
specificity in risk assessment.",n,,n,,n,,y,"The study included 20 participants aged 18 to 27 (mean 23.3,
SD 1.96) years with 60% (12/20) female and 40% (8/20) male.",n,,n,,n,,n,,n,,n,,n,,Comparative Analysis unclear in methodological aspects to me. 
2nd_search_9,2nd_search_349,Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users,Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3.0,1.0,2025.0,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,20,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,Elaborate server architecture around OpenAI API (see Fig. 1),"Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,Yes,n,y,y,y,,"The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"quantitative ux assessment (""overall satisfaction"")",b,l,"benchmarks are other mental health chatbots and general LLMs: Woebot, Happify, GPT-3.5, Google Bard",n,,,,n,,,,y,"We incorporated a risk detection function to identify potential
mental health crises and provide appropriate resources when
necessary",y,"In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.",n,,y,"Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].",n,,n,,n,,n,,n,,n,,n,,n,"Only: Integration with health care systems
• Investigate secure ways to integrate chatbot data with electronic health records, while maintaining user privacy.",
2nd_search_9,2nd_search_350,Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users,Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",3.0,1.0,2025.0,Other: Korea,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,20,,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,Elaborate server architecture around OpenAI API (see Fig. 1),"Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family,Yes,n,y,y,y,,"The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.

Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. ",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"quantitative ux assessment (""overall satisfaction"")",b,l,"benchmarks are other mental health chatbots and general LLMs: Woebot, Happify, GPT-3.5, Google Bard",n,,,,n,,,,y,"The risk detection system in HoMemeTown was developed
based on established clinical guidelines [19] and validated
screening tools [20], implementing a sophisticated approach to
identifying and responding to potential mental health concerns.
The system continuously monitors user interactions for primary
risk indicators, including expressions of suicidal ideation, severe
depression symptoms, and anxiety crisis signals, while also
tracking secondary indicators such as sleep disturbance patterns
and social withdrawal signs.
When potential risks are detected, the system implements a
graduated response protocol that has been carefully designed
to provide appropriate levels of support while avoiding
unnecessary escalation. For mild risk situations, the system
offers empathetic acknowledgment and self-help resources,
drawing from evidence-based interventions [21]. In cases of
moderate risk, the response includes more direct expressions
of concern and specific mental health resources, while severe
risk triggers an immediate crisis response protocol with direct
connections to professional support services. To address the challenge of potential false positives in risk
detection, we implemented a sophisticated validation system
that examines multiple contextual factors before triggering
interventions. This system uses NLP techniques to analyze the
broader context of user communications, helping to distinguish
between casual expressions and genuine indicators of distress.
Regular professional review of high-risk cases ensures the
ongoing refinement of detection algorithms and response
protocols, maintaining a balance between sensitivity and
specificity in risk assessment.",y,"In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.",n,,n,"Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].",n,"The study included 20 participants aged 18 to 27 (mean 23.3,
SD 1.96) years with 60% (12/20) female and 40% (8/20) male.",n,,n,,n,,n,,n,,n,,n,"Only: Integration with health care systems
• Investigate secure ways to integrate chatbot data with electronic health records, while maintaining user privacy.",Comparative Analysis unclear in methodological aspects to me. 
2nd_search_8,2nd_search_351,"Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study",Reviewer Two,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",13.0,6.0,2025.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,527,,Self-collected data,,,,,,,,,,CBT: Cognitive restructuring; Other CBT techniques,Other,Yes,y; working alliance inventory-short report? ,n,y,y,,"users expressed positive attitudes toward digital mental health solutions, with key motives including avoiding embarrassment (36%) and concerns about appearance in face-to-face consultations (35%). Expectations focused on emotional support (35%) and expressing feelings (32%)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,y,"Yes → Adheres to data protection principles, anonymized use",n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_8,2nd_search_352,"Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study",Richard Gaus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",13.0,6.0,2025.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,21,,No dataset used for development or evaluation,,,,,,,,Fine-tuning + other modules,"Clare is a standalone tool, based on fine-tuned LLMs plus other modules voice inputs, safety checks",CBT: Cognitive restructuring; Other CBT techniques,Other: Clare (R) by clare&me GmbH,Yes,y,y,n,y,User experience questionnaire (UEQ),,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,wai-sr working alliance,no benchmark,no benchmark,no benchmark,n,,,,n,,,,y,"If suicidality or severe distress is detected, users are directed to
psychological support hotlines and blocked from further use to
ensure safety.",y,"A custom moderation application programming interface (API)
filters inputs and outputs, flagging inappropriate content before
LLM processing. Key safety features include the following.",y,"Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.",n,did not describe how claire aligns with data protection regulations,y,Table 3,n,,y,See 3.2 Engagement patterns,n,,y,,n,,n,,n,,
2nd_search_8,2nd_search_353,"Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study",Consensus,Journal paper,"Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health)",13.0,6.0,2025.0,Germany,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,21,,No dataset used for development or evaluation,,,,,,,,Fine-tuning + other modules,"Clare is a standalone tool, based on fine-tuned LLMs plus other modules voice inputs, safety checks",CBT: Cognitive restructuring; Other CBT techniques,Other: Clare (R) by clare&me GmbH,Yes,y,n,y,y,User experience questionnaire (UEQ),"users expressed positive attitudes toward digital mental health solutions, with key motives including avoiding embarrassment (36%) and concerns about appearance in face-to-face consultations (35%). Expectations focused on emotional support (35%) and expressing feelings (32%)",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,wai-sr working alliance,no benchmark,no benchmark,no benchmark,n,,,,n,,,,y,"If suicidality or severe distress is detected, users are directed to
psychological support hotlines and blocked from further use to
ensure safety.",y,"A custom moderation application programming interface (API)
filters inputs and outputs, flagging inappropriate content before
LLM processing. Key safety features include the following.",n,"Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.
But not on-premise capable!",y,"Yes → Adheres to data protection principles, anonymized use
https://www.clareandme.com/post/what-happens-with-your-data-at-clare-me",y,Table 3,n,,y,See 3.2 Engagement patterns,n,,y,"UCLA, PHQ-D, PHQ-4, SWLS, Mini-SPIN",n,,n,,n,,
2nd_search_6,2nd_search_354,Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.,Reviewer Two,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12.0,2.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,,,,English,No,No,Psychopathology,Trained professionals,Only prompting,,"CBT: Motivational interviewing; Informal counseling (e.g., emotional support conversation); Other: “if possible,” obtain a commitment to start the treatment medication, buprenorphine. ",GPT-4 / GPT-4o family,Yes,n,y,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"Question ""Is this infomation safe?""",n,,n,,n,,n,,n,,n,,y,DSM 5,n,,n,,n,,Unsure about the metrics. We can discuss during the consensus. See table 4
2nd_search_6,2nd_search_355,Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.,Richard Gaus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12.0,2.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,No dataset used for development or evaluation,,,,,,,,Only prompting,"Prompt engineering on GPT-4. Authors describe the model is being ""tuned"" or ""fine-tuned"" but in fact they only iteratively adjust the prompt without any weight adjustment.",CBT: Motivational interviewing,GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Binary scoring rubrics for GPT-4 responses (see Table 4),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_6,2nd_search_356,Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.,Consensus,Journal paper,"Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports)",12.0,2.0,2025.0,USA,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,Self-collected data,see Generating Data for Evaluation,Emotional support dialogue -- chat logs,English,Yes,No,Other: not applicable,Other: not applicable,Only prompting,"Prompt engineering on GPT-4. Authors describe the model is being ""tuned"" or ""fine-tuned"" but in fact they only iteratively adjust the prompt without any weight adjustment.","CBT: Motivational interviewing; Informal counseling (e.g., emotional support conversation)",GPT-4 / GPT-4o family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,Binary scoring rubrics for GPT-4 responses (see Table 4),n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,Unsure about the metrics. We can discuss during the consensus. See table 4
2nd_search_5,2nd_search_357,Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,4.0,2025.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"The study uses data from YiXinli Community, a major Chinese online counseling platform (https://www.xinli001.com) frequented by nearly 40 million users worldwide for psychological support. On this platform, 
users can confidentially express their psychological concerns and receive advice from human counselors in the open Q&A section (https://www.xinli001.com/qa). We utilized a web data crawler called houyicaiji (https://www.houyicaiji.com/) to gather this Q&A data, which encompasses 10,903 distinct questions and 19,682 human counselor responses collected from November 3, 2022, to 
March 30, 2023.",Internet data -- mental health Q&A,Chinese,Yes,No,Unselected,Trained professionals,Prompting + other modules,,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,w,h,,perceived information quality (unpromted),w,h,,perceived information quality (promted),b/s/w (depending on risk situation),h,,n,,,,y,"In this study, the ethical risks associated with directly applying LLMs to individuals seeking mental 
health support were mitigated by utilizing pre-existing real-world data. ",n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,All comparisons between AI and human counselors based on ML Models. 
2nd_search_5,2nd_search_358,Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,4.0,2025.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"Data crawled from YiXinli Community (xinli001.com/qa). They crawled this data themselved ->  10,903 distinct questions and 19,682 human counselor responses",Internet data -- mental health Q&A,Chinese,No,Yes,Unselected,Unknown,Only prompting,The help-seeking questions were posted to the GPT-3.5-turbo model by OpenAI API to get responses from ChatGPT,"Informal counseling (e.g., emotional support conversation)",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"automatic ""perceived information quality"" (piq) rating via ml model",b,h,benchmark are real human responses from the Q&A dataset,various linguistic factors,mixed (table 9),h,benchmark are real human responses from the Q&A dataset,n,,,,n,,n,,n,,y,"First, although direct application risks were 
avoided, it is crucial to ensure that the real-world data used is properly protected and compliant with privacy regulations during both 
collection and processing (
). In this study, the data used was publicly available online, and all analysis was con­
ducted in a manner that safeguarded personal privacy. ",n,,n,,n,,n,,n,,n,,n,,n,,"This study also was a ""transcript analysis"" via BERT"
2nd_search_5,2nd_search_359,Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",3.0,4.0,2025.0,China,Empirical research involving an LLM,Client-facing application,One-turn chatbot (usually Q&A),No clients/patients involved,,,External data set,"The study uses data from YiXinli Community, a major Chinese online counseling platform (https://www.xinli001.com) frequented by nearly 40 million users worldwide for psychological support. On this platform, 
users can confidentially express their psychological concerns and receive advice from human counselors in the open Q&A section (https://www.xinli001.com/qa). We utilized a web data crawler called houyicaiji (https://www.houyicaiji.com/) to gather this Q&A data, which encompasses 10,903 distinct questions and 19,682 human counselor responses collected from November 3, 2022, to 
March 30, 2023.",Internet data -- mental health Q&A,Chinese,No,No,Unselected,Unknown,Only prompting,The help-seeking questions were posted to the GPT-3.5-turbo model by OpenAI API to get responses from ChatGPT,"Unspecified, might include formal therapy methods",GPT-3.5 family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,w,h,,"automatic ""perceived information quality"" (piq) rating via ml model (prompted chatgpt)",b,h,benchmark are real human responses from the Q&A dataset,various linguistic factors,mixed (table 9),h,benchmark are real human responses from the Q&A dataset,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"All comparisons between AI and human counselors based on ML Models. 

This study also was a ""transcript analysis"" via BERT"
2nd_search_4,2nd_search_360,Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots,Richard Gaus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),8.0,1.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,48,,No dataset used for development or evaluation,,,,,,,,Only prompting,Only sub-study 3 employed an LLM,Other: positive psychology intervention,GPT-3.5 family,Yes,n,y,n,y,,"Before the study’s completion, we conducted unstructured interviews to delve deeper into the participants’ experiences with the chatbots. Many participants expressed that the chatbot’s responses evoked feelings of surprise and novelty. They emphasized the importance of feeling understood as pivotal in enhancing their acceptance and engagement with the chatbot-based intervention. Since GPT-based chatbots can generate text in a human-like manner, mimicking the interaction between users and psychotherapists, five participants indicated that the anthropomorphic responses felt so authentic that they found it difficult to distinguish whether they were AI-generated. These qualitative insights reinforce our quantitative findings and underscore the critical importance of interaction in the design of chatbots.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,phq-9,b,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ","scales of psychological well-being (pwb), satisfaction with life scale (swls)",b,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ","gad-7, panas-p, panas-n",s,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",n,,n,,n,,n,,n,,n,,n,,n,,y,,y,,n,,n,,
2nd_search_4,2nd_search_361,Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots,Reviewer Two,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),8.0,1.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,"Substudy 1
ITT-Sample: 207
CC-Sample: 154

Substudy 2
ITT-Sample: 70
CC-Sample 53

Substudy 3
ITT-Sample: 72
CC-Sample: 48",,No dataset used for development or evaluation,,,,,,,,Prompting + other modules,"Pre-trained ChatGPT-3.5-Turbo API without fine-tuning but prompt engineering, integrating it with other system components.","Unspecified, might include formal therapy methods",GPT-3.5 family,Yes,n,y,n,y,,Only partialy described in the discussion section.,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"our exhaustive analysis of all chatbot- 
user dialogues recorded in this study did not reveal any inappropriate responses. This result, however, is not defini-
tive, and further research is required to ascertain the safety of chatbots in psychological research",n,,n,,n,,n,,n,,n,,y,"PHQ-9, GAD-7, PANAS-P, PANAS-N, SWLS, SVS",y,"Evident in whole study (e.g., Finally, it is worth noting that our choice of an active control group (that is, a non-trivial control group)...)",n,,n,,
2nd_search_4,2nd_search_362,Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots,Consensus,Journal paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),8.0,1.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,48,,No dataset used for development or evaluation,,,,,,,,Only prompting,Only sub-study 3 employed an LLM,Other: positive psychology intervention,GPT-3.5 family,Yes,n,y,n,y,,"Before the study’s completion, we conducted unstructured interviews to delve deeper into the participants’ experiences with the chatbots. Many participants expressed that the chatbot’s responses evoked feelings of surprise and novelty. They emphasized the importance of feeling understood as pivotal in enhancing their acceptance and engagement with the chatbot-based intervention. Since GPT-based chatbots can generate text in a human-like manner, mimicking the interaction between users and psychotherapists, five participants indicated that the anthropomorphic responses felt so authentic that they found it difficult to distinguish whether they were AI-generated. These qualitative insights reinforce our quantitative findings and underscore the critical importance of interaction in the design of chatbots.",n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,"phq-9, panas-p, satisfaction with life scale (swls)",b,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",scales of psychological well-being (pwb),unknown,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ","gad-7, panas-n",s,l,"Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. ",n,,n,,n,,n,,n,,n,,n,,n,,y,"PHQ-9, GAD-7, PANAS-P, PANAS-N, SWLS, SVS",y,"Evident in whole study (e.g., Finally, it is worth noting that our choice of an active control group (that is, a non-trivial control group)...)",n,,n,,
2nd_search_3,2nd_search_363,VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model,Richard Gaus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",26.0,11.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,People with some symptoms but not disorder (determined by symptom scales or questionnaires),49,,Self-collected data,zhang 2024 dataset,Other: Descriptions of mental disorders in the DSM-5,Other: unknown,Other: unknown,No,Other: n.a.,Other: n.a.,Fine-tuning + other modules,Fine-tuning + knowledge retrieval,"Unspecified, might include formal therapy methods",Other: ChatGLM2,Yes,y,n,y,y,"Counselor rating form‑short (CRF‑S), Client satisfaction scale (CSS)",,n,,,,n,,,,n,,,,n,,,,y,s,l,"We invited three counselors to evaluate 
these conversation cases. Benchmark is ""LLM-Counselor Support System""",n,,,,n,,,,n,,,,client satisfaction scale (css),b,l,Benchmars are other LLM-based models,counselor rating form‑short (crf‑s),b,l,Benchmars are other LLM-based models,sentiment score,b,l,Benchmars are other LLM-based models,n,,n,,y,,n,,n,,n,,n,,n,,n,,y,,n,,n,,
2nd_search_3,2nd_search_364,VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model,Reviewer Two,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",26.0,11.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,Other: Patients with psychological problems and having sought professional help,49,,Self-collected data,,"Other: Synthetic, self-created counseling dialogues for fine-tuning.",Other: Either Chines (most likely) or English,Yes,No,Unknown,Unknown,Fine-tuning + other modules; Prompting + other modules,"A base LLM was fine-tuned on 80 structured psychological counseling cases to adapt it to clinical contexts. Additionally, the model uses dynamic prompting with a structured DSM-5-based knowledge base. This is integrated into a larger pipeline involving text summarization, keyword extraction, and similarity analysis, alongside multimodal outputs (e.g., facial expressions, voice, and a visual avatar) to enable affective interaction.","Informal counseling (e.g., emotional support conversation); Unspecified, might include formal therapy methods",Other: ChatGLM2-6B ,Yes,y,n,y,y,Client satisfaction scale (CSS),,n,,,,n,,,,n,,,,n,,,,y,s/b,l,The LLM-Counselor Support System,n,,,,n,,,,n ,,,,counselor rating form-short (crf-s),b,l,Other LLMs/Models,n,,,,n,,,,n,,y ,"A professional counselor with three years of experience 
monitored the entire conversation to ensure the safety of the 
counseling process",y,GLM2-6B model ,n,,y,"Finally, 49 participants were selected to participate in 
this study for the experiment. The age range of participants 
is 15–40 years old. Among them, 29 (59.18%) were male 
and 20 (40.82%) were female; 15 (30.61%) were middle 
school students, 6 (12.24%) were undergraduate students, 
12 (24.49%) were graduate students, and 16 (32.65%) were 
already working; 12 (24.49%) were 15–18 years old, 21 
(42.86%) were 18–24 years old, and 16 people (32.65%) 
were over 24 years old. All participants did not have a pro-
fessional background in psychology.",n,,n,,n,,n,,n,,n,,n,,
2nd_search_3,2nd_search_365,VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model,Consensus,Journal paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",26.0,11.0,2024.0,China,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,General population,49,,Self-collected data,zhang 2024 dataset,Other: Descriptions of mental disorders in the DSM-5,Other: unknown,Other: unknown,No,Other: n.a.,Other: n.a.,Fine-tuning + other modules; Prompting + other modules,"A base LLM was fine-tuned on 80 structured psychological counseling cases to adapt it to clinical contexts. Additionally, the model uses dynamic prompting with a structured DSM-5-based knowledge base. This is integrated into a larger pipeline involving text summarization, keyword extraction, and similarity analysis, alongside multimodal outputs (e.g., facial expressions, voice, and a visual avatar) to enable affective interaction.","Unspecified, might include formal therapy methods",Other: ChatGLM2-6B ,Yes,y,n,y,y,"Counselor rating form‑short (CRF‑S), Client satisfaction scale (CSS)",,n,,,,n,,,,n,,,,n,,,,y,b,l,The LLM-Counselor Support System. Benchmark: GPT-4 with zero-shot CoT,n,,,,n,,,,n ,,,,client satisfaction scale (css),b,l,Benchmars are other LLM-based models,counselor rating form‑short (crf‑s),b,l,Benchmars are other LLM-based models,sentiment score,b,l,Benchmars are other LLM-based models,n,,n,"A professional counselor with three years of experience 
monitored the entire conversation to ensure the safety of the 
counseling process",y,GLM2-6B model ,n,,y,"Finally, 49 participants were selected to participate in 
this study for the experiment. The age range of participants 
is 15–40 years old. Among them, 29 (59.18%) were male 
and 20 (40.82%) were female; 15 (30.61%) were middle 
school students, 6 (12.24%) were undergraduate students, 
12 (24.49%) were graduate students, and 16 (32.65%) were 
already working; 12 (24.49%) were 15–18 years old, 21 
(42.86%) were 18–24 years old, and 16 people (32.65%) 
were over 24 years old. All participants did not have a pro-
fessional background in psychology.",n,,n,,n,,n,,n,,n,,n,,
2nd_search_2,2nd_search_366,Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach,Richard Gaus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",20.0,2.0,2025.0,Other: Vietnam,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"https://huggingface.co/datasets/jkhedri/psychology-dataset; Only questions embedded, not answers.",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules; Other: RAG,"Uses RAG with prompt engineering: general LLM is guided by structured prompt templates for intent detection and empathetic response generation, while a vector-based QA database supplies domain-specific context.","Unspecified, might include formal therapy methods","Other: No specific model chosen, but as a conclusion claude-sonnet-3.5 suggested. ",No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,"Three LLMs (Claude-Sonnet-3.5, gemini-1.5-pro, gpt-4o) used for evaluation of optimal base model for developed application. Then used LLM (gpt-4o) as a judge to compare reults agains each other. "
2nd_search_2,2nd_search_367,Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach,Reviewer Two,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,11.0,2024.0,Other: Vietnam,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"
1 https://huggingface.co/datasets/jkhedri/psychology-dataset.",Emotional support dialogue -- chat logs,English,No,Yes,Unselected,Unknown,Prompting + other modules,,"Informal counseling (e.g., emotional support conversation)",Other: unclear,,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,a little,facilitate access to professional care by linking users with nearby psychiatrists or psychology centers,
2nd_search_2,2nd_search_368,Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach,Consensus,Conference paper,"Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings)",11.0,11.0,2024.0,Other: Vietnam,Empirical research involving an LLM,Client-facing application,Multi-turn chatbot,No clients/patients involved,,,External data set,"https://huggingface.co/datasets/jkhedri/psychology-dataset; Only questions embedded, not answers.",Internet data -- mental health Q&A,English,No,Yes,Unknown,Unknown,Prompting + other modules,"Uses RAG with prompt engineering: general LLM is guided by structured prompt templates for intent detection and empathetic response generation, while a vector-based QA database supplies domain-specific context.","Unspecified, might include formal therapy methods",GPT-4 / GPT-4o family; Gemini / Bard family; Claude family,No users involved,n,n,n,n,,,n,,,,n,,,,n,,,,n,,,,n,,,,y,no benchmark,no benchmark,No indexmodel but three equaly ranked models were compared ,n,,,,n,,,,llm-as-a-judge approval and reassurance,no benchmark,no benchmark,no benchmark,llm-as-a-judge general response quality,no benchmark,no benchmark,no benchmark,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_1,2nd_search_369,""" I've talked to ChatGPT about my issues last night."": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",Reviewer Two,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),29.0,4.0,2025.0,USA,Other: content analysis of SM posts,Analysis of conversation transcripts,,General population,,,External data set,reddit mental health conversations mentioning ChatGPT,Internet data -- mental health forum,English,No,Yes,Unselected,Lay people,,,Other: unsure,"ChatGPT, model unspecified",Yes,n,y,n,y,themtaic coding of user posts // themes: ,,n,,,,n,,,,n,,,,n,,,,y,,,thematic content analysis by authors?,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"discuss misinformation, over-validation, and ethical risks reported by users",n,,y,Ethical use of Reddit data with disclaimers in 3.3.1,n,,n,,n,,y,"Somehow: 
However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT’s overly validating nature, and privacy concerns",n,,n,,y,Somehow: user-reported perceptions from Reddit,y,"Speculative discussion on integration with clinical care, but not implemented",
2nd_search_1,2nd_search_370,""" I've talked to ChatGPT about my issues last night."": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",Richard Gaus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),18.0,10.0,2025.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,177 Reddit posts,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,Thematic analysis of Reddit posts and comments,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,n,,
2nd_search_1,2nd_search_371,""" I've talked to ChatGPT about my issues last night."": Examining Mental Health Conversations with Large Language Models through Reddit Analysis",Consensus,Conference paper,Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems),18.0,10.0,2025.0,USA,Population survey,Client-facing application,Multi-turn chatbot,General population,177 Reddit posts,,No dataset used for development or evaluation,,,,,,,,,,"Unspecified, might include formal therapy methods","ChatGPT, model unspecified",Yes,n,y,n,y,,Thematic analysis of Reddit posts and comments,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,,,n,,y,"discuss misinformation, over-validation, and ethical risks reported by users",n,,y,Ethical use of Reddit data with disclaimers in 3.3.1,n,,n,,n,,y,"Somehow: 
However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT’s overly validating nature, and privacy concerns",n,,n,,y,Somehow: user-reported perceptions from Reddit,y,"Speculative discussion on integration with clinical care, but not implemented",
