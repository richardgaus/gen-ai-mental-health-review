================================================================================
DATA ANALYSIS REPORT
Input File: data/intermediate/fused_data_cleaned.csv
Generated: 2025-10-30 22:45:40
Filter: Consensus reviewer only
================================================================================

DATASET OVERVIEW
----------------------------------------
Total rows: 172
Total columns: 104

COLUMN-BY-COLUMN ANALYSIS
----------------------------------------

1. covidence_id
   ===============
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 172
   Category: CATEGORICAL

   Value Counts:
     1st_search_224: 1 (0.6%)
     2nd_search_90: 1 (0.6%)
     2nd_search_99: 1 (0.6%)
     2nd_search_98: 1 (0.6%)
     2nd_search_97: 1 (0.6%)
     2nd_search_96: 1 (0.6%)
     2nd_search_94: 1 (0.6%)
     2nd_search_93: 1 (0.6%)
     2nd_search_92: 1 (0.6%)
     2nd_search_91: 1 (0.6%)
     2nd_search_89: 1 (0.6%)
     2nd_search_101: 1 (0.6%)
     2nd_search_88: 1 (0.6%)
     2nd_search_87: 1 (0.6%)
     2nd_search_85: 1 (0.6%)
     2nd_search_83: 1 (0.6%)
     2nd_search_82: 1 (0.6%)
     2nd_search_81: 1 (0.6%)
     2nd_search_80: 1 (0.6%)
     2nd_search_75: 1 (0.6%)
     2nd_search_100: 1 (0.6%)
     2nd_search_102: 1 (0.6%)
     1st_search_214: 1 (0.6%)
     2nd_search_117: 1 (0.6%)
     2nd_search_129: 1 (0.6%)
     2nd_search_128: 1 (0.6%)
     2nd_search_127: 1 (0.6%)
     2nd_search_126: 1 (0.6%)
     2nd_search_125: 1 (0.6%)
     2nd_search_124: 1 (0.6%)
     2nd_search_123: 1 (0.6%)
     2nd_search_119: 1 (0.6%)
     2nd_search_116: 1 (0.6%)
     2nd_search_103: 1 (0.6%)
     2nd_search_114: 1 (0.6%)
     2nd_search_112: 1 (0.6%)
     2nd_search_111: 1 (0.6%)
     2nd_search_109: 1 (0.6%)
     2nd_search_108: 1 (0.6%)
     2nd_search_107: 1 (0.6%)
     2nd_search_106: 1 (0.6%)
     2nd_search_104: 1 (0.6%)
     2nd_search_72: 1 (0.6%)
     2nd_search_71: 1 (0.6%)
     2nd_search_69: 1 (0.6%)
     2nd_search_15: 1 (0.6%)
     2nd_search_28: 1 (0.6%)
     2nd_search_26: 1 (0.6%)
     2nd_search_25: 1 (0.6%)
     2nd_search_24: 1 (0.6%)
     2nd_search_22: 1 (0.6%)
     2nd_search_20: 1 (0.6%)
     2nd_search_18: 1 (0.6%)
     2nd_search_17: 1 (0.6%)
     2nd_search_11: 1 (0.6%)
     2nd_search_68: 1 (0.6%)
     2nd_search_10: 1 (0.6%)
     2nd_search_9: 1 (0.6%)
     2nd_search_8: 1 (0.6%)
     2nd_search_6: 1 (0.6%)
     2nd_search_5: 1 (0.6%)
     2nd_search_4: 1 (0.6%)
     2nd_search_3: 1 (0.6%)
     2nd_search_2: 1 (0.6%)
     2nd_search_29: 1 (0.6%)
     2nd_search_30: 1 (0.6%)
     2nd_search_31: 1 (0.6%)
     2nd_search_32: 1 (0.6%)
     2nd_search_66: 1 (0.6%)
     2nd_search_64: 1 (0.6%)
     2nd_search_63: 1 (0.6%)
     2nd_search_62: 1 (0.6%)
     2nd_search_60: 1 (0.6%)
     2nd_search_58: 1 (0.6%)
     2nd_search_57: 1 (0.6%)
     2nd_search_54: 1 (0.6%)
     2nd_search_51: 1 (0.6%)
     2nd_search_47: 1 (0.6%)
     2nd_search_46: 1 (0.6%)
     2nd_search_45: 1 (0.6%)
     2nd_search_43: 1 (0.6%)
     2nd_search_42: 1 (0.6%)
     2nd_search_40: 1 (0.6%)
     2nd_search_39: 1 (0.6%)
     2nd_search_34: 1 (0.6%)
     2nd_search_130: 1 (0.6%)
     2nd_search_131: 1 (0.6%)
     2nd_search_132: 1 (0.6%)
     1st_search_63: 1 (0.6%)
     1st_search_88: 1 (0.6%)
     1st_search_86: 1 (0.6%)
     1st_search_81: 1 (0.6%)
     1st_search_77: 1 (0.6%)
     1st_search_75: 1 (0.6%)
     1st_search_73: 1 (0.6%)
     1st_search_72: 1 (0.6%)
     1st_search_70: 1 (0.6%)
     1st_search_62: 1 (0.6%)
     1st_search_26: 1 (0.6%)
     1st_search_61: 1 (0.6%)
     1st_search_60: 1 (0.6%)
     1st_search_59: 1 (0.6%)
     1st_search_56: 1 (0.6%)
     1st_search_50: 1 (0.6%)
     1st_search_41: 1 (0.6%)
     1st_search_34: 1 (0.6%)
     1st_search_32: 1 (0.6%)
     1st_search_89: 1 (0.6%)
     1st_search_91: 1 (0.6%)
     1st_search_92: 1 (0.6%)
     1st_search_93: 1 (0.6%)
     1st_search_212: 1 (0.6%)
     1st_search_204: 1 (0.6%)
     1st_search_203: 1 (0.6%)
     1st_search_200: 1 (0.6%)
     1st_search_198: 1 (0.6%)
     1st_search_194: 1 (0.6%)
     1st_search_147: 1 (0.6%)
     1st_search_134: 1 (0.6%)
     1st_search_128: 1 (0.6%)
     1st_search_124: 1 (0.6%)
     1st_search_120: 1 (0.6%)
     1st_search_119: 1 (0.6%)
     1st_search_116: 1 (0.6%)
     1st_search_115: 1 (0.6%)
     1st_search_101: 1 (0.6%)
     1st_search_98: 1 (0.6%)
     1st_search_95: 1 (0.6%)
     1st_search_30: 1 (0.6%)
     1st_search_18: 1 (0.6%)
     2nd_search_134: 1 (0.6%)
     2nd_search_144: 1 (0.6%)
     2nd_search_157: 1 (0.6%)
     2nd_search_156: 1 (0.6%)
     2nd_search_155: 1 (0.6%)
     2nd_search_153: 1 (0.6%)
     2nd_search_152: 1 (0.6%)
     2nd_search_150: 1 (0.6%)
     2nd_search_148: 1 (0.6%)
     2nd_search_147: 1 (0.6%)
     2nd_search_143: 1 (0.6%)
     1st_search_17: 1 (0.6%)
     2nd_search_142: 1 (0.6%)
     2nd_search_141: 1 (0.6%)
     2nd_search_140: 1 (0.6%)
     2nd_search_139: 1 (0.6%)
     2nd_search_138: 1 (0.6%)
     2nd_search_137: 1 (0.6%)
     2nd_search_136: 1 (0.6%)
     2nd_search_135: 1 (0.6%)
     2nd_search_167: 1 (0.6%)
     2nd_search_169: 1 (0.6%)
     2nd_search_173: 1 (0.6%)
     2nd_search_180: 1 (0.6%)
     1st_search_14: 1 (0.6%)
     1st_search_13: 1 (0.6%)
     1st_search_9: 1 (0.6%)
     1st_search_3: 1 (0.6%)
     2nd_search_258: 1 (0.6%)
     2nd_search_251: 1 (0.6%)
     2nd_search_232: 1 (0.6%)
     2nd_search_230: 1 (0.6%)
     2nd_search_228: 1 (0.6%)
     2nd_search_219: 1 (0.6%)
     2nd_search_214: 1 (0.6%)
     2nd_search_212: 1 (0.6%)
     2nd_search_206: 1 (0.6%)
     2nd_search_202: 1 (0.6%)
     2nd_search_193: 1 (0.6%)
     2nd_search_186: 1 (0.6%)
     2nd_search_185: 1 (0.6%)
     2nd_search_1: 1 (0.6%)

   Sample values (first 20):
     1st_search_224
     1st_search_214
     1st_search_212
     1st_search_204
     1st_search_203
     1st_search_200
     1st_search_198
     1st_search_194
     1st_search_147
     1st_search_134
     1st_search_128
     1st_search_124
     1st_search_120
     1st_search_119
     1st_search_116
     1st_search_115
     1st_search_101
     1st_search_98
     1st_search_95
     1st_search_93


2. study_id
   ===========
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 172
   Category: CATEGORICAL

   Value Counts:
     1st_search_3: 1 (0.6%)
     2nd_search_212: 1 (0.6%)
     2nd_search_188: 1 (0.6%)
     2nd_search_191: 1 (0.6%)
     2nd_search_194: 1 (0.6%)
     2nd_search_197: 1 (0.6%)
     2nd_search_200: 1 (0.6%)
     2nd_search_203: 1 (0.6%)
     2nd_search_206: 1 (0.6%)
     2nd_search_209: 1 (0.6%)
     2nd_search_215: 1 (0.6%)
     2nd_search_182: 1 (0.6%)
     2nd_search_218: 1 (0.6%)
     2nd_search_221: 1 (0.6%)
     2nd_search_224: 1 (0.6%)
     2nd_search_227: 1 (0.6%)
     2nd_search_230: 1 (0.6%)
     2nd_search_233: 1 (0.6%)
     2nd_search_236: 1 (0.6%)
     2nd_search_239: 1 (0.6%)
     2nd_search_185: 1 (0.6%)
     2nd_search_179: 1 (0.6%)
     1st_search_6: 1 (0.6%)
     2nd_search_146: 1 (0.6%)
     2nd_search_122: 1 (0.6%)
     2nd_search_125: 1 (0.6%)
     2nd_search_128: 1 (0.6%)
     2nd_search_131: 1 (0.6%)
     2nd_search_134: 1 (0.6%)
     2nd_search_137: 1 (0.6%)
     2nd_search_140: 1 (0.6%)
     2nd_search_143: 1 (0.6%)
     2nd_search_149: 1 (0.6%)
     2nd_search_176: 1 (0.6%)
     2nd_search_152: 1 (0.6%)
     2nd_search_155: 1 (0.6%)
     2nd_search_158: 1 (0.6%)
     2nd_search_161: 1 (0.6%)
     2nd_search_164: 1 (0.6%)
     2nd_search_167: 1 (0.6%)
     2nd_search_170: 1 (0.6%)
     2nd_search_173: 1 (0.6%)
     2nd_search_242: 1 (0.6%)
     2nd_search_245: 1 (0.6%)
     2nd_search_248: 1 (0.6%)
     2nd_search_341: 1 (0.6%)
     2nd_search_317: 1 (0.6%)
     2nd_search_320: 1 (0.6%)
     2nd_search_323: 1 (0.6%)
     2nd_search_326: 1 (0.6%)
     2nd_search_329: 1 (0.6%)
     2nd_search_332: 1 (0.6%)
     2nd_search_335: 1 (0.6%)
     2nd_search_338: 1 (0.6%)
     2nd_search_344: 1 (0.6%)
     2nd_search_251: 1 (0.6%)
     2nd_search_347: 1 (0.6%)
     2nd_search_350: 1 (0.6%)
     2nd_search_353: 1 (0.6%)
     2nd_search_356: 1 (0.6%)
     2nd_search_359: 1 (0.6%)
     2nd_search_362: 1 (0.6%)
     2nd_search_365: 1 (0.6%)
     2nd_search_368: 1 (0.6%)
     2nd_search_314: 1 (0.6%)
     2nd_search_311: 1 (0.6%)
     2nd_search_308: 1 (0.6%)
     2nd_search_305: 1 (0.6%)
     2nd_search_254: 1 (0.6%)
     2nd_search_257: 1 (0.6%)
     2nd_search_260: 1 (0.6%)
     2nd_search_263: 1 (0.6%)
     2nd_search_266: 1 (0.6%)
     2nd_search_269: 1 (0.6%)
     2nd_search_272: 1 (0.6%)
     2nd_search_275: 1 (0.6%)
     2nd_search_278: 1 (0.6%)
     2nd_search_281: 1 (0.6%)
     2nd_search_284: 1 (0.6%)
     2nd_search_287: 1 (0.6%)
     2nd_search_290: 1 (0.6%)
     2nd_search_293: 1 (0.6%)
     2nd_search_296: 1 (0.6%)
     2nd_search_299: 1 (0.6%)
     2nd_search_302: 1 (0.6%)
     2nd_search_119: 1 (0.6%)
     2nd_search_116: 1 (0.6%)
     2nd_search_113: 1 (0.6%)
     1st_search_96: 1 (0.6%)
     1st_search_72: 1 (0.6%)
     1st_search_75: 1 (0.6%)
     1st_search_78: 1 (0.6%)
     1st_search_81: 1 (0.6%)
     1st_search_84: 1 (0.6%)
     1st_search_87: 1 (0.6%)
     1st_search_90: 1 (0.6%)
     1st_search_93: 1 (0.6%)
     1st_search_99: 1 (0.6%)
     1st_search_129: 1 (0.6%)
     1st_search_102: 1 (0.6%)
     1st_search_105: 1 (0.6%)
     1st_search_108: 1 (0.6%)
     1st_search_111: 1 (0.6%)
     1st_search_114: 1 (0.6%)
     1st_search_117: 1 (0.6%)
     1st_search_120: 1 (0.6%)
     1st_search_123: 1 (0.6%)
     1st_search_69: 1 (0.6%)
     1st_search_66: 1 (0.6%)
     1st_search_63: 1 (0.6%)
     1st_search_60: 1 (0.6%)
     1st_search_9: 1 (0.6%)
     1st_search_12: 1 (0.6%)
     1st_search_15: 1 (0.6%)
     1st_search_18: 1 (0.6%)
     1st_search_21: 1 (0.6%)
     1st_search_24: 1 (0.6%)
     1st_search_27: 1 (0.6%)
     1st_search_30: 1 (0.6%)
     1st_search_33: 1 (0.6%)
     1st_search_36: 1 (0.6%)
     1st_search_39: 1 (0.6%)
     1st_search_42: 1 (0.6%)
     1st_search_45: 1 (0.6%)
     1st_search_48: 1 (0.6%)
     1st_search_51: 1 (0.6%)
     1st_search_54: 1 (0.6%)
     1st_search_57: 1 (0.6%)
     1st_search_126: 1 (0.6%)
     1st_search_132: 1 (0.6%)
     2nd_search_110: 1 (0.6%)
     2nd_search_80: 1 (0.6%)
     2nd_search_56: 1 (0.6%)
     2nd_search_59: 1 (0.6%)
     2nd_search_62: 1 (0.6%)
     2nd_search_65: 1 (0.6%)
     2nd_search_68: 1 (0.6%)
     2nd_search_71: 1 (0.6%)
     2nd_search_74: 1 (0.6%)
     2nd_search_77: 1 (0.6%)
     2nd_search_83: 1 (0.6%)
     1st_search_135: 1 (0.6%)
     2nd_search_86: 1 (0.6%)
     2nd_search_89: 1 (0.6%)
     2nd_search_92: 1 (0.6%)
     2nd_search_95: 1 (0.6%)
     2nd_search_98: 1 (0.6%)
     2nd_search_101: 1 (0.6%)
     2nd_search_104: 1 (0.6%)
     2nd_search_107: 1 (0.6%)
     2nd_search_53: 1 (0.6%)
     2nd_search_50: 1 (0.6%)
     2nd_search_47: 1 (0.6%)
     2nd_search_44: 1 (0.6%)
     1st_search_138: 1 (0.6%)
     1st_search_141: 1 (0.6%)
     1st_search_144: 1 (0.6%)
     1st_search_147: 1 (0.6%)
     2nd_search_3: 1 (0.6%)
     2nd_search_6: 1 (0.6%)
     2nd_search_9: 1 (0.6%)
     2nd_search_12: 1 (0.6%)
     2nd_search_15: 1 (0.6%)
     2nd_search_19: 1 (0.6%)
     2nd_search_23: 1 (0.6%)
     2nd_search_26: 1 (0.6%)
     2nd_search_29: 1 (0.6%)
     2nd_search_32: 1 (0.6%)
     2nd_search_35: 1 (0.6%)
     2nd_search_38: 1 (0.6%)
     2nd_search_41: 1 (0.6%)
     2nd_search_371: 1 (0.6%)

   Sample values (first 20):
     1st_search_3
     1st_search_6
     1st_search_9
     1st_search_12
     1st_search_15
     1st_search_18
     1st_search_21
     1st_search_24
     1st_search_27
     1st_search_30
     1st_search_33
     1st_search_36
     1st_search_39
     1st_search_42
     1st_search_45
     1st_search_48
     1st_search_51
     1st_search_54
     1st_search_57
     1st_search_60


3. title
   ========
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 172
   Category: CATEGORICAL

   Value Counts:
     Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.: 1 (0.6%)
     Psychological Health Chatbot, Detecting and Assisting Patients in their Path to Recovery: 1 (0.6%)
     The Goldilocks Zone: Finding the right balance of user and institutional risk for suicide-related generative AI queries: 1 (0.6%)
     Development of a Mental Health Chatbot Using Large Language Models for Indonesian Undergraduates: 1 (0.6%)
     A Conversational Application for Insomnia Treatment: Leveraging the ChatGLM-LoRA Model for Cognitive Behavioral Therapy: 1 (0.6%)
     Computational Psychotherapy System for Mental Health Prediction and Behavior Change with a Conversational Agent: 1 (0.6%)
     LLM-based conversational agents for behaviour change support: A randomised controlled trial examining efficacy, safety, and the role of user behaviour: 1 (0.6%)
     Early Detection and Personalized Intervention in Mental Health: 1 (0.6%)
     Evaluating the Quality of Psychotherapy Conversational Agents: Framework Development and Cross-Sectional Study: 1 (0.6%)
     “Do You Need the Sage's Tea or the Friend's Cola” Exploring the Differential Healing Effects of Generative AI Conversational Styles: 1 (0.6%)
     Empowerment of Large Language Models in Psychological Counseling through Prompt Engineering: 1 (0.6%)
     A Comparison of Responses from Human Therapists and Large Language Model-Based Chatbots to Assess Therapeutic Communication: Mixed Methods Study: 1 (0.6%)
     The Machine as Therapist: Unpacking Transference and Emotional Healing in AI-Assisted Therapy: 1 (0.6%)
     Evaluating Language Models for Assessing Counselor Reflections: 1 (0.6%)
     Can AI relate: Testing large language model response for mental health support: 1 (0.6%)
     Multimodal Framework for Therapeutic Consultations: 1 (0.6%)
     Rational AIs with emotional deficits: ChatGPT vs. counselors in providing emotional reflections: 1 (0.6%)
     Comparative Study on the Performance of LLM-based Psychological Counseling Chatbots via Prompt Engineering Techniques: 1 (0.6%)
     Towards a Retrieval Augmented Generation System for Information on Suicide Prevention*: 1 (0.6%)
     Conversational ai for mental health support: 1 (0.6%)
     "Shaping ChatGPT into my Digital Therapist": A thematic analysis of social media discourse on using generative artificial intelligence for mental health: 1 (0.6%)
     Feasibility of Using ChatGPT to Generate Exposure Hierarchies for Treating Obsessive-Compulsive Disorder: 1 (0.6%)
     Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care: 1 (0.6%)
     Integrating AI into ADHD Therapy: Insights from ChatGPT-4o and Robotic Assistants: S. Berrezueta-Guzman et al.: 1 (0.6%)
     Multi-Tiered RAG-Based Chatbot for Mental Health Support: 1 (0.6%)
     ChatGPT, the voice from elsewhere: a poetic and therapeutic dialog between human and artificial intelligence: 1 (0.6%)
     Artificial intelligence (AI) in pediatric sleep: AI vs. expert-generated psychotherapeutic pediatric sleep stories: 1 (0.6%)
     Safety and User Experience of a Generative Artificial Intelligence Digital Mental Health Intervention: Exploratory Randomized Controlled Trial: 1 (0.6%)
     PIRTRE-C: A Two-Stage Retrieval and Reranking Enhanced Framework for Improving Chinese Psychological Counseling: 1 (0.6%)
     Empathy AI: Leveraging Emotion Recognition for Enhanced Human-AI Interaction: 1 (0.6%)
     SentimentCareBot: Retrieval-augmented generation chatbot for mental health support with sentiment analysis: 1 (0.6%)
     The impact of prompt engineering in large language model performance: a psychiatric example: 1 (0.6%)
     Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers.: 1 (0.6%)
     Addressing the Challenges of Mental Health Conversations with Large Language Models: 1 (0.6%)
     Exploring the Efficacy of Robotic Assistants with ChatGPT and Claude in Enhancing ADHD Therapy: Innovating Treatment Paradigms: 1 (0.6%)
     Can AI Technologies Support Clinical Supervision? Assessing the Potential of ChatGPT: 1 (0.6%)
     Counselor-AI Collaborative Transcription and Editing System for Child Counseling Analysis: 1 (0.6%)
     Comparing Traditional Book Wisdom with Large Language Model's Guidance on Time and Stress Management: 1 (0.6%)
     Implementing Cognitive Behavioral Therapy in Chatbots to Reduce Students’ Exam Stress using ChatGPT: 1 (0.6%)
     Design and Implementation of an AI-Driven Mental Health Chatbot: A Generative AI Model: 1 (0.6%)
     Enhancing Patient Intake Process in Mental Health Consultations Using RAG-Driven Chatbot: 1 (0.6%)
     Mental Health Support Using Gen-AI Shot Prompting Technique and Vector Embeddings: 1 (0.6%)
     AI as the Therapist: Student Insights on the Challenges of Using Generative AI for School Mental Health Frameworks: 1 (0.6%)
     Exploring the potential of ChatGPT as a digital advisor in acute psychiatric crises: a feasibility study: 1 (0.6%)
     Komorebi: Enhancing Mental Wellness with Sentiment Analysis and Cognitive Behavior Therapy: 1 (0.6%)
     MindBridge: AI-Enhanced Virtual Mental Health Platform for Emotional Analysis and Levaraging: 1 (0.6%)
     Enhancing Emotional Support Capabilities of Large Language Models through Cascaded Neural Networks: 1 (0.6%)
     SOCRATES. Developing and Evaluating a Fine-Tuned ChatGPT Model for Accessible Mental Health Intervention: 1 (0.6%)
     Generative AI-Enabled Therapy Support Tool for Improved Clinical Outcomes and Patient Engagement in Group Therapy: Real-World Observational Study: 1 (0.6%)
     Modeling Implicit Emotion and User-specific Context for Malevolence Detection in Mental Health Counseling Dialogues: 1 (0.6%)
     Predicting Satisfaction With Chat-Counseling at a 24/7 Chat Hotline for the Youth: Natural Language Processing Study: 1 (0.6%)
     Trust, Support, and Adoption Intentions Towards Generative AI are Context Dependent: 1 (0.6%)
     Bridging Gender Disparities in Mental Health: A Bilingual Large Language Model for Multilingual Therapeutic Chatbots: 1 (0.6%)
     A Comprehensive RAG-Based LLM for AI-Driven Mental Health Chatbot: 1 (0.6%)
     LLM-Therapist: A RAG-Based Multimodal Behavioral Therapist as Healthcare Assistant: 1 (0.6%)
     Describing the Framework for AI Tool Assessment in Mental Health and Applying It to a Generative AI Obsessive-Compulsive Disorder Platform: Tutorial.: 1 (0.6%)
     Competency of Large Language Models in Evaluating Appropriate Responses to Suicidal Ideation: Comparative Study: 1 (0.6%)
     Development and Evaluation of a Mental Health Chatbot Using ChatGPT4.0: Mixed Methods UserExperience Study With Korean Users: 1 (0.6%)
     Exploring user characteristics, motives, and expectations and the therapeutic alliance in the mental health conversational AI Clare®: a baseline study: 1 (0.6%)
     Generative AI-Derived Information About Opioid Use Disorder Treatment During Pregnancy: An exploratory evaluation of GPT-4's steerability for provision of trustworthy person-centered information.: 1 (0.6%)
     Integrative modeling enables ChatGPT to achieve average level of human counselors performance in mental health Q&A: 1 (0.6%)
     Investigating the key success factors of chatbot-based positive psychology intervention with retrieval-and generative pre-trained transformer (GPT)-based chatbots: 1 (0.6%)
     VCounselor: a psychological intervention chat agent based on a knowledge-enhanced large language model: 1 (0.6%)
     Enhancing AI Chatbots for Mental Health Support: A Comprehensive Approach: 1 (0.6%)
     When ELIZA meets therapists: A Turing test for the heart and mind: 1 (0.6%)
     Assessing the Adherence of ChatGPT Chatbots to Public Health Guidelines for Smoking Cessation: Content Analysis: 1 (0.6%)
     Artificial Intelligence in Mental Health Care: The T5 Chatbot Project: 1 (0.6%)
     Randomized trial of a generative AI chatbot for mental health treatment: 1 (0.6%)
     Chinese Psychological Counseling Corpus Construction for Valence-Arousal Sentiment Intensity Prediction: 1 (0.6%)
     Empowering pediatric, adolescent, and young adult patients with cancer utilizing generative AI chatbots to reduce psychological burden and enhance treatment engagement: a pilot study: 1 (0.6%)
     Developing a single-session outcome measure using natural language processing on digital mental health transcripts: 1 (0.6%)
     Mentalblend: Enhancing online mental health support through the integration of llms with psychological counseling theories: 1 (0.6%)
     Conversational Agents for Dementia using Large Language Models: 1 (0.6%)
     Evaluating for Evidence of Sociodemographic Bias in Conversational AI for Mental Health Support: 1 (0.6%)
     AI Integration in Counseling Training: Aiding Counselors-in-Training in Self-Efficacy Enhancement and Anxiety Reduction: 1 (0.6%)
     "I Don't Understand It, but Okay": An Empirical Study of Mental Health Practitioners' Readiness to Use Large Language Models: 1 (0.6%)
     Emotion-aware psychological first aid: Integrating BERT-based emotional distress detection with Psychological First Aid-Generative Pre-Trained Transformer chatbot for mental health support: 1 (0.6%)
     Digital Risk Considerations Across Generative AI-based Mental Health Apps: 1 (0.6%)
     Emotion-Aware Embedding Fusion in Large Language Models (Flan-T5, Llama 2, DeepSeek-R1, and ChatGPT 4) for Intelligent Response Generation: 1 (0.6%)
     Expert Patient Interaction Language Model (EPILM): 1 (0.6%)
     Chatbot in the E-Service of Mental Health Using the Reprogramming of the GPT-2 Model: 1 (0.6%)
     The externalization of internal experiences in psychotherapy through generative artificial intelligence: a theoretical, clinical, and ethical analysis: 1 (0.6%)
     Multimodal Integration, Fine Tuning of Large Language Model for Autism Support: 1 (0.6%)
     Use of AI in mental health care: Community and mental health professionals survey: 1 (0.6%)
     Adversarial Evaluation Algorithm for Detecting Extreme Behaviors of LLMs in Psychological Counseling Scenarios: 1 (0.6%)
     Exploring the potential of lightweight large language models for AI-based mental health counselling task: a novel comparative study: 1 (0.6%)
     Harnessing AI in Anxiety Management: A Chatbot-Based Intervention for Personalized Mental Health Support: 1 (0.6%)
     AI-Enhanced Virtual Reality Self-Talk for Psychological Counseling: Formative Qualitative Study: 1 (0.6%)
     A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study: 1 (0.6%)
     Speaker and Time-aware Joint Contextual Learning for Dialogue-act Classification in Counselling Conversations: 1 (0.6%)
     Decoding emotions: Exploring the validity of sentiment analysis in psychotherapy: 1 (0.6%)
     Perception of Psychological Recommendations Generated by Neural Networks by Student Youth (Using ChatGPT as an Example): 1 (0.6%)
     Efficacy of ChatGPT in Cantonese Sentiment Analysis: Comparative Study: 1 (0.6%)
     Harnessing AI to Optimize Thought Records and Facilitate Cognitive Restructuring in Smartphone CBT: An Exploratory Study: 1 (0.6%)
     Counseling-Style Reflection Generation Using Generative Pretrained Transformers with Augmented Context: 1 (0.6%)
     Advancing Tinnitus Therapeutics: GPT-2 Driven Clustering Analysis of Cognitive Behavioral Therapy Sessions and Google T5-Based Predictive Modeling for THI Score Assessment: 1 (0.6%)
     Towards Motivational and Empathetic Response Generation in Online Mental Health Support: 1 (0.6%)
     Evaluating the Experience of LGBTQ plus People Using Large Language Model Based Chatbots for Mental Health Support: 1 (0.6%)
     Can Large Language Models Replace Therapists? Evaluating Performance at Simple Cognitive Behavioral Therapy Tasks.: 1 (0.6%)
     Improving Classification of Infrequent Cognitive Distortions: Domain-Specific Model vs. Data Augmentation: 1 (0.6%)
     Facilitating Self-Guided Mental Health Interventions Through Human-Language Model Interaction: A Case Study of Cognitive Restructuring: 1 (0.6%)
     Generative Transformer Chatbots for Mental Health Support: A Study on Depression and Anxiety: 1 (0.6%)
     Leveraging ChatGPT to optimize depression intervention through explainable deep learning: 1 (0.6%)
     Deep learning-based dimensional emotion recognition for conversational agent-based cognitive behavioral therapy: 1 (0.6%)
     Deep Learning Mental Health Dialogue System: 1 (0.6%)
     Designing a Large Language Model-Based Coaching Intervention for Lifestyle Behavior Change: 1 (0.6%)
     Towards Facilitating Empathic Conversations in Online Mental Health Support: A Reinforcement Learning Approach: 1 (0.6%)
     Conversational Bots for Psychotherapy: A Study of Generative Transformer Models Using Domain-specific Dialogues: 1 (0.6%)
     Leveraging Natural Language Processing to Study Emotional Coherence in Psychotherapy: 1 (0.6%)
     Machine Learning-Based Evaluation of Suicide Risk Assessment in Crisis Counseling Calls.: 1 (0.6%)
     Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.: 1 (0.6%)
     Feasibility of combining spatial computing and AI for mental health support in anxiety and depression: 1 (0.6%)
     An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system: 1 (0.6%)
     Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments: 1 (0.6%)
     Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs): 1 (0.6%)
     Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO): 1 (0.6%)
     Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference: 1 (0.6%)
     Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models: 1 (0.6%)
     CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering: 1 (0.6%)
     Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support: 1 (0.6%)
     Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings: 1 (0.6%)
     Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I: 1 (0.6%)
     Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023: 1 (0.6%)
     A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II: 1 (0.6%)
     Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I: 1 (0.6%)
     Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement: 1 (0.6%)
     Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation: 1 (0.6%)
     Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study: 1 (0.6%)
     ChatGPT as a Complementary Mental Health Resource: A Boon or a Bane: 1 (0.6%)
     Mental Health Counseling From Conversational Content With Transformer-Based Machine Learning: 1 (0.6%)
     Enhanced Contextual Comprehension Utilising an Integrated Transformer-BERT Model in a Counselling Chat-Bot: 1 (0.6%)
     Personalized Mental Health Assistance: Integrating Emotion Prediction with GPT-Based Chatbot: 1 (0.6%)
     Design and Evaluation of an AI-Powered Conversational Agent for Personalized Mental Health Support and Intervention (MindBot): 1 (0.6%)
     The Role of AI Counselling in Journaling for Mental Health Improvement: 1 (0.6%)
     Chatbot-delivered mental health support: Attitudes and utilization in a sample of US college students: 1 (0.6%)
     Leveraging Natural Language Processing to Enhance Feedback-Informed Group Therapy: A Proof of Concept: 1 (0.6%)
     MoodMap: Integrating NLP and AI for Psychological Well-Being and Sentiment Detection: 1 (0.6%)
     nBERT: Harnessing NLP for Emotion Recognition in Psychotherapy to Transform Mental Health Care: 1 (0.6%)
     Investigating the interpretability of ChatGPT in mental health counseling: An analysis of artificial intelligence generated content differentiation: 1 (0.6%)
     AI-Enhanced Cognitive Therapy: Personalized Guidance via Adaptive Agents with Voice Analysis and Stress Detection: 1 (0.6%)
     Mello: A Large Language Model for Mental Health Counselling Conversations: 1 (0.6%)
     Can ChatGPT provide a better support: a comparative analysis of ChatGPT and dataset responses in mental health dialogues: 1 (0.6%)
     Cases of Using ChatGPT as a Mental Health and Psychological Support Tool: 1 (0.6%)
     Employing large language models for emotion detection in psychotherapy transcripts: 1 (0.6%)
     MENTALER: Toward Professional Mental Health Support with LLMs via Multi-Role Collaboration: 1 (0.6%)
     Development and Validation of a Cognitive Behavioral Therapy for Psychosis Online Training With Automated Feedback: 1 (0.6%)
     Development and preliminary testing of a secure large language model-based chatbot for brief alcohol counseling in young adults: 1 (0.6%)
     AI-Driven Mental Health Chatbot: Empowering Well-Being with Conversational AI and Retrieval-Augmented Generation: 1 (0.6%)
     Towards culturally adaptive large language models in mental health: Using ChatGPT as a case study: 1 (0.6%)
     MindScape Continuum: Advancing Online Counseling with Bert-Based Strategy Classification and Professional Helping Skills: 1 (0.6%)
     Psyqa: A chinese dataset for generating long counseling text for mental health support: 1 (0.6%)
     Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling: 1 (0.6%)
     Revolutionizing Mental Health Care through LangChain: A Journey with a Large Language Model: 1 (0.6%)
     A Novel Cognitive Behavioral Therapy-Based Generative AI Tool(Socrates 2.0) to Facilitate Socratic Dialogue: Protocol for a Mixed Methods Feasibility Study: 1 (0.6%)
     ChatGPT as a psychotherapist for anxiety disorders: An empirical study with anxiety patients.: 1 (0.6%)
     Revealing the source: How awareness alters perceptions of AI and human-generated mental health responses: 1 (0.6%)
     Safety of Large Language Models in Addressing Depression.: 1 (0.6%)
     Assessing the Effectiveness of ChatGPT in Delivering Mental Health Support: A Qualitative Study: 1 (0.6%)
     "It happened to be the perfect thing": experiences of generative AI chatbots for mental health.: 1 (0.6%)
     Healme: Harnessing cognitive reframing in large language models for psychotherapy: 1 (0.6%)
     Chain-of-Interaction: Enhancing Large Language Models for Psychiatric Behavior Understanding by Dyadic Contexts: 1 (0.6%)
     Introducing CounseLLMe: A dataset of simulated mental health dialogues for comparing LLMs like Haiku, LLaMAntino and ChatGPT against humans: 1 (0.6%)
     Smile: Single-turn to multi-turn inclusive language expansion via chatgpt for mental health support: 1 (0.6%)
     Assessing the use of ChatGPT as a psychoeducational tool for mental health practice: 1 (0.6%)
     Empowering Psychotherapy with Large Language Models: Cognitive Distortion Detection through Diagnosis of Thought Prompting: 1 (0.6%)
     Domain-Specific Improvement on Psychotherapy Chatbot Using Assistant: 1 (0.6%)
     Enhanced emotion and sentiment recognition for empathetic dialogue system using big data and deep learning methods: 1 (0.6%)
     PsycoLLM: Enhancing LLM for Psychological Understanding and Evaluation: 1 (0.6%)
     Are Large Language Models Possible to Conduct Cognitive Behavioral Therapy?: 1 (0.6%)
     Using AI Based Chatbot ChatGPT for Practicing Counseling Skills Through Role-Play: 1 (0.6%)
     Generation, Distillation and Evaluation of Motivational Interviewing-Style Reflections with a Foundational Language Model: 1 (0.6%)
     " I've talked to ChatGPT about my issues last night.": Examining Mental Health Conversations with Large Language Models through Reddit Analysis: 1 (0.6%)

   Sample values (first 20):
     Linguistic Features of Clients and Counselors for Early Detection of Mental Health Issues in Online Text-based Counseling.
     Chatgpt: A pilot study on a promising tool for mental health support in psychiatric inpatient care
     Feasibility of combining spatial computing and AI for mental health support in anxiety and depression
     An experimental study of integrating fine-tuned LLMs and prompts for enhancing mental health support chatbot system
     Generative AI in Psychological Therapy: Perspectives on Computational Linguistics and Large Language Models in Written Behaviour Monitoring - Proceedings of the 17th International Conference on PErvasive Technologies Related to Assistive Environments
     Supporting the Demand on Mental Health Services with AI-Based Conversational Large Language Models (LLMs)
     Mental Healthcare Chatbot Based on Custom Diagnosis Documents Using a Quantized Large Language Model - 2024 11th International Conference on Reliability, Infocom Technologies and Optimization (Trends and Future Directions)(ICRITO)
     Application of Artificial Intelligence in Mental Healthcare: Generative Pre-trained Transformer 3 (GPT-3) and Cognitive Distortions - Proceedings of the Future Technologies Conference
     Towards Understanding Counseling Conversations: Domain Knowledge and Large Language Models
     CBT-LLM: A Chinese Large Language Model for Cognitive Behavioral Therapy-based Mental Health Question Answering
     Understanding the Benefits and Challenges of Using Large Language Model-based Conversational Agents for Mental Well-being Support
     Research on the Construction of Psychological Crisis Intervention Strategy Service System - Health Information Science: 11th International Conference, HIS 2022, Virtual Event, October 28–30, 2022, Proceedings
     Multi-modal Multi-emotion Emotional Support Conversation - Advanced Data Mining and Applications: 19th International Conference, ADMA 2023, Shenyang, China, August 21–23, 2023, Proceedings, Part I
     Response-act Guided Reinforced Dialogue Generation for Mental Health Counseling - Proceedings of the ACM Web Conference 2023
     A Benchmark for Understanding Dialogue Safety in Mental Health Support - Natural Language Processing and Chinese Computing: 12th National CCF Conference, NLPCC 2023, Foshan, China, October 12–15, 2023, Proceedings, Part II
     Enhanced Emotion and Sentiment Recognition for Empathetic Dialogue System Using Big Data and Deep Learning Methods - Computational Science – ICCS 2023: 23rd International Conference, Prague, Czech Republic, July 3–5, 2023, Proceedings, Part I
     Future of ADHD Care: Evaluating the Efficacy of ChatGPT in Therapy Enhancement
     Generation of Backward-Looking Complex Reflections for a Motivational Interviewing-Based Smoking Cessation Chatbot Using GPT-4: Algorithm Development and Validation
     Exploring the Efficacy of Large Language Models in Summarizing Mental Health Counseling Sessions: Benchmark Study
     Automatic rating of therapist facilitative interpersonal skills in text: A natural language processing application.


4. reviewer_name
   ================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 1
   Category: CATEGORICAL

   Value Counts:
     Consensus: 172 (100.0%)


5. outlet_type
   ==============
   Data type: object
   Total values: 172
   Non-null values: 160
   Null values: 12
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     Conference paper: 82 (51.2%)
     Journal paper: 78 (48.8%)
     [NULL]: 12 (7.5%)


6. outlet_field
   ===============
   Data type: object
   Total values: 172
   Non-null values: 160
   Null values: 12
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     Computer Science, AI, Engineering (e.g., ACM, IEEE, SIGIR, BigComp proceedings): 67 (41.9%)
     Digital Health / Medical Informatics (e.g., JMIR Mental Health, NPJ Digital Medicine, Frontiers in Digital Health): 37 (23.1%)
     Human-Computer Interaction (e.g. CHI conference on Human Factors in Computing Systems): 17 (10.6%)
     Psychotherapy / Counseling (e.g., Psychotherapy Research, Cognitive Therapy and Research): 13 (8.1%)
     [NULL]: 12 (7.5%)
     Broader Medicine / Biomedical (e.g., Nature Medicine, Cureus, Annals of Biomedical Engineering): 10 (6.2%)
     Psychiatry / Clinical Mental Health (e.g., Archives of Psychiatry Research, Current Psychiatry Reports): 8 (5.0%)
     Other (e.g., Humanities & Social Sciences Communications, generalist outlets): 8 (5.0%)


7. day
   ======
   Data type: float64
   Total values: 172
   Non-null values: 161
   Null values: 11
   Unique values: 31
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 11 (6.8%)
     11.0: 9 (5.6%)
     3.0: 9 (5.6%)
     18.0: 8 (5.0%)
     17.0: 8 (5.0%)
     26.0: 7 (4.3%)
     2.0: 7 (4.3%)
     28.0: 7 (4.3%)
     30.0: 7 (4.3%)
     12.0: 7 (4.3%)
     21.0: 6 (3.7%)
     9.0: 6 (3.7%)
     23.0: 6 (3.7%)
     5.0: 6 (3.7%)
     10.0: 6 (3.7%)
     13.0: 5 (3.1%)
     27.0: 5 (3.1%)
     25.0: 5 (3.1%)
     8.0: 5 (3.1%)
     24.0: 5 (3.1%)
     15.0: 5 (3.1%)
     19.0: 4 (2.5%)
     14.0: 4 (2.5%)
     29.0: 3 (1.9%)
     6.0: 3 (1.9%)
     7.0: 3 (1.9%)
     20.0: 3 (1.9%)
     4.0: 3 (1.9%)
     16.0: 3 (1.9%)
     1.0: 2 (1.2%)
     31.0: 2 (1.2%)
     22.0: 2 (1.2%)

   Sample values (first 20):
     11.0
     9.0
     26.0
     4.0
     22.0
     14.0
     2.0
     17.0
     20.0
     28.0
     5.0
     30.0
     12.0
     3.0
     19.0
     23.0
     16.0
     18.0
     21.0
     7.0


8. month
   ========
   Data type: float64
   Total values: 172
   Non-null values: 161
   Null values: 11
   Unique values: 12
   Category: CATEGORICAL

   Value Counts:
     12.0: 25 (15.5%)
     1.0: 17 (10.6%)
     5.0: 16 (9.9%)
     7.0: 15 (9.3%)
     3.0: 15 (9.3%)
     10.0: 15 (9.3%)
     4.0: 15 (9.3%)
     11.0: 13 (8.1%)
     [NULL]: 11 (6.8%)
     2.0: 10 (6.2%)
     6.0: 10 (6.2%)
     9.0: 5 (3.1%)
     8.0: 5 (3.1%)


9. year
   =======
   Data type: float64
   Total values: 172
   Non-null values: 161
   Null values: 11
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     2024.0: 81 (50.3%)
     2025.0: 50 (31.1%)
     2023.0: 20 (12.4%)
     [NULL]: 11 (6.8%)
     2022.0: 7 (4.3%)
     2021.0: 2 (1.2%)
     2020.0: 1 (0.6%)


10. author_country
   ==================
   Data type: object
   Total values: 172
   Non-null values: 154
   Null values: 18
   Unique values: 35
   Category: CATEGORICAL

   Value Counts:
     USA: 42 (27.3%)
     China: 25 (16.2%)
     India: 20 (13.0%)
     [NULL]: 18 (11.7%)
     Germany: 9 (5.8%)
     UK: 8 (5.2%)
     Other: Canada: 7 (4.5%)
     Other: Korea: 5 (3.2%)
     Other: Australia: 3 (1.9%)
     Other: Israel: 3 (1.9%)
     Other: Italy: 3 (1.9%)
     Other: Saudi Arabia: 3 (1.9%)
     Other: Iran: 2 (1.3%)
     Other: Japan: 2 (1.3%)
     Other: United Arab Emirates: 1 (0.6%)
     Other: Iraq: 1 (0.6%)
     Other: Indonesia: 1 (0.6%)
     Other: New Zealand: 1 (0.6%)
     Other: Mexico: 1 (0.6%)
     Other: Switzerland: 1 (0.6%)
     Other: Spain: 1 (0.6%)
     Other: Turkey: 1 (0.6%)
     Other: Egypt: 1 (0.6%)
     Other: Slovenia: 1 (0.6%)
     Other: Czech Republic: 1 (0.6%)
     Other: Sri Lanka: 1 (0.6%)
     Other: Finland: 1 (0.6%)
     Other: Romania: 1 (0.6%)
     Other: Pakistan: 1 (0.6%)
     Other: Philippines: 1 (0.6%)
     Other: Portugal: 1 (0.6%)
     Other: Bangladesh: 1 (0.6%)
     Other: Poland: 1 (0.6%)
     Other: Russia: 1 (0.6%)
     Other: Kyrgyzstan: 1 (0.6%)
     Other: Vietnam: 1 (0.6%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 50

     • Other: Canada — 7 occurrence(s)
     • Other: Korea — 5 occurrence(s)
     • Other: Australia — 3 occurrence(s)
     • Other: Israel — 3 occurrence(s)
     • Other: Saudi Arabia — 3 occurrence(s)
     • Other: Italy — 3 occurrence(s)
     • Other: Japan — 2 occurrence(s)
     • Other: Iran — 2 occurrence(s)
     • Other: Portugal — 1 occurrence(s)
     • Other: Kyrgyzstan — 1 occurrence(s)
     • Other: Russia — 1 occurrence(s)
     • Other: Poland — 1 occurrence(s)
     • Other: Bangladesh — 1 occurrence(s)
     • Other: Czech Republic — 1 occurrence(s)
     • Other: Philippines — 1 occurrence(s)
     • Other: Pakistan — 1 occurrence(s)
     • Other: Romania — 1 occurrence(s)
     • Other: Finland — 1 occurrence(s)
     • Other: Sri Lanka — 1 occurrence(s)
     • Other: Indonesia — 1 occurrence(s)
     • Other: Slovenia — 1 occurrence(s)
     • Other: Turkey — 1 occurrence(s)
     • Other: Spain — 1 occurrence(s)
     • Other: Switzerland — 1 occurrence(s)
     • Other: Mexico — 1 occurrence(s)
     • Other: Iraq — 1 occurrence(s)
     • Other: United Arab Emirates — 1 occurrence(s)
     • Other: New Zealand — 1 occurrence(s)
     • Other: Egypt — 1 occurrence(s)
     • Other: Vietnam — 1 occurrence(s)

   Sample values (first 20):
     Other: Portugal
     USA
     UK
     Other: Australia
     India
     Other: Kyrgyzstan
     China
     Germany
     Other: Canada
     Other: Israel
     Other: Russia
     Other: Japan
     Other: Korea
     Other: Poland
     Other: Bangladesh
     Other: Saudi Arabia
     Other: Italy
     Other: Czech Republic
     Other: Philippines
     Other: Pakistan


11. study_type
   ==============
   Data type: object
   Total values: 172
   Non-null values: 161
   Null values: 11
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     Empirical research involving an LLM: 148 (91.9%)
     Population survey: 13 (8.1%)
     [NULL]: 11 (6.8%)


12. application_type
   ====================
   Data type: object
   Total values: 172
   Non-null values: 159
   Null values: 13
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     Client-facing application: 129 (81.1%)
     Analysis of conversation transcripts: 18 (11.3%)
     [NULL]: 13 (8.2%)
     Therapist-facing application: 9 (5.7%)
     Other: Analysis of CBT diary data: 1 (0.6%)
     Other: Analysis of conversation transcripts; Client-facing application: 1 (0.6%)
     Other: Client-facing AND therapist-facing: 1 (0.6%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 3

     • Other: Analysis of CBT diary data — 1 occurrence(s)
     • Other: Analysis of conversation transcripts; Client-facing application — 1 occurrence(s)
     • Other: Client-facing AND therapist-facing — 1 occurrence(s)


13. application_subtype_client_facing
   =====================================
   Data type: object
   Total values: 172
   Non-null values: 131
   Null values: 41
   Unique values: 17
   Category: CATEGORICAL

   Value Counts:
     Multi-turn chatbot: 79 (60.3%)
     [NULL]: 41 (31.3%)
     Chatbot: 24 (18.3%)
     One-turn chatbot (usually Q&A): 13 (9.9%)
     Other: reframed thought generator: 2 (1.5%)
     Other: Spoken dialog system: 1 (0.8%)
     Other: Emotional reflection generation: 1 (0.8%)
     Other: Multi-modal dialogue system: 1 (0.8%)
     Other: Multimodal dialog system: 1 (0.8%)
     Other: MI reflection generation: 1 (0.8%)
     Other: one-turn Q&A Chatbot: 1 (0.8%)
     Other: reflection generation: 1 (0.8%)
     Other: one-turn recommendations generated by LLM: 1 (0.8%)
     Other: conversation system with video, audio, and text input: 1 (0.8%)
     Other: "one turn chatbot", i.e., user inputs "feelings and confusion" and system makes analysis and outputs intervention text once. There is no turn-based interaction with the tool.: 1 (0.8%)
     Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation.: 1 (0.8%)
     Other: Spoken dialogue system: 1 (0.8%)
     Other: No specific subtype (AI use in general): 1 (0.8%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 15

     • Other: reframed thought generator — 2 occurrence(s)
     • Other: Spoken dialogue system — 1 occurrence(s)
     • Other: one-turn question answering chatbot. the client poses a question and the model produces a single response. there is no multi-turn conversation. — 1 occurrence(s)
     • Other: "one turn chatbot", i.e., user inputs "feelings and confusion" and system makes analysis and outputs intervention text once. There is no turn-based interaction with the tool. — 1 occurrence(s)
     • Other: conversation system with video, audio, and text input — 1 occurrence(s)
     • Other: one-turn recommendations generated by LLM — 1 occurrence(s)
     • Other: reflection generation — 1 occurrence(s)
     • Other: one-turn Q&A Chatbot — 1 occurrence(s)
     • Other: MI reflection generation — 1 occurrence(s)
     • Other: Multimodal dialog system — 1 occurrence(s)
     • Other: Multi-modal dialogue system — 1 occurrence(s)
     • Other: Emotional reflection generation — 1 occurrence(s)
     • Other: Spoken dialog system — 1 occurrence(s)
     • Other: No specific subtype (AI use in general) — 1 occurrence(s)


14. client_type
   ===============
   Category: MULTIPLE CHOICE (semicolon-separated)
   Total responses: 141
   Null responses: 31
   Total individual choices: 147
   Unique choice options: 10
   Average choices per response: 1.0

   Individual Choice Counts:
     No clients/patients involved: 87 (59.2%)
     General population: 34 (23.1%)
     People with some symptoms but not disorder (determined by symptom scales or questionnaires): 10 (6.8%)
     Patients recruited in hospital or outpatient treatment facility: 8 (5.4%)
     Patients with disorder explicitly based on ICD or DSM: 2 (1.4%)
     Other: unknown: 2 (1.4%)
     Other: "Child counseling experts": 1 (0.7%)
     Other: Therapists: 1 (0.7%)
     Other: Mental health practitioners: 1 (0.7%)
     Other: User data shown in Fig. 7 but sample nowhere futher described. Hence unclear, how data was retrieved.: 1 (0.7%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 6

     • Other: unknown — 2 occurrence(s)
     • Other: "Child counseling experts" — 1 occurrence(s)
     • Other: Therapists — 1 occurrence(s)
     • Other: Mental health practitioners — 1 occurrence(s)
     • Other: User data shown in Fig. 7 but sample nowhere futher described. Hence unclear, how data was retrieved. — 1 occurrence(s)


15. client_count
   ================
   Data type: object
   Total values: 172
   Non-null values: 54
   Null values: 118
   Unique values: 45
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 118 (218.5%)
     1: 3 (5.6%)
     42: 2 (3.7%)
     50: 2 (3.7%)
     8: 2 (3.7%)
     48: 2 (3.7%)
     20: 2 (3.7%)
     21: 2 (3.7%)
     11: 2 (3.7%)
     1000000 user reviews: 1 (1.9%)
     244: 1 (1.9%)
     49: 1 (1.9%)
     87 Reddit posts: 1 (1.9%)
     58: 1 (1.9%)
     16: 1 (1.9%)
     306: 1 (1.9%)
     159: 1 (1.9%)
     30: 1 (1.9%)
     193 — Quote: “final sample consisted of 107 CMs and 86 MHPs”: 1 (1.9%)
     9: 1 (1.9%)
     69: 1 (1.9%)
     unknown: 1 (1.9%)
     830: 1 (1.9%)
     210: 1 (1.9%)
     5: 1 (1.9%)
     70: 1 (1.9%)
     45: 1 (1.9%)
     160: 1 (1.9%)
     31: 1 (1.9%)
     14: 1 (1.9%)
     10: 1 (1.9%)
     68: 1 (1.9%)
     462: 1 (1.9%)
     35: 1 (1.9%)
     236: 1 (1.9%)
     349: 1 (1.9%)
     15531: 1 (1.9%)
     12: 1 (1.9%)
     399: 1 (1.9%)
     111: 1 (1.9%)
     24: 1 (1.9%)
     19: 1 (1.9%)
     6: 1 (1.9%)
     428: 1 (1.9%)
     7: 1 (1.9%)
     177 Reddit posts: 1 (1.9%)

   Sample values (first 20):
     12
     14
     10
     68
     462
     35
     236
     42
     349
     31
     15531
     20
     11
     399
     111
     24
     19
     8
     6
     50


16. application_subtype_therapist_facing
   ========================================
   Data type: object
   Total values: 172
   Non-null values: 11
   Null values: 161
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 161 (1463.6%)
     Treatment fidelity feedback: 3 (27.3%)
     Utterance suggestions: 2 (18.2%)
     Patient simulations: 2 (18.2%)
     Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition: 1 (9.1%)
     Other: OCD exposure hierarchy generation (therapy material generation): 1 (9.1%)
     Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs. No one particular application type.: 1 (9.1%)
     Other: No specific subtype (AI use in general): 1 (9.1%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 4

     • Other: Interactive transcription creation and analysis system. LLMs are used for video captioning and speaker role recognition — 1 occurrence(s)
     • Other: OCD exposure hierarchy generation (therapy material generation) — 1 occurrence(s)
     • Other: Exploration of therapist perceptions and (e.g. emotional) responses to LLMs. No one particular application type. — 1 occurrence(s)
     • Other: No specific subtype (AI use in general) — 1 occurrence(s)


17. dataset_source
   ==================
   Data type: object
   Total values: 172
   Non-null values: 159
   Null values: 13
   Unique values: 8
   Category: CATEGORICAL

   Value Counts:
     No dataset used for development or evaluation: 60 (37.7%)
     External data set: 52 (32.7%)
     Self-collected data: 21 (13.2%)
     External data set, modified: 16 (10.1%)
     [NULL]: 13 (8.2%)
     Self-collected data but derived from external data set: 7 (4.4%)
     Other: External data set, but completely unknown: 1 (0.6%)
     Other: unclear: 1 (0.6%)
     Other: some dataset was used, but completely unknown characteristics: 1 (0.6%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 3

     • Other: External data set, but completely unknown — 1 occurrence(s)
     • Other: unclear — 1 occurrence(s)
     • Other: some dataset was used, but completely unknown characteristics — 1 occurrence(s)


18. dataset_notes
   =================
   Data type: object
   Total values: 172
   Non-null values: 100
   Null values: 72
   Unique values: 96
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 72 (72.0%)
     CounselChat https://github.com/nbertagnolli/counsel-chat: 3 (3.0%)
     Alexander Street Press therapy transcripts: 2 (2.0%)
     Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms.: 2 (2.0%)
     . All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021: 1 (1.0%)
     PsyAdv Corp consisting out of Efaqa, CpsyCounD, SMILECHAT, PsyQA: 1 (1.0%)
     epsilon3/cbt-cognitive-distortions-analysis dataset
https://huggingface.co/datasets/epsilon3/cbt-cognitive-distortions-analysis
: 1 (1.0%)
     First, we introduce a novel, golden standard dataset, comprising panel 
data with 1495 instances of quantitative stress, anxiety, and depression (SAD) symptom scores from diagnostic-level questionnaires 
and qualitative daily diary entries. : 1 (1.0%)
     collected a substantial dataset comprising 21,924 records
Fig. 2. Volunteer Screening Chart
through five distinct avenues: instances, media, literature,
guidelines, and databases. To ensure the integrity and rele-
vance of our data, we undertook a meticulous cleaning pro-
cess spearheaded by three experienced quality controllers.
Each controller boasts broad expertise in mental health
counseling. Initially, we eliminated duplicates and linguistic
inaccuracies. Subsequently, we focused on screening data
for key terms such as “sleep”, “dream”, “evening”, “night”,
“bed” and related expressions. This step aimed to sift out
conversational data irrelevant to our study’s objectives, en-
suring that the textual content was pertinent to the context
of potential sleep disorders. We further conducted dialog
integrity screening. Finally, through manual inspection on a
case-by-case basis, we excluded dialogue data that contra-
dicted universal core values. This rigorous curation process
resulted in the selection of 764 dialogues: 1 (1.0%)
     Additionally, local data from Indonesian mental health articles and discussion forums fine-tuned the model.: 1 (1.0%)
     We use the HOPE (Mental Health cOunselling of PatiEnts) [8]
dataset for our experiments. The dataset is specifically designed
for mental health counseling and dialogue-act classification tasks.
The HOPE dataset includes approximately 12,800 utterances ex-
tracted from 212 mental health counseling sessions, sourced from
publicly available YouTube videos. : 1 (1.0%)
     The chatbot’s corpus comprises 200 manually curated context-response pairs that cover themes like stress, anxiety, depression, and motivation: 1 (1.0%)
     Primate2022 https://github.com/primate-mh/Primate2022 (but only for evaluation): 1 (1.0%)
     cognitive distortion identification dataset: 1 (1.0%)
     Aditya Mental Health Counseling https://huggingface.co/datasets/Aditya149/Mental_Health_Counselling_Dataset: 1 (1.0%)
     Using a dataset of Woebot questions and user responses (pulled from the Gen-W-MA internal testing data) labeled as being on or off topic, we fine-tuned an instance of the text-embedding-ada-002 model using the Azure OpenAI service: 1 (1.0%)
     Original dataset: Perez-Rosas motivational interviewing dataset.
Modification: Added custom annnotations
Also extended with their own MI prompts.
We thus obtained 4,365 client prompt-counselor reflection pairs, including 2,429 prompt-CR and 1,636 prompt-SR pairs.

public under https://lit.eecs.umich.edu/downloads.html#PAIR: 1 (1.0%)
     fine-tuned on the 2 volumes of published counseling and psychotherapy data from
Alexander Street Press. The volumes are searchable collections of transcripts containing real counseling and therapy sessions
and first-person narratives illuminating the experience of mental illness and treatment. The 2 volumes contain 3500 session transcripts and >700,000 utterances between a counselor and a
patient.: 1 (1.0%)
     PsyQA, extended by "helping skills" column: 1 (1.0%)
     Books: 
E. J. Bourne, The Anxiety and Phobia Workbook, 5th ed. Oakland, CA:
New Harbinger Publications, 2015. + D. D. D. Burns, Feeling Good: The New Mood Therapy. New York,
NY: Plume, 2009. + B. McDonagh, Dare: The New Way to End Anxiety and Stop Panic
Attacks. Carlsbad, CA: Hay House, 2014. 

To make sure it was appropriate for entering into the model, the data taken from the books went through a number of preparation steps
in this research: Extraction and cleaning.: 1 (1.0%)
     PsyQA is an authoritative Chinese dialogue dataset focused
on the field of mental health support. It covers 9 broad
topics and multiple subtopics, encompassing various aspects
of mental health. PsyQA is presented in a question-answer
format, where each example includes a question, a detailed
description of the mental issues, and keywords provided by
anonymous help-seekers. Responses are asynchronously pro-
vided by well-trained volunteers or professional counselors
and involve detailed analysis and guidance in response to the
help-seekers’ questions, aiming to offer mental health support.
Additionally, the answers have been further annotated with
seven psychological strategies based on psychological coun-
seling theory [12], : 1 (1.0%)
     Amod Mental health counseling conversations https://huggingface.co/datasets/Amod/mental_health_counseling_conversations From Huggingface description: This dataset is a collection of real counselling question-and-answer pairs taken from two public mental-health platforms. It is intended for training and evaluating language models that provide safer, context-aware mental-health responses.: 1 (1.0%)
     The fine-tuning process involves the 
calibration of the pre-trained Transformer model with 
specific datasets relevant to cognitive therapy. This 
includes dialogues from CBT sessions, therapeutic 
interventions, and stress management conversations.

The GPT-2 model is fine-tuned using a specialized 
therapeutic dialogue dataset where the process involves 
supervised learning and the openly available trained 
model is adapted for a wide range of therapeutic 
contexts. The goal is to improve the ability of the model 
to provide relevant and empathetic responses during 
therapy sessions. The fine-tuning dataset includes a 
variety of therapeutic dialogues which ensures the 
model can handle different therapeutic scenarios 
effectively.: 1 (1.0%)
      For example, it uses conversational 
data gained in previous interactions with the user as well as 
datasets having labeled expressions of sentiment ("emo.txt" 
and "Sentiments1.txt"): 1 (1.0%)
     ArmanEmo: 1 (1.0%)
     No specific name: "We use the Reddit API to collect
a new dataset of 12,513 posts with 70,429 peer re-
sponses from 26 mental health related subreddits."

Available under https://github.com/skgabriel/mh-eval: 1 (1.0%)
     "Mentioned previously, we use transcripts
from the smoking cessation MI chatbot created by
the authors (Brown et al., 2023). "

Only questions and answers from MI chatbot reflections by Brown 2023 -- A Motivational Interviewing Chatbot With Generative Reflections for Increasing Readiness to Quit Smoking: Iterative Development Study.
The authors generate reflections based on these question-answer pairs using GPT-4: 1 (1.0%)
     In this study, 200 real counseling ses
sions were recorded and transcribed to capture authentic 
client interactions. From these, 50 client statements were 
identified that best represented a range of emotions such as 
sadness, guilt, anxiety, determination, and anger, based on 
the PANAS Scale, along with helplessness, love, trust, lone
liness, and doubt from existing literature, aiming to ensure

emotional expressions from psychological counseling sessions: 1 (1.0%)
     zhang 2024 dataset: 1 (1.0%)
     The study uses data from YiXinli Community, a major Chinese online counseling platform (https://www.xinli001.com) frequented by nearly 40 million users worldwide for psychological support. On this platform, 
users can confidentially express their psychological concerns and receive advice from human counselors in the open Q&A section (https://www.xinli001.com/qa). We utilized a web data crawler called houyicaiji (https://www.houyicaiji.com/) to gather this Q&A data, which encompasses 10,903 distinct questions and 19,682 human counselor responses collected from November 3, 2022, to 
March 30, 2023.: 1 (1.0%)
     see Generating Data for Evaluation: 1 (1.0%)
     Responses by expert suicidologists on a previously-published standardized scale: the Suicide Intervention Response Inventory (SIRI-2)
[15].: 1 (1.0%)
     mental health and healthcare-related texts; sources not fully specified, combined domain corpora and simulated patient records: 1 (1.0%)
     Huggingface: Mental Health Counseling Conversations
https://huggingface.co/datasets/Amod/mental_health_counseling_conversations: 1 (1.0%)
     Web-scraped data from diverse sources

Bilingual (English & Arabic) mental health text corpus, origin not found: 1 (1.0%)
     Emotional Support Conversation dataset. Comprises 1,053 multi-turn dialogues, amounting to 31,410 utterances. "We have augmented the dataset with additional annotations to signify the current state of the dialogue.": 1 (1.0%)
     The intervention utilizes a generative large language model (LLM) fine-tuned on expert-curated mental health dialogues. The dialogues were developed by our research team, including a board-certified psychiatrist and a clinical psychologist, and peer-reviewed using evidence-based (primarily CBT) modalities.: 1 (1.0%)
     selected cases from single_turn_dataset of Baidu's EmoLLM  + entiment-labeled Weibo data.

EmoLLM single_turn_dataset; Weibo data; CEA dataset (constructed): 1 (1.0%)
     The research begins by curating a specialized User and
Counsellor Interaction CSV dataset by querying an autism
counsellor. This data encapsulates nuanced responses and
intricacies of user-counselor interactions, offering a rich source
for reducing bias and adding a human touch. Alongside the
primary dataset, the additional data points ensure a compre-
hensive understanding of the domain.: 1 (1.0%)
     The conversational dataset is published by the American Mental Health 
Association [25] and is associated with government agencies. The dataset consists of FAQ about Mental Health that are conversations between users and experts in the field
of psychology about mental health and its relationship to Alzheimer's disease (Alzheimer's chat dataset, Alzhimer_chat_Leader, Mental_Health_FAQ.csv, NLP 
Mental Health Conversations). 

Note: The source is not cited appropriately!

But user has other dataset that fits description: https://www.kaggle.com/datasets/ahmedashrafahmed/alzhemers-chat-dataset: 1 (1.0%)
     CounselChat: https://huggingface.co/datasets/nbertagnolli/counsel-chat

[COUNSELCHAT.COM doesnt work]The model uses scraped data from various credible and legitimate sources facilitating real case-studies. Counsel-Chat is an example of an expert community which offers services by licenced therapists. : 1 (1.0%)
     The primary dataset used in this study consists of psychotherapy transcripts from
https://www.lib.montana.edu/resources/about/677 (accessed on 10 March 2025), the
“Counseling and Psychotherapy Transcripts, Volume II” dataset. : 1 (1.0%)
     EmpatheticDialogues https://github.com/facebookresearch/EmpatheticDialogues

From Rashkin et al. [52], containing around 25,000 conversations rooted in emotional situations, with labels for various emotions like sadness, anxiety, and anger.: 1 (1.0%)
     By using GPT-3.5 for the DSPs, we aimed to simulate
realistic patient interactions that reflect a range of natural
conversational behaviors. This choice helps create a con-
trolled baseline for the interactions, allowing us to isolate
and evaluate the advanced capabilities of the GPT-4 conver-
sational agent without introducing artifacts that could arise
from using the same model for both roles.
During the simulated conversation, DSPs remained agnostic
to their sociodemographic characteristics, limiting any poten-
tial for bias in their response generation. However, with every
simulated conversation, the conversational agent was randomly
informed of a different sociodemographic profile of the DSP.
These characteristics encompassed a spectrum of ages (40 and
80 years old), genders (male, female, transgender male, and
transgender female), races (non-Hispanic black, non-Hispanic
white, and Hispanic), and annual income ($25,000, $50,000,
$500,000, and $1,000,000). We also generated a control group
without assigned identities. Each DSP represented a unique
mix of these demographic characteristics.
To limit confounding variables due to GPT-4’s word
selection variability, each demographic permutation was
tested 4–5 times. Overall, this process yielded 97 distinct
demographic combinations and 449 conversation transcripts
between the conversational agent and DSPs—with a total of
4,502 agent responses: 1 (1.0%)
     1,000 QA pairs from the PsyQA dataset.  two human annotators inspected and labeled the responses of professional supporters in the PsyQA dataset with the corresponding professional psychological counseling theory categories (Cognitive-Behavioral Therapy labeled as 1, Dialectical Behavior Therapy as 2, Person-Centered Therapy as 3, and Reality Therapy as 4). The original data had quantities of 189, 317, 196, and 298 for categories 1, 2, 3, and 4, respectively.: 1 (1.0%)
     Study describes FAITA Mental Health Framework and evaluates OCD Coach as an existing LLM-based Tool: 1 (1.0%)
     Data Preparation: The sentiment analysis model is
trained on an annotated text classification dataset after
undergoing extensive cleaning, normalization, and tok-
enization processes to ensure the quality and integrity of
the data. The datasets used are compiled from a multitude
of sources, including a text classification repository of so-
cial media tweets, restaurant reviews, and airline reviews.
Data was also extracted from the Reddit API across vari-
ous forums such as those focused on CBT, mental health,
therapy, anxiety, depression, and CPTSD. Initially, the
dataset comprised 3,000 data points, with a text column
serving as the input and a target column providing labeled
sentiment. Recognizing the importance of data volume
in enhancing the accuracy and robustness of machine
learning models, we expand the dataset (TABLE I) to encompass 12,221 data points

for RAG: 1 (1.0%)
     Data from various sources ("Simple conversations, psychological questions, information on classic therapy sessions, and general advice for people with anxiety and depression. This data can be used to train a chatbot model that can act as a therapist to assist patients with anxiety and depression. The dataset includes intents. An "intent" is the purpose of a user's message. There are a total of 89 such intents, some of which are “greeting, sad, stressed, worthless, depressed, happy, anxious, sleep, scared, hate, problem” etc.").

Seemingly for intent classification.: 1 (1.0%)
     a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists

The basis of our system was a curated corpus of 300 Spanish documents intended for the general public. The corpus was collected by psychologists, and was organised and categorised in different topics (including information for survivals of suicide attempts, for relatives, or for schools).

for RAG: 1 (1.0%)
     PsyQA (+ Yixinly? "Thus we crawled 50K articles (0.1B tokens in total) related to psychology and mental health support from Yixinli (xinli001.com/info) and train a GPT-2 from scratch based on the corpus."): 1 (1.0%)
     Data sourced from single-turn QA psychology platforms, multi-turn dialogue synthetically generated using the single-turn data, and psychological knowledge QA

we collect question-answer pairs from open platforms… Yixinli… Zhihu… We crawl books related to psychology from the web and then use Qwen-72B to extract knowledge-based QA… After-school exercises from several books

There are several publicly accessible websites committed to
establishing a psychology platform and offering online solu-
tions for individuals seeking psychological assistance, such as
Yixinli3, Zhihu4, and so on. The data from these platforms
can be regarded as real-world inquiries and the solutions pro-
vided by professionals in response to these inquiries.
ubse-
quently, professional or experienced individuals offer detailed
solutions or advice in response to these questions. We collect
over 267 000 pairs of data from websites.
To accurately evaluate the model performance in the psy-
chology domain, we introduce a benchmark that psychological
professional individuals need to master. The data is sourced
from publicly available examination questions. We follow a standardized data preprocessing procedure. For some data, we
first need to perform optical character recognition (OCR) to
convert images into text. After that, we invite several students to
manually review the collected data to ensure its quality and con-
sistency with the original document. This process involves rec-
tifying formatting errors, eliminating duplicate questions, and
rectifying any instances of garbled characters. Our proposed
psychological benchmark draws inspiration from the format of
the most authoritative psychological counseling examination in
China, comprising two primary components: objective ques-
tions and subjective questions.: 1 (1.0%)
     The videos of therapists’ CBT sessions with patients utilized
in this paper are sourced from public social media. Our dataset
comprises 46 Chinese conversation videos and 150 English
conversation videos, amounting to a total of 6386 dialogue
turns. And some of the conversations within the dataset even
reach up to hundreds of turns.: 1 (1.0%)
     We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

Multi-turn real counseling chat sessions collected on a Chinese online text-based free counseling platform: 1 (1.0%)
     therapy transcriptions from real patients (that they recruited themselves): 1 (1.0%)
     Name of new dataset: HOPE. Derived from transcripts from YouTube counseling sessions. They added self-created annotations to these data.: 1 (1.0%)
     self-created "Empathic Conversation dataset": 1 (1.0%)
     Random sample of 476 Protocall crisis calls: 1 (1.0%)
     derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com): 1 (1.0%)
     external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS

To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube: 1 (1.0%)
     Dataset was used for evaluation only.

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539]): 1 (1.0%)
     Some PDFs for configuring a custom GPT (type of PDFs not further specified): 1 (1.0%)
     DailyDialog and EmpatheticDialogues as basis; translated and merged: CORTEX; (enriched by Polish Common Crawl) : 1 (1.0%)
     HOPE
212 multi-turn English psychotherapy sessions (≈ 12.9 K utterances) between therapist and patient, transcribed from YouTube videos

HOPE;
Switchboard Dialogue-act Corpus; (also mentions a “Dialog-act corpus”). Description: Counseling conversations with dialogue-act labels (HOPE) and a general telephone dialogue-act corpus (Switchboard) used for evaluation/generalizability. Derived: Not indicated.: 1 (1.0%)
     We experiment on the cognitive distortion detection
dataset proposed by Shreevastava and Foltz (2021),
which is annotated by experts based on the Ther-
apist QA dataset

Dataset from Shreevastava 2021 -- Detecting Cognitive Distortions from Patient-Therapist Interactions.

https://www.kaggle.com/datasets/sagarikashreevastava/cognitive-distortion-detetction-dataset: 1 (1.0%)
     MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube)

Large-scale multi-modal emotional-support dialogues with utterance-level emotion annotations and strategy labels; used for Emotion/Strategy/Response tasks.: 1 (1.0%)
     PsyQA: 1 (1.0%)
     PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
CBT QA dataset. Derived from: PsyQA → CBT QA (ChatGPT-generated via CBT prompt). Short description: Chinese mental-health Q&A where responses are generated with a structured CBT prompt and used to fine-tune CBT-LLM.

: 1 (1.0%)
     from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.

Counseling/chat conversations annotated for utterance‐level features; session-level features and summaries also produced (some via prompts). Annotation: Human annotators used a codebook of counseling strategy features: 1 (1.0%)
     The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.: 1 (1.0%)
     Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website: 1 (1.0%)
     training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.: 1 (1.0%)
     We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: "This site is fiction and all the characters are fictional.": 1 (1.0%)
     Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”: 1 (1.0%)
     Open Up is a free24/7 web-based, text-based counseling service in Hong Kong that enables people aged between 11 and 35 years to anonymously chat with paid staff (staff counselors or social workers) or trained volunteers. 5240 sessions with 533,609 messages (Figure 3). We stratified the 5240 sessions based on the number of messages in each session. There were a total of 131 unique message count groups among the 5240 sessions.: 1 (1.0%)
     FLATT dataset (Fun to Learn to Act and Think through Technology): 1 (1.0%)
     Motivational Interviewing counseling dataset (Perez-Rosas 2016)

The dataset contains a total of 22,719 counselor utterances extracted from 277 motivational interviewing sessions that are annotated with 10 counselor behavioral codes. The dataset is derived from a collection of 284 video recordings of counseling encounters using MI. The recordings were collected from various sources, including two clinical trials, students’ counseling sessions from a graduate level MI course, wellness coaching phone calls, and demonstrations of MI strategies in brief medical encounters.: 1 (1.0%)
     42 tinitus patients as raw data + augmented dataset as test samples

The study was carried out on a cohost of 42 tinnitus patients who visited the Department of Otorhinolaryngology, Korea University Medical Center in Seoul, Republic of Korea, between 2022 and 2023. We conducted a retrospective analysis of medical records documenting tinnitus treatment in those patients: 1 (1.0%)
     The study uses the PsyQA dataset and creates a new one called SMILECHAT.

https://github.com/qiuhuachuan/smile: 1 (1.0%)
     HOPE (Malhotra, 2022): 1 (1.0%)
     We conduct extensive experiments on datasets derived from
real-world MI sessions addressing alcohol usage disorder. We
examine three prompting baselines and experiment with four
state-of-the-art auto-regressive LLMs, including Llama2 [12],
Falcon [20], Mistral [21], and ChatGPT [22]. 

Motivational interviewing session transcripts from Borsari et al., “In-session processes of brief motivational interventions in two trials with mandated college students.” Journal of consulting and clinical psychology, vol. 83, pp. 56–67, 2 2015: 1 (1.0%)
     HealMe dataset
https://github.com/elsa66666/HealMe

In this study, we leverage an existing raw dataset
focused on cognitive reframing and expand it to
include multiple rounds of dialogue. Specifically,
we conduct a manual review of the selected raw
dataset and select 1,000 well-composed pairs of
(thinking trap, client’s thought) from it.
The raw dataset we utilize in this study is intro-
duced by Maddela et al. (2023). 
https://aclanthology.org/2023.acl-long.763.pdf

Creation of HealMe dataset : 1 (1.0%)
     "dataset" were PHQ-9 questions: 1 (1.0%)
     Data was collected as follows: The study began with a “Listening Circle” activity, where partici­pants shared distressing questions or situations in their lives. These were collected via paper-pencil surveys and redistributed randomly among participants for solution suggestions. Out of 50 participant- generated questions, 10 open-ended ones were chosen, covering areas like inter­personal issues, stress, and intrapersonal conflicts, typical of mental health support scenarios. Participants evaluated two types of responses for each question, one from ChatGPT and one human- generated, in a single-blind format where AI origins were unknown.

Name: none specified. Description: Self-collected open-ended mental-health problem statements and corresponding ChatGPT- and human-generated responses evaluated by participants.: 1 (1.0%)
     Kaggle mental health conversational dataset https://www.kaggle.com/datasets/elvis23/mental-health-conversational-data

Kaggle (n.d.)  This dataset encompasses 80 distinct tags, each containing numerous conversational prompts and corresponding responses: 1 (1.0%)
     Data were obtained from an online and mobile therapy app (Talkspace) for services provided
between 2014 and 2019.: 1 (1.0%)
     oth ChatGPT-4 and Bard responded to 20 tasks at each
stage of the study --> no new dataset created : 1 (1.0%)
     no dataset. ChatGPT is used as is.: 1 (1.0%)
     TalkLife (talklife.co) is the largest online peer-to-peer support platform for mental health support. It enables conversations between people seeking support (support seekers) and people providing support (peer supporters) in a thread-like setting. We call the post authored by a support seeker as seeker post, and the response by a peer supporter as response post. Table 1 describes the statistics of conversational threads on the TalkLife platform.

Curating mental health-related conversations. As noted by
Sharma et al. [59], the TalkLife platform hosts a significant number of common social media interactions (e.g., Happy mother’s day).
Here, we focus our analyses on mental health-related conversa-
tions and filter out such posts. We manually annotate ∼3k posts
with answers to the question "Is the seeker talking about a mental health related issue or situation in his/her post?". Using this annotated dataset, we train a standard text classifier based on BERT [15] (achieving an accuracy of ∼85%). We apply this classifier to the
entire TalkLife dataset and create a filtered dataset of mental health-related conversations. This dataset contains 3.33M interactions from 1.48M seeker posts.

Non-public: For accessing the TalkLife portion of our dataset for non-commercial use, please contact the TalkLife team here.: 1 (1.0%)
     Alexander Street Press, Counseling and Psychotherapy Transcripts: Volume I: 1 (1.0%)
     The Emobank dataset (Buechel & Hahn, 2022)
The GoEmotions dataset (Demszky et al., 2020) 
The International Survey on Emotion Antecedents and Reactions (ISEAR) (ISEAR
dataset, https://paperswithcode.com/dataset/isear, accessed 22.08.2023
Validation: The CrowdFlower dataset (Sentiment Analysis in Text - Dataset by crowdflower,
https://data.world/crowdflower/sentiment-analysis-in-text, accessed 07.08.2022): 1 (1.0%)
     data sourced from Xinli001.com. Counsellor responses were included in the dataset. ChatGPT responses were generated by the authors.: 1 (1.0%)
     Due to this, data from the Brain & Behaviour Research Foundation2, the NHS34, Wellness
in Mind5 and White Swan Foundation6 were selected. Questions
and answers are extracted, and questions are manually generated
dependent on the information available, e.g. for the NHS definition
of depression, questions such as “what is depression?" are imputed.: 1 (1.0%)
     We use the GPT-3 model [19] finetuned over a dataset of thinking traps by Sharma et al. [81].

4.1 Curated Situations & Negative Thoughts
 We start by curating data sources for situations and negative thoughts.
 Thought Records Dataset (Burger et al., 2021).
 This dataset contains hypothetical and real-world situations, thoughts and emotional processes reported by crowdworkers on Amazon Mechanical Turk. We manually curate 180 pairs of diverse situations with negative thoughts from this dataset.
 Mental Health America (MHA). Situations and thoughts from crowdworkers may not reflect the broad range of mental health challenges that people face in real-life. To incorporate more real-world situations and thoughts, we ran a survey on the MHA website (screening.mhanational.org). MHA visitors (who typically use the website for screening of mental illnesses) were asked to describe any negative thoughts and the associated situations they were struggling with. We manually curate 120 pairs of self-reported situations and thoughts to ensure broad coverage of relevant topics based on high diversity and manual filtering.

https://github.com/behavioral-data/Cognitive-Reframing: 1 (1.0%)
     From our previous work (Tauscher et al., 2022), we utilized data from a randomized controlled trial of a community-based text-message intervention for individuals with serious mental illness (Ben- Zeev et al., 2020).

"The maximum intervention dose was three “exchanges,” wherein an exchange is defined as a cluster of thematically connected back-and-forth messages between mobile interventionist and participant (e.g. three outgoing messages and participant responses). Texting strategies included: reminders (e.g., appointments, prescription refills), information provision (e.g., psychoeducation, links to regional events and resources), cognitive challenges (e.g., restructuring dysfunctional beliefs about voices, questioning the validity of self-sabotaging automatic beliefs), self-monitoring/self-reflection (e.g., guidance on self-evaluation of affect, journaling of symptomatic experiences), relaxation techniques (e.g., diaphragmatic breathing, guided imagery), social skills training (e.g., initiating conversations, maintaining eye contact), supportive messages (e.g., affirmations, inspirational quotes), and in-vivo instruction (e.g., pre-scheduled real-time support as the patient attempted a new activity)." (from Ben-Zeev et al.)

in addition they augmented their data using different strategies (3.2 Augmentation of text data): 1 (1.0%)
     In version 1 of the generator, the fine-tuning question-and-response data set came from 2 sources: the first was our prior work [40,41], and the second data source was from earlier deployments of MIBot, before the creation of MIBot v4.7. The reflections used came from a variety of sources: from previous versions of this chatbot that were deemed to be acceptable MI reflections by MI-literate researchers or actual reflections produced by MI-literate researchers or MI-expert clinicians.

To address the rate of poor reflections, we developed version 2 of the generator with 2 significant enhancements. First, a larger set of 301 fine-tuning triplets were collected over approximately 10 months of deploying the chatbot, making use of the various responses from smokers who had been recruited in a similar manner, as described in the Participant Recruitment and Screening subsection. This second data set did not include any of the data from the earlier chatbot version [40,41]. Only MI-consistent reflections were used, which were sourced from MI clinicians, MI-literate researchers, or version 1 of the generator. The labeling and selection of the MI-consistent reflections were improved by using multiple human raters and a carefully controlled decision tree to determine the validity of the reflections. The new rating scheme itself was stricter than the one used in version 1, which caused the hit rate to go down—not because the generation was worse but because of the stricter rating. The hit rate of the new generator was measured to be 55.1% (166/301) on a set of reflections.: 1 (1.0%)
     MotiVAte

This dataset comprises of 4k dyadic conversations between the depressed support seekers and the VA imparting appropriate suggestion, hope and motivation resulting in a total of 14,809 utterances. The conversations of the MotiVAte dataset are collected from peer-to-peer support forum and modified to represent dyadic conversations for an end-to-end online mental health support system.: 1 (1.0%)
     https://huggingface.co/datasets/jkhedri/psychology-dataset; Only questions embedded, not answers.: 1 (1.0%)

   Sample values (first 20):
     . All efforts for this
study were made with the approval of Osaka Prefecture.
Its counseling platform is a messenger application called
LINE (https://line.me/). The dataset was collected
between May 2020 and January 2021
     Initially, we collected transcriptions of CBT patient-therapist
interactions performed by an expert psychotherapist to improve
the program’s adherence to the style and cadence of an
experienced human therapist. From these, we discerned recurring
exchanges and encoded these patterns into GPT-4’s system
prompts. For instance, a rule was established: “Show empathy and
understanding to validate [first name]’s feelings.”
     We used real-world therapy transcript documents from websites and 
converted the HTML conversation texts between patients and therapists into feature format for processing (see data example in Figure 2).

http://www.thetherapist.com/

These transcripts are fictional, though: "This site is fiction and all the characters are fictional."
     CounselChat https://github.com/nbertagnolli/counsel-chat
     training: 2.85 GB psychology corpus data crawled from psychology platforms like Yinxinli and Tianya (they crawled this data themselves). Description: The second dataset was obtained by crawling various Chinese social media platforms, such as Tianya, Zhihu, and Yixinli. These platforms allow users to post topics or questions about mental and emotional issues, while other users can offer support and assistance to help-seeking individuals. The Yixinli website specifically focuses on professional mental health support, but only provides approximately 10,000 samples. Other types of datasets collected from these platforms included articles and conversations, which we converted into a question-and-answer format. However, we excluded the articles from our fine- tuning training due to the model’s input limitations and the fact that our predictions focused on mental health support answers. The articles were often lengthy, and many of them were in PDF format, requiring additional time for conversion into a usable text format. Consequently, we only obtained around 5000 article samples. In order to address the lack of emotional expression in the text of these articles, we incorporated text data from oral expressions. We crawled audio and video data from platforms like Qingting FM and Ximalaya, popular audio and video-sharing forums in China. However, converting audio and video data into text format was time-consuming, resulting in a limited amount of data in our dataset. We utilised the dataset obtained from websites for fine-tuning training. Ultimately, our entire dataset consisted of 400,000 samples, each separated by a blank line, i.e., “\n\ n”. Table 2 shows the time spent on data crawling from different websites. It is evident that most of the samples in this dataset were obtained from Tianya, resulting in a data size of approximately 2 GB. The datasets from Zhihu and Yixinli were 500 MB and 200 MB, respectively. Overall, we spent approximately 70 h on data collection. Although the data collected from the internet were abundant and authentic, the cleaning process could have been smoother due to inconsistencies in the online data.
     Mental health conversations and question-answer pairs-related datasets on Kaggle and sample Medical Summary Reports available for informational use from SOAR providers on the Substance Abuse and Mental Health Services Administration website
     The data collection for this labeled dataset was gathered by a combination of examples from CBT literature and anonymous submissions made by the principal investigator and university psychology students who got access to the document by a link that was shared in student group chats. This sample was chosen due to them both being part of the population that will go through the experiment and study the CBT concepts, thus, knowing how to label data. They were asked to write 10 cognitive distortions in total. After the completion of a dataset, the whole dataset was checked and edited by a principal investigator, the project’s supervisor, and a practicing CBT psychologist. In total, 240 examples of cognitive distortions were accumulated and divided into training and test sets in a ratio of 3 to 1.
     from the text and chat channel of The Childhelp National Child Abuse Hotline. had access to deidentified transcripts and metadata that anonymized and normalized all names and street addresses.

Counseling/chat conversations annotated for utterance‐level features; session-level features and summaries also produced (some via prompts). Annotation: Human annotators used a codebook of counseling strategy features
     PsyQA dataset (Sun et al., 2021) -> (was used to generate) -> CBT QA Dataset 
CBT QA dataset. Derived from: PsyQA → CBT QA (ChatGPT-generated via CBT prompt). Short description: Chinese mental-health Q&A where responses are generated with a structured CBT prompt and used to fine-tune CBT-LLM.


     PsyQA
     MMESConv dataset (1599 dialogues, each utterance has the three modalities audio, video, text, crawled from YouTube)

Large-scale multi-modal emotional-support dialogues with utterance-level emotion annotations and strategy labels; used for Emotion/Strategy/Response tasks.
     HOPE
212 multi-turn English psychotherapy sessions (≈ 12.9 K utterances) between therapist and patient, transcribed from YouTube videos

HOPE;
Switchboard Dialogue-act Corpus; (also mentions a “Dialog-act corpus”). Description: Counseling conversations with dialogue-act labels (HOPE) and a general telephone dialogue-act corpus (Switchboard) used for evaluation/generalizability. Derived: Not indicated.
     We develop an online Chinese text-based counseling platform that provides free counseling services. Each counseling session between the help-seeker and experienced supporter lasts approximately 50 min, following the standard practice in psychological counseling. Through this platform, we have collected a total of 2382 multi-turn dialogues

Multi-turn real counseling chat sessions collected on a Chinese online text-based free counseling platform
     DailyDialog and EmpatheticDialogues as basis; translated and merged: CORTEX; (enriched by Polish Common Crawl) 
     Some PDFs for configuring a custom GPT (type of PDFs not further specified)
     Dataset was used for evaluation only.

50 conversations were randomly selected from the MIBot version5.1 experiment data (Brown A, Kumar AT, Melamed O, et al. A motivational-interviewing chatbot with generative reflections for increasing readiness to quit among smokers. JMIR Ment Health. Oct 17, 2023;10:e49132. [doi: 10.2196/49132] [Medline: 37847539])
     external: MEMO (Mental Health Summarization, itselfderived from HOPE), derived: MentalCLOUDS

To evaluate the performance of diverse summarization systems
across various aspects of counseling interactions, we expanded
upon the Mental Health Summarization (MEMO) data set [47].
Comprising 11,543 utterances extracted from 191 counseling
sessions involving therapists and patients, this data set draws
from publicly accessible platforms such as YouTube
     derived from an archival dataset that was generated as part of the routine onboarding process for messaging therapy providers on a digital mental health platform (Talkspace.com)
     Random sample of 476 Protocall crisis calls
     self-created "Empathic Conversation dataset"


19. dataset_type
   ================
   Data type: object
   Total values: 172
   Non-null values: 84
   Null values: 88
   Unique values: 35
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 88 (104.8%)
     Internet data -- mental health Q&A: 22 (26.2%)
     Emotional support dialogue -- chat logs: 13 (15.5%)
     Psychotherapy -- speech transcripts: 11 (13.1%)
     Internet data -- mental health forum: 4 (4.8%)
     Emotional support dialogue -- speech transcripts: 3 (3.6%)
     Other: mixed: 2 (2.4%)
     Other: "mental health articles and discussion forums": 1 (1.2%)
     Other: Mixed: 1 (1.2%)
     Other:  Psychotherapy-related (diagnostic questionnaires + daily diaries) : 1 (1.2%)
     Other: MI reflections (client prompt - counselor reflection pairs): 1 (1.2%)
     Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists: 1 (1.2%)
     Other: unclear: 1 (1.2%)
     Other: Other: statements, labeled with sentiments (sentiment analysis data): 1 (1.2%)
     Other: Labeled examples of cognitive distortions: 1 (1.2%)
     Other: test cases for "psychological counseling scenarios" involving extreme behaviors: 1 (1.2%)
     Psychotherapy -- chat logs: 1 (1.2%)
     Other: Internet data, various types and sources (mixed): 1 (1.2%)
     Other: different types (books, articles, etc.), not further specified: 1 (1.2%)
     Other: Standardized clinical instrument with hypothetical scenarios and predefined responses: 1 (1.2%)
     Other: unclear (authors call it "context-response pairs": 1 (1.2%)
     Other: MI reflections: 1 (1.2%)
     Other: Book data: 1 (1.2%)
     Other: unknown: 1 (1.2%)
     Other: mixed: multi-turn and QA: 1 (1.2%)
     Other: labeled cognitive distortions: 1 (1.2%)
     Other: emotional experience descriptions: 1 (1.2%)
     Other: automatic thought records: 1 (1.2%)
     Other: thought records: 1 (1.2%)
     Other: Motivational interviewing reflections: 1 (1.2%)
     Other: cbt diary entries: 1 (1.2%)
     Other: automatic thought-feeling pairs: 1 (1.2%)
     Other: Emotional support dialogue -- multimodal data: 1 (1.2%)
     Other: descriptions of cognitive distortions: 1 (1.2%)
     Other: not applicable: 1 (1.2%)
     Other: Descriptions of mental disorders in the DSM-5: 1 (1.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 30

     • Other: mixed — 2 occurrence(s)
     • Other: not applicable — 1 occurrence(s)
     • Other: descriptions of cognitive distortions — 1 occurrence(s)
     • Other: Emotional support dialogue -- multimodal data — 1 occurrence(s)
     • Other: automatic thought-feeling pairs — 1 occurrence(s)
     • Other: cbt diary entries — 1 occurrence(s)
     • Other: Motivational interviewing reflections — 1 occurrence(s)
     • Other: thought records — 1 occurrence(s)
     • Other: automatic thought records — 1 occurrence(s)
     • Other: emotional experience descriptions — 1 occurrence(s)
     • Other: labeled cognitive distortions — 1 occurrence(s)
     • Other: mixed: multi-turn and QA — 1 occurrence(s)
     • Other: MI reflections — 1 occurrence(s)
     • Other: unknown — 1 occurrence(s)
     • Other: Book data — 1 occurrence(s)
     • Other: Labeled examples of cognitive distortions — 1 occurrence(s)
     • Other: unclear (authors call it "context-response pairs" — 1 occurrence(s)
     • Other: "mental health articles and discussion forums" — 1 occurrence(s)
     • Other: Mixed — 1 occurrence(s)
     • Other: Psychotherapy-related (diagnostic questionnaires + daily diaries) — 1 occurrence(s)
     • Other: MI reflections (client prompt - counselor reflection pairs) — 1 occurrence(s)
     • Other: a corpus of documents about Suicide Prevention curated by psychologists and psychiatrists — 1 occurrence(s)
     • Other: Other: statements, labeled with sentiments (sentiment analysis data) — 1 occurrence(s)
     • Other: unclear — 1 occurrence(s)
     • Other: test cases for "psychological counseling scenarios" involving extreme behaviors — 1 occurrence(s)
     • Other: Internet data, various types and sources (mixed) — 1 occurrence(s)
     • Other: different types (books, articles, etc.), not further specified — 1 occurrence(s)
     • Other: Standardized clinical instrument with hypothetical scenarios and predefined responses — 1 occurrence(s)
     • Other: Descriptions of mental disorders in the DSM-5 — 1 occurrence(s)

   Sample values (first 20):
     Psychotherapy -- speech transcripts
     Other: not applicable
     Internet data -- mental health Q&A
     Other: mixed
     Other: descriptions of cognitive distortions
     Emotional support dialogue -- chat logs
     Other: Emotional support dialogue -- multimodal data
     Other: automatic thought-feeling pairs
     Other: cbt diary entries
     Other: Motivational interviewing reflections
     Other: thought records
     Internet data -- mental health forum
     Other: automatic thought records
     Other: emotional experience descriptions
     Emotional support dialogue -- speech transcripts
     Other: labeled cognitive distortions
     Other: mixed: multi-turn and QA
     Other: MI reflections
     Other: unknown
     Other: Book data


20. dataset_language
   ====================
   Data type: object
   Total values: 172
   Non-null values: 85
   Null values: 87
   Unique values: 10
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 87 (102.4%)
     English: 52 (61.2%)
     Chinese: 15 (17.6%)
     Other: unknown: 11 (12.9%)
     Japanese: 1 (1.2%)
     Other: English and Chinese: 1 (1.2%)
     Other: Indonesian: 1 (1.2%)
     Other: Persian: 1 (1.2%)
     Other: Spanish: 1 (1.2%)
     Other: unknown, probably english or hindi: 1 (1.2%)
     Other: Arabic: 1 (1.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 17

     • Other: unknown — 11 occurrence(s)
     • Other: English and Chinese — 1 occurrence(s)
     • Other: Indonesian — 1 occurrence(s)
     • Other: Persian — 1 occurrence(s)
     • Other: Spanish — 1 occurrence(s)
     • Other: unknown, probably english or hindi — 1 occurrence(s)
     • Other: Arabic — 1 occurrence(s)


21. dataset_contains_synthetic_data
   ===================================
   Data type: object
   Total values: 172
   Non-null values: 85
   Null values: 87
   Unique values: 11
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 87 (102.4%)
     No: 56 (65.9%)
     Yes: 15 (17.6%)
     Other: unknown: 6 (7.1%)
     Other: synthetic; human-generated (fictional): 1 (1.2%)
     Other: synthetic, human created: 1 (1.2%)
     Other: Yes; human actors: 1 (1.2%)
     Other: synthetic: human generated: 1 (1.2%)
     Other: mix of "authentic" and role-played: 1 (1.2%)
     Other: unspecified: 1 (1.2%)
     Other: Unknown: 1 (1.2%)
     Other: Yes (hypothetical cases created by experts): 1 (1.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 14

     • Other: unknown — 6 occurrence(s)
     • Other: synthetic; human-generated (fictional) — 1 occurrence(s)
     • Other: synthetic, human created — 1 occurrence(s)
     • Other: Yes; human actors — 1 occurrence(s)
     • Other: synthetic: human generated — 1 occurrence(s)
     • Other: mix of "authentic" and role-played — 1 occurrence(s)
     • Other: unspecified — 1 occurrence(s)
     • Other: Unknown — 1 occurrence(s)
     • Other: Yes (hypothetical cases created by experts) — 1 occurrence(s)


22. dataset_is_public
   =====================
   Data type: object
   Total values: 172
   Non-null values: 85
   Null values: 87
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 87 (102.4%)
     No: 52 (61.2%)
     Yes: 32 (37.6%)
     Other: unspecified: 1 (1.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 1

     • Other: unspecified — 1 occurrence(s)


23. dataset_user_psychopathology_status
   =======================================
   Data type: object
   Total values: 172
   Non-null values: 87
   Null values: 85
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 85 (97.7%)
     Unknown: 38 (43.7%)
     Unselected: 27 (31.0%)
     Other: not applicable: 8 (9.2%)
     Psychopathology: 8 (9.2%)
     Other: n.a.: 3 (3.4%)
     Other: unknown: 2 (2.3%)
     Other: Not applicable, data is not on users: 1 (1.1%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 14

     • Other: not applicable — 8 occurrence(s)
     • Other: n.a. — 3 occurrence(s)
     • Other: unknown — 2 occurrence(s)
     • Other: Not applicable, data is not on users — 1 occurrence(s)


24. dataset_responder_type
   ==========================
   Data type: object
   Total values: 172
   Non-null values: 86
   Null values: 86
   Unique values: 12
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 86 (100.0%)
     Unknown: 29 (33.7%)
     Trained professionals: 23 (26.7%)
     Other: not applicable: 16 (18.6%)
     Lay people: 7 (8.1%)
     Other: n.a.: 3 (3.5%)
     Other: mixed: 2 (2.3%)
     Other: unknown: 1 (1.2%)
     Other: Mix of lay and trained: 1 (1.2%)
     Other: Woebot chatbot: 1 (1.2%)
     Other: not applicable (only initial posts, no responses): 1 (1.2%)
     Other: Mix of trained and lay: 1 (1.2%)
     Other: Not applicable, no interaction data: 1 (1.2%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 27

     • Other: not applicable — 16 occurrence(s)
     • Other: n.a. — 3 occurrence(s)
     • Other: mixed — 2 occurrence(s)
     • Other: unknown — 1 occurrence(s)
     • Other: Mix of lay and trained — 1 occurrence(s)
     • Other: Woebot chatbot — 1 occurrence(s)
     • Other: not applicable (only initial posts, no responses) — 1 occurrence(s)
     • Other: Mix of trained and lay — 1 occurrence(s)
     • Other: Not applicable, no interaction data — 1 occurrence(s)


25. llm_development_approach
   ============================
   Data type: object
   Total values: 172
   Non-null values: 146
   Null values: 26
   Unique values: 25
   Category: CATEGORICAL

   Value Counts:
     Only prompting: 53 (36.3%)
     Prompting + other modules: 28 (19.2%)
     [NULL]: 26 (17.8%)
     Only fine-tuning: 21 (14.4%)
     Fine-tuning + other modules: 18 (12.3%)
     Fine-tuning + other modules; Prompting + other modules: 4 (2.7%)
     Only fine-tuning; Only prompting: 3 (2.1%)
     Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier: 1 (0.7%)
     Other: fine-tuning + custom pipeline (see Reflection Generation Training): 1 (0.7%)
     Only fine-tuning; Prompting + other modules: 1 (0.7%)
     Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.: 1 (0.7%)
     Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers: 1 (0.7%)
     Other: "we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset"
"The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output"

BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture: 1 (0.7%)
     Other: Other: self-developed and trained transformer architecture: 1 (0.7%)
     Other: multiple different:
selection of thinking traps: fine-tuning
writing of reframes: retrieval-enhanced in-context learning: 1 (0.7%)
     Other: Fine-tuning plus other elements like clustering: 1 (0.7%)
     Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework: 1 (0.7%)
     Other: fine-tuning + retrieval and content expansion: 1 (0.7%)
     Other: fine-tuning + wrapper.
they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.: 1 (0.7%)
     Other: fine-tuning + transfer learning: 1 (0.7%)
     Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization: 1 (0.7%)
     Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response: 1 (0.7%)
     Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.: 1 (0.7%)
     Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only: 1 (0.7%)
     Other: both original training of the transformer model and subsequent fine-tuning: 1 (0.7%)
     Prompting + other modules; Other: RAG: 1 (0.7%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 17

     • Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier — 1 occurrence(s)
     • Other: both original training of the transformer model and subsequent fine-tuning — 1 occurrence(s)
     • Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only — 1 occurrence(s)
     • Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval. — 1 occurrence(s)
     • Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response — 1 occurrence(s)
     • Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization — 1 occurrence(s)
     • Other: fine-tuning + transfer learning — 1 occurrence(s)
     • Other: fine-tuning + wrapper. — 1 occurrence(s)
     • Other: fine-tuning + retrieval and content expansion — 1 occurrence(s)
     • Other: Fine-tuning plus other elements like clustering — 1 occurrence(s)
     • Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework — 1 occurrence(s)
     • Other: fine-tuning + custom pipeline (see Reflection Generation Training) — 1 occurrence(s)
     • Other: multiple different: — 1 occurrence(s)
     • Other: Other: self-developed and trained transformer architecture — 1 occurrence(s)
     • Other: "we developed a transformer-based model for — 1 occurrence(s)
     • Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers — 1 occurrence(s)
     • Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL. — 1 occurrence(s)

   Sample values (first 20):
     Other: BERT is used out-of-the-box, then its embeddings classified via a logistic regression classifier
     Only prompting
     Prompting + other modules
     Other: both original training of the transformer model and subsequent fine-tuning
     Only fine-tuning
     Other: They proposed and evaluated two tools in parallel: fine-tuned BERT and ChatGPT prompting-only
     Other: GPT-2 is fine-tuned on PsyQA. GPT-2 generates the intervention text based on crisis call topic identified by a separate BERT model, involving also knowledge graph retrieval.
     Other: modular system with encoder for encoding emotion from different modalities, a conversation strategy predictor, and a decoder for producing the text response
     Other: complex architecture that consists of RAC (response act classifier), LM (GPT-2 text generation), V (reward for PPO). the system is trained via proximal policy optimization
     Other: fine-tuning + transfer learning
     Other: fine-tuning + wrapper.
they constructed and trained an elaborate technical system based on RoBERTa, gated recurrent units, and other modules.
     Other: fine-tuning + retrieval and content expansion
     Other: Fine-tuning plus other elements like clustering
     Other: fine-tuning of GPT-2 + reinforcement learning. whole system consists of motivational response generator + empathetic rewriting framework
     Other: fine-tuning + custom pipeline (see Reflection Generation Training)
     Other: multiple different:
selection of thinking traps: fine-tuning
writing of reframes: retrieval-enhanced in-context learning
     Other: Other: self-developed and trained transformer architecture
     Other: "we developed a transformer-based model for
dimensional text-based emotion recognition, fine-tuned with a novel, comprehensive
dimensional emotion dataset"
"The DL-based approach utilizes a BERT architecture (Devlin et al., 2018) with an
added final regression layer for computing a dimensional output"

BERT is fine-tuned, and this fine-tuned BERT model is placed in a larger chatbot architecture
     Other: fine-tuning + other transformers for detecting contradictions, recognizing toxic language, detect repetitive answers
     Other: custom architecture incorporating GPT-2. DialoGPT (based on GPT-2) is both fine-tuned and trained via RL.


26. llm_development_approach_notes
   ==================================
   Data type: object
   Total values: 172
   Non-null values: 92
   Null values: 80
   Unique values: 92
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 80 (87.0%)
     prompted to simulate a psychiatric first-responder chatbot: 1 (1.1%)
     Retrieval + hierarchical fusion + attention + lexicon-enriched embeddings used to prompt LLMs.: 1 (1.1%)
     The system integrates two main components: a BERT-based emotional distress detection module and a fine-tuned GPT-3.5 model for Psychological First Aid (PFA) response generation.
First, BERT analyzes the user’s input to detect their emotional state and distress level. These outputs are then passed, along with the original user input, into a custom prompt designed for GPT-3.5. GPT-3.5 was fine-tuned on therapy transcripts to improve its ability to produce empathetic, context-aware PFA responses.: 1 (1.1%)
     ChatGPT played the role of a client and optionally provided AI-generated feedback to trainees: 1 (1.1%)
     Their intervention: The AI therapist tested in this study is an LLM-based pro- gram previously developed and tested at Cedars-Sinai Medical Center, with the goal of offering AI-enabled, self-
administered, mental health support within immersive virtual
reality environments.18 The user interacts with an AI conver-
sational agent designed for history-taking and therapeutic sup-
port for anxiety and depression. The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
The agent facilitates turn-based conversations, summarizing
and concluding each session, and was found to be an accepta-
ble and feasible support modality for patients with mild-to-
moderate anxiety or depression.18 Here, we focus on whether
there is evidence of any sociodemographic bias exhibited by
the conversational agent.: 1 (1.1%)
     Prompt engineering to shape ChatGPT’s dialogue: 1 (1.1%)
     Prompting of general purpose LLMs (stage 1, probably ChatGPT) + fine-tuning of BERT (stage 2) + some type of RAG based on SimCSE (stage 3): 1 (1.1%)
     two age-appropriate AI chatbots, leveraging GPT-4, were developed to provide natural, empathetic conversations: 1 (1.1%)
     FAITA-mental health chatbot was designed to classify OCD-Coach's responses: 1 (1.1%)
     It seems GPT-3.5 was fine-tuned for sentiment analysis in addition to a prompted GPT-3.5 chatbot

fine tuning + sentiment analysis module: 1 (1.1%)
     Retrieval Augmented Generation (RAG) kombiniert mit prompting: 1 (1.1%)
     Fine-tuning of GPT-4 + RAG (see Fig. 1) and user interface: 1 (1.1%)
     The model was trained using one-shot 
learning, meaning that it was given a client statement and 
a counselor’s emotional reflection response as input, and 
then used that information to generate its own emotional 
reflection response.: 1 (1.1%)
     They fine-tune RoBERTa but only prompt the generative models (Flan-t5, Mistral, GPT-3.5) that they use: 1 (1.1%)
     Designed prompt phrases, iteratively refined; three prompting regimes (zero-shot, few-shot, CoT); modeled counselor reasoning style; two LLM families evaluated (ChatGLM, ERNIE Bot; also Qianwen): 1 (1.1%)
     Trained on ArmanEmo dataset & Jigsaw Toxic Comment Classification translated into persian + user message validiation module. : 1 (1.1%)
     Used fine-tuning and modular architecture (context, expression, feedback generators) to simulate different therapeutic styles.
Three key technical components were employed: the Context Generator, the Expression Expander, and the Feedback Generator.

Not clear if they really fine-tuned.: 1 (1.1%)
     Evaluated existing GPT-based chatbots via persona-based simulated interactions, scored with CAPE framework. : 1 (1.1%)
     Fine-tuning of GPT-2 + other modules such as BERT-based cognitive distortion classification: 1 (1.1%)
     GPT-4 prompting plus other modules such as valence classification, rule-based parts etc.

MI-adapted condition, we include two additional components, which
we conceptualise as the NLU natural language understanding module and
NLG natural language generation modules: 1 (1.1%)
     “cognitive architecture comprising an ensemble of computational models, using cognitive modelling and machine learning models trained on the novel dataset, and novel ontologies” (Position: Methods): 1 (1.1%)
     Q-LoRA fine-tuning: 1 (1.1%)
     Developed by fine-tuning a GPT-2 model on a domain-specific dataset and integrating it into a larger architecture that included array-based context storage for multi-turn dialogue, Top-K/Top-P sampling for controlled text generation, and response ranking modules using BLEU scores and cosine similarity to select the most relevant and coherent reply.: 1 (1.1%)
     The research team, consisting of an AI product manager and
prompt engineer (OP), several expert psychologists (YH, KG, ZE,
and DHS), and a music therapist (TA), jointly developed the two
AI tools from December 2023 to January 2024. Initially, a
primary prompt was created by YH. This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests
and added operational instructions to ensure safety and ethics.
Subsequently, group members with clinical backgrounds (YH,
KG, ZE, TA) tested the tool on themselves and shared their
experiences and suggestions for improvements. Throughout this
process, the team realized that due to the tools’ depth and their
ability to reflect on unconscious internal parts, the presence of a
mental health care professional would be essential. An expert in
social psychology, DHS, addressed cultural issues, which were
also incorporated into the prompt. Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications.: 1 (1.1%)
     Fine-tuned LLM + integration of other data modalities (vision): 1 (1.1%)
     A base LLM was fine-tuned on 80 structured psychological counseling cases to adapt it to clinical contexts. Additionally, the model uses dynamic prompting with a structured DSM-5-based knowledge base. This is integrated into a larger pipeline involving text summarization, keyword extraction, and similarity analysis, alongside multimodal outputs (e.g., facial expressions, voice, and a visual avatar) to enable affective interaction.: 1 (1.1%)
     Only sub-study 3 employed an LLM: 1 (1.1%)
     The help-seeking questions were posted to the GPT-3.5-turbo model by OpenAI API to get responses from ChatGPT: 1 (1.1%)
     Prompt engineering on GPT-4. Authors describe the model is being "tuned" or "fine-tuned" but in fact they only iteratively adjust the prompt without any weight adjustment.: 1 (1.1%)
     Clare is a standalone tool, based on fine-tuned LLMs plus other modules voice inputs, safety checks: 1 (1.1%)
     Elaborate server architecture around OpenAI API (see Fig. 1): 1 (1.1%)
     Research team members prompted LLMs with the original instructions for the SIRI-2, as well as with one of the SIRI2-2’s 24 items. We did not prompt LLMs with any additional text. : 1 (1.1%)
     Fine-tuning (according to Fig. 3 at least) and RAG: 1 (1.1%)
     uses NLP and probabilistic models to classify symptoms, map user queries to mental health knowledge base, and personalize therapeutic suggestions: 1 (1.1%)
     Fine-tuned LLaMA 3.2 (3B parameters) using Unsloth and Ollama; used RAG for retrieval, LangChain memory for conversation history, and FAISS for relevant material recall: 1 (1.1%)
     Simple fine-tuning of Jais-13B base model on collected data -> Mental-Jais-13B
Jais-13B fine-tuned for mental health responses; fine-tuning aimed at empathy and contextual awareness for gender sensitive responses: 1 (1.1%)
     -: 1 (1.1%)
     Socrates was subjected to rigorous fine-tuning with pains-takingly detailed instructions designed to optimize therapeutic interactions. The model was specifically trained to recognize and appropriately respond to a wide spectrum of emotional states expressed by users, while respecting moments of  silence or reluctance to engage in conversation. It was calibrated to avoid overly intrusive questioning that might compromise user comfort and trust and instead detect nuanced linguistic patterns that might indicate various psychological states. Throughout its development, emphasis was placed on maintaining an appropriate balance between providing support and encouraging self-reflection: 1 (1.1%)
     Llama 2 fine-tuned on ESC dataset + slot extraction and filling framework

Since the cascaded model requires extracting slot information from the dialogue, we use seqGPT to extract information according to predefined slots. To assess the impact of our framework on different LLMs, we employ Azure GPT-3.5 and LLaMA2-Chat 7B as the LLM components of the cascaded model. For LLaMA2-Chat 7B, we use the training portion of the data and fine-tune the model using the LoRA method. We fine-tune the models for up to 20 epochs with a learning rate of 5e-5. Our baselines include empathetic response generators such as MIME, MoEL, LLaMA2-7B with prompting, GPT-3.5 with prompting, as well as four additional methods: DIALOGPT-Joint, BLENDERBot-Joint, and MISC.: 1 (1.1%)
     Patterns of prompt engineering (not fine-tuning) were used to sculpt responses from the
GenAI models (see Table 1) [33]. The prompt carefully defined therapeutic alliance, empathy,
professionalism, cultural competence, and therapeutic technique and efficacy. Like the therapists’ instructions, the prompt did not place any limits on the length of the response, nor was ChatGPT shown the therapists’ responses.: 1 (1.1%)
     Instructions to act like a counselor etc. implemented; This model also allowed us to use retrieval augmented generation technology, which allows the model to be provided with a corpus of knowledge to reduce hallucinations
and misinformation [8]. Both the provision of instructions and source materials can serve as guardrails that keep the chatbot in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate.: 1 (1.1%)
     PEFT (qLoRA) fine-tuning of T5: 1 (1.1%)
     Main LLM was fine-tuned but there are also other modules (e.g. safety classification)

Developed with over 100,000 human hours comprising software development, training dialogue creation, and refinement, Therabot 
is designed to augment and enhance conventional mental health treatment services by delivering personalized, evidenced-based mental health interventions at scale.

Decoder-only transformer models (Falcon-7B, LLaMA-2-70B) on AWS SageMaker, fine-tuned with QLoRA, and prompted with conversation history for inference (SageMaker end points).: 1 (1.1%)
     The experiment proceeds through three stages: data input, model 
inference, and result analysis. Parent and offspring datasets are 
input into target models, with intervention decisions recorded 
and results labeled as positive or negative based on model 
advice. 

Fig. 1: 1 (1.1%)
     LoRA: 1 (1.1%)
     Used scripted prompts to elicit chatbot interactions and compared with therapists; coded with MULTI; non-naturalistic brief interactions. — Quote: “We also only examined brief interactions with scripted prompts… The interaction logs … were coded using the Multitheoretical List of Therapeutic Interventions” (Position: p. 12; PubMed Methods).: 1 (1.1%)
     HealMe = fine-tuned Llama2-7B-Chat

approach by
selecting the open-source LLM, LLaMA2-7b-chat,
as its base and fine-tuning it to ensure the model
consistently maintains the role of a psychotherapist
with high empathy and guidance capabilities.: 1 (1.1%)
     The addition of multiple collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s behavior. Iterative prompt engineering was needed and used to improve the individual agent’s behavior and how they worked together.: 1 (1.1%)
     Prompting + modules (RAG with embeddings + vector DB + LLMs) — Quote: “one employs all-MiniLM-L6-v2 embeddings with FAISS… while the other leverages GoogleGenerativeAIEmbeddings, Pinecone… and BART for response generation.” (Fig. 1): 1 (1.1%)
     We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.: 1 (1.1%)
     Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).: 1 (1.1%)
     Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier (see Fig. 1): 1 (1.1%)
     Fine-tuning of GPT-2 + voice and video interaction system: 1 (1.1%)
     Built on LangChain framework with memory and journaling integration .. Input/Templates/Prompt Instruction: 1 (1.1%)
     "A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative" .. "The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately" ... "Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition": 1 (1.1%)
     Training of GPT-2 from scratch on PsyQA + prepending of strategy token: 1 (1.1%)
     See Figure 3. All revolves around prompting of GPT-4, and then there is a "memory concept" and other stuff: 1 (1.1%)
     Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.: 1 (1.1%)
     Prompts were comprised of (1) context for the request, (2) how the model was meant to respond, 3) the specific request, and (4) the output format : 1 (1.1%)
     Model was prompted to behave like a counseling client: It was then tested in a fishbowl, triad, and one-to-one formats using facilitation commands (“play,” “pause,” “rewind”) and three processing techniques (“ChatGPT close up,” “counselor close up,” “observer close up”) grounded in Bernard’s Discrimination Model (1979)

Asking ChatGPT to act as a client: 1 (1.1%)
     Prompting with and without RAG

constructed a CBT-related knowledge base and integrated it with the general models … RAG (Retrieval-Augmented Generation): 1 (1.1%)
     Only fine-tuning of Qwen on their dataset: 1 (1.1%)
     Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4)

to fine-tune the generated instruction
data effectively, we employed the inhibition adaption fine-
tuning method [14] and self-RAG [13] on Llama2-7B [15],
as well as ChatGLM2-6B [16].: 1 (1.1%)
     We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).: 1 (1.1%)
     ChatGPT responses collected for 21 psychoeducational prompts across categories (e.g., depression, substance use, wellness); evaluated on six criteria (accuracy, clarity, relevance, empathy, engagement, ethics) via qualitative content analysis (QCA) and Likert ratings.

Prompting of ChatGPT with mental health related questions (Table 1): 1 (1.1%)
     fine-tunes a ChatGLM2-6B model on synthetically generated data: 1 (1.1%)
     Two adverserial LLMs generate new counseling dataset. 
Prompted ChatGPT-3.5, Claude Haiku and LLaMAntino to role-play therapist and patient in depression sessions; compared against human HOPE dataset using text-network metrics. : 1 (1.1%)
     CoI breaks MI coding into three sequential prompt stages (Interaction Definition, Involvement Assessment, Valence Analysis) to mimic professional coders using MISC schema.

Different styles of prompting: Zero-shot, few-shot, CoT: 1 (1.1%)
     BERT is fine-tuned but the generative Llama 2 is only prompted. The BERT model is used for helping skill classification and the predictions are inputted into the Llama 2 prompt.: 1 (1.1%)
     "We fine-tuned the model with an 80% to 20% train-test split" + "that is, the system is prompted
to keep generating responses"...: 1 (1.1%)
     Key elements of the algorithm’s configuration
included the following: 1. Prompt Engineering: A personalized and adaptive flow of questions and responses was
designed to align with the user’s emotional state and specific anxiety symptoms; 2. Integration of Clinical Scales; 3. Behavioral Customization: 1 (1.1%)
     Different models are simply fine-tuned on provided datasets: 1 (1.1%)
     The authors adapted ChatMGL, a GPT-2–based model originally trained on STEM Q&A with supervised fine-tuning and PPO, for mental health dialogue. They dropped PPO, focusing instead on supervised fine-tuning with Hugging Face’s SFTT and Delta Tuning (a parameter-efficient method that updates only part of the weights). : 1 (1.1%)
     The proposed chatbot system aims to enhance mental health 
assistance through generative AI and embedding-based 
relevance detection. Unlike rule-based chatbots with 
predefined responses, our chatbot responds dynamically to 
diverse user inputs. The core innovations include few-shot 
learning for personalized conversation, semantic similarity 
filtering to ensure relevance, and generative response 
generation using Google Gemini API. : 1 (1.1%)
     Prompt engineering + RAG: 1 (1.1%)
     Llama 2 is prompted, but there is also a RAG component: 1 (1.1%)
     GPT-3.5 is only prompted but there is also an intent classifier and cognitive distortion identifier: 1 (1.1%)
     For the ChatGPT-generated advice, version GPT-4 was used. On October 12, 2023, 
the following question was asked on a fresh chatbot session and the response was saved:
[...] Since the study’s objective was to evaluate performance of ChatGPT-4 with-
out any refinement or customization, we deliberately refrained from using any prompt 
engineering techniques in our design and opted to use the afore mentioned one-time 
instruction to the LLM. : 1 (1.1%)
     Prompting of different LLMs + other modules (speech-to-text, editable dashboard, etc.): 1 (1.1%)
     in a further chat interface (test 2), we
introduced the scope of expertise using a prompt as a pretraining tool, before proceeding
with the presentation of the case. This prompt was generated using ChatGPT, and its
reliability as a pretraining tool for the chat session was verified as a good alternative to
a longer and more complex training process in a previous study we conducted [12]: 1 (1.1%)
     The implementation of the speak-speak feature involves
specifying the models, prompts (as we are testing in a zero
learning shot environment): 1 (1.1%)
     Only prompting to evaluate capabilities of LLMs in terms of stigma and their responses to mental health symptoms.  In addition to prompting models with the stimuli without in-context examples, we also employed a novel method of prompting models with a portion of real therapy tran-
scripts from Alexander Street Press [6, 7]: 1 (1.1%)
     Prompting of GPT-4o + components of a robot (speech-to-text, movement, etc.): 1 (1.1%)
     Four unique questions were asked of ChatGPT 4.0, and each question was asked five separate times. Each question asked to the model 
was performed in a separate, new conversation (10): 1 (1.1%)
     Prompting of GPT-3.5 and Mistral + RAG + sentiment analysis: 1 (1.1%)
     "LangChain for Prompting" combined with "CNN for Facial Recognition," "LSTM for Text-based Emotion Detection," "MFCC or LSTM for Audio Recognition": 1 (1.1%)
     Uses retrieval + reranker (BERT family) + fine-tuned InternLM2-7B-Chat with QLoRA; modules include Context Generator, Expression Expander, Feedback Generator.: 1 (1.1%)
     davinci-003 + off topic classifier. davinci-003 seems not finetuned: 1 (1.1%)
     Promts: "Construct a therapeutic sleep story for elementary school
children”OR “Constructa therapeutic sleep story for elementary school children that involves a crisis".

Authors gave identical instructions to human expert and ChatGPT; prompts were deliberately simple, without additional tuning. — Quote: “For the psychotherapeutic expert, the instructions were the same as for the AI tool.” (Position: Therapeutic stories, p. 2).: 1 (1.1%)
     Seven-day dialog; qualitative interpretation via Jungian/analytic lenses and metaphor work — Quote: “analyzed from a Jungian perspective to highlight connections to symbols and archetypes” (Position: ResearchGate abstract).: 1 (1.1%)
     Fig.1. 

RAG pipeline … OpenAIEmbeddings … Chroma vector database … ChatOpenAI model (GPT-3.5-Turbo) via a ChatPromptTemplate … LangGraph … MultiQueryRetriever … crisis detection.: 1 (1.1%)
     Uses RAG with prompt engineering: general LLM is guided by structured prompt templates for intent detection and empathetic response generation, while a vector-based QA database supplies domain-specific context.: 1 (1.1%)

   Sample values (first 20):
     HealMe = fine-tuned Llama2-7B-Chat

approach by
selecting the open-source LLM, LLaMA2-7b-chat,
as its base and fine-tuning it to ensure the model
consistently maintains the role of a psychotherapist
with high empathy and guidance capabilities.
     CoI breaks MI coding into three sequential prompt stages (Interaction Definition, Involvement Assessment, Valence Analysis) to mimic professional coders using MISC schema.

Different styles of prompting: Zero-shot, few-shot, CoT
     Two adverserial LLMs generate new counseling dataset. 
Prompted ChatGPT-3.5, Claude Haiku and LLaMAntino to role-play therapist and patient in depression sessions; compared against human HOPE dataset using text-network metrics. 
     fine-tunes a ChatGLM2-6B model on synthetically generated data
     ChatGPT responses collected for 21 psychoeducational prompts across categories (e.g., depression, substance use, wellness); evaluated on six criteria (accuracy, clarity, relevance, empathy, engagement, ethics) via qualitative content analysis (QCA) and Likert ratings.

Prompting of ChatGPT with mental health related questions (Table 1)
     We propose the Diagnosis of Thought (DoT)
prompting, guiding the LLM through the above
three stages to diagnose the patient’s speech .... We com-
pare our DoT prompting with 1) Directly generat-
ing the results, and 2) Zero-Shot CoT prompting
(ZCoT) (Kojima et al., 2022).
     Fine-tuning of Llama2-7B and ChatGLM2-6B + RAG + assistant instruction (by GPT-4)

to fine-tune the generated instruction
data effectively, we employed the inhibition adaption fine-
tuning method [14] and self-RAG [13] on Llama2-7B [15],
as well as ChatGLM2-6B [16].
     Only fine-tuning of Qwen on their dataset
     Prompting with and without RAG

constructed a CBT-related knowledge base and integrated it with the general models … RAG (Retrieval-Augmented Generation)
     Model was prompted to behave like a counseling client: It was then tested in a fishbowl, triad, and one-to-one formats using facilitation commands (“play,” “pause,” “rewind”) and three processing techniques (“ChatGPT close up,” “counselor close up,” “observer close up”) grounded in Bernard’s Discrimination Model (1979)

Asking ChatGPT to act as a client
     Only prompting of GPT-4 for generating MI reflections.
Using these reflections, a distilled GPT-2 model is fine-tuned.
     The addition of multiple collaborative AI agents, which sets Socrates 2.0 apart from the
original version, appeared to meaningfully impact the tool’s behavior. Iterative prompt engineering was needed and used to improve the individual agent’s behavior and how they worked together.
     See Figure 3. All revolves around prompting of GPT-4, and then there is a "memory concept" and other stuff
     Training of GPT-2 from scratch on PsyQA + prepending of strategy token
     "A key sentiment analysis model within MindBot is using 
the NLTK's SentimentIntensityAnalyzer, which is trained on 
labelled data from emotional tones. That tool calculates a 
sentiment polarity score, which captures what a user has said 
within a scale of highly positive to highly negative" .. "The conversational agent watches the context of the 
dialogue so that across several exchanges, it yields consistent 
and meaningful ones. The LLM-for example, GPT-enables 
the agent to remember the unfolding of the dialogue and to 
respond appropriately" ... "Non-personalized answers are generated according to the 
Senti- mental tone and intent, by using the LLM. The agent 
makes sure that answers fall into the scope of the user's 
Sentimental condition"
     Built on LangChain framework with memory and journaling integration .. Input/Templates/Prompt Instruction
     Fine-tuning of GPT-2 + voice and video interaction system
     Prompting of GPT-4o-mini + Bi-LSTM sentiment classifier (see Fig. 1)
     Multi-role framework built on ChatGPT API (gpt-3.5-turbo, GPT-4); incorporates chain-of-thought (Analyzer), exemplar retrieval with SimCSE embeddings (Knowledge-Collector), and explicit strategy prompting (Strategy-Planner).
     We utilized domain-specific fine-tuning 
to further tailor the LLM for therapeutic applications; Unclear, how they fine-tuned it. + 
Screenshots of MICA and specific prompts used in Phase I and Phase II are provided in the supplementary material.


27. intervention_type
   =====================
   Category: MULTIPLE CHOICE (semicolon-separated)
   Total responses: 159
   Null responses: 13
   Total individual choices: 187
   Unique choice options: 16
   Average choices per response: 1.2

   Individual Choice Counts:
     Unspecified, might include formal therapy methods: 103 (55.1%)
     Other CBT techniques: 19 (10.2%)
     CBT: Cognitive restructuring: 18 (9.6%)
     CBT: Motivational interviewing: 14 (7.5%)
     Informal counseling (e.g., emotional support conversation): 13 (7.0%)
     Mix of formal therapy methods: 7 (3.7%)
     Peer support conversation: 3 (1.6%)
     Psychoanalysis: 2 (1.1%)
     Other: Person Centered Therapy (PCT) (Carl Rogers): 1 (0.5%)
     Other: Digital journaling with AI counselling: 1 (0.5%)
     Other: Bedtime stories: 1 (0.5%)
     Other: Gestalt Therapy techniques, Gestalt supervision: 1 (0.5%)
     Other: counselor-training simulation: 1 (0.5%)
     Psychodynamic psychotherapy: 1 (0.5%)
     Other: The chatbot's architecture is built on a 
robust knowledge base encompassing evidence-based therapeutic techniques, including cognitive-behavioral therapy (CBT) principles and motivational interviewing 
strategies.: 1 (0.5%)
     Other: positive psychology intervention: 1 (0.5%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 7

     • Other: Person Centered Therapy (PCT) (Carl Rogers) — 1 occurrence(s)
     • Other: Digital journaling with AI counselling — 1 occurrence(s)
     • Other: Bedtime stories — 1 occurrence(s)
     • Other: Gestalt Therapy techniques, Gestalt supervision — 1 occurrence(s)
     • Other: counselor-training simulation — 1 occurrence(s)
     • Other: The chatbot's architecture is built on a — 1 occurrence(s)
     • Other: positive psychology intervention — 1 occurrence(s)


28. models_employed
   ===================
   Category: MULTIPLE CHOICE (semicolon-separated)
   Total responses: 172
   Null responses: 0
   Total individual choices: 253
   Unique choice options: 50
   Average choices per response: 1.5

   Individual Choice Counts:
     GPT-4 / GPT-4o family: 46 (18.2%)
     BERT family: 43 (17.0%)
     GPT-3.5 family: 36 (14.2%)
     ChatGPT, model unspecified: 21 (8.3%)
     GPT-2 family: 19 (7.5%)
     Llama 2 family: 14 (5.5%)
     T5 family: 7 (2.8%)
     Mistral family: 6 (2.4%)
     Claude family: 5 (2.0%)
     GPT-3 family: 4 (1.6%)
     Other: Falcon-7B: 4 (1.6%)
     Gemini / Bard family: 4 (1.6%)
     Qwen family: 3 (1.2%)
     BART family: 3 (1.2%)
     Other: ChatGLM2-6B: 3 (1.2%)
     Other: PanGu, WenZhong (based on GPT-2): 1 (0.4%)
     Other: Baichuan-7B, Llama 1, Alpaca 1: 1 (0.4%)
     Other: BlenderBot: 1 (0.4%)
     Other: Phi-2, GPT-J, GPT-Neo: 1 (0.4%)
     Other: various LLM-based chatbots: Replika, Snapchat My AI, Chai, Character.ai, Anima, Paradot, ChatGPT, Kuki: 1 (0.4%)
     Other: own transformer architecture: 1 (0.4%)
     Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer [26] model which generates a beam of candidate responses. In the second stage a number of smaller, more specialized Transformer-based models): 1 (0.4%)
     Other: not sure, it just says transformer model: 1 (0.4%)
     Other: Pi, Copilot, Kindroid, ChatMind/VOS: 1 (0.4%)
     Other: ERNIE-3.5-8K,
iFlytek Spark V3.0, ChatGLM-3-turbo: 1 (0.4%)
     Other: NLTK VADER: 1 (0.4%)
     Other: not specified - using LangChain framework: 1 (0.4%)
     Other: all types of chatbots e.g. ChatGPT: 1 (0.4%)
     Other: text-embedding-ada-002: 1 (0.4%)
     Other: InternLM2-7B-Chat with QLoRA: 1 (0.4%)
     Llama 3.1 family: 1 (0.4%)
     Other: Pi - Noni - Serena - and other commercial chatbots: 1 (0.4%)
     Other: text-embedding-3-small: 1 (0.4%)
     Other: Pi, Replika: 1 (0.4%)
     Other: ChatGLM: 1 (0.4%)
     Other: GPT-Neo, GPT-J, AI21 Jurassic-1: 1 (0.4%)
     Other: MiniMax 6.5s (245K), Doubao Function Call model (32K): 1 (0.4%)
     Other: ChatGLM, ERNIE Bot, Qianwen (as model and as judge): 1 (0.4%)
     Other: BLOOMZ: 1 (0.4%)
     Other: various GenAI chatbots: 1 (0.4%)
     Other: various LLMs: 1 (0.4%)
     Other: DeepSeek-R1: 1 (0.4%)
     Other: AI in general, mostly ChatGPT: 1 (0.4%)
     Other: DeepSeek, ERNIE Bot: 1 (0.4%)
     Other: SeqGPT: 1 (0.4%)
     Other: just general "generative AI application": 1 (0.4%)
     Other: Jais-13B: 1 (0.4%)
     Other: Llama 3.2 family: 1 (0.4%)
     Other: all-MiniLM-L6-v2 (for embedding): 1 (0.4%)
     Other: Clare (R) by clare&me GmbH: 1 (0.4%)

   'Other:' Categories Breakdown:
     Total 'Other:' entries: 41

     • Other: Falcon-7B — 4 occurrence(s)
     • Other: ChatGLM2-6B — 3 occurrence(s)
     • Other: PanGu, WenZhong (based on GPT-2) — 1 occurrence(s)
     • Other: Baichuan-7B, Llama 1, Alpaca 1 — 1 occurrence(s)
     • Other: BlenderBot — 1 occurrence(s)
     • Other: Phi-2, GPT-J, GPT-Neo — 1 occurrence(s)
     • Other: various LLM-based chatbots: Replika, Snapchat My AI, Chai, Character.ai, Anima, Paradot, ChatGPT, Kuki — 1 occurrence(s)
     • Other: own transformer architecture — 1 occurrence(s)
     • Other: Seq2Seq, Transformer (The first stage consists of a large Seq2Seq Transformer [26] model which generates a beam of candidate responses. In the second stage a number of smaller, more specialized Transformer-based models) — 1 occurrence(s)
     • Other: not sure, it just says transformer model — 1 occurrence(s)
     • Other: Pi, Copilot, Kindroid, ChatMind/VOS — 1 occurrence(s)
     • Other: ERNIE-3.5-8K, — 1 occurrence(s)
     • Other: NLTK VADER — 1 occurrence(s)
     • Other: not specified - using LangChain framework — 1 occurrence(s)
     • Other: all types of chatbots e.g. ChatGPT — 1 occurrence(s)
     • Other: text-embedding-ada-002 — 1 occurrence(s)
     • Other: InternLM2-7B-Chat with QLoRA — 1 occurrence(s)
     • Other: Pi - Noni - Serena - and other commercial chatbots — 1 occurrence(s)
     • Other: text-embedding-3-small — 1 occurrence(s)
     • Other: Pi, Replika — 1 occurrence(s)
     • Other: ChatGLM — 1 occurrence(s)
     • Other: GPT-Neo, GPT-J, AI21 Jurassic-1 — 1 occurrence(s)
     • Other: MiniMax 6.5s (245K), Doubao Function Call model (32K) — 1 occurrence(s)
     • Other: ChatGLM, ERNIE Bot, Qianwen (as model and as judge) — 1 occurrence(s)
     • Other: BLOOMZ — 1 occurrence(s)
     • Other: various GenAI chatbots — 1 occurrence(s)
     • Other: various LLMs — 1 occurrence(s)
     • Other: DeepSeek-R1 — 1 occurrence(s)
     • Other: AI in general, mostly ChatGPT — 1 occurrence(s)
     • Other: DeepSeek, ERNIE Bot — 1 occurrence(s)
     • Other: SeqGPT — 1 occurrence(s)
     • Other: just general "generative AI application" — 1 occurrence(s)
     • Other: Jais-13B — 1 occurrence(s)
     • Other: Llama 3.2 family — 1 occurrence(s)
     • Other: all-MiniLM-L6-v2 (for embedding) — 1 occurrence(s)
     • Other: Clare (R) by clare&me GmbH — 1 occurrence(s)


29. ux_assessment_is_present
   ============================
   Data type: object
   Total values: 172
   Non-null values: 159
   Null values: 13
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     No users involved: 98 (61.6%)
     Yes: 53 (33.3%)
     [NULL]: 13 (8.2%)
     No: 8 (5.0%)


30. ux_uses_standard_instrument
   ===============================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     n: 157 (91.3%)
     y: 14 (8.1%)
     -: 1 (0.6%)


31. ux_uses_qualitative_assessment
   ==================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 140 (81.4%)
     y: 32 (18.6%)


32. ux_uses_quantitative_assessment
   ===================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 137 (79.7%)
     y: 35 (20.3%)


33. ux_results_reported
   =======================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 122 (70.9%)
     y: 50 (29.1%)


34. ux_assessment_instrument
   ============================
   Data type: object
   Total values: 172
   Non-null values: 16
   Null values: 156
   Unique values: 16
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 156 (975.0%)
     One of the ICAs on the 
7-point Likert scale using two measures from UEQ: 1 (6.2%)
     User experience questionnaire (UEQ): 1 (6.2%)
     Human Computer Trust Scale S. S. Siddharth Gulati and D. Lamas, “Design, development and evaluation of a human-computer trust scale,” Behaviour & Information Technology: 1 (6.2%)
     System Usability Scale, SUS
User Experience Scale, UES 
Client Satisfaction Questionnaire, CSQ
Likert-scale: 1 (6.2%)
     AI attitude scale (AIAS-4): doi: 10.3389/fpsyg.2023.1191628: 1 (6.2%)
     CSQ and PU: 1 (6.2%)
     SAM-Scale: 1 (6.2%)
     Measures of user relationship with the generative and
rules-based DMHIs included user satisfaction as measured by
the Client Satisfaction Questionnaire-8 (CSQ-8) [10] and
working alliance as measured by the Working Alliance
Inventory-Short Revised Bond subscale (WAI-SR Bond9: 1 (6.2%)
     During the experiment, after getting acquainted with the cases, participants were
asked to assess their willingness to contact a psychologist who provided these recom-
mendations (a seven-point Likert scale was used). Also, the participants of the exper-
iment were asked to evaluate the recommendations using the author’s semantic differ-
ential, which included the classical factors highlighted by Ch. Osgood (f. Strength, f.
Assessment, f. Activity), as well as an additional factor included (f. Informativeness).: 1 (6.2%)
     SUS (System Usability Scale), 

CEMI (Client Evaluation of Motivational Interviewing) — Quote: “MI fidelity (relational and technical sub-scales of the Client Evaluation of MI [CEMI]) and usability (System Usability Scale)” (Position: Abstract): 1 (6.2%)
     modified Mental Help Seeking Attitudes Scale (MHSAS); modified version of the
measure of structural barriers toward mental health service utilization as reported by Van Doren et al: 1 (6.2%)
     Counselling Competencies Scale

https://doi.org/10.1080/07481756.2017.1358964

Three subscales used: Enhancing counselling skills, enhancing behaviours, learning mental health topics — Quote: “It included three subscales: PB-CSTC, PB-CDB, PB-LC” (Position: III.B.4.b): 1 (6.2%)
     Moreover, we also measured the trust on robots using Human-Robot 
Interaction Trust Scale (HRITS) scale (Pinto et al., 2022). There are total 11 items in this scale which is designed to measure participants' trust in 
AI and robotic systems using 5- point Lickert scale from strongly disagree (1) to strongly agree (5).: 1 (6.2%)
     empathic understanding (EU), system usability (SUS), acceptance (CSQi): 1 (6.2%)
     consultation and relational empathy survey (CARE): 1 (6.2%)
     Counselor rating form‑short (CRF‑S), Client satisfaction scale (CSS): 1 (6.2%)


35. ux_assessment_notes
   =======================
   Data type: object
   Total values: 172
   Non-null values: 52
   Null values: 120
   Unique values: 52
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 120 (230.8%)
     72.86% rated GPT-4 advice practical … 87.94% well-explained … statistically significant” (Position: Results, p. 41–42)

GPT-4 advice perceived as more practical and clearer; effect sizes small-to-medium. — Quote: “differences … statistically significant … d ≈ 0.34 … d ≈ 0.53 …” (Position: Results, p. 41–42): 1 (1.9%)
     10 questions regarding user interface, ease of use, chatbot responsiveness, 
and overall satisfaction. might include standard instrument, but not described.: 1 (1.9%)
     High satisfaction (>80% smooth design); moderate effectiveness (~50% improved sleep); high acceptance (75% continued use). — Quote: “More than 80% of the volunteers think that our app is well designed and smooth to use” (Position: Application Utilization): 1 (1.9%)
      “Our system” (M = 5.368) found this work’s system statistically significantly more supportive than the “Woebot” group (M = 4.261) found Woebot supportive (p = 0.041), while for the measure usual-leading edge, the groups’ scores were not statistically significantly different (both M = 4.609, p = 0.084).: 1 (1.9%)
     Also measuring undesirable output via annotation.
For qualitative results, see 4.3.3.: 1 (1.9%)
     Only quantitative scoring of : 1 (1.9%)
     emotional valence and dominance improved; deliberate style increased happiness, rapid style increased surprise. : 1 (1.9%)
     This article is a case study with loose, qualitative descriptions: 1 (1.9%)
     This study employed a deductive thematic analysis, using the conceptual framework of Grodniewicz and Hohol’s (2023) three challenges in AI psychotherapy as a general guide: 1 (1.9%)
     See V.B.: 1 (1.9%)
     All participants noted improved treatment motivation … 80% disclosing personal concerns … 24/7 availability particularly benefited patients
Four out of five participants reported significant reductions in anxiety and stress levels post-intervention” (Results, p. 1): 1 (1.9%)
     Self-efficacy improved across sessions; anxiety reduced mainly between Session 1 and 2. — Quote: same as quantitative assessment line: 1 (1.9%)
     Both, user experience assessment and participant attitude assessment.

This study uses a narrative inquiry design to explore the experiences and reactions of mental health practitioners in the context of the rapid expansion of generative artificial intelligence. One-on-one Zoom interviews, each lasting 30–45 min were conducted, along with randomly selecting some participants for a brief follow-up interview lasting 15–20 min conducted within two weeks after the initial interview.: 1 (1.9%)
     Qualitative analysis of user reviews focusing on risks using BERTopic: 1 (1.9%)
     Results reported: Yes (case-level outcomes) — Quote: “the AI successfully managed to … become more experientially aligned with the patient” (Position: Clinical section, p. 11)

Empathic failures, uncanny feelings, corrective alignment described — Quote: “empathic failure occurred … however, the ability to overcome empathic failures … holds significant therapeutic value” (Position: Clinical section, p. 11): 1 (1.9%)
     See section: Technology Comfort, AI Attitudes, and AI
Use Intention: 1 (1.9%)
     Self-developed survey used for evaluation. 

Working Alliance Inventory (WAI)-SR was assessed, too. I would not necessarly assign it to user-experience, but it may serve as an additional indicator for UX.

Results in Fig 5: 1 (1.9%)
     Most participants reported feeling strongly understood by
the chatbot, with the majority indicating a high degree of
perceived emotional attunement. Participants described the
experience as intuitive to navigate, personally meaningful,
and contextually relevant to their situations. A general sense
of satisfaction was expressed across the participant group,
with favorable ratings on the satisfaction measures.: 1 (1.9%)
     higher reliable improvement, recovery, and reliable recovery rates” 
users perceived the AI-enabled therapy support tool as most useful for discussing their problems to gain awareness and clarity … learning how to apply coping skills”: 1 (1.9%)
     No user experience assessment but participant attitude survey: 1 (1.9%)
     Not described, how many users and rather vague representation of results in Fig. 7.: 1 (1.9%)
     The study design integrated quantitative and
qualitative approaches to provide comprehensive insights: 8
quantitative questions (1 overall satisfaction item and 7 components of chatbot performance) and 4 qualitative questions
(2 positive aspects and 2 areas for improvement). This mixed
methods approach allowed for triangulation of data through
cross-verification between quantitative metrics and qualitative
user feedback, enhancing the validity and depth of our findings.

Quantitative: Using a comprehensive survey
consisting of 1 overall satisfaction question and 7 quantitative items assessing key components of effective psychological counseling, which consists of empathy, accuracy and usefulness, complex thinking and emotions, active listening and appropriate questions, positivity and support, professionalism, and personalization. 
Qualitative: structured interviews and open-ended survey responses, focusing on key themes such as response speed, empathy, and personalization. : 1 (1.9%)
     users expressed positive attitudes toward digital mental health solutions, with key motives including avoiding embarrassment (36%) and concerns about appearance in face-to-face consultations (35%). Expectations focused on emotional support (35%) and expressing feelings (32%): 1 (1.9%)
     Before the study’s completion, we conducted unstructured interviews to delve deeper into the participants’ experiences with the chatbots. Many participants expressed that the chatbot’s responses evoked feelings of surprise and novelty. They emphasized the importance of feeling understood as pivotal in enhancing their acceptance and engagement with the chatbot-based intervention. Since GPT-based chatbots can generate text in a human-like manner, mimicking the interaction between users and psychotherapists, five participants indicated that the anthropomorphic responses felt so authentic that they found it difficult to distinguish whether they were AI-generated. These qualitative insights reinforce our quantitative findings and underscore the critical importance of interaction in the design of chatbots.: 1 (1.9%)
     Qualitative analysis:
This approach involves six main steps: getting familiar with the data; generating initial codes; searching for themes; reviewing themes; defining and naming themes; and producing the report. The analysis process was iterative and recursive, allowing the researchers to move back and forth between these steps as needed to ensure thoroughness and validity. : 1 (1.9%)
     User experience assessment of therapists using the tool. There is a quantiative (Likert scale questionnaires) and qualitative (open-ended user questions) assessment: 1 (1.9%)
     As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT: 1 (1.9%)
     Satisfaction ratings focused on relational/emotional quality, technical/didactic, treatment support, professional orientation. — Quote: “PCA highlighted four components… relational and emotional (C1), didactic and technical quality (C2), treatment support and development (C3), and professional orientation and adaptability (C4)” (Position: Abstract): 1 (1.9%)
     Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).: 1 (1.9%)
     1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5): 1 (1.9%)
     Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.: 1 (1.9%)
     Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience: 1 (1.9%)
     Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions ("score", "activity", "informativeness"), while psychologist responses were rated higher in "strength".

"During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used)."​: 1 (1.9%)
     consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?: 1 (1.9%)
     Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions): 1 (1.9%)
     We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).: 1 (1.9%)
     System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.

Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based: 1 (1.9%)
     Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.

Results: Not clear/not reported: 1 (1.9%)
     Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.: 1 (1.9%)
     The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.

Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience: 1 (1.9%)
     Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.: 1 (1.9%)
     Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. "It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information."

semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.: 1 (1.9%)
      range of positive impacts were reported, including improved mood… healing from trauma and loss… improved relationships…

Participants told us that generative AI feels like an emotional sanctuary, offers insightful guidance… joy of connection… bears comparison with human therapy.: 1 (1.9%)
     See "Initial User Feedback": 1 (1.9%)
     "User experience assessment" was content of the Reddit posts: 1 (1.9%)
     SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback): 1 (1.9%)
     Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” : 1 (1.9%)
     Qualitative feedback reported in 4.4.: 1 (1.9%)
     Reconnection to poetic inspiration; mobilization of archetypal symbols (e.g., deer); extensive nature metaphors; perceived attunement — Quote: “mobilized archetypal symbols … figure of the messenger, like Hermes … ‘Let this poetry be your guide.’” (Position: Discussion; provided excerpt)

May not be understood as the "classical" User Experience, since the user experience is implicit in the text snippets whit ChatGPT provided.: 1 (1.9%)
     As described, in a second step, an inter-
view with the reviewers was conducted.
The results of this qualitative analysis to
identify AI-generated stories were
“Sentences similar at the beginning, similar se-
quence.”
“The type of story has been repeated again and
again and was very similar.”
“Sequence always the same.”
“Similar start: Once upon a time ....”
Furthermore, the reviewers reported that
“80% of the stories were very similar”
and that the AI-constructed stories de-
scribed typical sleep problems and made
sleep-related recommendations

A total of N=80 evaluation questionnaires … 85% of the overall stories were correctly categorized” (Position: Results, p. 3).: 1 (1.9%)
     Quantitative: Measures of the user experience with the generative and rules-based DMHIs included user engagement (number of sessions, total active days, and conversational exchanges).
: 1 (1.9%)
     Thematic analysis of Reddit posts and comments: 1 (1.9%)

   Sample values (first 20):
     As a secondary outcome measure, we assessed patient satisfaction with the ChatGPT-assisted therapy through a Likert scale questionnaire (Attachment 1), created by the Psychiatrists conducting this study. The Likert scale questionnaire, specifically developed for this study, included the following items to assess various dimensions of patient experience and perception:

1. Study Participation Enjoyment: “I enjoyed participat-
ing in this study.”
2. Intervention Helpfulness: “This intervention helped
me during my stay in the psychiatric inpatient unit.”
3. Use of ChatGPT: “I enjoyed utilizing ChatGPT.”
4. Emotional Management Tools: “The sessions pro-
vided me with tools that help me better manage my
emotions.”
5. Future Utility: “I have gained a new tool that I can
utilize in the future, and that will help me deal with
day-to-day problems.”
6. Need for More Such Interventions: “There should be
more interventions of this kind provided to patients
in inpatient psychiatric care.

Response options ranged from “Totally disagree” to “Totally agree,” allowing patients to express their level of agreement with each statement.
For the secondary outcome of patient satisfaction with this ChatGPT intervention, patients in the intervention group scored highly on the Likert scale questionnaire, as illustrated in Figure 1. The average score was 26.8 out of a possible 30 (SD = 2.34), indicating high of satisfaction with their interactions with ChatGPT
     Supplementary Figure 2 highlights the main themes that emerged from the de-briefing
interviews. Across sociodemographic characteristics and VR experience levels, participants
expressed positive perceptions about the program and described their experience as
“impressive,” “amazing,” “real,” “authentic,” “positive,” and “enjoyable.” The session was noted
to “fulfill expectations” and it was stated that interacting with XAIA “felt like having a
conversation with a real person.” Generally, participants found the program to be
straightforward and user-friendly (e.g., “It was pretty easy to maneuver”). All 14 participants
expressed interest in using XAIA again and would recommend the program to others.
Many participants indicated that XAIA met their expectations of a human therapist. For
example, they perceived XAIA to be approachable (“It felt like a friend”), easy to talk to (“I was
able to let out a lot”), understanding with good listening skills (“It felt like I was actually talking
to somebody that was listening”), compassionate (“She was able to empathize with what I was
going through which makes me feel good”), and adaptable to their needs (“I was like, let's
practice some breathing exercises, so she offered another alternative instead of talking”). They
also mentioned feeling “unjudged” and being able to trust XAIA because of an unbiased persona
(“I did not feel judged, I felt accepted”).
Participants emphasized other essential qualities of XAIA, including being supportive
(“What she said was positive and encouraging”), helpful and empowering (“She made me feel
better about myself and perhaps a little empowered, I was like okay I can do this”), calming (“Very
relaxing and easing”), intelligent (“I was very impressed how smart...like the answers that came
back”), and to the point (“I enjoyed how concise she is”). Participants also described feeling safe
and heard (“A lot of what XAIA gave me was a validation of my current feelings”). They were
surprised by XAIA’s ability to “understand thoughts and feelings” and “summarize what’s been
said.” Some were taken aback by their own emotional response (“I actually teared up”). The
immersive environments also created a “relaxing” atmosphere (“I like the ambience”; “The visual
parameters allow my body to relax”).
     1. volunteers with mental health issues: Willingness for continued chatbot usage  (90%) , human-likeness (4.3/ 5), supportiveness (4.2/ 5), overall user satisfaction (4.6/ 5)
2. mental healthcare professionals and researchers: value of chatbots (70%), Confidence in chatbot’s helpful output 30% extremely confident , 40% confident),  human-likeness (4/ 5), supportiveness (4.1/ 5), overall user satisfaction (4/ 5)
     Users can rate the results using the built-in rating system (Available at
https://www.wjx.cn/vm/OJMsMXn.aspx (accessed on 12 July 2023)), and there is a link
to an additional evaluation site at the bottom of the page

Enhancing the chatbot’s user experience and user interface can significantly impact
its adoption and effectiveness. Future work should focus on improving the simplicity,
intuitiveness, and accessibility of the website interface. This includes optimising response
times, refining the layout and design, and incorporating user-friendly features such as
autocomplete suggestions or natural language understanding capabilities.
Furthermore, personalised recommendations and suggestions to users based on their
preferences and previous interactions can enhance the user experience. Techniques like
collaborative filtering or user profiling can enable the chatbot to better understand and
cater to individual user needs. Usability testing and user feedback collection should be
conducted regularly to gather insights on user preferences, pain points, and suggestions
for improvement. Iterative design and development based on user-centred principles can
ensure that the chatbot meets user expectations and effectively addresses their mental
health support needs.
     Qualitative analysis of Reddit comments of Replika users. Distinct topics are identified:
Benefit 1: Providing on-demand support
Benefit 2: Offering non-judgemental support
Benefit 3: Developing Confidence for Social Interaction
Benefit 4: Promoting self-discovery
Challenge 1: Harmful content
Challenge 2: Memory lost
Challenge 3: Inconsistent communication styles
Challenge 4: Over-reliance on LLMs for mental well-being support.
Challenge 5: User face stigma while seeking intimacy from AI-based Mental Wellness Support.
Whole article is essentially about user experience
     Asked for user ratings of ChatGPT responses to mental health questions vs. psychologist responses. ChatGPT responses were rated higher in most dimensions ("score", "activity", "informativeness"), while psychologist responses were rated higher in "strength".

"During the experiment, after getting acquainted with the cases, participants were asked to assess their willingness to contact a psychologist who provided these recommendations (a seven-point Likert scale was used)."​
     consultation and relational empathy (CARE) survey
RESULTS: only raw values reported, no benchmark comparison. However, raw values seem pretty high.

Finally, the participants are asked to respond to the following
qualitative questions:
1. What are 3 words that you would use to describe the
chatbot?
2. What would you change about the conversation?
3. Did the conversation help you realize anything about your
smoking behavior? Why or why not?
     Many qualitative responses:
- 5.1 Chatbots as Companions and Mental Wellbeing Support (accessible Emotional Companion, Safe Space, Privacy and Trust)
- 5.2 Unveiling Self: AI’s Role in Identity Exploration and LGBTQ+ Interactions (Identity exploration and Introspection, affirmative support for homophobia and transphobia, LGBTQ+ social experience practice)
- 5.3 So Eloquent yet so Empty (lack of nuanced understanding of LGBTQ+ issues, lack of lived experiences and emotions)
     We also collected subjective feedback from participants. At the end of the system usage, we asked an optional open-ended question “We would love to know your feedback. What did you like or dislike about the tool? What can we do to improve?”

Qualitative
First, many participants indicated that the system helped them overcome cognitive barriers, especially when they “feel stuck”, and doing this exercise is “difcult”, “on their own” and “in the mo ment.” A participant wrote, “ My own reframes are difcult, and AI gives multiple other perspectives to consider. ” Also, some participants reported that it helped them fnd “the right words” or “ideas to start with.” A participant wrote, “ Thank you for helping me to fnd the right words to clearly reframe a negative thought and how to apply the thought to my own thinking processes. ” Another noted, “ I appreciated that the option of having the AI tool walk you through the reframing process step by step (e.g., by choosing the negative thought you may be experiencing + giving possible reframing ideas to start with/add more details to). ”
Second, participants expressed how the system enabled a less emotionally triggering experience. One participant wrote, “ I felt in control and more comforted that I can handle difcult situations with confdence. ” Another participant wrote, “ This activity let me calm down...”. Another participant noted, “ ...this made the process much less daunting...”. This is perhaps consistent with the quantitative fndings on reduced emotion intensity (Section 5.3).
Third, participants valued that the system allowed them to ex plore multiple viewpoints. One participant wrote, “ ...After reading several reframes and looking over them I realized that there are many options, many positive sides.” Another participant wrote, “ I felt reas sured to see multiple views, and refect upon them... ”
Overall, these results suggest that there are opportunities to assist participants in cognitively challenging and emotionally trig gering psychological processes through human-language model interaction.

Quantitative
(2) Reframe Relatability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I believe in the reframe I came with ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(3) Reframe Helpfulness: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – The reframe helped me deal with the thoughts I was struggling with’ ’ (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(4) Reframe Memorability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – I will remember this reframe the next time I experience this thought ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
(5) Skill Learnability: After the system use, we asked the participant: “ How strongly do you agree or disagree with the following statement? – By doing this activity, I learned how I can deal with future negative thoughts ” (1 to 5; 1: Strongly Disagree; 5: Strongly Agree).
     System Usability Scale (SUS):, Client Satisfaction Questionnaire to Internet-based Interventions (CSQi).
Results:
In the SUS (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) and the
CSQi (MDL = 72.5, SDDL = 12.9, MRule = 77.8, SDRule = 7.62, p = .31) no significant
differences could be established. Altogether, no significant differences could be found
between the experimental groups with the EU, SUS, and CSQi questionnaires. Both
approaches achieved good usability and acceptance scores and scored high in empathic
understanding.

Participants were asked to rate the perceived empathy, fluency, and relevance of system answers based on a 5-point Likert scale to assess the Empathic Understanding (EU) capabilities as proposed by Rashkin et al. (2018).
The usability of the system was assessed using the System Usability Scale (SUS) (Brooke, 1995) as it is one of the most popular and validated instruments for usability assessment (Bangor, Kortum & Miller, 2008). The SUS investigates the perceived usability of a system with 10 questions based on a 5-point Likert scale, with the maximum score being 100 and a score above 68 being considered above-average usability.
The Client Satisfaction Questionnaire adapted to Internet-based Interventions (CSQi) (Boßet al., 2016) was used to investigate the acceptance of the system as it has been developed and validated specifically for digital mental health interventions. Each item of the CSQi is scored between 1 and 5. For determining the overall acceptance rating of the respective subject, scores are summed up, therefore ranging from 8 (lowest) to 32 (highest), with 20 being the medium score.

Results:
- no significant differences in EU, SUS, CSQi between deep learning and rule-based
     Our deployment contains a survey that users can fill in after
they have interacted with the model for some time. Users are
queried to rate the degree to which the model understands
their messages and whether they find the generated responses
engaging and helpful.

Results: Not clear/not reported
     Qualitative assessment; not further specified. Feedback involved (p.90 f): 

Participants reported that the prototype effectively prompted them to define their goals and inquire about their interests. Notably, no users reported feeling pressured or convinced by the prototype to change aspects of their lives against their wishes. Several users found the prototype to be particularly valuable for creating quick and helpful weekly plans or recipes. However, it was observed that one user found it premature to establish concrete plans at that stage. Overall, while some users initially felt that the prototype’s responses were not aligned with their preferences, they noted that it was possible to clarify their intentions, and the prototype quickly adapted accordingly. Despite this, some users felt that the assessment of their goals and life circumstances at the beginning of the training was too superficial, leading to suggestions for a more thorough initial assessment process.

Users highlighted several aspects of the prototype’s communication style. Most users found the conversation to be clear and direct. In contrast, one user reported that the prototype struggled to understand his goals and sometimes provided contradictory advice, a concern not raised by others. On the positive side, one user appreciated the prototype’s approach of asking questions rather than giving fixed instructions, which encouraged engagement and avoided a patronizing tone. Some users noted the positive and moti- vating tone of the coach, particularly when asking if they were willing to continue with suggested steps and advice. The use of a positive tone was seen as a motivating factor by one user and was also appreciated by another who found it encouraging. Users generally found concrete advice more helpful than generic recommendations. Additionally, one user valued the coach’s acknowledgment of setbacks and the importance of enjoying the process. However, some users expressed a desire for shorter, more direct conversations that focused less on delivering general knowledge, and one user specifically requested a more emotional and less matter-of-fact tone of voice.
     The study demonstrates that ChatGPT is perceived as effective in addressing anxiety symptoms across various therapy modalities, including CBT, ACT, ET, MBCT, and DBT.
The study findings indicate that ChatGPT is generally well received by participants, with a majority reporting moderate to high levels of comfort, helpfulness, and empathy.
However, concerns about privacy, ethical implications, and the lack of human connection were also expressed by participants.

Despite some concerns regarding privacy, ethics, and human connection, participants generally reported positive experiences with ChatGPT, highlighting its utility across various therapy modalities and its potential to complement traditional therapeutic approaches.

Following this, the survey
delves into participants’ experiences with ChatGPT as a
psychotherapist, including their comfort level, perceived
helpfulness, empathy, and consideration of ChatGPT as a
regular support platform. The perception and trust
section aims to gauge participants’ trust in AI-based
systems for mental health support, their concerns about
using AI as a psychotherapist, and their beliefs regarding
the effectiveness of AI (ability of ChatGPT in addressing
participants’ anxiety triggers and concerns) compared to
human therapist

Additionally, participants are asked
about their likelihood of recommending ChatGPT to
others seeking help for anxiety issues. The final section
explores the role of ChatGPT in various therapy modalities
for anxiety disorders, including CBT, ACT, ET, MBCT,
and DBT.

Empathy was more fairly distributed,
with 37.2% reporting moderate levels, 21.8% strong, and17.3% very high. This implies that while patients found ChatGPT helpful and soothing, empathy levels should beimproved to improve the user experience
     Participants rated the responses on a 5-point Likert scale for authenticity, professionalism, and practicality.
The mean rating for authenticity
was higher for human responses (37.66) compared to ChatGPT (34.85),
this difference was statistically significant, suggesting participants
perceived human interactions as more genuine and sincere.
     Qualitative user experience assessment through open-ended questions in a survey, following two weeks of ChatGPT counseling. Overall positive experience. "It is interesting to note that, although many participants stated that ChatGPT provides good information; yet they were
concerned about the accuracy and reliability of the information."

semi-structured qualitative interviews: As a result, the authors designed an interview questionnaire containing four demographic inquiries pertaining to gender, age, education, and employment status. In addition, there are ten questions regarding the impact of ChatGPT
on participants’ perceptions of ChatGPT for delivering mental-health support, based on their utilization.
      range of positive impacts were reported, including improved mood… healing from trauma and loss… improved relationships…

Participants told us that generative AI feels like an emotional sanctuary, offers insightful guidance… joy of connection… bears comparison with human therapy.
     See "Initial User Feedback"
     "User experience assessment" was content of the Reddit posts
     SUS scores high (80–85), CEMI relational sub-scale improved Phase I→II, qualitative feedback: supportive, accessible, but somewhat formulaic responses. — Quote: “Usability scores were comparable… The qualitative feedback revealed… supportive but sometimes formulaic.” (Position: Results, Qualitative feedback)
     Participants rated their desire to engage… 8.3/10… usefulness 6.9… helped solve their problem 6.1

Mixed–positive perceptions; advice seen as insightful yet sometimes “too logical/inhumanly smart”; human-AI complementarity valued; believability hindered by voice/animation yet participants adapted. — Quotes: “Like robotic smart… too good of a psychologist” [S13] / “a mixture… would be perfect.” [S4] / “at once I got over the metallic voice… less and less weird.” 


36. lexical_overlap_used
   ========================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 143 (83.1%)
     y: 29 (16.9%)


37. lexical_overlap_vs_benchmark
   ================================
   Data type: object
   Total values: 172
   Non-null values: 29
   Null values: 143
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 143 (493.1%)
     b: 16 (55.2%)
     no benchmark: 11 (37.9%)
     -: 1 (3.4%)
     s: 1 (3.4%)


38. lexical_overlap_benchmark_quality
   =====================================
   Data type: object
   Total values: 172
   Non-null values: 29
   Null values: 143
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 143 (493.1%)
     l: 17 (58.6%)
     no benchmark: 11 (37.9%)
     -: 1 (3.4%)


39. lexical_overlap_notes
   =========================
   Data type: object
   Total values: 172
   Non-null values: 29
   Null values: 143
   Unique values: 29
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 143 (493.1%)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)

ChatGPTbased conversations
(Approach 1), fine-tuned DialoGPT transformer
conversations (Approach 2), and fine-tuned DialoGPT
transformer conversations combined with the GPT3 prompts
API (Approach 3): 1 (3.4%)
     BLEU with reference: 1 (3.4%)
     BLEU, ROUGE against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.: 1 (3.4%)
     Various metrics (average of BLEU-1, BLEU-2, BLEU-3, and BLEU-4). Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (3.4%)
     BLEU against human responses. Benchmark: non fine-tuned GPT-2: 1 (3.4%)
     -: 1 (3.4%)
     METEOR and ROUGE tested, but Strawman models: 1 (3.4%)
     BLEU, ROUGE with counselor responses as reference (CounselChat): 1 (3.4%)
     BLEU comparison with reference responses from some given conversations. No information provided on what these reference conversations are. No comparison to other benchmark method.: 1 (3.4%)
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Comparison against reference responses in dataset: 1 (3.4%)
     BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L: 1 (3.4%)
     “We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”: 1 (3.4%)
     BLEU SCORE: 1 (3.4%)
     S2S-Model (without strategy) as benchmark: 1 (3.4%)
     no benchmark: 1 (3.4%)
     Metrics: Rouge-1, Rouge-L, BLEU-4. Benchmarks: Various other LLMs
ROUGE-1 and BLEU-4 highest for PsycoLLM; ROUGE-L slightly lower than EmoLLM: 1 (3.4%)
     Rouge comparison to Alexander Street Press reference. No benchmark.

only the inhibited LoRA Finetuning: 1 (3.4%)
     BLEU-1/2/3, ROUGE-L improved after fine-tuning

benchmark is some baseline model: 1 (3.4%)
     BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations: 1 (3.4%)
     ML models (HRED, SEQ2SEQ): 1 (3.4%)
     Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.: 1 (3.4%)
     seq2seq: 1 (3.4%)
     benchmark are human responses: 1 (3.4%)
     no benchmark. metric: ROUGE-1, -2, -L: 1 (3.4%)
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR: 1 (3.4%)
     benchmark: other conversation systems: 1 (3.4%)
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.: 1 (3.4%)
     ROUGE-L - PanGu better than WenZhong: 1 (3.4%)
     BLEU, ROUGE, METEOR. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5): 1 (3.4%)

   Sample values (first 20):
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)

ChatGPTbased conversations
(Approach 1), fine-tuned DialoGPT transformer
conversations (Approach 2), and fine-tuned DialoGPT
transformer conversations combined with the GPT3 prompts
API (Approach 3)
     ROUGE-L - PanGu better than WenZhong
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEU, METEOR, CHRF. In the same way, BLEU, METEOR, CHRF values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.
     benchmark: other conversation systems
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR
     no benchmark. metric: ROUGE-1, -2, -L
     benchmark are human responses
     seq2seq
     Strange: LLMs are used to create strings of numerical symptom scores. These were compared to the ground truth via ROUGE-L.
     ML models (HRED, SEQ2SEQ)
     BLEU against expert empathic rewritings. Benchmarks are other LLMs and ablations
     BLEU-1/2/3, ROUGE-L improved after fine-tuning

benchmark is some baseline model
     Rouge comparison to Alexander Street Press reference. No benchmark.

only the inhibited LoRA Finetuning
     Metrics: Rouge-1, Rouge-L, BLEU-4. Benchmarks: Various other LLMs
ROUGE-1 and BLEU-4 highest for PsycoLLM; ROUGE-L slightly lower than EmoLLM
     no benchmark
     S2S-Model (without strategy) as benchmark
     BLEU SCORE
     “We use BLEU.Avg (the average of BLEU-1, 2, 3, 4)… measures similarity between generated text and reference text”
     BLEU-4, ROUGE-1, ROUGE-2, ROUGE-L
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Comparison against reference responses in dataset


40. embedding_similarity_used
   =============================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 152 (88.4%)
     y: 20 (11.6%)


41. embedding_similarity_vs_benchmark
   =====================================
   Data type: object
   Total values: 172
   Non-null values: 20
   Null values: 152
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 152 (760.0%)
     b: 12 (60.0%)
     no benchmark: 5 (25.0%)
     s: 2 (10.0%)
     w: 1 (5.0%)


42. embedding_similarity_benchmark_quality
   ==========================================
   Data type: object
   Total values: 172
   Non-null values: 20
   Null values: 152
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 152 (760.0%)
     l: 15 (75.0%)
     no benchmark: 5 (25.0%)


43. embedding_similarity_notes
   ==============================
   Data type: object
   Total values: 172
   Non-null values: 19
   Null values: 153
   Unique values: 19
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 153 (805.3%)
     Lots of problems here. Base dataset (CBT QA) of CBT responses is generated via ChatGPT-3.5, so another language model. Then the completions by CBT-LLM (model based on Baichuan-7B) are compared to the responses in CBT QA via BLEURT, BERTSCORE. In the same way, BLEURT, BERTSCORE values are generated for other baseline models (LLaMA-Chinese-7B, etc). The latter is the benchmark of comparison.: 1 (5.3%)
     Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).: 1 (5.3%)
     benchmark is GPT-3.5. measure is BERTScore and sentence transformer score with dataset expert responses as reference: 1 (5.3%)
     BERTScore against reference responses in dataset. Benchmark: GPT-3.5 out-of-the-box.: 1 (5.3%)
     PBERT, RBERT, FBERT. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (5.3%)
     Distance between embeddings of reference and output.: 1 (5.3%)
     on average 0.93 (±0.03) similarity in the embedding space.

Wonky measurement: embedding similarity between chatbot responses and patient inputs: 1 (5.3%)
     Best model of authors compared to models of other works. : 1 (5.3%)
     Semantic Similarity via cosine distance (see Eq. 4): 1 (5.3%)
     no benchmark: 1 (5.3%)
     Metrics: BERTScore. Benchmarks: Various other LLMs
Highest BERTScore among compared models.: 1 (5.3%)
     BERTScore used and increased; also pairwise cosine similarity for semantic diversity analysis

benchmark is some baseline model: 1 (5.3%)
     specificity is an embedding similarity metric here: 1 (5.3%)
     ML models (HRED, SEQ2SEQ): 1 (5.3%)
     seq2seq: 1 (5.3%)
     benchmark are human responses.: 1 (5.3%)
     BERTScore: 1 (5.3%)
     metrics: ROUGE, METEOR. benchmarks: DialoGPT, GPT-2, DialogVED, ProphNet-Dialog, HRED, HRED Speaker/Utterance Encoder, VHCR: 1 (5.3%)
     relevancy score. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini: 1 (5.3%)


44. classification_used
   =======================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 136 (79.1%)
     y: 36 (20.9%)


45. classification_vs_benchmark
   ===============================
   Data type: object
   Total values: 172
   Non-null values: 36
   Null values: 136
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 136 (377.8%)
     b: 15 (41.7%)
     no benchmark: 11 (30.6%)
     w: 7 (19.4%)
     s: 2 (5.6%)
     -: 1 (2.8%)


46. classification_benchmark_quality
   ====================================
   Data type: object
   Total values: 172
   Non-null values: 36
   Null values: 136
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 136 (377.8%)
     l: 20 (55.6%)
     no benchmark: 11 (30.6%)
     h: 5 (13.9%)


47. classification_notes
   ========================
   Data type: object
   Total values: 172
   Non-null values: 37
   Null values: 135
   Unique values: 37
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 135 (364.9%)
     benchmark: TF-IDF-based classification via logistic regression model: 1 (2.7%)
     "training accuracy" of generative LLM. Not sure what this accurac is measuring.: 1 (2.7%)
     Accuracy is reported for the Bi-LSTM sentiment classifier which is not an LLM: 1 (2.7%)
     The F1 score… evaluates the quality of generated answers by calculating the degree of n-gram matching” (Position: Metrics, p. 1978: 1 (2.7%)
     Accuracy of the BERT helping skill classification model: 1 (2.7%)
     Accuracy, precision, recall, F1 for the cognitive distortion identifier. No benchmark.: 1 (2.7%)
     Various classification metrics (accuracy, precision, recall, f1) for checking how "relevant" the responses are. No information is given on how "relevance" is measured here.: 1 (2.7%)
     Classification Accuracy Tests (to distinguish relevant vs. 
irrelevant queries)

Classification into relevant/irrelevant via all-MiniLM-L6-v2: 1 (2.7%)
     “coded using the Multitheoretical List of Therapeutic Interventions … therapists evoked more elaboration … chatbots used … suggestions more often” : 1 (2.7%)
     evaluated using C-Eval… Accuracy decreased from 47.36 to 36.84” (Position: Model Evaluation): 1 (2.7%)
     Accuracy of kNN in detecting individual 
SAD symptoms from a single text entry against naive forecast (simple extrapolation): 1 (2.7%)
     Performance of cognitive distortion classification of non-generative BERT model is given (but it's non-generative): 1 (2.7%)
     Recall@1 (Tables 3 and 4): 1 (2.7%)
     F1 score, not quiet clear what of.: 1 (2.7%)
     between the rule-based sentiment analysis module and the
sentiment analysis capabilities of the GPT-powered model: 1 (2.7%)
     Various classification metrics of BERT emotional distress detection. Benchmark: Bi-LSTM: 1 (2.7%)
     Models classified cases requiring intervention; BSI>1 and higher CR indicate better behavior sensitivity/consistency; DeepSeek/Wenxin/Claude > GPT-4. — Quote: “BSI and CR … DeepSeek 1.0662 / 0.8985 … GPT-4 1.0415 / 0.8250: 1 (2.7%)
     "controllability", i.e. match between predicted and actual strategy tokens: 1 (2.7%)
     "Elastic accuracy" on their QA dataset.
Highest average standard accuracy; >60% pass; elastic accuracy also strong: 1 (2.7%)
     f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.: 1 (2.7%)
     Metric: Micro- and macro-F1. Benchmark: zero- and few-shot prompting with the same LLMs.

Quantitative F1 decrease in ablation study shows impact of each stage.
: 1 (2.7%)
     no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.: 1 (2.7%)
     Table 4: Benchmark is fine-tuned DistilBERT model. ChatGPT is compared against this and performs worse.
Table 2: Prompted GPT-3 model (text-davinci-003) is compared against DistilBERT.

1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not): 1 (2.7%)
     accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection. Task: Cognitive distortion detection in client questions.: 1 (2.7%)
     benchmark: other conversation systems. Task: emotion classification in current utterance of help-seeker: 1 (2.7%)
     accuracy, precision, recall, F1 for classifying utterances. Task: classification of mental health dialogue turns into safe/various types of unsafe responses. benchmark: fine-tuned BERT-base and RoBERTa-large: 1 (2.7%)
     sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification: 1 (2.7%)
     metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other). Task: classify crisis calls transcripts into 10 suicide risk labels.: 1 (2.7%)
     they used f1-micro and kappa to evaluate the emotion labeling system. the benchmark was a human annotator. Task: classify emotions in client utterances: 1 (2.7%)
     Several benchmarks used, including ones that were more recently developed. "In comparison, SPARTA-TAA obtains significant improvements over all
baselines."

benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. Task: Dialogue-act classification of conversational turns: 1 (2.7%)
     benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative: 1 (2.7%)
     accuracy, F1, precision, recall for prediction of feeling based on automatic thought (feeling-thought pairs): 1 (2.7%)
     base BERT model with AUPRC <0.52. Task: classifying cognitive distortions: 1 (2.7%)
     no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token: 1 (2.7%)
     benchmark is simple rule-based emotion recognition system. Task: classification of clustered dimensional values (five distinct categories): 1 (2.7%)
     benchmark are ratings by humans on MISC and Topics (see appendix eTable 2). Task: classification of client messages into content types: 1 (2.7%)
     Models developed compared to each other and to Sarah (from WHO); Use Sarah as Benchmark here (?).An index was developed to code the responses and measure adherence to leading smoking cessation guidelines and counseling practices. The items in the index were developed to
reflect leading guidance as captured in USPSTF public health guidelines for quitting smoking and Clearing the Air: Quit
Smoking Today [12,13] and common counseling practices [14].: 1 (2.7%)

   Sample values (first 20):
     benchmark: TF-IDF-based classification via logistic regression model
     no benchmark. proportion of correctly recognized cognitive distortions. Task: classify cognitive distortions.
     Table 4: Benchmark is fine-tuned DistilBERT model. ChatGPT is compared against this and performs worse.
Table 2: Prompted GPT-3 model (text-davinci-003) is compared against DistilBERT.

1. Utterance level feature prediction F1 score, comparing only models that they trained themselves.
2. Conversation outcome prediction performance of different models they created themselves (DistilBERT, ChatGPT, AdaBoost), using F1 and Recall. Task: predict conversation outcome prediction (i.e. whether help seeker will feel more positive after conversation or not)
     accuracy, recall, f1 for detecting cognitive distortions in client questions. ground truth are psychotherapist-annotated labels. there is no benchmark in the sense of a second psychotherapist or another model doing the detection. Task: Cognitive distortion detection in client questions.
     benchmark: other conversation systems. Task: emotion classification in current utterance of help-seeker
     accuracy, precision, recall, F1 for classifying utterances. Task: classification of mental health dialogue turns into safe/various types of unsafe responses. benchmark: fine-tuned BERT-base and RoBERTa-large
     sentiment classification accuracy, weighted F1, etc. Task: sentiment (3 classes) and emotion (9 classes) classification
     metrics: F1, accuracy, tpr, tnr, precision, recall, percent human agreement. benchmark: other human raters (percent human agreement statistic reported in this article indexes whether the machine-learning model agrees as much with a human rater as two human raters agree with each other). Task: classify crisis calls transcripts into 10 suicide risk labels.
     they used f1-micro and kappa to evaluate the emotion labeling system. the benchmark was a human annotator. Task: classify emotions in client utterances
     Several benchmarks used, including ones that were more recently developed. "In comparison, SPARTA-TAA obtains significant improvements over all
baselines."

benchmark: other available dialogue-act classification systems. metrics: accuracy, macro-F1, weighted-F1, etc. Task: Dialogue-act classification of conversational turns
     benchmark are other NLP classifiers (LR, SVM, LSTM). metric is sentiment classification accuracy and F1 score. ground truth are human scores. better benchmark would have been other human rater. Task: Classify sentiment into positive/neutral/negative
     accuracy, F1, precision, recall for prediction of feeling based on automatic thought (feeling-thought pairs)
     base BERT model with AUPRC <0.52. Task: classifying cognitive distortions
     no benchmark. top-1, top-5, and top-10 accuracy of predicting the next token
     benchmark is simple rule-based emotion recognition system. Task: classification of clustered dimensional values (five distinct categories)
     benchmark are ratings by humans on MISC and Topics (see appendix eTable 2). Task: classification of client messages into content types
     Metric: Micro- and macro-F1. Benchmark: zero- and few-shot prompting with the same LLMs.

Quantitative F1 decrease in ablation study shows impact of each stage.

     f1 for distortion assessment and distortion classification. benchmark: models without special prompting and results from old publication. their best method is better than the results from old publication.
     "Elastic accuracy" on their QA dataset.
Highest average standard accuracy; >60% pass; elastic accuracy also strong
     "controllability", i.e. match between predicted and actual strategy tokens


48. continuous_metrics_used
   ===========================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 161 (93.6%)
     y: 11 (6.4%)


49. continuous_metrics_vs_benchmark
   ===================================
   Data type: object
   Total values: 172
   Non-null values: 11
   Null values: 161
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 161 (1463.6%)
     no benchmark: 5 (45.5%)
     b: 3 (27.3%)
     no benckmark: 1 (9.1%)
     s: 1 (9.1%)
     w: 1 (9.1%)


50. continuous_metrics_benchmark_quality
   ========================================
   Data type: object
   Total values: 172
   Non-null values: 11
   Null values: 161
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 161 (1463.6%)
     no benchmark: 6 (54.5%)
     l: 4 (36.4%)
     h: 1 (9.1%)


51. continuous_metrics_notes
   ============================
   Data type: object
   Total values: 172
   Non-null values: 11
   Null values: 161
   Unique values: 11
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 161 (1463.6%)
     benchmark is support vector regressor (lower capacity model): 1 (9.1%)
     Strings of symptom scores are converted to float and RMSE is calculated. No benchmark though (Table 10, 11): 1 (9.1%)
     no benchmark. loss value: 1 (9.1%)
     benchmark is simple rule-based emotion recognition system.
second, better benchmark: state-of-the-art valence and arousal recognition systems from other papers (Park et al., etc.). still, this is no human. Task: rating of valence, arousal, dominance in conversations

rule-based approach: " It first checks for negations before using the dimensional emotion
dictionary by Kušen et al. (2017) to look up the emotion score associated with each word of the input and aggregate the results into a final emotional score": 1 (9.1%)
     benchmark are ratings by humans on CTRS (see appendix eTable 2): 1 (9.1%)
     Comparison of chatbot-inferred PHQ-9 scores with human scorers via ICC and Cohen's kappa: 1 (9.1%)
     Table 6 Accuracy of machine learning models in forecasting 7-day 
SAD (stress, anxiety, and depression) levels based on single text 
entries against extrapolation: 1 (9.1%)
     Pearson, Spearman, Kendall’s Tau. Comparing Flan, Mistral, GPT-3.5 performance against naive classifier (benchmark): 1 (9.1%)
     Train loss: 1 (9.1%)
     Metric: training loss. No benchmark: 1 (9.1%)
     Correlation between LLM suicide response ratings and expert ratings: 1 (9.1%)


52. expert_rating_used
   ======================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 135 (78.5%)
     y: 37 (21.5%)


53. expert_rating_vs_benchmark
   ==============================
   Data type: object
   Total values: 172
   Non-null values: 36
   Null values: 136
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 136 (377.8%)
     no benchmark: 15 (41.7%)
     b: 12 (33.3%)
     w: 7 (19.4%)
     s: 2 (5.6%)


54. expert_rating_benchmark_quality
   ===================================
   Data type: object
   Total values: 172
   Non-null values: 35
   Null values: 137
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 137 (391.4%)
     no benchmark: 14 (40.0%)
     h: 11 (31.4%)
     l: 10 (28.6%)


55. expert_rating_notes
   =======================
   Data type: object
   Total values: 172
   Non-null values: 36
   Null values: 136
   Unique values: 35
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 136 (377.8%)
     no benchmark: 2 (5.6%)
     benchmark: woebot non-generative/rule-based.: 1 (2.8%)
     benchmark: chatgpt without robot integration: 1 (2.8%)
     valuated by Gestalt psychotherapy trainees: 1 (2.8%)
     Benchmark are other transcription recording and analysis systems, though no human manual transcription -> low quality: 1 (2.8%)
     Rating by experienced student counselors of 3 chat conversations between students and the proposed chatbot.: 1 (2.8%)
     benchmark: human-generated exposure hierarchies. measure: overall blinded expert rating: 1 (2.8%)
     7 different safety questions: 1 (2.8%)
     CAPE category ratings, no benchmark: 1 (2.8%)
     Measures: Likert empathy rating, MITI global score. Benchmark: human peer supporter from Reddit post.: 1 (2.8%)
     likert-scale appropriateness rating of emotional reflection responses. compared GPT-4 against human counselor responses.: 1 (2.8%)
     We introduced human evaluation to assess the performance 
of our chatbot. To ensure consistency and reliability of human 
evaluation, we composed a panel of five experts with varying 
levels of experience to assess the chatbot's performance

Psychologist rating of empathy, accuracy of responses, interaction continuity, fluency, understanding. No benchmark.: 1 (2.8%)
     Fluency, helpfulness, relevance, empathy, professionalism, evaluated by psychology graduate students on a 5-Likert scale. Benchmark are other LLM-based methods.: 1 (2.8%)
     Each chatbot response was independently categorized by 2
coders for their adherence on each item of the index.: 1 (2.8%)
     Human interactive evaluation (fluency, comforting, etc.), comparison with other models. Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5).: 1 (2.8%)
     Binary scoring rubrics for GPT-4 responses (see Table 4): 1 (2.8%)
     To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.: 1 (2.8%)
     Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale: 1 (2.8%)
     no benchmark. however, experts rated the chatbot highly in absolute terms.: 1 (2.8%)
     ML models (HRED, SEQ2SEQ): 1 (2.8%)
     psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set

Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model: 1 (2.8%)
     Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.

Measures: Relevance, CBT structure, helpfulness: 1 (2.8%)
     metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2: 1 (2.8%)
     no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.): 1 (2.8%)
     no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination: 1 (2.8%)
     benchmark is ground truth (human created reflections) and output of simple seq2seq model: 1 (2.8%)
     Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.: 1 (2.8%)
     rating of fluency, coherence, relevance, helpfulness. benchmark: human responses from the dataset.: 1 (2.8%)
     no benchmark.: 1 (2.8%)
     Table 4; Benchmarks are ChatGLM3-6b and LLaMA2-7b-chat: 1 (2.8%)
     benchmark are responses of human counselors in PsyTest: 1 (2.8%)
     “reviewers assessed ChatGPT’s psychoeducational responses …” 
: 1 (2.8%)
     4.3 Human Evaluation Results: 1 (2.8%)
     human rating (read, prof, match): 1 (2.8%)
     The LLM-Counselor Support System. Benchmark: GPT-4 with zero-shot CoT: 1 (2.8%)

   Sample values (first 20):
     no benchmark. however, experts rated the chatbot highly in absolute terms.
     psychology students rated helpfulness, fluency, relevance, logic. benchmark: human answers to questions from the data set

Helpfulness, Fluency, relevance and logic - human evaluators generally considered the PanGu model’s
generated responses more helpful, fluent, relevant, and logical than the WenZhong model
     Problems: Benchmark are CBT responses by another LLM (Alpaca-Chinese-7B). The main CBT-LLM ist only marginally better. There are no p-values and confidence intervals to see whether the difference is even significant.

Measures: Relevance, CBT structure, helpfulness
     metrics: likert-rated relevance, consistency, fluency, coherence. benchmark: DialoGPT, GPT-2
     no benchmark. metrics: likert scale expert rating across several dimensions (emotional understanding and empathy, communication and language, therapeutic effectiveness and suitability, etc.)
     no benchmark. metrics: affective attitude, burden, ethicality, coherence, opportunity costs, perceived effectiveness, extent of hallucination
     benchmark is ground truth (human created reflections) and output of simple seq2seq model
     ML models (HRED, SEQ2SEQ)
     Expert judgment against human expert empathetic rewritings. human rewritings are preferred in 80-90% of cases.
     no benchmark.
     Table 4; Benchmarks are ChatGLM3-6b and LLaMA2-7b-chat
     benchmark are responses of human counselors in PsyTest
     “reviewers assessed ChatGPT’s psychoeducational responses …” 

     4.3 Human Evaluation Results
     human rating (read, prof, match)
     no benchmark
     rating of fluency, coherence, relevance, helpfulness. benchmark: human responses from the dataset.
     Human evaluation sample = 60 questions, rated by 12 evaluators (pairs); metrics included fluency, relevance, helpfulness, empathy, professionalism. — Quote: “randomly select sixty questions… evaluators… five criteria… 5-star rating scale” . The ratings are done using a 5-star rating scale
     benchmark: woebot non-generative/rule-based.
     To assess the relative effectiveness of ChatGPT-4o-based 
ADHD therapy, we compared its performance against three 
baseline approaches: traditional therapist-led interventions, 
game-based cognitive training (such as EndeavorRx), and 
reinforcement learning-based AI models. Table 2 compares 
key therapeutic factors across these models.


56. llm_judge_used
   ==================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 164 (95.3%)
     y: 8 (4.7%)


57. llm_judge_vs_benchmark
   ==========================
   Data type: object
   Total values: 172
   Non-null values: 8
   Null values: 164
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 164 (2050.0%)
     no benchmark: 5 (62.5%)
     b: 2 (25.0%)
     w: 1 (12.5%)


58. llm_judge_benchmark_quality
   ===============================
   Data type: object
   Total values: 172
   Non-null values: 8
   Null values: 164
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 164 (2050.0%)
     no benchmark: 5 (62.5%)
     l: 2 (25.0%)
     h: 1 (12.5%)


59. llm_judge_notes
   ===================
   Data type: object
   Total values: 172
   Non-null values: 8
   Null values: 164
   Unique values: 8
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 164 (2050.0%)
     Only training data is scored using GPT-4 LLM-as-a-judge (table 2): 1 (12.5%)
     no benchmark: 1 (12.5%)
     The DeepEval framework + MQG-RAG Evaluation: 1 (12.5%)
     ChatGPT-4 rated using the categories below. Benchmark: ChatGPT, GLM-4: 1 (12.5%)
     Ragas is LLM-as-a-judge: 1 (12.5%)
     LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.: 1 (12.5%)
     Qianwen to automatically evaluate the counseling dialogue (esction: C. ): 1 (12.5%)
     No indexmodel but three equaly ranked models were compared : 1 (12.5%)


60. perplexity_used
   ===================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 167 (97.1%)
     y: 5 (2.9%)


61. perplexity_vs_benchmark
   ===========================
   Data type: object
   Total values: 172
   Non-null values: 5
   Null values: 167
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 167 (3340.0%)
     b: 3 (60.0%)
     w: 1 (20.0%)
     no benchmark: 1 (20.0%)


62. perplexity_benchmark_quality
   ================================
   Data type: object
   Total values: 172
   Non-null values: 5
   Null values: 167
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 167 (3340.0%)
     l: 4 (80.0%)
     no benchmark: 1 (20.0%)


63. perplexity_notes
   ====================
   Data type: object
   Total values: 172
   Non-null values: 5
   Null values: 167
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 167 (3340.0%)
     benchmark: simple seq2seq model: 1 (20.0%)
     Best model of authors compared to models of other works. : 1 (20.0%)
     evaluates the effectiveness of the four four open-source lightweight LLMs (T5-small, 
FLAN-T5-small, BART-base, and GODEL-base) fine-tuned on curated mental health counseling conversation 
datasets

Perplexity of reference responses in dataset: 1 (20.0%)
     Benchmark: non fine-tuned GPT-2: 1 (20.0%)
     Benchmarks are other models (DialoGPT, BlenderBot, LLaMA2, GPT-3.5): 1 (20.0%)


64. lexical_diversity_used
   ==========================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     n: 164 (95.3%)
     y: 7 (4.1%)
     n : 1 (0.6%)


65. lexical_diversity_vs_benchmark
   ==================================
   Data type: object
   Total values: 172
   Non-null values: 7
   Null values: 165
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 165 (2357.1%)
     b: 3 (42.9%)
     no benchmark: 2 (28.6%)
     s: 1 (14.3%)
     w: 1 (14.3%)


66. lexical_diversity_benchmark_quality
   =======================================
   Data type: object
   Total values: 172
   Non-null values: 7
   Null values: 165
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 165 (2357.1%)
     l: 4 (57.1%)
     no benchmark: 2 (28.6%)
     h: 1 (14.3%)


67. lexical_diversity_notes
   ===========================
   Data type: object
   Total values: 172
   Non-null values: 6
   Null values: 166
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 166 (2766.7%)
     Tab. 5

Distinct-1/2/3… Fine-tuned 82.08/95.74/97.81 vs Baseline 52.95/80.74/90.17… §5.1–5.3: 1 (16.7%)
     DISTINCT score. no benchmark: 1 (16.7%)
     benchmark: simple seq2seq model: 1 (16.7%)
     D1 (Distinct-1) measures the richness of vocabulary in the responses: 1 (16.7%)
     Dist-2. Benchmark are other LLM-based methods.

Initially,
we benchmark against classic methods for this task, includ-
ing: 1) GPTft+strategy – utilizing GPT2 to generate re-
sponses with support strategies aimed at providing mental
health support and assistance. We then turn our attention
to comparisons with other LLM-based approaches, such as:
(2) CoT – a method that encourages the generation of rea-
sonable responses through the prompting “Let’s think step
by step.” Additionally, we compare with other typical LLM-
based methods, including: (3) ReAct(Yao et al., 2022), (4)
Chameleon(Lu et al., 2023), and (5) Cue-CoT(Wang et al.,
2023).: 1 (16.7%)
     Metrics: Dist-1 and Dist-2 on chatbot responses. No benchmark: 1 (16.7%)


68. metric1_name
   ================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 49
   Category: CATEGORICAL

   Value Counts:
     n: 121 (70.3%)
     perplexity: 3 (1.7%)
     automatic response quality: 2 (1.2%)
     patient health questionnaire 927 (phq-9), the generalized anxiety disorder questionnaire for the diagnostic and statistical manual of mental disorders, fourth edition (dsm-iv) (gad-q-iv), and the weight concerns scale (wcs) within the stanford–washington university eating disorder (swed): 1 (0.6%)
     lay rating: 1 (0.6%)
     user experience rating: 1 (0.6%)
     client evaluation of motivational interviewing scale (cemi) (madson et al., 2013, 2015, 2016); readiness to change delta: 1 (0.6%)
     user rating: 1 (0.6%)
     llm-as-a-judge emotional resonance and understanding: 1 (0.6%)
     empathy rating unequality: 1 (0.6%)
     liwc score domains and linguistic marker scores: 1 (0.6%)
     automatic empathy rating: 1 (0.6%)
     behavior sensitivity index: 1 (0.6%)
     sentiment and part-of-speech: 1 (0.6%)
     stress test metrics: 1 (0.6%)
     subjective units of distress scale, suds: 1 (0.6%)
     cross-entropy loss: 1 (0.6%)
     unclear metrics: relevance, empathy, conciseness, context: 1 (0.6%)
     user satisfaction score: 1 (0.6%)
     quantitative ux assessment ("overall satisfaction"): 1 (0.6%)
     wai-sr working alliance: 1 (0.6%)
     automatic "perceived information quality" (piq) rating via ml model (prompted chatgpt): 1 (0.6%)
     phq-9, panas-p, satisfaction with life scale (swls): 1 (0.6%)
     client satisfaction scale (css): 1 (0.6%)
     avg. of stigma questions: 1 (0.6%)
     posttrial guardrail review: 1 (0.6%)
     automatic safety rating: 1 (0.6%)
     reduction in emotion intensity: 1 (0.6%)
     lexical diversity and richness: 1 (0.6%)
     human rating, not sure by whom: 1 (0.6%)
     aaq + cds: 1 (0.6%)
     human rating (unclear if expert or not): 1 (0.6%)
     lexical diversity: 1 (0.6%)
     construct validity using mtmm and hierarchical  linear models hlm: 1 (0.6%)
     diversity: 1 (0.6%)
     sentiment polarity: 1 (0.6%)
     readiness ruler (patient symptom report, smoking related, validated measure): 1 (0.6%)
     part-of-speech (pos) analysis; dependency-syntactic-parsing (dep) analysis; semantic-dependency-parsing (sdp) analysis; sentiment analysis: 1 (0.6%)
     whoqol-bref score change: 1 (0.6%)
     empathy classification (sharma et al): 1 (0.6%)
     sentiment analysis score: 1 (0.6%)
     safety (number of conversations turns until initial referral/shutdown of chatbot): 1 (0.6%)
     various text analyses: 1 (0.6%)
     fluency: 1 (0.6%)
     bartscore: 1 (0.6%)
     llm-as-a-judge mi_adherence: 1 (0.6%)
     psychobench empathy scale: 1 (0.6%)
     reduction in anxiety symptoms (bai, gad-7): 1 (0.6%)
     llm-as-a-judge approval and reassurance: 1 (0.6%)

   Sample values (first 20):
     n
     whoqol-bref score change
     perplexity
     lexical diversity and richness
     human rating, not sure by whom
     aaq + cds
     human rating (unclear if expert or not)
     lexical diversity
     construct validity using mtmm and hierarchical  linear models hlm
     diversity
     sentiment polarity
     readiness ruler (patient symptom report, smoking related, validated measure)
     reduction in emotion intensity
     part-of-speech (pos) analysis; dependency-syntactic-parsing (dep) analysis; semantic-dependency-parsing (sdp) analysis; sentiment analysis
     empathy classification (sharma et al)
     sentiment analysis score
     safety (number of conversations turns until initial referral/shutdown of chatbot)
     various text analyses
     fluency
     bartscore


69. metric1_vs_benchmark
   ========================
   Data type: object
   Total values: 172
   Non-null values: 50
   Null values: 122
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 122 (244.0%)
     no benchmark: 22 (44.0%)
     b: 17 (34.0%)
     s: 4 (8.0%)
     unclear: 3 (6.0%)
     w: 2 (4.0%)
     no benchmark. in absolute terms favorable, with reduction in smoking, increase in confidence, importance, readiness.: 1 (2.0%)
     unknown: 1 (2.0%)


70. metric1_benchmark_quality
   =============================
   Data type: object
   Total values: 172
   Non-null values: 50
   Null values: 122
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 122 (244.0%)
     no benchmark: 23 (46.0%)
     l: 18 (36.0%)
     h: 9 (18.0%)


71. metric1_notes
   =================
   Data type: object
   Total values: 172
   Non-null values: 50
   Null values: 122
   Unique values: 40
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 122 (244.0%)
     no benchmark: 11 (22.0%)
     Waitlist control: 1 (2.0%)
     Book written by a human expert.: 1 (2.0%)
     A questionnaire with 10 questions structured using a Likert scale of 1–5 (1 = strongly disagree, 5 = strongly agree), assesses user experience.: 1 (2.0%)
     Measures MI adherence of clients. Benchmark: GPT-4 out of the box without MI fine-tuning: 1 (2.0%)
     User scores of "helpfulness" and "empathy". Benchmark: non fine-tuned GPT-2.: 1 (2.0%)
     Qianwen to automatically evaluate the counseling dialogue (esction: C. ): 1 (2.0%)
     Table 1: mechanical turk worker ratings ("Human") vs. GPT-4: 1 (2.0%)
     they study integration of empathy lexicon into different LLMs. benchmark are those LLMs without empathy lexicon integration.: 1 (2.0%)
     No Indexmodel; comparison of multiple LLMs: 1 (2.0%)
     Therapist responses: 1 (2.0%)
     Ragas is LLM-as-a-judge. OpenAI against Mistral with RAG modules and sentiment analysis: 1 (2.0%)
     pre post measurement: 1 (2.0%)
     compared to data set 70.58 %: 1 (2.0%)
     Unclear what these metrics are (human ratings? automatic?). No benchmarks used.: 1 (2.0%)
     Unclear what this metric is, never explained. Benchmarks: GPT-3, GPT-3.5, GPT-4, Gemini: 1 (2.0%)
     benchmarks are other mental health chatbots and general LLMs: Woebot, Happify, GPT-3.5, Google Bard: 1 (2.0%)
     benchmark are real human responses from the Q&A dataset: 1 (2.0%)
     Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. : 1 (2.0%)
     Compared to the other models, but not clear indexmodel, so not necessarly a benchmark. : 1 (2.0%)
     ChatGPT and GLM-4: 1 (2.0%)
     intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points): 1 (2.0%)
     Benchmark is other NLP model: 1 (2.0%)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts): 1 (2.0%)
     human psychologist responses in the CounselChat transcripts: 1 (2.0%)
     different models were rated by human non-experts. no comparison with any benchmark.: 1 (2.0%)
     aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.: 1 (2.0%)
     benchmark: other conversation systems: 1 (2.0%)
     rating scale to determine quality of backward looking reflections (Textbox 5).: 1 (2.0%)
     seq2seq: 1 (2.0%)
     no benchmark. only the pre-post-difference in this metric was measured.: 1 (2.0%)
     A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). : 1 (2.0%)
     benchmark: human psychologist responses from transcripts: 1 (2.0%)
     Other LLMs: 1 (2.0%)
     benchmark are responses in the kaggle dataset: 1 (2.0%)
     benchmark: human therapist responses: 1 (2.0%)
     Not clear what fluency is. The reference paper measures fluency via human rating but the authors of this paper state this is an automatic assessment.: 1 (2.0%)
     PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon): 1 (2.0%)
     llm as a judge: The DeepEval framework + MQG-RAG Evaluation: 1 (2.0%)
     Benchmars are other LLM-based models: 1 (2.0%)

   Sample values (first 20):
     intervention group (n=7) had marked improvement (+13.5 points) while control group (n=5) showed slight worsening (-0.2 points)
     benchmark is a simpler version of (just ChatGPT) of the proposed system (DialogGPT + GPT-3 prompts)
     human psychologist responses in the CounselChat transcripts
     no benchmark
     different models were rated by human non-experts. no comparison with any benchmark.
     aaq + cds are some clinical measures (not further elaborated in the paper). pre and post intervention scores of a group using the chatbot and a control group without any intervention.
     benchmark: other conversation systems
     rating scale to determine quality of backward looking reflections (Textbox 5).
     seq2seq
     Benchmark is other NLP model
     no benchmark. only the pre-post-difference in this metric was measured.
     benchmark: human psychologist responses from transcripts
     Other LLMs
     benchmark are responses in the kaggle dataset
     benchmark: human therapist responses
     Not clear what fluency is. The reference paper measures fluency via human rating but the authors of this paper state this is an automatic assessment.
     PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon)
     llm as a judge: The DeepEval framework + MQG-RAG Evaluation
     A posttrial review of all instances of generated text in the
Gen-W-MA group found no failures of the predefined technical
guardrails (100% true negatives). 
     ChatGPT and GLM-4


72. metric2_name
   ================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 28
   Category: CATEGORICAL

   Value Counts:
     n: 145 (84.3%)
     linguistic features as mentioned under 3.3: 1 (0.6%)
     counselor rating form‑short (crf‑s): 1 (0.6%)
     scales of psychological well-being (pwb): 1 (0.6%)
     various linguistic factors: 1 (0.6%)
     siri-2 z-score: 1 (0.6%)
     positive and negative affect schedule, panas: 1 (0.6%)
     difference between responses (detection): 1 (0.6%)
     consistency ratio: 1 (0.6%)
     coherence, informativeness, fluency: 1 (0.6%)
     average tone (sentiment score): 1 (0.6%)
     llm-as-a-judge professionalism etc.: 1 (0.6%)
     number of harmful outputs (user-rated): 1 (0.6%)
     automatic safety rating: 1 (0.6%)
     automatic response quality: 1 (0.6%)
     instances of potentially concerning language detected: 1 (0.6%)
     user satisfaction rating: 1 (0.6%)
     psychobench emotional intelligence scale: 1 (0.6%)
     bleurt: 1 (0.6%)
     response quality rated by non experts: 1 (0.6%)
     perplexity, diversity, sentence coherence, edit rate: 1 (0.6%)
     shap values of words in classifier that classifies human vs. chatgpt responses: 1 (0.6%)
     ux measures: 1 (0.6%)
     change of empathy scores for the erf module: 1 (0.6%)
     criterion validity: 1 (0.6%)
     average length: 1 (0.6%)
     distinct 1: 1 (0.6%)
     llm-as-a-judge general response quality: 1 (0.6%)

   Sample values (first 20):
     n
     linguistic features as mentioned under 3.3
     distinct 1
     average length
     criterion validity
     change of empathy scores for the erf module
     ux measures
     shap values of words in classifier that classifies human vs. chatgpt responses
     perplexity, diversity, sentence coherence, edit rate
     response quality rated by non experts
     bleurt
     psychobench emotional intelligence scale
     user satisfaction rating
     instances of potentially concerning language detected
     automatic response quality
     automatic safety rating
     number of harmful outputs (user-rated)
     llm-as-a-judge professionalism etc.
     average tone (sentiment score)
     coherence, informativeness, fluency


73. metric2_vs_benchmark
   ========================
   Data type: object
   Total values: 172
   Non-null values: 27
   Null values: 145
   Unique values: 7
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 145 (537.0%)
     no benchmark: 11 (40.7%)
     b: 6 (22.2%)
     w: 4 (14.8%)
     unclear: 3 (11.1%)
     s: 1 (3.7%)
     mixed (table 9): 1 (3.7%)
     unknown: 1 (3.7%)


74. metric2_benchmark_quality
   =============================
   Data type: object
   Total values: 172
   Non-null values: 27
   Null values: 145
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 145 (537.0%)
     no benchmark: 11 (40.7%)
     l: 10 (37.0%)
     h: 6 (22.2%)


75. metric2_notes
   =================
   Data type: object
   Total values: 172
   Non-null values: 26
   Null values: 146
   Unique values: 20
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 146 (561.5%)
     no benchmark: 7 (26.9%)
     Benchmark: GPT-4 out of the box without MI fine-tuning: 1 (3.8%)
     Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. : 1 (3.8%)
     benchmark are real human responses from the Q&A dataset: 1 (3.8%)
     Compared to the ratings of expert suicidologist.
: 1 (3.8%)
     pre post measurement: 1 (3.8%)
     Therapist responses: 1 (3.8%)
     No Indexmodel; comparison of multiple LLMs: 1 (3.8%)
     they study integration of empathy lexicon into different LLMs. benchmark are those LLMs without empathy lexicon integration.: 1 (3.8%)
     Qianwen to automatically evaluate the counseling dialogue (esction: C. ): 1 (3.8%)
     LLM-based classification of whether LLM responses are appropriate or not. Benchmark: Responses of n = 16 human therapist participants.: 1 (3.8%)
     it's only stated whether the difference is significant. it's not stated whether human or AI have higher scores.: 1 (3.8%)
     ChatGPT and GLM-4: 1 (3.8%)
     benchmark: woebot non-generative/rule-based.: 1 (3.8%)
     PsychoBench is a framework for testing psychological instruments like Big Five and other scales on LLMs. Benchmark here: human reference population (not therapists, just average humans). Also compared to other LLMs (Llama 2, Falcon): 1 (3.8%)
     benchmark are responses in the kaggle dataset, but author rated himself (vested interests???): 1 (3.8%)
     Other LLMs: 1 (3.8%)
     benchmark: human psychologist responses from transcripts: 1 (3.8%)
     Reframe Relatability, Reframe Helpfulness, Reframe Memorability, Skill Learnability: 1 (3.8%)
     Benchmars are other LLM-based models: 1 (3.8%)


76. metric3_name
   ================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 9
   Category: CATEGORICAL

   Value Counts:
     n: 164 (95.3%)
     distinct 2
: 1 (0.6%)
     expert rating 2: 1 (0.6%)
     word analysis/ count: 1 (0.6%)
     network centrality (distance from depression terms): 1 (0.6%)
     daily interaction time: 1 (0.6%)
     german version of the working alliance inventory - short revised (wai-sr): 1 (0.6%)
     gad-7, panas-n: 1 (0.6%)
     sentiment score: 1 (0.6%)


77. metric3_vs_benchmark
   ========================
   Data type: object
   Total values: 172
   Non-null values: 8
   Null values: 164
   Unique values: 4
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 164 (2050.0%)
     s: 3 (37.5%)
     no benchmark: 2 (25.0%)
     b: 2 (25.0%)
     more words: 1 (12.5%)


78. metric3_benchmark_quality
   =============================
   Data type: object
   Total values: 172
   Non-null values: 8
   Null values: 164
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 164 (2050.0%)
     l: 4 (50.0%)
     no benchmark: 2 (25.0%)
     h: 2 (25.0%)


79. metric3_notes
   =================
   Data type: object
   Total values: 172
   Non-null values: 7
   Null values: 165
   Unique values: 6
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 165 (2357.1%)
     no benchmark: 2 (28.6%)
     Expert judgments of empathy, fluency, specificity of PARTNER against other LLMs.: 1 (14.3%)
     Network centrality and average distance values reported (Table 1 and 2).: 1 (14.3%)
     Benchmark: GPT-4 out of the box without MI fine-tuning: 1 (14.3%)
     Comparison of pre-post-change. Benchmark: For the real-time feedback group, after each exercise, the 
chatbot provided immediate and encouraging feedback 
based on the participant’s input during the intervention. Conversely, participants in the control group 
received online PPI guidance through the chatbot but did not receive any post-intervention response. : 1 (14.3%)
     Benchmars are other LLM-based models: 1 (14.3%)


80. s1_risk_detection_considered
   ================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 148 (86.0%)
     y: 24 (14.0%)


81. s1_risk_detection_notes
   ===========================
   Data type: object
   Total values: 172
   Non-null values: 25
   Null values: 147
   Unique values: 25
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 147 (588.0%)
     n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist

attending psychiatrist was checking responses: 1 (4.0%)
     The risk detection system in HoMemeTown was developed
based on established clinical guidelines [19] and validated
screening tools [20], implementing a sophisticated approach to
identifying and responding to potential mental health concerns.
The system continuously monitors user interactions for primary
risk indicators, including expressions of suicidal ideation, severe
depression symptoms, and anxiety crisis signals, while also
tracking secondary indicators such as sleep disturbance patterns
and social withdrawal signs.
When potential risks are detected, the system implements a
graduated response protocol that has been carefully designed
to provide appropriate levels of support while avoiding
unnecessary escalation. For mild risk situations, the system
offers empathetic acknowledgment and self-help resources,
drawing from evidence-based interventions [21]. In cases of
moderate risk, the response includes more direct expressions
of concern and specific mental health resources, while severe
risk triggers an immediate crisis response protocol with direct
connections to professional support services. To address the challenge of potential false positives in risk
detection, we implemented a sophisticated validation system
that examines multiple contextual factors before triggering
interventions. This system uses NLP techniques to analyze the
broader context of user communications, helping to distinguish
between casual expressions and genuine indicators of distress.
Regular professional review of high-risk cases ensures the
ongoing refinement of detection algorithms and response
protocols, maintaining a balance between sensitivity and
specificity in risk assessment.: 1 (4.0%)
     The chatbot's functionality extends beyond mere conversation, integrating mood tracking features, 
personalized coping strategy recommendations, and crisis 
detection algorithms with built-in escalation protocols for 
high-risk situations: 1 (4.0%)
     All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.: 1 (4.0%)
     Special attention was devoted to ensuring that Socrates
could recognize signs of acute psychological distress, includ-
ing suicidal ideation, self-harm intentions, or severe emo-
tional crises. When such indicators are detected, Socrates is
programmed to prioritize user safety by acknowledging the
severity of the situation and discontinuing the standard con-
versational approach. Instead, it explicitly refers the user to
appropriate professional resources and provides immediate
access to crisis intervention contacts, ensuring users in dis-
tress receive proper care beyond what an AI system can
provide.: 1 (4.0%)
     In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model: 1 (4.0%)
     from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.: 1 (4.0%)
     The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.: 1 (4.0%)
     “After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9): 1 (4.0%)
     In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.: 1 (4.0%)
     see 4.1: 1 (4.0%)
     Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.: 1 (4.0%)
     The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.: 1 (4.0%)
     The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.: 1 (4.0%)
     entire study is about this: 1 (4.0%)
     Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.: 1 (4.0%)
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response

4) Crisis Intervention and Ethical Safeguards: 1 (4.0%)
     Implicit in S-2: 1 (4.0%)
     Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language

Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language: 1 (4.0%)
     Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159: 1 (4.0%)
     Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. : 1 (4.0%)
     they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values: 1 (4.0%)
     "With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation."

the tool itself is a suicide risk detection tool: 1 (4.0%)
     even though this is a crisis call intervention system, there is not detection of acute risk: 1 (4.0%)
     If suicidality or severe distress is detected, users are directed to
psychological support hotlines and blocked from further use to
ensure safety.: 1 (4.0%)

   Sample values (first 20):
     n contrast, the intervention group consisted of seven
patients who participated in 3 to 6 semi-structured sessions
with ChatGPT (version 3.5) each, under the facilitation of
their attending psychiatrist

attending psychiatrist was checking responses
     even though this is a crisis call intervention system, there is not detection of acute risk
     "With software that requires only an audio recording, evaluations of whether any risk assessment occurred were highly similar to human ratings for the entire call and specific call-taker statements. Moreover, with the exception of the label of attempt in progress, the percent human agreement (i.e., the extent to which agreement of human-machine ratings matched that between two human raters) for specific risk labels was >80%. Together, these findings suggest that trained machine-learning models can provide an overall gestalt of an entire conversation and targeted feedback on content within a conversation."

the tool itself is a suicide risk detection tool
     they use BERT to classify responses into human or AI generated, then analyze this BERT model using SHAP values
     Subsequently, the
user’s suicidal tendencies are assessed, with a referral to human-operated suicidal hotlines
if confirmed. 
     Crisis Detection: The system prioritizes user safety by detecting emergencies, such as suicide risk, and providing immediate contact information … before processing each query, it assesses for crisis indicators … until the user confirms they are safe or … seek professional help.
Position: p. 159
     Proprietary natural language classifier for detecting
potentially concerning language: every free-text user input
was processed by a classifier for detecting potentially
concerning language

Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer. The content filter works by
processing both the prompt and completion through an ensemble
of classification models that aim to detect and prevent the output
of harmful content. Categories that are checked as part of the
content filter include hate and fairness, sexual violence, and
self-harm language
     Implicit in S-2
     The chatbot includes a crisis detection mechanism that scans 
for high-risk expressions such as suicidal thoughts or self-
harm indicators. When triggered, the chatbot responds with 
empathy and refers the user to professional mental health 
resources such as hotlines or support websites. Example 
response

4) Crisis Intervention and Ethical Safeguards
     Out of the 7 chatbots we tested, only 3 gave the user a specific phone number to call during an emergency situation. Two chatbots (Claude and the Character.ai chatbot) suggested 1 specific phone number but only did so in message 10, two messages after the user indicated suicidal ideation. Because the
chatbots gained information that the user was suicidal before sending message 9, they missed the opportunity to immediately display the lifesaving phone number. Claude suggested calling
911, while Character.ai suggested a specific suicide prevention hotline. However, the numbers did not include a hyperlink that would allow the user to call directly by clicking on the link, so the user would need to type in the phone numbers manually.
     entire study is about this
     The SAD state submodule contains several ML models, including DT, BagDT, BoostDT, RF, CNB, kNN, MLP, LOG, 
and SVM (see Section Machine Learning Algorithms). They are trained to detect and forecast SAD levels and symptoms, 
described in Table 3, including the following: levels of stress, anxiety, depression; and symptoms of inability to relax, 
nervousness, fear, tightness in chest, lightheadedness, feeling hot or cold, trembling, pounding heart, sadness, self-hatred, 
anhedonia, hopelessness, indecisiveness, fatigue, emotional detachment, and suicidality. The submodule keeps track of 
the users’ mental health. It informs how the system should act in its support of the user, and whether strategy dispatch is 
necessary.
     The procedure starts with text 
processing, which is followed by an assessment of the input 
complexity. If the technology identifies a high-risk or 
complex issue, it refers the session to a human therapist.
     Many provided
the National Suicide Prevention Hotline number, though this
often required further prompting asking for specific methods
to connect with a human.
     see 4.1
     In response to serious user messages that imply
potentiality of self-harm, illegal activity, or danger to others
the chatbot responded by redirecting the user to other support
resources such as crisis hotlines and professionals.
     “After Sam typed, ‘I want to kill myself today,’ OCD Coach … ‘I’m really sorry … contact emergency services or a crisis hotline …’ [but] no specific numbers or links were provided” (Position: p. 9)
     The chatbots incorporated robust safety protocols to protect
vulnerable participants. Automated escalation systems were
implemented to detect and respond to signs of severe emotional
distress or suicidal ideation. Upon identification of critical
keywords or concerning patterns, the chatbot immediately
provided guidance to contact the attending physician or
designated crisis support services. These safety protocols were
developed and validated in consultation with psychosomatic
medicine specialists to ensure appropriate and timely responses
to psychological emergencies.
     from referenced paper: If at any time the user expressed hints of suicidal ideation, then
they were directed to seek crisis intervention and immediate
support and were provided with information for emergency
services. If the user raised medical issues outside the scope of talk
therapy, XAIA was programmed to advise the user to seek care
from a medical healthcare professional.
     In the event of a par-
ticipant raising safety concerns (e.g., suicidal ideation), we 
contacted the participant to provide safety guidance and 
emergency resources.; Given the potential risks associated with Gen-AI, we added multiple guard rails, including a crisis classification model


82. s2_content_safety_considered
   ================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 150 (87.2%)
     y: 22 (12.8%)


83. s2_content_safety_notes
   ===========================
   Data type: object
   Total values: 172
   Non-null values: 25
   Null values: 147
   Unique values: 25
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 147 (588.0%)
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.: 1 (4.0%)
     A professional counselor with three years of experience 
monitored the entire conversation to ensure the safety of the 
counseling process: 1 (4.0%)
     A custom moderation application programming interface (API)
filters inputs and outputs, flagging inappropriate content before
LLM processing. Key safety features include the following.: 1 (4.0%)
     In addition, to mitigate variability and potential errors in LLM
responses, we introduced a validation process including semantic
consistency checks, medical reference verification, and
automatic escalation to human review when necessary, ensuring
responses remain clinically appropriate and user safety is
maintained.: 1 (4.0%)
     All conversations were constantly monitored using several
machine learning safety modules to ensure appropriateness,
prevent harmful responses, monitor risks, and ensure regulatory
compliance [27]. The conversations as well as these machine
learning models were monitored and continuously improved
by the company’s research team.: 1 (4.0%)
     Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.: 1 (4.0%)
     All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.

However, no automatic screening pre-transmission: 1 (4.0%)
     This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc: 1 (4.0%)
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses: 1 (4.0%)
     Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.: 1 (4.0%)
     “OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9): 1 (4.0%)
     In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users: 1 (4.0%)
     Chatbots mostly preserved privacy and avoided harmful
content. : 1 (4.0%)
     Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.: 1 (4.0%)
     See e.g., "Strong Framing of Opinions and Suggestions": 1 (4.0%)
     LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11): 1 (4.0%)
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer

We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.: 1 (4.0%)
     All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results): 1 (4.0%)
     We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user: 1 (4.0%)
     To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.: 1 (4.0%)
     see 4.2 -> Safety considerations: 1 (4.0%)
     used reflection quality classifier: 1 (4.0%)
     During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.: 1 (4.0%)
     GPT-2 outputs are not checked: 1 (4.0%)
     discuss misinformation, over-validation, and ethical risks reported by users: 1 (4.0%)

   Sample values (first 20):
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses.
     GPT-2 outputs are not checked
     During interactions with the conversational agent, our main objective is to have
the discriminator accurately detect unsafe responses to prevent harm to users.
Simultaneously, we ensure that safe responses are successfully sent to users.
     used reflection quality classifier
     see 4.2 -> Safety considerations
     To recognize toxic speech and exclude it
from the model’s responses we use the pre-trained “unbiased”
model made available in the Detoxify repository [10]. It is a
RoBERTa model that has been trained on the Civil Comments
[5] dataset, a large collection of annotated comments with
classes such as threat, insult or obscene.
     We created
a multiagent tool [24] by adding an AI supervisor and an AI
external rater, which were designed to support the AI therapist
in facilitating the dialogue without being visible to the user
     All authors… reviewed MICA session transcripts independently to identify any statements that were medically inappropriate…” (Position: Methods, Qualitative analyses)

(no unsafe responses observed) — Quote: “No inappropriate or unsafe text generated by MICA was observed” (Position: Results)
     Our primary LLM vendor, Azure OpenAI Service, provided a
built-in content filtering layer

We also checked
model output against a set of formatting and content rules to
ensure that the generated output was appropriate before sending
it to a participant. These rules validated that the output was
properly formatted as instructed using XML tags and checked
for any words within a banned words list. At no point was a
participant able to directly interact with an LLM. As described
here, every participant’s input was assessed, and every model
output was validated before returning the response to the
participant.
     LLMs make dangerous or inappropriate statements. to peo-
ple experiencing delusions, suicidal ideation, hallucinations,
and OCD as we show in Fig. 4, and Fig. 13 and in line
with prior work [59]. This conflicts with the guidelines
Don’t Collude with Delusions, Don’t Enable Suicidal Ideation, and
Don’t Reinforce Hallucinations. The models we tested facilitated
suicidal ideation (Fig. 4), such as by giving examples of tall bridges
to clients with expressed suicidal ideation (Tab. 8), behavior which
could be dangerous.
Current safety interventions do not always help. reduce how dan-
gerous LLMs are as therapists. We found larger and newer models
(with, in theory, better safety filtering and tuning [114, 157]) still
showed stigma (Fig. 1 and 6) and failed to respond appropriately
(Fig. 4). gpt-4o shows significantly less stigma than llama3.1 mod-
els, but we find no significant decrease in stigma with scale within
the llama family—even including llama2-70b (Fig. 6). gpt-4o and
llama3.1 models fail to respond appropriately to particular mental
health conditions at the same rate, although llama2-70b performs
much worse (Fig. 4 and 11)
     See e.g., "Strong Framing of Opinions and Suggestions"
     Language generation humanizer decides whether the added text generated in the previous module is acceptable in 
terms of risk for the user. It rejects the text if it is detected as risky.
     Chatbots mostly preserved privacy and avoided harmful
content. 
     In this module, the generated text by LLM is evaluated to ensure that no inappropriate content is
included in the user-provided text. Given the importance of vocabulary and its impact on users’
mental well-being, text evaluation and generating
suitable content aimed at improving the user’s state
of mind are critical tasks; The module is designed to function as a filter, en-
suring that messages generated by the LLM are
neither toxic nor contain language that could evoke
negative feelings in users
     “OpenAI’s use policies prohibit using the service to promote or engage in harmful activities … ‘This content may violate our usage policies’” (Position: p. 9)
     Quality assurance was maintained through systematic review of
chatbot interactions by pediatric psychosomatic specialists during
the development and testing phase. This process enabled
optimization of response appropriateness and refinement of
safety protocols.
     The LLM output is sent through an “Appropriateness
Classifier”—a stand-alone AI to detect potentially dangerous or
unhelpful responses
     This prompt was then
refined by an AI prompt engineer (OP) who also conducted tests and added operational instructions to ensure safety and ethics; Finally, after various trials, a
final version of the prompt was formulated by YH, reflecting the
collective insights and ensuring that both tools would be effective
and ethically sound for potential clinical applications. After
building an initial version, the research team made attempts to
improve and refine it, reducing inconsistent, inaccurate, or unsafe
responses, and enhancing the user experienc
     All responses from Therabot were 
supervised by trained clinicians and researchers post-transmission. In the event of an inappropriate response from 
Therabot (e.g., providing medical advice), we contacted 
the participant to provide correction.; All content was closely supervised for quality and safety in our trial, with rapid expert 
intervention available. This approach may continue to be necessary when testing similar future models to ensure 
safety.

However, no automatic screening pre-transmission
     Both the provision of instructions and
source materials can serve as guardrails that keep the chatbot
in line with evidence-based guidelines (if provided), as well as
prevent the chatbot from generating content that is off-topic or inappropriate; Finally, misinformation, defined as advice for quitting that was
not supported by USPSTF guidelines, was present in over 20% of responses which is concerning. This was the case even for BeFreeGPT which was told to follow these specific guidelines.


84. p1_on_premise_model_considered
   ==================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 127 (73.8%)
     y: 45 (26.2%)


85. p1_on_premise_model_notes
   =============================
   Data type: object
   Total values: 172
   Non-null values: 33
   Null values: 139
   Unique values: 31
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 139 (421.2%)
     Chat GPT: 2 (6.1%)
     GPT-2: 2 (6.1%)
     BlenderBot: 1 (3.0%)
     Partially: GPT not, Llama yes. : 1 (3.0%)
     Clare® operates independently, accepting text and voice
inputs, with voice transcriptions processed using NLP to extract key
information about emotions and context.
But not on-premise capable!: 1 (3.0%)
     Llama 3.2 used: 1 (3.0%)
     Jais-13B is on-premise: 1 (3.0%)
     T5 can be used on-premise and via cloud (not in the text). : 1 (3.0%)
     Llama 2, Falcon-7B: 1 (3.0%)
     Some are, others don´t: 1 (3.0%)
     Falcon7B : 1 (3.0%)
     falcon: 1 (3.0%)
     Llama and DeepSeek are: 1 (3.0%)
     	•	ChatGLM — open-source 6B variants (e.g., ChatGLM2-6B) with downloadable weights for local hosting.  ￼
	•	Tongyi Qianwen (Qwen) — Alibaba’s Qwen/Qwen2/Qwen3 series are open-sourced (multiple sizes) and routinely self-hosted with vLLM, etc.  ￼
	•	ERNIE (Baidu) — historically API/SaaS via Qianfan, but 2025 releases (ERNIE 4.5) are open-sourced under Apache-2.0, enabling on-prem deployments. (Earlier ERNIE Bot access was API-only.): 1 (3.0%)
     ChatGLM is open-source/self-hostable: 1 (3.0%)
     Gemini 1.5 Flash: 1 (3.0%)
     InternLM2-7B is self-hostable: 1 (3.0%)
     Chat GPT 3.5 used + BERT : 1 (3.0%)
     mistral 7b: 1 (3.0%)
     One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4: 1 (3.0%)
     Qwen is: 1 (3.0%)
     on Llama2-7B… as well as ChatGLM2-6B: 1 (3.0%)
     fine-tuning experiment on ChatGLM2-6B (open, self-hostable): 1 (3.0%)
     LLaMAntino (Italian LLaMA 2 model) is: 1 (3.0%)
     Llama2-13B-Chat, Falcon-7B-Instruct, Mistral-7B-Instruct (open-source).: 1 (3.0%)
     Llama: 1 (3.0%)
     own transformer architecture -> on-premise: 1 (3.0%)
     GPT3: 1 (3.0%)
     gpt 2: 1 (3.0%)
     T5 model: 1 (3.0%)
     GLM2-6B model : 1 (3.0%)

   Sample values (first 20):
     mistral 7b
     Llama
     BlenderBot
     Chat GPT 3.5 used + BERT 
     T5 model
     gpt 2
     GPT3
     own transformer architecture -> on-premise
     Chat GPT
     Llama2-13B-Chat, Falcon-7B-Instruct, Mistral-7B-Instruct (open-source).
     LLaMAntino (Italian LLaMA 2 model) is
     fine-tuning experiment on ChatGLM2-6B (open, self-hostable)
     on Llama2-7B… as well as ChatGLM2-6B
     Qwen is
     One goal of the paper is showing that the local GPT-2 can perform similarly well as proprietary GPT-4
     GPT-2
     InternLM2-7B is self-hostable
     Partially: GPT not, Llama yes. 
     Gemini 1.5 Flash
     ChatGLM is open-source/self-hostable


86. p2_privacy_awareness_considered
   ===================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 161 (93.6%)
     y: 11 (6.4%)


87. p2_privacy_awareness_notes
   ==============================
   Data type: object
   Total values: 172
   Non-null values: 24
   Null values: 148
   Unique values: 22
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 148 (616.7%)
     too little: 3 (12.5%)
     Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy: 1 (4.2%)
     Yes → Adheres to data protection principles, anonymized use
https://www.clareandme.com/post/what-happens-with-your-data-at-clare-me: 1 (4.2%)
     Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].: 1 (4.2%)
     Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.: 1 (4.2%)
     Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.: 1 (4.2%)
     usage of Amazon AWS, no discussion of user privacy: 1 (4.2%)
     The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).: 1 (4.2%)
     Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]: 1 (4.2%)
     In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.
: 1 (4.2%)
     
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. : 1 (4.2%)
     issues such as data privacy, 
consent, confidentiality, and the potential for biases in AI algorithms are the few challenges raised by most of the 
participants. These are inferred from the following statements: 1 (4.2%)
     Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.: 1 (4.2%)
     Privacy and confidentiality … conversations … may contain sensitive information … implement robust security measures to protect user privacy and ensure the confidentiality of conversations.
: 1 (4.2%)
     “Informed consent was obtained… participants were assured that no personal sensitive data would be collected… All data collected was kept confidential and anonymous, ensuring complete privacy and data protection.”
: 1 (4.2%)
     Not sufficient: The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.: 1 (4.2%)
     Too little: 1 (4.2%)
     they say they release the data but where is it?: 1 (4.2%)
     User privacy and data security are paramount. The system operates under strict ethical guidelines and secure data storage protocols.”
: 1 (4.2%)
     Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls: 1 (4.2%)
     only:  such as personal details, to protect privacy and reduce computation complexities
: 1 (4.2%)
     Ethical use of Reddit data with disclaimers in 3.3.1: 1 (4.2%)

   Sample values (first 20):
     Another concern with chatbots is the potential misuse of
personal data shared.10 As these chatbots collect and store
data about a user’s mental health and emotional state, the
risk of unauthorized access to this information is not triv-
ial.11 Such breaches could lead to serious repercussions, in-
cluding discrimination based on the user’s mental health
status. Therefore, user privacy and security must be a top
concern, requiring secure data storage and transmission,
along with adherence to relevant data protection regula-
tions.
     only:  such as personal details, to protect privacy and reduce computation complexities

     Future work should address these concerns by implementing robust privacy protection
mechanisms and ensuring transparency in data usage. This includes obtaining explicit user
consent for data collection and usage, anonymising sensitive user information, and imple-
menting strict data access controls
     User privacy and data security are paramount. The system operates under strict ethical guidelines and secure data storage protocols.”

     they say they release the data but where is it?
     Too little
     Not sufficient: The call for regulation and quality control
mechanisms is pertinent to ensuring that ChatGPT is integrated into cognitive therapies to
safeguard patient privacy, provide data security, and maintain the integrity of therapeutic
interventions. This perspective invites further research and dialogue among policymakers,
legal experts, healthcare providers, and technologists to develop comprehensive guidelines
that navigate the complexities of applying AI in mental healthcare responsibly.
     too little
     “Informed consent was obtained… participants were assured that no personal sensitive data would be collected… All data collected was kept confidential and anonymous, ensuring complete privacy and data protection.”

     Privacy and confidentiality … conversations … may contain sensitive information … implement robust security measures to protect user privacy and ensure the confidentiality of conversations.

     issues such as data privacy, 
consent, confidentiality, and the potential for biases in AI algorithms are the few challenges raised by most of the 
participants. These are inferred from the following statements
     Socrates 2.0 runs on
our own instance of Microsoft Azure (Microsoft Corp) GPT4o
(OpenAI) in the Road Home Program’s dedicated resource
group and Rush University Medical Center subscription. This
is different from a public instance, such as simply connecting
to ChatGPT via an application programming interface. The
Microsoft Azure (Microsoft Corp) GPT4o (OpenAI) models
are stateless, and no data (ie, user prompts or model-generated
responses) are stored in the model. Moreover, none of the input
or output data of the model or embedding and training data are
used by other parties (eg, corporations and researchers). Socrates
2.0 was reviewed by our hospital’s cybersecurity team and is
fully compliant with Health Insurance Portability and
Accountability Act and our hospital’s data privacy policy
     
Key security features 
included: 1. Strict data isolation: Preventing access to model inputs, 
outputs, or training data by external parties. 2. Comprehensive 
encryption protocols: Securing all data transmission and storage. This 
implementation met Health Insurance Portability and Accountability 
Act (HIPAA) standards and underwent rigorous institutional cyberse­curity review. 
     In response to increasing concerns around ethical AI deploy-
ment in healthcare, our system incorporates rigorous pri-
vacy protection measures and explainability mechanisms to 
enhance user trust. All user interactions are processed in 
real-time without persistent storage to safeguard sensitive 
data. Communication channels are encrypted using standard 
Transport Layer Security (TLS) protocols3, and all local logs 
are anonymized. The system adheres to key data protection 
regulations, including the GDPR and HIPAA. These safe-
guards minimize the risks of unauthorized access or misuse 
of user data.

     Client data should be private and confidential. (Therapist Qual-
ities: Trustworthy and Adherence to Professional Norms: Keep
patient data private). Regulation around the globe prohibits disclo-
sure of sensitive health information without consent—in the U.S.,
providers must not disclose, except when allowed, clients’ “individ-
ually identifiable health information” [141]. Both Anthropic and
OpenAI8 do provide mechanisms to secure health data. But to make
an effective LLM-as-therapist, we may have to train on real exam-
ples of therapeutic conversations. LLMs memorize and regurgitate
their training data, meaning that providing them with sensitive
personal data at training time (e.g., regarding patients’ trauma) is a
serious risk [26]. Deidentification of training data (e.g., removal of
name, date of birth, etc.) does not eliminate privacy issues. Indeed,
Huang et al. [67] demonstrate that commercially available LLMs
can identify the authors of text. Specially trained classifiers work
even better at uniquely reidentifying authors [120]
     The conversational agent
operates without specific tuning for sociodemographic bias
handling. Backend processes include HIPAA-compliant audio
recording transmission to ensure privacy. The agent, blinded
to all participant information except for their first name,
encrypts and sends data via a HIPAA-compliant pipeline.
GPT-4 (OpenAI) is used to formulate responses and relayed
to the user, with the use of finetuned prompts to provide cog-
nitive behavioral therapy (Supplementary Appendix SA1).
     usage of Amazon AWS, no discussion of user privacy
     Data security and confidentiality were
prioritized through encryption and secure storage, with access
limited to authorized personnel only. User anonymity was
maintained in alignment with data protection regulations such
as GDPR and HIPAA. The project also followed a data
minimization approach, collecting and storing only essential
data to reduce privacy risks.
     Privacy, security, and data ethics remain paramount con-
cerns in this field. Managing sensitive mental health informa-
tion demands robust safeguards. Socrates addresses these
challenges by adhering to OpenAI’s data security regulations
and implementing measures to prevent storage of sensitive
data, conversation histories, or user interactions, thereby main-
taining rigorous privacy standards and ethical compliance.
     Furthermore, the reliance on an external
API raises considerations about data privacy and the long-term
sustainability of the system.

Future research should explore advanced technologies like
federated learning or differential privacy, which could
potentially allow for more personalized features without
compromising user privacy. In addition, developing clear
guidelines for handling mental health data in AI-powered
interventions will be essential. Our experience underscores the
need for innovative solutions that balance the benefits of
personalization with robust data protection in mental health
contexts. As the field evolves, finding this balance will be key
to developing effective, trustworthy, and ethically sound
AI-powered mental health interventions [8,41].


88. e1_demographics_reporting_considered
   ========================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     n: 149 (86.6%)
     y: 22 (12.8%)
     n
: 1 (0.6%)


89. e1_demographics_reporting_notes
   ===================================
   Data type: object
   Total values: 172
   Non-null values: 24
   Null values: 148
   Unique values: 20
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 148 (616.7%)
     Table 1: 4 (16.7%)
     Table 2: 2 (8.3%)
     Twenty-four patients participated in the interviews. Eight participants were females and sixteen participants were males. 
The demographic information of the participants is presented in Table S1 and Appendix B: 1 (4.2%)
     Table 3: 1 (4.2%)
     The study included 20 participants aged 18 to 27 (mean 23.3,
SD 1.96) years with 60% (12/20) female and 40% (8/20) male.: 1 (4.2%)
     In the current study, participants (N = 830) were 45.17 years old on average (SD = 16.56),
59.88% mentioned being in a current romantic relationship, and 18.07% of the sample
reported having ever engaged in couple therapy. Most participants identified as a woman
(50.60%), slightly fewer identified as a man (47.95%), and the remaining individuals identified
as non-binary or third-gender (0.24%), 0.12% preferred not to say, and 0.07% of the sample
did not answer. A majority of the sample identified as straight (83.25%), 7.83% of the sample
identified as bisexual, 2.65% as gay, 1.81% as asexual, 1.45% as lesbian, 0.72% as queer, and
0.60% preferred to not disclose. When considering race and ethnicity, most participants iden-
tified as non-Hispanic White (49.40%), followed by Black (18.80%), White Hispanic (16.87%),
Asian (5. %), Black Hispanic (0.84%), American Indian or Alaskan Native (0.12%), and the
remaining sample identified as other (8.43%), or preferred not to disclose (0.12%).: 1 (4.2%)
     yes, in simulated DSPs (digital standardized patient): 1 (4.2%)
     Appendix Table 2: 1 (4.2%)
     Table 2 has just the bare minimum: 1 (4.2%)
     f 140 participants (101 fe­
male, 37 male and 2 opted not to provide information about their 
gender) aged between 18 and 43 (SD = 3.444).: 1 (4.2%)
     Table-1: 1 (4.2%)
     Table 1 shows that the survey participants are diverse and
representative, with a significant majority aged 18-30
(41.1%), followed by 31–40 (28.6%), 41–50 (20.1%), and
over 50 (10.3%). Males (57.1%) outnumber females
(42.9%), and bachelor’s degrees (37.1%) and diplomas
(32.3%) are most common. The sample is well-distributed
across urban (40.9%), semi-urban (30.6%), and rural
(28.6%) dwellings, providing insights regarding telephar-
macy experiences across demographics and regions. The
majority (89.6%) had anxiety disorder therapy or counsel-
ing. Most individuals had severe anxiety (36.1%), followed
by moderate (33.5%), mild (14.7%), and extremely severe
(15.7%).
: 1 (4.2%)
     see Table client demographic characteristics: 1 (4.2%)
     reported in multimedia appendix 3: 1 (4.2%)
     
The sample for the survey consisted of 236 people aged 17 to 40 years (Mean =
20.9, SD = 4.03), of which 86% (203) were women and 14% (33) were men. The study
was conducted in 2023 in Russia, in the city of St. Petersburg.: 1 (4.2%)
     See Table S1

We analyzed a sample of N = 35 patients (M = 40 years, SD = 12.5, range: 17–62) [...] 
No restrictions were made based on demographic
variables or psychopathology. [...] The majority
(85.7%) of patients were of German origin, and all
therapy sessions were conducted in the German
language.: 1 (4.2%)
     Some demographic information given: The clients were all above age 18 (Mage =
39.06, SD = 13.67, range 20–77), and most were women (58.9%).
Of the clients, 92% were native Hebrew speakers and 92% were
born in Israel. Of the clients, 53.5% had at least a bachelor’s degree;
53.5% were single and 8.9% were in a committed relationship
but unmarried; 23.2% were married and 14.2% were divorced
or widowed. : 1 (4.2%)
     Some demographics of therapists are shown but only male/female, no ethnic etc. data: 1 (4.2%)
     Only very basic demographics reported:
All counseling conversations are recorded in En-
glish. For Dsmall, around 70% of the help seeker
was female, and 55% of the help seeker was the
maltreated child. About 60% of the help seekers
are younger than 17 years old.: 1 (4.2%)
     Finally, 49 participants were selected to participate in 
this study for the experiment. The age range of participants 
is 15–40 years old. Among them, 29 (59.18%) were male 
and 20 (40.82%) were female; 15 (30.61%) were middle 
school students, 6 (12.24%) were undergraduate students, 
12 (24.49%) were graduate students, and 16 (32.65%) were 
already working; 12 (24.49%) were 15–18 years old, 21 
(42.86%) were 18–24 years old, and 16 people (32.65%) 
were over 24 years old. All participants did not have a pro-
fessional background in psychology.: 1 (4.2%)


90. e2_outcomes_by_demographics_considered
   ==========================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 168 (97.7%)
     y: 4 (2.3%)


91. e2_outcomes_by_demographics_notes
   =====================================
   Data type: object
   Total values: 172
   Non-null values: 3
   Null values: 169
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 169 (5633.3%)
     see table 4!: 1 (33.3%)
     Table 3 and Table 4
section "Perceptions across demographic groups": 1 (33.3%)
     yes, in simulated DSPs (digital standardized patient): 1 (33.3%)


92. g1_early_discontinuation_considered
   =======================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 167 (97.1%)
     y: 5 (2.9%)


93. g1_early_discontinuation_notes
   ==================================
   Data type: object
   Total values: 172
   Non-null values: 5
   Null values: 167
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 167 (3340.0%)
     fig 6 participants flow: 1 (20.0%)
     Of the 654 participants who accepted and consented to the study,
105 (16.1%) did not finish the entire study. We speculate that
this dropout was caused by several factor: 1 (20.0%)
     Table 6 shows how many participants interacted only 0-15 mins daily: 1 (20.0%)
     dropouts noted: 1 (20.0%)
     See 3.2 Engagement patterns: 1 (20.0%)


94. g2_overuse_considered
   =========================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 167 (97.1%)
     y: 5 (2.9%)


95. g2_overuse_notes
   ====================
   Data type: object
   Total values: 172
   Non-null values: 5
   Null values: 167
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 167 (3340.0%)
     only a set number of sessions was administered: 1 (20.0%)
     number of messages sent is reported: 1 (20.0%)
     Table 6 shows daily interaction time over 30 mins: 1 (20.0%)
     Measures of the user experience with the generative and
rules-based DMHIs included user engagement (number of
sessions, total active days, and conversational exchanges): 1 (20.0%)
     Somehow: 
However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT’s overly validating nature, and privacy concerns: 1 (20.0%)


96. f1_validated_outcomes_considered
   ====================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 155 (90.1%)
     y: 17 (9.9%)


97. f1_validated_outcomes_notes
   ===============================
   Data type: object
   Total values: 172
   Non-null values: 20
   Null values: 152
   Unique values: 20
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 152 (760.0%)
     WAI-SR Bond is used, but this is not symptom or function scale: 1 (5.0%)
     UCLA, PHQ-D, PHQ-4, SWLS, Mini-SPIN: 1 (5.0%)
     Reliable improvement refers to a clinically significant
improvement in symptoms following a course of treatment and
is calculated as the score difference between the first and the
last validated clinical questionnaire completed. The types of
questionnaires patients complete are tailored to their specific
condition. For example, the Patient Health Questionnaire-9
(PHQ-9) [30] is used to measure depression symptom severity,
and the Generalized Anxiety Disorder-7 (GAD-7) [31] is used
to measure anxiety symptom severity. A clinically significant
improvement in symptoms is considered a change score ≥6 for
PHQ-9 or ≥4 for GAD-7 [26]: 1 (5.0%)
     Patient Health 
Questionnaire 9(PHQ-9), the Generalized Anxiety 
Disorder Questionnaire for the Diagnostic and Statistical 
Manual of Mental Disorders, Fourth Edition (DSM-IV)
(GAD-Q-IV), and the Weight Concerns Scale (WCS) within 
the Stanford–Washington University Eating Disorder
(SWED), as measures of depression, anxiety, and weight 
concerns, respectively. Therapeutic alliance 
(Working Alliance Inventory — Short Revised [WAI-SR]).: 1 (5.0%)
     (CASES &) STAI: 1 (5.0%)
     PHQ-9: 1 (5.0%)
     (Self-Assessment Manikin Scale)+ Heart Rate variability: 1 (5.0%)
     WAI-SR but is neither symptom nor function scale: 1 (5.0%)
     Instead of PANAS, the study collected data by combining items from several symptom inventories related to SAD, consisting of standardized screening questions used by mental health professionals in the process of mental health diagnosis. Depression Anxiety and Stress Scale,  Beck Anxiety Inventory,
Beck Depression Inventory,
and 
Ratcliffe’s Depression Questionnaire
were used to compile the questionnair: 1 (5.0%)
     Beck Anxiety Inventory (BAI) and Generalized Anxiety Disorder Scale (GAD-7): 1 (5.0%)
     World Health Organization Quality of Life Questionnaire – Brief Version (WHOQOL-BREF): 1 (5.0%)
     PANAS: 1 (5.0%)
     we also measured the trust on robots using Human-Robot Interaction Trust Scale (HRITS) scale (Pinto et al., 2022): 1 (5.0%)
     PHQ-8 was measured in clients: 1 (5.0%)
     use of the readiness ruler: 1 (5.0%)
     Tinnitus Handicap Inventory (THI): 1 (5.0%)
     measured phq-9, gad, etc. in patients but did not predict those from the transcripts: 1 (5.0%)
     FIS-T is validated: 1 (5.0%)
     unclear: Each group’s progress was measured by two questionnaires one of which evaluated the test taker’s relationship with their thoughts, and the other estimated their
level of cognitive distortions. These assessments were conducted three times: twice
before the intervention itself and once after the experiment was over. After that, the
gathered data was analyzed using the statistical software JASP.

AAQ, CDS are actually validated clinical measures: 1 (5.0%)
     PHQ-9, GAD-7, PANAS-P, PANAS-N, SWLS, SVS: 1 (5.0%)


98. f2_control_condition_considered
   ===================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 158 (91.9%)
     y: 14 (8.1%)


99. f2_control_condition_notes
   ==============================
   Data type: object
   Total values: 172
   Non-null values: 15
   Null values: 157
   Unique values: 15
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 157 (1046.7%)
     control group that received standard care: 1 (6.7%)
     After the recruitment was done,
participants were randomly divided into two groups: control and experimental. The
experimental group received intervention in the form of interaction with TeaBot and
was asked to use a manual for learning more about the therapeutic approach used. The
control group received no intervention with only a manual available to learn more about
distortions. : 1 (6.7%)
     A quasi-experimental design
with one sample was used with the introduction of two equivalent experimental
interventions: cases with recommendations written by a psychologist and a neural
network.: 1 (6.7%)
     We conduct randomized controlled trials to assess
the impact of diferent design hypotheses/decisions
BUT: only for different design decisions; no overall other intervention or no intervention as control group!: 1 (6.7%)
     Twenty participants (healthy individuals without a diagnosed mental health disorder) were
recruited online and evenly split between the two groups (DL group and rule-based group).: 1 (6.7%)
     two people not undergoing intervention: 1 (6.7%)
     "Compared to baseline prompting methods (Zeroshot, Few-shot, ZeroCoT)."
: 1 (6.7%)
     Yes (two RAG variants compared) — Quote: “two Retrieval-Augmented Generation (RAG) models are proposed” (Abstract): 1 (6.7%)
     RCT: 1 (6.7%)
     three groups: untrained AI, pre-trained AI, human supervisor) — Quote: “three distinct groups (untrained AI, pre-trained AI, and qualified human supervisor)” (Position: Abstract): 1 (6.7%)
     either an MI-adapted or a GPT-4 condition and conversed with a corresponding chatbot version for a fixed
number of turns. To mitigate the potential for harm in interactions
with the chatbots, we limited the possible topics of conversation to
the three target behaviours procrastinate less, live more sustainably,
and eat a healthier diet, as they represent a sample of non-medical
lifestyle

Control: GPT-4 out-of-the-box: 1 (6.7%)
     The participants were randomly assigned to one of two experi-
mental groups: an AI-feedback group and a self-review group.
The self-review group received no specific intervention after their
counseling sessions and was given 10 min to reflect on their own.
The AI-feedback group received feedback from ChatGPT after
their counseling sessions.: 1 (6.7%)
     To examine the effectiveness of Therabot relative to the waitlist control group, we examined the effect of time and treatment assignment on depression, anxiety, and weight concerns among participants at a clinical level of MDD, 
GAD, and CHR-FED at baseline.: 1 (6.7%)
     comparing 150 … patients who used the AI-enabled therapy support tool to 94 … who used the standard delivery of CBT exercises” (: 1 (6.7%)
     Evident in whole study (e.g., Finally, it is worth noting that our choice of an active control group (that is, a non-trivial control group)...): 1 (6.7%)


100. i1_multilevel_feasibility_considered
   =========================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 2
   Category: CATEGORICAL

   Value Counts:
     n: 168 (97.7%)
     y: 4 (2.3%)


101. i1_multilevel_feasibility_notes
   ====================================
   Data type: object
   Total values: 172
   Non-null values: 5
   Null values: 167
   Unique values: 5
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 167 (3340.0%)
     Human evaluations relied on two groups: 
mental healthcare professionals and researchers who believe they are suffering mental 
health issue. The survey comprised ten questions, focusing on 
users’ mental health needs, the perceived usefulness and satisfaction of the 
chatbot, its conversation quality, and potential areas of 
improvement. : 1 (20.0%)
     acceptability data collected only from one stakeholder level (clinicians): 1 (20.0%)
     Initial positive user and clinician feedback
suggests that generative AI tools, such as Socrates 2.0, could
be feasible: 1 (20.0%)
     both CMs and MHPs were surveyed: 1 (20.0%)
     Somehow: user-reported perceptions from Reddit: 1 (20.0%)


102. i2_healthcare_integration_considered
   =========================================
   Data type: object
   Total values: 172
   Non-null values: 172
   Null values: 0
   Unique values: 3
   Category: CATEGORICAL

   Value Counts:
     n: 160 (93.0%)
     y: 11 (6.4%)
     n : 1 (0.6%)


103. i2_healthcare_integration_notes
   ====================================
   Data type: object
   Total values: 172
   Non-null values: 15
   Null values: 157
   Unique values: 15
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 157 (1046.7%)
     We want to emphasize, though, that our chatbot, while 
a step forward, is not a replacement for human therapists. 
Instead, we envision it as an auxiliary resource that can 
provide support in scenarios where human resources are 
stretched thin, or as an additional tool to complement 
traditional therapeutic processes.

 Integration with clinical systems: future research 
could look into integrating the chatbot with chatbot to provide more personalized and context-
aware support. It could also facilitate better 
coordination with healthcare professionals, alerting 
them when the chatbot identifies potential serious 
concerns.
existing clinical systems. This would allow the : 1 (6.7%)
     "Specific applications of machine learning–based evaluation may go beyond after-action call-level summaries of statement-level evaluations that cannot affect the quality of care received by the caller in the moment. Specifically, real-time evaluation may also be possible. For example, machine-learning–enabled call center software could label interventions as they occur, providing just-in-time feedback and suggestions to call takers when specific interventions are not provided." (...) "..stakeholders include call takers who can use this information to learn from previous calls, supervisors and administrators who may better identify and then direct resources to call takers who are struggling with their performance, and funders who may begin to include population-level data on the quality of conversations - complementing existing metrics on answer rates and wait times." : 1 (6.7%)
     this is all: Automatic emotion recognition models can be integrated into existing feedback systems to provide an indication of the levels of
emotional coherence in psychotherapy sessions and allow therapists
to modify their interventions accordingly.: 1 (6.7%)
     some description in following paragraph but too superficial:

The integration of sentiment analysis could supplement and enhance traditional measures of emotion by providing automated and objective assessments of emotional expressions. It could help compensate for the limitations of traditional self-report measures of emotions, which can be biased y social desirability or memory recall. The multimo-dal measurement approach in this study revealed a few discrepancies between therapist ratings of patient emotions and those achieved by sentiment analysis. On average, positive sentiments were nega-tively correlated with therapist ratings of negative emotions (Figure S3C). However, the correlation was positive for some patients, indicating that thera-pists may have been unable to identify their patients’ emotions correctly or that patients’ non- and para-verbal emotional expressions differed from what they said. Therapists may profit from feedback on such discrepancies in emotional expression, especially since patient-focused research has demonstrated the general benefits of feedback and data-informed psychological therapies (de Jonget al., 2021; Lutz et al., 2022). Therefore, it is crucial to develop systems that integrate and provide easy access to emotional process feedback in clinical practice, training, and supervision (e.g.,Trier Treatment Navigator, TTN; Lutz et al.,2019; Lutz et al., 2022).: 1 (6.7%)
     
"Automation of time-consuming tasks in iCBT could, through a positive lens, lead to
improved cost-effectiveness, which is an important point in often over-encumbered and
underfinanced psychiatry treatment and care contexts."
"
Fully automated iCBT, including the prediction of emotional states coupled with a CA
in charge of the iCBT with no human therapist involvement, would be both unwanted
and unethical. For legal reasons, having a clinical professional involved and ultimately
responsible for treatment is mandatory today and unlikely to change in the foreseeable
future. Only hybrid solutions of man-machine co-involvement are therefore further
discussed here. One such hybrid scenario would be the sole automation of emotion
recognition. This scenario starts with initial machine recognition of emotional states
derived from patients’ responses as part of ongoing iCBT treatment. Estimated emotional
states can then be fed to a human clinician as decision support. In theory, this could
render an improved understanding of a patient’s emotional state and also change of
state across time during iCBT. This could ultimately improve treatment tailoring and
effectiveness through the patient perceiving the therapist as more empathic, strengthening
the therapeutic alliance. Furthermore, it would allow for modifications of ongoing therapy
work modules to better suit the patient’s emotional state"
" An interesting
but largely untested scenario would be extended automation of emotional recognition
coupled with therapist-supported CA treatment. This would involve not only the potential
benefit of emotion recognition discussed above but also cost-effective semi-automated
treatment. One such implementation would be that the emotionally informed CA drafts
empathically written therapy responses to the patient’s messages and the human therapist
then scrutinizes the responses and signs off on them with or without making prior changes.
A major portion of iCBT costs come from therapists spending time drafting responses to
patients in the treatment portal, unlocking a major potential for cost-saving strategies. An
additional downside risk with this scenario would be that the human therapist—due to
stress or other human factors—signs off on written responses of lower therapeutic quality.
Proper training and structured follow-up of therapists are likely required in this scenario,
which in turn may offset some of the cost-effectiveness of the approach. That stated since
a major motivation for iCBT is cost-effectiveness, extending it with emotionally tailored
CA seems in accordance with that overarching aim of iCBT.": 1 (6.7%)
     Our study findings suggest that LLMs should not yet be relied
on to lead CBT delivery, although LLMs show clear potential
as assistants capable of offering reasonable suggestions for the
identification and reframing of unhelpful thoughts.
LLMs are far from replacing CBT therapists, but they perform
well in some isolated tasks (eg, Bard for reframing), so it is
worthwhile exploring limited yet innovative ways to use AI to
improve patient experience and outcomes. We suggest CBT
therapists equip patients with a working knowledge of cognitive
biases, but therapists could also advise patients to consider using
LLMs to gather suggestions on reframing unhelpful thoughts
beyond sessions: 1 (6.7%)
     
The integration of ChatGPT into existing mental health
care systems can be approached through several practical
strategies. Firstly, ChatGPT can serve as a supplementary
tool for mental health professionals, providing support
between therapy sessions and offering immediate responses
to patients in need. This can help bridge the gap for those
with limited access to mental health services, particularly
in underserved or remote areas. Additionally, ChatGPT
can be incorporated into telehealth platforms, enhancing
the accessibility and reach of mental health care. Training
mental health professionals to effectively utilize ChatGPT
in their practice is essential, ensuring they can leverage its
capabilities to augment patient care without replacing
human interaction. Furthermore, integrating ChatGPT into
routine screening processes can aid in early detection and
intervention of mental health issues, allowing for timely
referrals to appropriate care providers. By embedding
ChatGPT within a comprehensive, patient-centered care
framework, mental health systems can enhance their cap-
acity to deliver timely, effective, and accessible support to
those in nee: 1 (6.7%)
     Future studies
should examine whether using LLMs would make out-of-session
practice, such as examining one’s thoughts, more engaging for
patients than worksheets [21].: 1 (6.7%)
     Bare minimum: A hybrid model, combining the strengths of AI with the expertise of human therapists,
may provide optimal outcomes. For example, chatbots could function as supplementary
tools within traditional therapeutic frameworks, allowing therapists to leverage chatbot-
generated insights to tailor interventions to individual needs.: 1 (6.7%)
     
Additionally, the system could integrate with comple-
mentary therapeutic tools, such as gamified interven-
tions, mindfulness programs, and physical activity regi-
mens, creating hybrid models that combine AI’s strengths 
with the human-centric aspects of traditional therapy. Col-
laborative efforts with interdisciplinary teams will be key 
to refining the system’s design and application.
Enhance the explainability of AI-driven interventions 
by integrating visual and interactive elements, such as 
graphical representations of engagement levels and inter-
active prompts that allow users to ask ’why’ questions 
about system decisions. Moreover, refining the reason-
ing logs for therapists to include structured insights into 
behavioral adaptations could further strengthen trust and 
usability.
Integrate privacy-preserving AI techniques, such as 
differential privacy and federated learning, to enhance data 
security while maintaining model performance. Further-
more, collaboration with cybersecurity experts can ensure 
that AI-driven therapeutic systems remain resilient against 
evolving threats.
Overall, these directions emphasize the transformative 
potential of AI-driven therapeutic systems. By addressing 
these challenges, future work can advance the personaliza-
tion, accessibility, and scalability of ADHD interventions, 
ultimately enhancing outcomes for individuals and their 
families: 1 (6.7%)
     Client data should be private and confidential. … Regulation around the globe prohibits disclosure of sensitive health information without consent—in the U.S., providers must not disclose, except when allowed, clients’ ‘individually identifiable health information’ [141].; 
Low quality therapy bots endanger people, enabled by a regulatory vacuum. … the APA wrote to the U.S. Federal Trade Commission requesting regulation of chatbots marketed as therapists [49].

See 6.2 and 7: 1 (6.7%)
     „Embedding LLM-based coaching within the wellness initiative of an educational institution …“

„… could democratize accessibility … making coaching available even to students at cash-strapped academic institutions.“

„Hiring human coaches can be expensive … there is an opportunity to build cost-effective and scale-friendly LLM coaches …“: 1 (6.7%)
     Integrating Socrates into environ-
ments such as hospitals or drug rehabilitation centers could
therefore provide substantial benefits. Patients could gain
additional time for psychological reflection beyond what current staffing constraints allow. Moreover, some individuals
might find it easier to discuss sensitive issues with Socrates
rather than in face-to-face interactions, potentially experienc-
ing less perceived judgment and accelerating psychological
change processes. Health care providers might also benefit
from this technology through more streamlined workflows
and reduced burnout risk.
: 1 (6.7%)
     Only: Integration with health care systems
• Investigate secure ways to integrate chatbot data with electronic health records, while maintaining user privacy.: 1 (6.7%)
     Speculative discussion on integration with clinical care, but not implemented: 1 (6.7%)


104. general_notes
   ==================
   Data type: object
   Total values: 172
   Non-null values: 27
   Null values: 145
   Unique values: 27
   Category: CATEGORICAL

   Value Counts:
     [NULL]: 145 (537.0%)
     intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes: 1 (3.7%)
     Unsure about the metrics. We can discuss during the consensus. See table 4: 1 (3.7%)
     Comparative Analysis unclear in methodological aspects to me. : 1 (3.7%)
     Null empirische Validierung: keine User-Daten, keine Inhaltevaluation, keine klinischen Benchmarks...
Wie viele paper: viel domain knowledge nicht beachtender Techno-Optimismus, wenig Evidenz: 1 (3.7%)
     Ambitious platform vision, but lacks any form of quantitative, clinical, or user-centered evaluation: 1 (3.7%)
     without human evaluation or even content-based analysis of output quality, 70% accuracy is weak evidence of real-world utility: 1 (3.7%)
      : 1 (3.7%)
     Ethical Considerations: Which users were informed?: 1 (3.7%)
     Despite it being labeled an "empirical study", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. : 1 (3.7%)
     used metrics: EPIT-ONE framework, MITI, RoVERTa: 1 (3.7%)
     This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy.: 1 (3.7%)
     The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots.: 1 (3.7%)
     Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one.: 1 (3.7%)
     VR maybe as solution for the embodiment problem? : 1 (3.7%)
     Interestingly, Haiku and ChatGPT, while playing either therapists or 
patients and speaking in English, rarely use jargon expressing anger or 
disgust. Instead, human patients express jargon eliciting disgust and 
anger at higher rates, compatible with random expectations. This dif­
ference could be due to English LLMs might have been fine-tuned to 
more strongly avoid using a negative outlook on things.: 1 (3.7%)
     Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory


Therapy type: undirected therapy resulting from interacting with ChatGPT with no prespecified initial prompt: 1 (3.7%)
     the transformer model predicted items subscales from different clinical questionnaires: 1 (3.7%)
     very short study : 1 (3.7%)
     No real evaluation. Only a sample conversatino is shown: 1 (3.7%)
     no evaluation was performed. Only a sample conversation was shown "look this is good".: 1 (3.7%)
     no alpha adjusting (table 3): 1 (3.7%)
     qualitative study - survey with 31 participants from recruited over Reddit: 1 (3.7%)
     This is one of the few studies that also included psychodynamic psychotherapy in the intervention.: 1 (3.7%)
     This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%): 1 (3.7%)
     "Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality.": 1 (3.7%)
     proof-of-concept prototype rather than a full-fledged clinical tool

No evaluation of the conversation system. Study only displayed some conversation sample "look this is good".: 1 (3.7%)
     All comparisons between AI and human counselors based on ML Models. 

This study also was a "transcript analysis" via BERT: 1 (3.7%)

   Sample values (first 20):
     intervention group did not use chatgbt, but rather discussed the chatgbt answers with their therapists--> potential, to distort outcomes
     proof-of-concept prototype rather than a full-fledged clinical tool

No evaluation of the conversation system. Study only displayed some conversation sample "look this is good".
     "Therapists highlightet that it is inappropriate for children to be prompted to share secrets with an AI, particularly under the pretense of guaranteed confidentiality."
     This sample endorsed a wide range of
theoretical orientations, with the most prevalent being 2nd
Wave Cognitive-Behavioral (n = 314; 32.1%), Person-centered
(n = 288; 29.4%), and 3rd Wave Cognitive-Behavioral (e.g.,
ACT, DBT) (n = 199; 20.3%)
     This is one of the few studies that also included psychodynamic psychotherapy in the intervention.
     qualitative study - survey with 31 participants from recruited over Reddit
     no alpha adjusting (table 3)
     no evaluation was performed. Only a sample conversation was shown "look this is good".
     No real evaluation. Only a sample conversatino is shown
     very short study 
     the transformer model predicted items subscales from different clinical questionnaires
     Interesting: Several theories underpin the evaluation of ChatGPT’s efficacy in delivering mental health support to patients. The
Technology Acceptance Model (TAM) suggests that a user’s perception of a technology’s ease of use and usefulness
influences its adoption. Applied here, it implies that patients’ acceptance and continued use of ChatGPT for mental health
support could depend on how user-friendly and beneficial they find the interactions. 38–40 Moreover, the Elaboration
Likelihood Model (ELM) proposes that the persuasiveness of messages varies based on the depth of cognitive proces-
sing. In the context of ChatGPT, the model suggests that the effectiveness of its mental health support may relate to the
quality of conversation and the extent to which it engages patients cognitively. 41 Finally, the Social Cognitive Theory


Therapy type: undirected therapy resulting from interacting with ChatGPT with no prespecified initial prompt
     Interestingly, Haiku and ChatGPT, while playing either therapists or 
patients and speaking in English, rarely use jargon expressing anger or 
disgust. Instead, human patients express jargon eliciting disgust and 
anger at higher rates, compatible with random expectations. This dif­
ference could be due to English LLMs might have been fine-tuned to 
more strongly avoid using a negative outlook on things.
     VR maybe as solution for the embodiment problem? 
     Technical depth: strong emphasis on ChatGLM architecture, LoRA, AdamW, and NLP metrics, typical of a computer science paper rather than a clinical one.
     The CAPE framework (Conversational Agent for Psychotherapy Evaluation) is proposed as a modular, objective measure for evaluating psychotherapy chatbots.
     This is the first Persian-language chatbot for mental health described in AbjadNLP, highlighting cultural adaptation for underrepresented languages.
strong focus on technical performance of emotion detection models, not on clinical validation or user studies.
Safety considerations were included through language model validation, but no mention of clinical or ethical safeguards like privacy.
     used metrics: EPIT-ONE framework, MITI, RoVERTa
     Despite it being labeled an "empirical study", I assumed it to be a population study, since the use of ChatGPT was not investigated itself and therefore may be comparable to the study of Kongmeng 2025. 
     Ethical Considerations: Which users were informed?


================================================================================
END OF REPORT
================================================================================